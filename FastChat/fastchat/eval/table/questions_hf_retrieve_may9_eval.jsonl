{"question_id": 1, "text": " We are a medical research institute and want to extract features from biomedical texts to improve literature-based predictions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", \"api_call\": \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", \"api_arguments\": \"input_ids, attention_mask\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", \"performance\": {\"dataset\": \"UMLS\", \"accuracy\": \"N/A\"}, \"description\": \"SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.\"}}", "category": "generic"}
{"question_id": 2, "text": " I'm organizing an online conference with participants from different countries who speak different languages. I want to find a tool to extract information from their forum discussions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"rasa/LaBSE\", \"api_call\": \"AutoModel.from_pretrained('rasa/LaBSE')\", \"api_arguments\": \"input_text\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.\"}}", "category": "generic"}
{"question_id": 3, "text": " Our company plans to build a video-sharing platform that automatically generates video descriptions. We need a model that extracts video features and understands human language.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"microsoft/xclip-base-patch16-zero-shot\", \"api_call\": \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": [{\"name\": \"HMDB-51\", \"accuracy\": 44.6}, {\"name\": \"UCF-101\", \"accuracy\": 72.0}, {\"name\": \"Kinetics-600\", \"accuracy\": 65.2}]}, \"description\": \"X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\"}}", "category": "generic"}
{"question_id": 4, "text": " We are working on a project that can generate images from textual description for an online art gallery. We need to create diverse images from a given text prompt.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"stabilityai/stable-diffusion-2-1-base\", \"api_call\": \"pipe(prompt).images[0]\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": {\"install_dependencies\": \"pip install diffusers transformers accelerate scipy safetensors\", \"code\": \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\"}, \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\"}}", "category": "generic"}
{"question_id": 5, "text": " I want to create a document digitization tool that converts printed documents into text.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-base-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 6, "text": " We would like to present images of our product to create a caption for each image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"AICVTG_What_if_a_machine_could_create_captions_automatically\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de') .generate(pixel_values, **gen_kwargs)\", \"api_arguments\": {\"image_paths\": \"List of image file paths\", \"max_length\": 20, \"num_beams\": 8}, \"python_environment_requirements\": {\"transformers\": \"from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\", \"torch\": \"import torch\", \"Image\": \"from PIL import Image\"}, \"example_code\": \"predict_step(['Image URL.jpg'])\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"This is an image captioning model training by Zayn\"}}", "category": "generic"}
{"question_id": 7, "text": " I need to build an interactive robot that can answer questions using an image input, process questions in natural language, and generate a response.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"blip2-flan-t5-xxl\", \"api_call\": \"Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\", \"api_arguments\": {\"raw_image\": \"Image\", \"question\": \"Text\"}, \"python_environment_requirements\": [\"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not provided\"}, \"description\": \"BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 8, "text": " I need assistance to describe an image of a stop sign in Australia.\\n###Input: The image URL is https://www.ilankelman.org/stopsigns/australia.jpg\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/pix2struct-textcaps-base\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", \"api_arguments\": {\"images\": \"image\", \"text\": \"text\", \"return_tensors\": \"pt\", \"max_patches\": 512}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": [\"import requests\", \"from PIL import Image\", \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\", \"url = https://www.ilankelman.org/stopsigns/australia.jpg\", \"image = Image.open(requests.get(url, stream=True).raw)\", \"model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)\", \"processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)\", \"inputs = processor(images=image, return_tensors=pt)\", \"predictions = model.generate(**inputs)\", \"print(processor.decode(predictions[0], skip_special_tokens=True))\"], \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"state-of-the-art\"}, \"description\": \"Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.\"}}", "category": "generic"}
{"question_id": 9, "text": " We have an old dataset of charts from previous reports, and we want to generate tables from these charts.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/deplot\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", \"api_arguments\": {\"images\": \"image\", \"text\": \"question\", \"return_tensors\": \"pt\", \"max_new_tokens\": 512}, \"python_environment_requirements\": {\"transformers\": \"Pix2StructForConditionalGeneration, Pix2StructProcessor\", \"requests\": \"requests\", \"PIL\": \"Image\"}, \"example_code\": \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"ChartQA\", \"accuracy\": \"24.0% improvement over finetuned SOTA\"}, \"description\": \"DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.\"}}", "category": "generic"}
{"question_id": 10, "text": " I am a clothing designer and I need to analyze an image of a new fabric pattern with a model. Can you suggest how to do that to get the description of the fabric pattern?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"ivelin/donut-refexp-combined-v1\", \"api_call\": \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", \"performance\": {\"dataset\": \"ivelin/donut-refexp-combined-v1\", \"accuracy\": \"N/A\"}, \"description\": \"A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\"}}", "category": "generic"}
{"question_id": 11, "text": " We want to create an application that allows users to upload photos of tourist attractions and provides them with information about the images. Implement a system that can answer questions about these images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-large-vqav2\", \"api_call\": \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", \"api_arguments\": {\"model\": \"microsoft/git-large-vqav2\", \"task\": \"visual-question-answering\", \"device\": 0}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", \"performance\": {\"dataset\": \"VQAv2\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}}", "category": "generic"}
{"question_id": 12, "text": " We are creating a virtual tour guide. The guide will answer the users' questions about the places they visit based on the images of those places.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"Salesforce/blip-vqa-capfilt-large\", \"api_call\": \"BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\", \"api_arguments\": {\"raw_image\": \"RGB image\", \"question\": \"string\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForQuestionAnswering\"}, \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"VQA\", \"accuracy\": \"+1.6% in VQA score\"}, \"description\": \"BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.\"}}", "category": "generic"}
{"question_id": 13, "text": " A bank provides us with account statements of their clients in PDF format. We need to extract information such as transaction date, transaction amount, and balance after each transaction.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased-finetuned-docvqa\", \"api_call\": \"layoutlmv2-base-uncased-finetuned-docvqa\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.12.2\", \"torch==1.8.0+cu101\", \"datasets==1.14.0\", \"tokenizers==0.10.3\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 1.194}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 14, "text": " I need a personal assistant to help me analyze the financial reports and answer the question: What is the total revenue for the company?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"vision-encoder-decoder\", \"api_name\": \"naver-clova-ix/donut-base-finetuned-docvqa\", \"api_call\": \"donut-base-finetuned-docvqa\", \"api_arguments\": {\"image\": \"path_to_image\", \"question\": \"your_question\"}, \"python_environment_requirements\": \"Transformers\", \"example_code\": \"from transformers import pipeline\\n# Initialize the pipeline\\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\\n# Load an image and ask a question\\nimage_path = 'path_to_image'\\nquestion = 'your_question'\\n# Get the answer\\nanswer = doc_qa({'image': image_path, 'question': question})\\nprint(answer)\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\"}}", "category": "generic"}
{"question_id": 15, "text": " Develop an AI model for a local art gallery accepting scanned artwork, to help the staff easily find information about any piece of art by asking questions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"tiny-random-LayoutLMv3ForQuestionAnswering\", \"api_call\": \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", \"api_arguments\": {\"image\": \"path/to/image/file\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"tensorflow\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.\"}}", "category": "generic"}
{"question_id": 16, "text": " Working in a government agency, I need to extract specific information from a variety of unstructured forms and documents. Could you help me extract the answer to my question from the given context?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_arguments\": \"{'question': 'your_question', 'context': 'your_context'}\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}", "category": "generic"}
{"question_id": 17, "text": " We are building a virtual makeover app and we need to estimate the depth of various surfaces in an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-230131-041708\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4425, \"Mae\": 0.427, \"Rmse\": 0.6196, \"Abs_Rel\": 0.4543, \"Log_Mae\": 0.1732, \"Log_Rmse\": 0.2288, \"Delta1\": 0.3787, \"Delta2\": 0.6298, \"Delta3\": 0.8083}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.\"}}", "category": "generic"}
{"question_id": 18, "text": " The digital marketing company needs to create a virtual reality environment for their product display. They asked us to estimate the depth of input images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Monocular Depth Estimation\", \"api_name\": \"Intel/dpt-large\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"Intel/dpt-large\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\", \"performance\": {\"dataset\": \"MIX 6\", \"accuracy\": \"10.82\"}, \"description\": \"Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\"}}", "category": "generic"}
{"question_id": 19, "text": " I am working on an indoor robot project that needs depth estimation for the robot's vision. Help sequence the proper code for depth estimation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"Intel/dpt-hybrid-midas\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"Intel/dpt-hybrid-midas\", \"low_cpu_mem_usage\": \"True\"}, \"python_environment_requirements\": [\"torch\", \"transformers\", \"PIL\", \"numpy\", \"requests\"], \"example_code\": \"from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()\", \"performance\": {\"dataset\": \"MIX 6\", \"accuracy\": \"11.06\"}, \"description\": \"Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\"}}", "category": "generic"}
{"question_id": 20, "text": " We are developing a smart mobility solution for the visually impaired. We want to estimate the depth of surrounding objects in real-time.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode-221116-054332\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", \"api_arguments\": {\"model_name\": \"sayakpaul/glpn-nyu-finetuned-diode-221116-054332\"}, \"python_environment_requirements\": {\"transformers\": \"4.24.0\", \"pytorch\": \"1.13.0+cu117\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.6028, \"Rmse\": \"nan\"}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 21, "text": " Let's find the depth estimation for an image captured in a warehouse.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221122-044810\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-044810')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, torch==1.12.1, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.369, \"Mae\": 0.2909, \"Rmse\": 0.4208, \"Abs Rel\": 0.3635, \"Log Mae\": 0.1224, \"Log Rmse\": 0.1793, \"Delta1\": 0.5323, \"Delta2\": 0.8179, \"Delta3\": 0.9258}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 22, "text": " I am the owner of an online pet store. I need a solution that categorizes the uploaded photos of pets into cat or dog.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/convnext-base-224\", \"api_call\": \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", \"api_arguments\": [\"image\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"datasets\"], \"example_code\": \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": null}, \"description\": \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\"}}", "category": "generic"}
{"question_id": 23, "text": " To create a photo gallery app, we need a way to categorize images according to their content.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"google/mobilenet_v2_1.0_224\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v2_1.0_224)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v2_1.0_224)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not specified\"}, \"description\": \"MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.\"}}", "category": "generic"}
{"question_id": 24, "text": " I am building an app that detects flower species from uploaded images. What model can I use?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/swin-tiny-patch4-window7-224\", \"api_call\": \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"AutoFeatureExtractor\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": \"from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not specified\"}, \"description\": \"Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.\"}}", "category": "generic"}
{"question_id": 25, "text": " Recommend me an object detection library that can detect hard hats in construction areas to ensure safety compliance.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-hard-hat-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-hard-hat-detection')\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.24\", \"ultralytics==8.0.23\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"hard-hat-detection\", \"accuracy\": 0.811}, \"description\": \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\"}}", "category": "generic"}
{"question_id": 26, "text": " We need to implement a zero-shot object detection system to detect cats and dogs in images for a pet adoption application.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"zero-shot-object-detection\", \"api_name\": \"google/owlvit-large-patch14\", \"api_call\": \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", \"api_arguments\": {\"model_name\": \"google/owlvit-large-patch14\"}, \"python_environment_requirements\": [\"torch\", \"transformers\", \"PIL\", \"requests\"], \"example_code\": [\"import requests\", \"from PIL import Image\", \"import torch\", \"from transformers import OwlViTProcessor, OwlViTForObjectDetection\", \"processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)\", \"model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)\", \"url = http://images.cocodataset.org/val2017/000000039769.jpg\", \"image = Image.open(requests.get(url, stream=True).raw)\", \"texts = [[a photo of a cat, a photo of a dog]\", \"inputs = processor(text=texts, images=image, return_tensors=pt)\", \"outputs = model(**inputs)\", \"target_sizes = torch.Tensor([image.size[::-1]])\", \"results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\", \"i = 0\", \"text = texts[i]\", \"boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]\", \"score_threshold = 0.1\", \"for box, score, label in zip(boxes, scores, labels):\", \" box = [round(i, 2) for i in box.tolist()]\", \" if score >= score_threshold:\", \" print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})\"], \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not specified\"}, \"description\": \"OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\"}}", "category": "generic"}
{"question_id": 27, "text": " Our company needs to detect license plates of vehicles in our parking lot. Please provide object detection solution for this purpose.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"License Plate Detection\", \"api_name\": \"keremberke/yolov5s-license-plate\", \"api_call\": \"model(img, size=640)\", \"api_arguments\": {\"img\": \"image url or path\", \"size\": \"image resize dimensions\", \"augment\": \"optional, test time augmentation\"}, \"python_environment_requirements\": \"pip install -U yolov5\", \"example_code\": [\"import yolov5\", \"model = yolov5.load('keremberke/yolov5s-license-plate')\", \"model.conf = 0.25\", \"model.iou = 0.45\", \"model.agnostic = False\", \"model.multi_label = False\", \"model.max_det = 1000\", \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model(img, size=640)\", \"results = model(img, augment=True)\", \"predictions = results.pred[0]\", \"boxes = predictions[:, :4]\", \"scores = predictions[:, 4]\", \"categories = predictions[:, 5]\", \"results.show()\", \"results.save(save_dir='results/')\"], \"performance\": {\"dataset\": \"keremberke/license-plate-object-detection\", \"accuracy\": 0.985}, \"description\": \"A YOLOv5 based license plate detection model trained on a custom dataset.\"}}", "category": "generic"}
{"question_id": 28, "text": " We are working on a smart camera for recognizing people in crowded areas, please segment the given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"openmmlab/upernet-convnext-small\", \"api_call\": \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\"}}", "category": "generic"}
{"question_id": 29, "text": " Our team at a blood test center is analyzing blood cell images. Find a way to detect blood cells in the images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Blood Cell Detection\", \"api_name\": \"keremberke/yolov8n-blood-cell-detection\", \"api_call\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": \"ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"blood-cell-object-detection\", \"accuracy\": 0.893}, \"description\": \"This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\"}}", "category": "generic"}
{"question_id": 30, "text": " We are an agricultural company aiming to detect and classify segments of crops in aerial images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-base-coco-panoptic\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/mask2former-swin-base-coco-panoptic\"}, \"python_environment_requirements\": {\"packages\": [\"requests\", \"torch\", \"PIL\", \"transformers\"]}, \"example_code\": \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO panoptic segmentation\", \"accuracy\": null}, \"description\": \"Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\"}}", "category": "generic"}
{"question_id": 31, "text": " We want to create an application to help enhance the security of buildings. It needs to detect and segment people who enter the building.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-large-coco-panoptic\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"requests\", \"torch\", \"PIL\", \"transformers\"], \"example_code\": \"processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not provided\"}, \"description\": \"Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\"}}", "category": "generic"}
{"question_id": 32, "text": " Help us design a solution to segment objects inside images and classify them.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-base-ade\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", \"api_arguments\": {\"from_pretrained\": \"facebook/maskformer-swin-base-ade\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"PIL\": \"latest\", \"requests\": \"latest\"}, \"example_code\": \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\"}}", "category": "generic"}
{"question_id": 33, "text": " The company has recently launched an image classification software which recognizes all the objects in an image. How the software can be improved by a more sophisticated instance segmentation model to recognize individual objects and their boundaries within the image?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-tiny-coco\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", \"api_arguments\": [\"image\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO panoptic segmentation\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\"}}", "category": "generic"}
{"question_id": 34, "text": " We have a production line creating computer chips. We need to identify and segment any defects in the printed circuit boards(PCB) of these products.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8s-pcb-defect-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\", \"api_arguments\": {\"image\": \"URL or local path to image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pcb-defect-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.515, \"mAP@0.5(mask)\": 0.491}}, \"description\": \"YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\"}}", "category": "generic"}
{"question_id": 35, "text": " The art gallery is exploring innovative ways to display their collections. We are helping them to generate abstract artworks with the help of AI.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"google/ddpm-ema-church-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception score\": 9.46, \"FID score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.\"}}", "category": "generic"}
{"question_id": 36, "text": " Present a code for generating a colorful butterfly using A.I. technology.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\", \"api_arguments\": {\"model_id\": \"clp/sd-class-butterflies-32\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies.\"}}", "category": "generic"}
{"question_id": 37, "text": " Develop an AI actor that can recognize scenes from a trailer and identify the specific movie genre.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-hr-finetuned-k400\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 448, 448))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\n  logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not specified\"}, \"description\": \"TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al.\"}}", "category": "generic"}
{"question_id": 38, "text": " To improve the customer experience on our video streaming platform, we need to classify videos into proper categories based on content. Develop a video classification model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-base-finetuned-ssv2\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-ssv2')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(8, 3, 224, 224))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-ssv2)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-ssv2)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something Something v2\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 39, "text": " Analyze a video and provide a classification based on the content.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset')\", \"api_arguments\": {\"learning_rate\": 5e-05, \"train_batch_size\": 8, \"eval_batch_size\": 8, \"seed\": 42, \"optimizer\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\", \"lr_scheduler_type\": \"linear\", \"lr_scheduler_warmup_ratio\": 0.1, \"training_steps\": 111}, \"python_environment_requirements\": {\"transformers\": \"4.24.0\", \"pytorch\": \"1.12.1+cu113\", \"datasets\": \"2.6.1\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 1.0}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 40, "text": " As someone who loves photography, I want my app to be able to automatically identify what the subject is in my uploaded images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\", \"api_arguments\": [\"image\", \"possible_class_names\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"80.1\"}, \"description\": \"A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 41, "text": " We are dealing with a project related to self-driving cars. In order to improve the navigation, we need to classify the location of an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Geolocalization\", \"api_name\": \"geolocal/StreetCLIP\", \"api_call\": \"CLIPModel.from_pretrained('geolocal/StreetCLIP')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"geolocal/StreetCLIP\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [{\"name\": \"IM2GPS\", \"accuracy\": {\"25km\": 28.3, \"200km\": 45.1, \"750km\": 74.7, \"2500km\": 88.2}}, {\"name\": \"IM2GPS3K\", \"accuracy\": {\"25km\": 22.4, \"200km\": 37.4, \"750km\": 61.3, \"2500km\": 80.4}}]}, \"description\": \"StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\"}}", "category": "generic"}
{"question_id": 42, "text": " I'm trying to determine if a list of German sentences are positive, negative, or neutral in sentiment.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"German Sentiment Classification\", \"api_name\": \"oliverguhr/german-sentiment-bert\", \"api_call\": \"Output: SentimentModel()\", \"api_arguments\": [\"texts\"], \"python_environment_requirements\": \"pip install germansentiment\", \"example_code\": [\"from germansentiment import SentimentModel\", \"model = SentimentModel()\", \"texts = [\", \" Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,\", \" Total awesome!,nicht so schlecht wie erwartet,\", \" Der Test verlief positiv.,Sie frt ein gres Auto.]\", \"result = model.predict_sentiment(texts)\", \"print(result)\"], \"performance\": {\"dataset\": [\"holidaycheck\", \"scare\", \"filmstarts\", \"germeval\", \"PotTS\", \"emotions\", \"sb10k\", \"Leipzig Wikipedia Corpus 2016\", \"all\"], \"accuracy\": [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, \"description\": \"This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.\"}}", "category": "generic"}
{"question_id": 43, "text": " I need to choose the right answer from a set of options for a given question.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Information Retrieval\", \"api_name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('model_name')\", \"api_arguments\": {\"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('model_name')\", \"features\": \"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\"}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": {\"import\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\", \"model\": \"model = AutoModelForSequenceClassification.from_pretrained('model_name')\", \"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('model_name')\", \"features\": \"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\", \"scores\": \"with torch.no_grad():\\n    scores = model(**features).logits\\n    print(scores)\"}, \"performance\": {\"dataset\": \"TREC Deep Learning 2019\", \"accuracy\": \"69.84 (NDCG@10)\"}, \"description\": \"This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\"}}", "category": "generic"}
{"question_id": 44, "text": " I need to extract all company names from a list of news articles.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Entity Extraction\", \"api_name\": \"903429548\", \"api_call\": \"Output: AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoTrain\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTokenClassification, AutoTokenizer\"}, \"example_code\": \"from transformers import AutoModelForTokenClassification, AutoTokenizer\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"ismail-lucifer011/autotrain-data-company_all\", \"accuracy\": 0.9979930566588805}, \"description\": \"A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.\"}}", "category": "generic"}
{"question_id": 45, "text": " Our company is working on a customer support tool that automatically extracts relevant information mentioned by users in their messages. Analyze customer messages to extract entities.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Entity Extraction\", \"api_name\": \"903929564\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-job_all-903929564', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoTrain\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTokenClassification, AutoTokenizer\"}, \"example_code\": \"from transformers import AutoModelForTokenClassification, AutoTokenizer\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-job_all-903929564, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-job_all-903929564, use_auth_token=True)\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"ismail-lucifer011/autotrain-data-job_all\", \"accuracy\": 0.9989412009896035}, \"description\": \"A Token Classification model trained using AutoTrain for Entity Extraction. The model is based on distilbert and achieves high accuracy, precision, recall, and F1 score.\"}}", "category": "generic"}
{"question_id": 46, "text": " We are developing a system to identify the named entities in a given sentence.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Token Classification\", \"api_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\", \"api_call\": \"Output: AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\\nmodel = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\", \"performance\": {\"dataset\": \"CoNLL-03\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification tasks.\"}}", "category": "generic"}
{"question_id": 47, "text": " Identify the named entities in the given news article.\\n###Input: On August 21st, Apple Inc. announced their new product, the iPhone 13, will be released on September 14th. CEO Tim Cook led the event and highlighted the new features, including an improved camera and longer battery life. The starting price for the device is $699.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Flair\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-ontonotes-large\", \"api_call\": \"SequenceTagger.load('flair/ner-english-ontonotes-large')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\n# load tagger\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\\n# make example sentence\\nsentence = Sentence('On September 1st George won 1 dollar while watching Game of Thrones.')\\n# predict NER tags\\ntagger.predict(sentence)\\n# print sentence\\nprint(sentence)\\n# print predicted NER spans\\nprint('The following NER tags are found:')\\n# iterate over entities and print\\nfor entity in sentence.get_spans('ner'):\\n    print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": 90.93}, \"description\": \"English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.\"}}", "category": "generic"}
{"question_id": 48, "text": " Design bedside nurse report for COVID-19 patients with easy comprehension. We need to provide an overview of a patient's condition to the nurse.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lysandre/tiny-tapas-random-sqa\", \"api_call\": \"TapasForCovid.from_pretrained('lysandre/tiny-tapas-random-sqa')\", \"api_arguments\": null, \"python_environment_requirements\": \"transformers\", \"example_code\": null, \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"A tiny TAPAS model for table question answering tasks.\"}}", "category": "generic"}
{"question_id": 49, "text": " My company needs to answer questions about sales data on a monthly basis.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"lysandre/tapas-temporary-repo\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\", \"api_arguments\": [\"model_name\", \"question\", \"table\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\noutputs = model(**inputs)\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\", \"performance\": {\"dataset\": \"SQA\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.\"}}", "category": "generic"}
{"question_id": 50, "text": " We are building a knowledge base management system with AI assistance, and we want to ask it to explain concepts for us.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/roberta-large-squad2\", \"api_call\": \"pipeline('question-answering', model='deepset/roberta-large-squad2')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": \"Not provided\"}, \"description\": \"A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\"}}", "category": "generic"}
{"question_id": 51, "text": " Collect information about global warming and answer this question: \\\"What are the main causes of global warming?\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/bert-medium-squad2-distilled\", \"api_call\": \"AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\", \"api_arguments\": {\"model\": \"deepset/bert-medium-squad2-distilled\"}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"qa({'context': 'This is an example context.', 'question': 'What is this example about?'})\", \"performance\": {\"dataset\": \"squad_v2\", \"exact\": 68.6431398972458, \"f1\": 72.7637083790805}, \"description\": \"This model is a distilled version of deepset/bert-large-uncased-whole-word-masking-squad2, trained on the SQuAD 2.0 dataset for question answering tasks. It is based on the BERT-medium architecture and uses the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 52, "text": " Our website contains news articles in French, and we need to categorize them into sports, politics, and science.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"BaptisteDoyen/camembert-base-xnli\", \"api_call\": \"pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\", \"api_arguments\": {\"sequence\": \"str\", \"candidate_labels\": \"List[str]\", \"hypothesis_template\": \"str\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"sequence = L'uipe de France joue aujourd'hui au Parc des Princes\\ncandidate_labels = [sport,politique,science]\\nhypothesis_template = Ce texte parle de {}.\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\", \"performance\": {\"dataset\": \"xnli\", \"accuracy\": {\"validation\": 81.4, \"test\": 81.7}}, \"description\": \"Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French.\"}}", "category": "generic"}
{"question_id": 53, "text": " We are a news outlet, and our editors need to categorize the incoming articles into politics, sports, and entertainment.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"joeddav/xlm-roberta-large-xnli\", \"api_call\": \"XLMRobertaForSequenceClassification.from_pretrained('joeddav/xlm-roberta-large-xnli')\", \"api_arguments\": [\"sequence\", \"candidate_labels\", \"hypothesis_template\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"sequence_to_classify = ' qui vas a votar en 2020?'\\ncandidate_labels = ['Europa', 'salud plica', 'polica']\\nhypothesis_template = 'Este ejemplo es {}.'\\nclassifier(sequence_to_classify, candidate_labels, hypothesis_template=hypothesis_template)\", \"performance\": {\"dataset\": {\"xnli\": \"56.6k\", \"multi_nli\": \"8.73k\"}, \"accuracy\": \"Not specified\"}, \"description\": \"This model takes xlm-roberta-large and fine-tunes it on a combination of NLI data in 15 languages. It is intended to be used for zero-shot text classification, such as with the Hugging Face ZeroShotClassificationPipeline.\"}}", "category": "generic"}
{"question_id": 54, "text": " We would like to identify if the following hypothesis \\\"A woman is walking a dog\\\" represents a contradiction, entailment, or neutral relationship with the given premise \\\"A woman is strolling with her pet.\\\"\\n###Input: premise = \\\"A woman is strolling with her pet.\\\", hypothesis = \\\"A woman is walking a dog.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"cross-encoder/nli-deberta-v3-small\", \"api_call\": \"CrossEncoder('cross-encoder/nli-deberta-v3-small')\", \"api_arguments\": [\"sentence1\", \"sentence2\"], \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from sentence_transformers import CrossEncoder\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\", \"performance\": {\"dataset\": {\"SNLI-test\": \"91.65\", \"MNLI-mismatched\": \"87.55\"}, \"accuracy\": {\"SNLI-test\": \"91.65\", \"MNLI-mismatched\": \"87.55\"}}, \"description\": \"Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\"}}", "category": "generic"}
{"question_id": 55, "text": " My software needs to improve in conversational understanding. I want it to predict the logical relationship of two short texts.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Natural Language Inference\", \"api_name\": \"cointegrated/rubert-base-cased-nli-threeway\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\", \"api_arguments\": [\"text1\", \"text2\"], \"python_environment_requirements\": [\"transformers\", \"sentencepiece\"], \"example_code\": \"import torch\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\nif torch.cuda.is_available():\\n model.cuda()\\ntext1 = '\\u0421\\u043e\\u043a\\u0440\\u0430\\u0442 - \\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a, \\u0430 \\u0432\\u0441\\u0435 \\u043b\\u044e\\u0434\\u0438 \\u0441\\u043c\\u0435\\u0440\\u0442\\u043d\\u044b.'\\ntext2 = '\\u0421\\u043e\\u043a\\u0440\\u0430\\u0442 \\u043d\\u0438\\u043a\\u043e\\u0433\\u0434\\u0430 \\u043d\\u0435 \\u0443\\u043c\\u0440\\u0451\\u0442.'\\nwith torch.inference_mode():\\n out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\\n proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\\nprint({v: proba[k] for k, v in model.config.id2label.items()})\", \"performance\": {\"dataset\": [\"JOCI\", \"MNLI\", \"MPE\", \"SICK\", \"SNLI\", \"ANLI\", \"NLI-style FEVER\", \"IMPPRES\"], \"accuracy\": {\"ROC AUC\": {\"entailment\": 0.91, \"contradiction\": 0.71, \"neutral\": 0.79}}}, \"description\": \"This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral.\"}}", "category": "generic"}
{"question_id": 56, "text": " We are a multinational company, and our website received a review in Russian. We want to understand the content.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-ru-en\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-ru-en')\", \"api_arguments\": {\"from_pretrained\": \"Helsinki-NLP/opus-mt-ru-en\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForSeq2SeqLM\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\ntokenizer = AutoTokenizer.from_pretrained(Helsinki-NLP/opus-mt-ru-en)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(Helsinki-NLP/opus-mt-ru-en)\", \"performance\": {\"dataset\": \"newstest2019-ruen.ru.en\", \"accuracy\": 31.4}, \"description\": \"A Russian to English translation model developed by the Language Technology Research Group at the University of Helsinki. It is based on the Transformer-align architecture and trained on the OPUS dataset. The model can be used for translation and text-to-text generation tasks.\"}}", "category": "generic"}
{"question_id": 57, "text": " We are working on a research paper and one of the paragraphs needs translation from Italian to English. Can you help me to do it?\\n###Input: During my trip to Italy, I found out Italians are incredibly passionate about food. \\\"essenziale per la salute e il benessere, coscome il piacere che si ottiene da un pasto delizioso.\\\" The Italian cuisine is famous for its simplicity, with most dishes comprising only a few high-quality ingredients.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-it-en\", \"api_call\": \"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')('Ciao mondo!')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newssyscomb2009.it.en\": 35.3, \"newstest2009.it.en\": 34.0, \"Tatoeba.it.en\": 70.9}, \"chr-F\": {\"newssyscomb2009.it.en\": 0.6, \"newstest2009.it.en\": 0.594, \"Tatoeba.it.en\": 0.808}}}, \"description\": \"A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\"}}", "category": "generic"}
{"question_id": 58, "text": " Create an email draft for a German colleague, but I need the mail to be available in Spanish since they requested it.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-de-es\", \"api_call\": \"Helsinki-NLP/opus-mt-de-es\", \"api_arguments\": {\"text\": \"string\"}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\ntranslated_text = translation('Guten Tag')\", \"performance\": {\"dataset\": \"Tatoeba.de.es\", \"accuracy\": {\"BLEU\": 48.5, \"chr-F\": 0.676}}, \"description\": \"A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization.\"}}", "category": "generic"}
{"question_id": 59, "text": " Prepare a software that extracts the main points of a German text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Summarization\", \"api_name\": \"Einmalumdiewelt/T5-Base_GNAD\", \"api_call\": \"pipeline('summarization', model='Einmalumdiewelt/T5-Base_GNAD')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 2.1025, \"Rouge1\": 27.5357, \"Rouge2\": 8.5623, \"Rougel\": 19.1508, \"Rougelsum\": 23.9029, \"Gen Len\": 52.7253}}, \"description\": \"This model is a fine-tuned version of Einmalumdiewelt/T5-Base_GNAD on an unknown dataset. It is intended for German text summarization.\"}}", "category": "generic"}
{"question_id": 60, "text": " I have a long Chinese article about sports news. I want to get a summary of the article.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Summarization\", \"api_name\": \"Randeng-Pegasus-238M-Summary-Chinese\", \"api_call\": \"PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese').generate(inputs['input_ids'])\", \"api_arguments\": {\"text\": \"string\", \"max_length\": \"integer\"}, \"python_environment_requirements\": [\"transformers\", \"tokenizers_pegasus.py\", \"data_utils.py\"], \"example_code\": \"from transformers import PegasusForConditionalGeneration\\nfrom tokenizers_pegasus import PegasusTokenizer\\nmodel = PegasusForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\ntokenizer = PegasusTokenizer.from_pretrained('IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese')\\ntext = '\\u5728\\u5317\\u4eac\\u51ac\\u5965\\u4f1a\\u81ea\\u7531\\u5f0f\\u6ed1\\u96ea\\u5973\\u5b50\\u5761\\u9762\\u969c\\u788d\\u6280\\u5de7\\u51b3\\u8d5b\\u4e2d\\uff0c\\u4e2d\\u56fd\\u9009\\u624b\\u8c37\\u7231\\u51cc\\u593a\\u5f97\\u94f6\\u724c\\u3002\\u795d\\u8d3a\\u8c37\\u7231\\u51cc\\uff01\\u4eca\\u5929\\u4e0a\\u5348\\uff0c\\u81ea\\u7531\\u5f0f\\u6ed1\\u96ea\\u5973\\u5b50\\u5761\\u9762\\u969c\\u788d\\u6280\\u5de7\\u51b3\\u8d5b\\u4e3e\\u884c\\u3002\\u51b3\\u8d5b\\u5206\\u4e09\\u8f6e\\u8fdb\\u884c\\uff0c\\u53d6\\u9009\\u624b\\u6700\\u4f73\\u6210\\u7ee9\\u6392\\u540d\\u51b3\\u51fa\\u5956\\u724c\\u3002\\u7b2c\\u4e00\\u8df3\\uff0c\\u4e2d\\u56fd\\u9009\\u624b\\u8c37\\u7231\\u51cc\\u83b7\\u5f9769.90\\u5206\\u3002\\u572812\\u4f4d\\u9009\\u624b\\u4e2d\\u6392\\u540d\\u7b2c\\u4e09\\u3002\\u5b8c\\u6210\\u52a8\\u4f5c\\u540e\\uff0c\\u8c37\\u7231\\u51cc\\u53c8\\u626e\\u4e86\\u4e2a\\u9b3c\\u8138\\uff0c\\u751a\\u662f\\u53ef\\u7231\\u3002\\u7b2c\\u4e8c\\u8f6e\\u4e2d\\uff0c\\u8c37\\u7231\\u51cc\\u5728\\u9053\\u5177\\u533a\\u7b2c\\u4e09\\u4e2a\\u969c\\u788d\\u5904\\u5931\\u8bef\\uff0c\\u843d\\u5730\\u65f6\\u6454\\u5012\\u3002\\u83b7\\u5f9716.98\\u5206\\u3002\\u7f51\\u53cb\\uff1a\\u6454\\u5012\\u4e86\\u4e5f\\u6ca1\\u5173\\u7cfb\\uff0c\\u7ee7\\u7eed\\u52a0\\u6cb9\\uff01\\u5728\\u7b2c\\u4e8c\\u8df3\\u5931\\u8bef\\u6454\\u5012\\u7684\\u60c5\\u51b5\\u4e0b\\uff0c\\u8c37\\u7231\\u51cc\\u9876\\u4f4f\\u538b\\u529b\\uff0c\\u7b2c\\u4e09\\u8df3\\u7a33\\u7a33\\u53d1\\u6325\\uff0c\\u6d41\\u7545\\u843d\\u5730\\uff01\\u83b7\\u5f9786.23\\u5206\\uff01\\u6b64\\u8f6e\\u6bd4\\u8d5b\\uff0c\\u517112\\u4f4d\\u9009\\u624b\\u53c2\\u8d5b\\uff0c\\u8c37\\u7231\\u51cc\\u7b2c10\\u4f4d\\u51fa\\u573a\\u3002\\u7f51\\u53cb\\uff1a\\u770b\\u6bd4\\u8d5b\\u65f6\\u6211\\u6bd4\\u8c37\\u7231\\u51cc\\u7d27\\u5f20\\uff0c\\u52a0\\u6cb9\\uff01'\\ninputs = tokenizer(text, max_length=1024, return_tensors='pt')\\nsummary_ids = model.generate(inputs['input_ids'])\\ntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\", \"performance\": {\"dataset\": \"LCSTS\", \"accuracy\": {\"rouge-1\": 43.46, \"rouge-2\": 29.59, \"rouge-L\": 39.76}}, \"description\": \"Randeng-Pegasus-238M-Summary-Chinese is a Chinese text summarization model based on Pegasus. It is fine-tuned on 7 Chinese text summarization datasets including education, new2016zh, nlpcc, shence, sohu, thucnews, and weibo. The model can be used to generate summaries for Chinese text inputs.\"}}", "category": "generic"}
{"question_id": 61, "text": " Generate a story for kids about space traveling with the Big Red Button that could change everything.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"TehVenom/PPO_Pygway-V8p4_Dev-6b\", \"api_call\": \"pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b') should be rewritten by looking at the example code and determining the right model instantiation. However, there is not enough information in the example code to determine the correct model instantiation. Therefore, no changes should be made to the value of the `api_call` field.\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"TODO card. Mix of (GPT-J-6B-Janeway + PPO_HH_GPT-J) + Pygmalion-6b-DEV (V8 / Part 4). At a ratio of GPT-J-6B-Janeway - 20%, PPO_HH_GPT-J - 20%, Pygmalion-6b DEV (V8 / Part 4) - 60%.\"}}", "category": "generic"}
{"question_id": 62, "text": " I want to start a new novel about my life. Can you please help me generate the first few lines?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"facebook/opt-350m\", \"api_call\": \"pipeline('text-generation', model='facebook/opt-350m')\", \"api_arguments\": {\"model\": \"facebook/opt-350m\", \"do_sample\": \"True\", \"num_return_sequences\": 5}, \"python_environment_requirements\": {\"transformers\": \"4.3.0\"}, \"example_code\": \"from transformers import pipeline, set_seed\\nset_seed(32)\\ngenerator = pipeline('text-generation', model='facebook/opt-350m', do_sample=True, num_return_sequences=5)\\ngenerator('The man worked as a')\", \"performance\": {\"dataset\": \"BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit, CCNewsV2\", \"accuracy\": \"Roughly matches GPT-3 performance\"}, \"description\": \"OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, developed by Meta AI. It is designed to enable reproducible and responsible research at scale and bring more voices to the table in studying the impact of large language models. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation. It can also be fine-tuned on a downstream task using the CLM example.\"}}", "category": "generic"}
{"question_id": 63, "text": " We need a powerful model to generate statistical analysis from a paragraph.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text2Text Generation\", \"api_name\": \"google/flan-t5-xxl\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained(\\\\google/flan-t5-xxl\\\\)\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import T5Tokenizer, T5ForConditionalGeneration\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-xxl)\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-xxl)\\ninput_text = translate English to German: How old are you?\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\noutputs = model.generate(input_ids)\\nprint(tokenizer.decode(outputs[0]))\", \"performance\": {\"dataset\": [{\"name\": \"MMLU\", \"accuracy\": \"75.2%\"}]}, \"description\": \"FLAN-T5 XXL is a fine-tuned version of the T5 language model, achieving state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. It has been fine-tuned on more than 1000 additional tasks covering multiple languages, including English, German, and French. It can be used for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning and question answering.\"}}", "category": "generic"}
{"question_id": 64, "text": " Let the program suggest the next word in the sentence \\\"The AI model can complete any unfinished task by [MASK].\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling\", \"api_name\": \"albert-base-v2\", \"api_call\": \"pipeline('fill-mask', model='albert-base-v2')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nunmasker = pipeline('fill-mask', model='albert-base-v2')\\nunmasker(Hello I'm a [MASK] model.)\", \"performance\": {\"dataset\": {\"SQuAD1.1\": \"90.2/83.2\", \"SQuAD2.0\": \"82.1/79.3\", \"MNLI\": \"84.6\", \"SST-2\": \"92.9\", \"RACE\": \"66.8\"}, \"accuracy\": \"82.3\"}, \"description\": \"ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English.\"}}", "category": "generic"}
{"question_id": 65, "text": " Write a code that helps me autocompleting an input text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling Prediction\", \"api_name\": \"CodeBERTa-small-v1\", \"api_call\": \"pipeline('fill-mask', model='huggingface/CodeBERTa-small-v1')\", \"api_arguments\": [\"task\", \"model\", \"tokenizer\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"fill_mask(PHP_CODE)\", \"performance\": {\"dataset\": \"code_search_net\", \"accuracy\": null}, \"description\": \"CodeBERTa is a RoBERTa-like model trained on the CodeSearchNet dataset from GitHub. It supports languages like Go, Java, JavaScript, PHP, Python, and Ruby. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face tokenizers. The small model is a 6-layer, 84M parameters, RoBERTa-like Transformer model.\"}}", "category": "generic"}
{"question_id": 66, "text": " We want to search for relevant content in customer feedback to improve our product. Identify the most similar feedback to a given query.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/all-MiniLM-L6-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"1B sentence pairs dataset\", \"accuracy\": \"https://seb.sbert.net\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 67, "text": " Our company is creating a chatbot. Can you go through user messages and find the ones that are most similar to each other?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Embeddings\", \"api_name\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": [\"sentence-transformers\"], \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 68, "text": " I want to build an AI to answer if Sentences A and B are similar or not in nature.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"nikcheerla/nooks-amd-detection-v2-full\", \"api_call\": \"SentenceTransformer('{MODEL_NAME}')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": [\"sentence-transformers\", \"transformers\"], \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('{MODEL_NAME}')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 69, "text": " I am an employee in a big corporation, I need to find similar sentences in a document with many sentences.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/distiluse-base-multilingual-cased-v1\", \"api_call\": \"SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"N/A\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 70, "text": " Design a notification system that reads out upcoming events from a user's calendar using synthesized speech.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"mio/Artoria\", \"api_call\": \"pipeline('text-to-speech', model='mio/Artoria')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; tts = pipeline('text-to-speech', model='mio/Artoria'); tts('\\u3053\\u3093\\u306b\\u3061\\u306f')\", \"performance\": {\"dataset\": \"fate\", \"accuracy\": \"Not provided\"}, \"description\": \"This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\"}}", "category": "generic"}
{"question_id": 71, "text": " My startup works within India, primarily in the Marathi language. Help me convert text to speech suited for a male voice in Marathi.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"SYSPIN/Marathi_Male_TTS\", \"api_call\": \"api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS').\", \"api_arguments\": [], \"python_environment_requirements\": [\"huggingface_hub\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Marathi Male Text-to-Speech model using ESPnet framework.\"}}", "category": "generic"}
{"question_id": 72, "text": " In our mobile application, we need a function to read the users messages in Telugu Male voice.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"SYSPIN/Telugu_Male_TTS\", \"api_call\": \"pipeline('text-to-speech', model='SYSPIN/Telugu_Male_TTS')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.\"}}", "category": "generic"}
{"question_id": 73, "text": " Create a system that translates the phrase \\\"Welcome to the team. We are excited to have you\\\" into speech.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"imdanboy/jets\", \"api_call\": \"pipeline('text-to-speech', model='imdanboy/jets')\", \"api_arguments\": null, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; tts = pipeline('text-to-speech', model='imdanboy/jets'); tts('Hello world')\", \"performance\": {\"dataset\": \"ljspeech\", \"accuracy\": null}, \"description\": \"This model was trained by imdanboy using ljspeech recipe in espnet.\"}}", "category": "generic"}
{"question_id": 74, "text": " Recently, a hearing-impaired colleague requested an app to convert audio meetings to written text. Develop a solution that transcribes audio files.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Recognition\", \"api_name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-english\", \"api_call\": \"Wav2Vec2Model.from_pretrained('jonatasgrosman/wav2vec2-large-xlsr-53-english')\", \"api_arguments\": [\"audio_paths\"], \"python_environment_requirements\": [\"huggingsound\", \"torch\", \"librosa\", \"datasets\", \"transformers\"], \"example_code\": \"from huggingsound import SpeechRecognitionModel\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-english')\\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\\ntranscriptions = model.transcribe(audio_paths)\", \"performance\": {\"dataset\": \"mozilla-foundation/common_voice_6_0\", \"accuracy\": {\"Test WER\": 19.06, \"Test CER\": 7.69, \"Test WER (+LM)\": 14.81, \"Test CER (+LM)\": 6.84}}, \"description\": \"Fine-tuned facebook/wav2vec2-large-xlsr-53 on English using the train and validation splits of Common Voice 6.1. When using this model, make sure that your speech input is sampled at 16kHz.\"}}", "category": "generic"}
{"question_id": 75, "text": " Develop a tool that can identify and differentiate speakers in a conversation for our transcription service.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"pyannote.audio\", \"functionality\": \"Speaker Diarization\", \"api_name\": \"pyannote/speaker-diarization\", \"api_call\": \"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', use_auth_token='ACCESS_TOKEN_GOES_HERE')\", \"api_arguments\": {\"num_speakers\": \"int (optional)\", \"min_speakers\": \"int (optional)\", \"max_speakers\": \"int (optional)\"}, \"python_environment_requirements\": \"pyannote.audio 2.1.1\", \"example_code\": [\"from pyannote.audio import Pipeline\", \"pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2.1, use_auth_token=ACCESS_TOKEN_GOES_HERE)\", \"diarization = pipeline(audio.wav)\", \"with open(audio.rttm, w) as rttm:\", \"  diarization.write_rttm(rttm)\"], \"performance\": {\"dataset\": \"ami\", \"accuracy\": {\"DER%\": \"18.91\", \"FA%\": \"4.48\", \"Miss%\": \"9.51\", \"Conf%\": \"4.91\"}}, \"description\": \"This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers.\"}}", "category": "generic"}
{"question_id": 76, "text": " We are building an app for transcribing audio calls between multiple users. It's essential to divide overlapping speech into separate audio recordings for each user.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"pyannote.audio\", \"functionality\": \"overlapped-speech-detection\", \"api_name\": \"pyannote/overlapped-speech-detection\", \"api_call\": \"pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\", \"api_arguments\": [\"audio.wav\"], \"python_environment_requirements\": [\"pyannote.audio 2.1\"], \"example_code\": \"from pyannote.audio import Pipeline\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\noutput = pipeline(audio.wav)\\nfor speech in output.get_timeline().support():\\n  # two or more speakers are active between speech.start and speech.end\\n  ...\", \"performance\": {\"dataset\": \"ami\", \"accuracy\": null}, \"description\": \"Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\"}}", "category": "generic"}
{"question_id": 77, "text": " We want to transcribe speech from a podcast into text.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"vitouphy/wav2vec2-xls-r-300m-phoneme\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('vitouphy/wav2vec2-xls-r-300m-phoneme')\", \"api_arguments\": [\"input_values\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"None\", \"accuracy\": {\"Loss\": 0.3327, \"Cer\": 0.1332}}, \"description\": \"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition tasks.\"}}", "category": "generic"}
{"question_id": 78, "text": " Play this Chinese video and provide transcriptions.\\n###Input: /path/to/chinese_video.mp4\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Recognition\", \"api_name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\", \"api_call\": \"Wav2Vec2Model.from_pretrained('jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn')\", \"api_arguments\": [\"audio_paths\"], \"python_environment_requirements\": [\"huggingsound\", \"torch\", \"librosa\", \"datasets\", \"transformers\"], \"example_code\": \"from huggingsound import SpeechRecognitionModel\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn')\\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\\ntranscriptions = model.transcribe(audio_paths)\", \"performance\": {\"dataset\": \"Common Voice zh-CN\", \"accuracy\": {\"WER\": 82.37, \"CER\": 19.03}}, \"description\": \"Fine-tuned XLSR-53 large model for speech recognition in Chinese. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.\"}}", "category": "generic"}
{"question_id": 79, "text": " I have an audio file containing the sound of a machine and a person talking in the background. I want to separate the two sounds for better analysis.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"SpeechBrain\", \"functionality\": \"Audio Source Separation\", \"api_name\": \"sepformer-wsj02mix\", \"api_call\": \"separator.from_hparams(source=speechbrain/sepformer-wsj02mix, savedir='pretrained_models/sepformer-wsj02mix')\", \"api_arguments\": [\"path\"], \"python_environment_requirements\": [\"pip install speechbrain\"], \"example_code\": [\"from speechbrain.pretrained import SepformerSeparation as separator\", \"import torchaudio\", \"model = separator.from_hparams(source=speechbrain/sepformer-wsj02mix, savedir='pretrained_models/sepformer-wsj02mix')\", \"est_sources = model.separate_file(path='speechbrain/sepformer-wsj02mix/test_mixture.wav')\", \"torchaudio.save(source1hat.wav, est_sources[:, :, 0].detach().cpu(), 8000)\", \"torchaudio.save(source2hat.wav, est_sources[:, :, 1].detach().cpu(), 8000)\"], \"performance\": {\"dataset\": \"WSJ0-2Mix\", \"accuracy\": \"22.4 dB\"}, \"description\": \"This repository provides all the necessary tools to perform audio source separation with a SepFormer model, implemented with SpeechBrain, and pretrained on WSJ0-2Mix dataset.\"}}", "category": "generic"}
{"question_id": 80, "text": " Our customer is English-speaking and needs to communicate with Hokkien-speaking locals. We need to provide speech-to-speech translation from English to Hokkien.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"xm_transformer_s2ut_en-hk\", \"api_call\": \"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\", \"api_arguments\": {\"arg_overrides\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\"}, \"cache_dir\": \"cache_dir\"}, \"python_environment_requirements\": [\"fairseq\", \"huggingface_hub\", \"torchaudio\"], \"example_code\": {\"import_modules\": [\"import json\", \"import os\", \"from pathlib import Path\", \"import IPython.display as ipd\", \"from fairseq import hub_utils\", \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\", \"from fairseq.models.speech_to_text.hub_interface import S2THubInterface\", \"from fairseq.models.text_to_speech import CodeHiFiGANVocoder\", \"from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\", \"from huggingface_hub import snapshot_download\", \"import torchaudio\"], \"load_model\": [\"cache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\", \"models, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\", \"model = models[0].cpu()\", \"cfg['task'].cpu = True\"], \"generate_prediction\": [\"generator = task.build_generator([model], cfg)\", \"audio, _ = torchaudio.load('/path/to/an/audio/file')\", \"sample = S2THubInterface.get_model_input(task, audio)\", \"unit = S2THubInterface.get_prediction(task, model, generator, sample)\"], \"speech_synthesis\": [\"library_name = 'fairseq'\", \"cache_dir = (cache_dir or (Path.home() / '.cache' / library_name).as_posix())\", \"cache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name=library_name)\", \"x = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\", \"with open(f'{x['args']['data']}/config.json') as f:\", \"  vocoder_cfg = json.load(f)\", \"assert (len(x['args']['model_path']) == 1), 'Too many vocoder models in the input'\", \"vocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\", \"tts_model = VocoderHubInterface(vocoder_cfg, vocoder)\", \"tts_sample = tts_model.get_model_input(unit)\", \"wav, sr = tts_model.get_prediction(tts_sample)\", \"ipd.Audio(wav, rate=sr)\"]}, \"performance\": {\"dataset\": \"MuST-C\", \"accuracy\": \"Not specified\"}, \"description\": \"Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.\"}}", "category": "generic"}
{"question_id": 81, "text": " We have an app that records sounds and we need to extract multiple voices from the environment. We'd like to try separating three speakers.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Asteroid\", \"api_name\": \"ConvTasNet_Libri3Mix_sepclean_8k\", \"api_call\": \"from_asteroid import ConvTasNet_Libri3Mix_sepclean_8k\", \"api_arguments\": {\"n_src\": 3, \"sample_rate\": 8000, \"segment\": 3, \"task\": \"sep_clean\", \"train_dir\": \"data/wav8k/min/train-360\", \"valid_dir\": \"data/wav8k/min/dev\", \"kernel_size\": 16, \"n_filters\": 512, \"stride\": 8, \"bn_chan\": 128, \"hid_chan\": 512, \"mask_act\": \"relu\", \"n_blocks\": 8, \"n_repeats\": 3, \"skip_chan\": 128, \"lr\": 0.001, \"optimizer\": \"adam\", \"weight_decay\": 0.0, \"batch_size\": 24, \"early_stop\": true, \"epochs\": 200, \"half_lr\": true, \"num_workers\": 4}, \"python_environment_requirements\": \"Asteroid\", \"example_code\": \"\", \"performance\": {\"dataset\": \"Libri3Mix\", \"accuracy\": {\"si_sdr\": 8.581797049575108, \"si_sdr_imp\": 11.977037288467368, \"sdr\": 9.305885208641385, \"sdr_imp\": 12.3943409734845, \"sir\": 16.42030534048559, \"sir_imp\": 19.508759460400984, \"sar\": 10.641943911079238, \"sar_imp\": -56.4345187842095, \"stoi\": 0.8365148408724333, \"stoi_imp\": 0.24401766199806396}}, \"description\": \"This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri3Mix dataset.\"}}", "category": "generic"}
{"question_id": 82, "text": " As a tourist to Romania, I just recorded a native speaker explaining directions, and I need a translation of this speech into English.\\n###Input: Romanian_speaker_audio_recording.wav\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"facebook/textless_sm_ro_en\", \"api_call\": \"pipeline('audio-to-audio', model='facebook/textless_sm_ro_en')\", \"api_arguments\": \"audio file or recording\", \"python_environment_requirements\": \"fairseq, huggingface_hub\", \"example_code\": \"https://huggingface.co/facebook/textless_sm_cs_en\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": \"unknown\"}, \"description\": \"A speech-to-speech translation model for Romanian to English developed by Facebook AI\"}}", "category": "generic"}
{"question_id": 83, "text": " There's a noisy background sound in the recorded meeting we had today. Can you please enhance the speech signal?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Enhancement\", \"api_name\": \"sepformer-wham-enhancement\", \"api_call\": \"model.separate_file(path='speechbrain/sepformer-wham-enhancement/example_wham.wav')\", \"api_arguments\": [\"path\"], \"python_environment_requirements\": [\"pip install speechbrain\"], \"example_code\": \"from speechbrain.pretrained import SepformerSeparation as separator\\nimport torchaudio\\nmodel = separator.from_hparams(source='speechbrain/sepformer-wham-enhancement', savedir='pretrained_models/sepformer-wham-enhancement')\\nest_sources = model.separate_file(path='speechbrain/sepformer-wham-enhancement/example_wham.wav')\\ntorchaudio.save('enhanced_wham.wav', est_sources[:, :, 0].detach().cpu(), 8000)\", \"performance\": {\"dataset\": \"WHAM!\", \"accuracy\": \"14.35 dB SI-SNR\"}, \"description\": \"This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.\"}}", "category": "generic"}
{"question_id": 84, "text": " We're developing a voice assistant with emotion recognition. Help us classify emotions in spoken language.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"superb/hubert-base-superb-er\", \"api_call\": \"pipeline('audio-classification', model='superb/hubert-base-superb-er')\", \"api_arguments\": {\"model\": \"superb/hubert-base-superb-er\"}, \"python_environment_requirements\": {\"libraries\": [\"transformers\", \"datasets\", \"librosa\"], \"versions\": [\"latest\"]}, \"example_code\": \"from datasets import load_dataset\\nfrom transformers import pipeline\\ndataset = load_dataset(anton-l/superb_demo, er, split=session1)\\nclassifier = pipeline(audio-classification, model=superb/hubert-base-superb-er)\\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"IEMOCAP\", \"accuracy\": {\"session1\": 0.6492, \"session2\": 0.6359}}, \"description\": \"Hubert-Base for Emotion Recognition is a ported version of S3PRL's Hubert for the SUPERB Emotion Recognition task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. The model is used for predicting an emotion class for each utterance, and it is trained and evaluated on the IEMOCAP dataset.\"}}", "category": "generic"}
{"question_id": 85, "text": " I am developing an AI tool to spot specific keywords present in different audio files to categorize them. Provide me with a solution to detect keywords in the given audio files.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"superb/wav2vec2-base-superb-ks\", \"api_call\": \"pipeline('audio-classification', model='superb/wav2vec2-base-superb-ks')\", \"api_arguments\": {\"model\": \"superb/wav2vec2-base-superb-ks\"}, \"python_environment_requirements\": [\"torch\", \"transformers\", \"torchaudio\", \"datasets\"], \"example_code\": \"from datasets import load_dataset\\nfrom transformers import pipeline\\ndataset = load_dataset(anton-l/superb_demo, ks, split=test)\\nclassifier = pipeline(audio-classification, model=superb/wav2vec2-base-superb-ks)\\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"Speech Commands dataset v1.0\", \"accuracy\": {\"s3prl\": 0.9623, \"transformers\": 0.9643}}, \"description\": \"Wav2Vec2-Base for Keyword Spotting (KS) task in the SUPERB benchmark. The base model is pretrained on 16kHz sampled speech audio. The KS task detects preregistered keywords by classifying utterances into a predefined set of words. The model is trained on the Speech Commands dataset v1.0.\"}}", "category": "generic"}
{"question_id": 86, "text": " A customer support center needs a tool to analyze customers' satisfaction based on their voice messages. Recommend a solution for evaluating sentiment in Spanish voice messages.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Classification\", \"api_name\": \"hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\", \"api_call\": \"Output: Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\", \"api_arguments\": {\"model_name\": \"hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\"}, \"python_environment_requirements\": {\"transformers\": \"4.17.0\", \"pytorch\": \"1.10.0+cu111\", \"datasets\": \"2.0.0\", \"tokenizers\": \"0.11.6\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"MESD\", \"accuracy\": 0.9308}, \"description\": \"This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.\"}}", "category": "generic"}
{"question_id": 87, "text": " Your company is building voice assistants for smartphones, and they must be able to recognize and execute spoken commands efficiently.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"ast-finetuned-speech-commands-v2\", \"api_call\": \"MIT/ast-finetuned-speech-commands-v2\", \"api_arguments\": \"audio file\", \"python_environment_requirements\": \"transformers library\", \"example_code\": \"result = audio_classifier('path/to/audio/file.wav')\", \"performance\": {\"dataset\": \"Speech Commands v2\", \"accuracy\": \"98.120\"}, \"description\": \"Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.\"}}", "category": "generic"}
{"question_id": 88, "text": " A finance company needs a way to predict whether potential clients will have an income above $50,000 per year.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Joblib\", \"functionality\": \"Transformers\", \"api_name\": \"abhishek/autotrain-adult-census-xgboost\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"scikit-learn/adult-census-income\", \"accuracy\": 0.8750191923844618}, \"description\": \"A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual's income is above or below $50,000 per year.\"}}", "category": "generic"}
{"question_id": 89, "text": " We are a data science agency, we are helping people to predict if a passenger of titanic will survive based on different features.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Scikit-learn\", \"functionality\": \"Binary Classification\", \"api_name\": \"danupurnomo/dummy-titanic\", \"api_call\": \"load_model(cached_download(hf_hub_url(REPO_ID, TF_FILENAME)))\", \"api_arguments\": [\"new_data\"], \"python_environment_requirements\": [\"huggingface_hub\", \"joblib\", \"pandas\", \"numpy\", \"tensorflow\"], \"example_code\": \"from huggingface_hub import hf_hub_url, cached_download\\nimport joblib\\nimport pandas as pd\\nimport numpy as np\\nfrom tensorflow.keras.models import load_model\\nREPO_ID = 'danupurnomo/dummy-titanic'\\nPIPELINE_FILENAME = 'final_pipeline.pkl'\\nTF_FILENAME = 'titanic_model.h5'\\nmodel_pipeline = joblib.load(cached_download(\\n hf_hub_url(REPO_ID, PIPELINE_FILENAME)\\n))\\nmodel_seq = load_model(cached_download(\\n hf_hub_url(REPO_ID, TF_FILENAME)\\n))\", \"performance\": {\"dataset\": \"Titanic\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a binary classifier for predicting whether a passenger on the Titanic survived or not, based on features such as passenger class, age, sex, fare, and more.\"}}", "category": "generic"}
{"question_id": 90, "text": " We are a museum and we need to classify the survivers from Titanic according to their age, gender, and passenger class.\\n###Input: data = pd.read_csv('data.csv')\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Binary Classification\", \"api_name\": \"harithapliyal/autotrain-tatanic-survival-51030121311\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"harithapliyal/autotrain-data-tatanic-survival\", \"accuracy\": 0.872}, \"description\": \"A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.\"}}", "category": "generic"}
{"question_id": 91, "text": " Estimate the carbon emissions of a vehicle using its underlying features.\\n###Input: {\\\"data\\\": {\\\"Engine Size(L)\\\": 1.5, \\\"Cylinders\\\": 4, \\\"Transmission\\\": \\\"Manual\\\", \\\"Fuel Type\\\": \\\"gas\\\", \\\"Fuel Consumption Comb (L/100 km)\\\": 6.0}}\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"1702259728\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"Validation Metrics\", \"accuracy\": 0.831}, \"description\": \"A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data.\"}}", "category": "generic"}
{"question_id": 92, "text": " A real estate company needs a prediction tool to estimate housing prices for different regions.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Single Column Regression\", \"api_name\": \"1771761513\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"jwan2021/autotrain-data-us-housing-prices\", \"accuracy\": {\"Loss\": 100581.032, \"R2\": 0.922, \"MSE\": 10116543945.03, \"MAE\": 81586.656, \"RMSLE\": 0.101}}, \"description\": \"A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework.\"}}", "category": "generic"}
{"question_id": 93, "text": " I want to predict the carbon emissions of a specific car with given features.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Hugging Face\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"49741119788\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"kochetkovIT/autotrain-data-ironhack\", \"accuracy\": {\"Loss\": 2.603, \"R2\": 0.013, \"MSE\": 6.776, \"MAE\": 1.666, \"RMSLE\": 0.502}}, \"description\": \"A tabular regression model trained using AutoTrain to predict carbon emissions (in grams) with an R2 score of 0.013.\"}}", "category": "generic"}
{"question_id": 94, "text": " As an environmental organization, we want to estimate the carbon emissions from various factories given specific input data.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"2311773112\", \"api_call\": \"model\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\", \"json\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"al02783013/autotrain-data-faseiii_diciembre\", \"accuracy\": {\"Loss\": 5487.957, \"R2\": 0.96, \"MSE\": 30117668.0, \"MAE\": 2082.499, \"RMSLE\": 1.918}}, \"description\": \"A tabular regression model trained using AutoTrain to predict carbon emissions based on input features.\"}}", "category": "generic"}
{"question_id": 95, "text": " I am a data scientist working on a project to predict carbon emissions based on a set of input data. Provide instructions to load and use the model to make predictions.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"autotrain-dragino-7-7-1860763606\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"pcoloc/autotrain-data-dragino-7-7\", \"accuracy\": {\"Loss\": 84.433, \"R2\": 0.54, \"MSE\": 7129.004, \"MAE\": 62.626, \"RMSLE\": 0.418}}, \"description\": \"A tabular regression model trained using AutoTrain for predicting carbon emissions. The model is trained on the pcoloc/autotrain-data-dragino-7-7 dataset and has an R2 score of 0.540.\"}}", "category": "generic"}
{"question_id": 96, "text": " A restaurant wants to predict tips given by customers based on factors like bill amount, day, time, and other attributes. We can use this to optimize staff allocation during peak hours.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Scikit-learn\", \"functionality\": \"baseline-trainer\", \"api_name\": \"merve/tips9y0jvt5q-tip-regression\", \"api_call\": \"pipeline('tabular-regression', model='merve/tips9y0jvt5q-tip-regression')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"dabl\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"tips9y0jvt5q\", \"accuracy\": {\"r2\": 0.41524, \"neg_mean_squared_error\": -1.098792}}, \"description\": \"Baseline Model trained on tips9y0jvt5q to apply regression on tip. The model uses Ridge(alpha=10) and is trained with dabl library as a baseline. For better results, use AutoTrain.\"}}", "category": "generic"}
{"question_id": 97, "text": " We need to predict fish's weight based on different measurements, as part of a fish farm management software.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Scikit-learn\", \"functionality\": \"GradientBoostingRegressor\", \"api_name\": \"Fish-Weight\", \"api_call\": \"load('path_to_folder/example.pkl')\", \"api_arguments\": {\"model_path\": \"path_to_folder/example.pkl\"}, \"python_environment_requirements\": {\"skops.hub_utils\": \"download\", \"skops.io\": \"load\"}, \"example_code\": \"from skops.hub_utils import download\\nfrom skops.io import load\\ndownload('brendenc/Fish-Weight', 'path_to_folder')\\nmodel = load('path_to_folder/example.pkl')\", \"performance\": {\"dataset\": \"Fish dataset\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a GradientBoostingRegressor on a fish dataset. This model is intended for educational purposes.\"}}", "category": "generic"}
{"question_id": 98, "text": " We are making an application that predicts stock closing prices using a regression model on financial data.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Scikit-learn\", \"functionality\": \"baseline-trainer\", \"api_name\": \"srg/outhimar_64-Close-regression\", \"api_call\": \"Output: Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\\\nDate False False False ... True False False\\\\nOpen True False False ... False False False\\\\nHigh True False False ... False False False\\\\nLow True False False ... False False False\\\\nAdj Close True False False ... False False False\\\\nVolume True False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))]).fit(X_train, y_train)\", \"api_arguments\": [\"X_train\", \"y_train\"], \"python_environment_requirements\": [\"scikit-learn\", \"dabl\"], \"example_code\": \"Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\nDate False False False ... True False False\\nOpen True False False ... False False False\\nHigh True False False ... False False False\\nLow True False False ... False False False\\nAdj Close True False False ... False False False\\nVolume True False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))])\", \"performance\": {\"dataset\": \"outhimar_64\", \"accuracy\": {\"r2\": 0.999858, \"neg_mean_squared_error\": -1.067685}}, \"description\": \"Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt.\"}}", "category": "generic"}
{"question_id": 99, "text": " Create an AI model that can play CartPole-v1 game and store the results.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"CartPole-v1\", \"api_name\": \"sb3/ppo-CartPole-v1\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": \"python -m rl_zoo3.load_from_hub --algo ppo --env CartPole-v1 -orga sb3 -f logs/\", \"performance\": {\"dataset\": \"CartPole-v1\", \"accuracy\": \"500.00 +/- 0.00\"}, \"description\": \"This is a trained model of a PPO agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 100, "text": " A soccer match is being co-organized by a couple of sports organizations. The sports organizers want to prepare a learning-based simulation for the match.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"Train and play SoccerTwos\", \"api_name\": \"ahmad-alismail/poca-SoccerTwos\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"ml-agents\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"Not specified\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 101, "text": " The game development team needs a learning-based agent to create a character that can learn to hop in a game environment.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"edbeeching/decision-transformer-gym-hopper-expert\", \"api_call\": \"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-expert')\", \"api_arguments\": {\"mean\": [1.3490015, -0.11208222, -0.5506444, -0.13188992, -0.00378754, 2.6071432, 0.02322114, -0.01626922, -0.06840388, -0.05183131, 0.04272673], \"std\": [0.15980862, 0.0446214, 0.14307782, 0.17629202, 0.5912333, 0.5899924, 1.5405099, 0.8152689, 2.0173461, 2.4107876, 5.8440027]}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"See our Blog Post, Colab notebook or Example Script for usage.\", \"performance\": {\"dataset\": \"Gym Hopper environment\", \"accuracy\": \"Not provided\"}, \"description\": \"Decision Transformer model trained on expert trajectories sampled from the Gym Hopper environment\"}}", "category": "generic"}
{"question_id": 102, "text": " Create a game application that requires the interaction between two soccer-playing agents. The agents will learn how to play from each interaction.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Unity ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"JYC333/poca-SoccerTwos-v1\", \"api_call\": \"No changes needed. The `api_call` field does not include any model instantiation.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"ml-agents\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"N/A\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 103, "text": " A new robotic toy is being developed for kids. It recognizes objects and interacts with them accordingly.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning Robotics\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"EmbodiedAI tasks\", \"api_name\": \"VC1_BASE_NAME\", \"api_call\": \"model_utils.load_model(model_utils.VC1_BASE_NAME)(transformed_img)\", \"api_arguments\": \"img\", \"python_environment_requirements\": \"from vc_models.models.vit import model_utils\", \"example_code\": \"model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\nimg = your_function_here ...\\ntransformed_img = model_transforms(img)\\nembedding = model(transformed_img)\", \"performance\": {\"dataset\": \"CortexBench\", \"accuracy\": \"Mean Success: 68.7%\"}, \"description\": \"The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.\"}}", "category": "generic"}
{"question_id": 104, "text": " A home robotics company requires object manipulation and indoor navigation capabilities. How can we achieve this using available APIs?\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning Robotics\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"EmbodiedAI tasks, such as object manipulation and indoor navigation\", \"api_name\": \"facebook/vc1-large\", \"api_call\": \"Output: model_utils.load_model(model_utils.VC1_BASE_NAME)(transformed_img)\", \"api_arguments\": \"img\", \"python_environment_requirements\": \"from vc_models.models.vit import model_utils\", \"example_code\": \"model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\nimg = your_function_here ...\\ntransformed_img = model_transforms(img)\\nembedding = model(transformed_img)\", \"performance\": {\"dataset\": \"CortexBench\", \"accuracy\": \"68.7 (Mean Success)\"}, \"description\": \"The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.\"}}", "category": "generic"}
{"question_id": 105, "text": " Create a new story to be read to a child by starting the story with \\\"Once upon a time in a land far away...\\\"\\n###Input: \\\"Once upon a time in a land far away...\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lewtun/tiny-random-mt5\", \"api_call\": \"pipeline('text-generation', model='lewtun/tiny-random-mt5')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"nlp('Once upon a time...')\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random mt5 model for text generation\"}}", "category": "generic"}
{"question_id": 106, "text": " In the next product launch, we want to showcase the company mascot. We need a high-quality anime character with specific features.\\n###Input: 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"hakurei/waifu-diffusion\", \"api_call\": \"pipe(prompt, guidance_scale=6)['sample'][0]\", \"api_arguments\": {\"prompt\": \"text\", \"guidance_scale\": \"number\"}, \"python_environment_requirements\": {\"torch\": \"torch\", \"autocast\": \"from torch\", \"StableDiffusionPipeline\": \"from diffusers\"}, \"example_code\": \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", \"performance\": {\"dataset\": \"high-quality anime images\", \"accuracy\": \"not available\"}, \"description\": \"waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\"}}", "category": "generic"}
{"question_id": 107, "text": " I need a script to create photorealistic images from a description I provide, so I can use the generated images for my art projects.\\n###Input: <noinput>\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"darkstorm2150/Protogen_v2.2_Official_Release\", \"api_call\": \"StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\", \"api_arguments\": {\"model_id\": \"darkstorm2150/Protogen_v2.2_Official_Release\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": {\"diffusers\": \"StableDiffusionPipeline, DPMSolverMultistepScheduler\", \"torch\": \"torch\"}, \"example_code\": \"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)\", \"performance\": {\"dataset\": \"Various datasets\", \"accuracy\": \"Not specified\"}, \"description\": \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\"}}", "category": "generic"}
{"question_id": 108, "text": " Develop a piece of code that generates a high-resolution image of a landscape using a given prompt and low-resolution latents in the provided AI model.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Upscaling\", \"api_name\": \"stabilityai/sd-x2-latent-upscaler\", \"api_call\": \"upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator)\", \"api_arguments\": {\"prompt\": \"text prompt\", \"image\": \"low resolution latents\", \"num_inference_steps\": 20, \"guidance_scale\": 0, \"generator\": \"torch generator\"}, \"python_environment_requirements\": [\"git+https://github.com/huggingface/diffusers.git\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)\", \"performance\": {\"dataset\": \"LAION-2B\", \"accuracy\": \"Not specified\"}, \"description\": \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\"}}", "category": "generic"}
{"question_id": 109, "text": " We have a scanned Japanese manga image and we want to extract the text from it.\\n###Input: image_path = \\\"scanned_manga_page.jpg\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"kha-white/manga-ocr-base\", \"api_call\": \"pipeline('ocr', model='kha-white/manga-ocr-base').model\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"manga109s\", \"accuracy\": \"\"}, \"description\": \"Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\"}}", "category": "generic"}
{"question_id": 110, "text": " Please help us with describing the image, which we have submitted for our advertisement campaign.\\n###Input: \\\"<the image provided>\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-coco\", \"api_call\": \"pipeline('text-generation', model='microsoft/git-base-coco')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"See the model hub for fine-tuned versions on a task that interests you.\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Refer to the paper for evaluation results.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 111, "text": " Our company is designing a virtual assistant that will answer customer questions based on images they submit. Implement a function that finds out how many people are in a picture submitted by the user.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"blip2-flan-t5-xxl\", \"api_call\": \"Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\", \"api_arguments\": {\"raw_image\": \"Image\", \"question\": \"Text\"}, \"python_environment_requirements\": [\"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not provided\"}, \"description\": \"BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 112, "text": " I would like to create an AI system that can provide a detailed textual description for a given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"blip2-opt-6.7b\", \"api_call\": \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", \"api_arguments\": \"image, optional text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"Refer to the documentation\", \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not specified\"}, \"description\": \"BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 113, "text": " A children's publisher requested a system to generate stories based on images. For this purpose, we need to find a solution.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-coco\", \"api_call\": \"Output: GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", \"api_arguments\": \"image, text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"See table 11 in the paper for more details.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 114, "text": " I want to analyze and answer questions about a chart using image and text data.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/pix2struct-chartqa-base\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\", \"api_arguments\": [\"t5x_checkpoint_path\", \"pytorch_dump_path\", \"use-large\"], \"python_environment_requirements\": \"transformers\", \"example_code\": \"python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE\", \"performance\": {\"dataset\": \"ChartQA\", \"accuracy\": \"Not provided\"}, \"description\": \"Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.\"}}", "category": "generic"}
{"question_id": 115, "text": " I want to develop an application to read traffic signboards and provide output text in real-time. Can you tell me the model I can use and code to apply the model?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"mgp-str\", \"api_call\": \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", \"api_arguments\": {\"model_name\": \"alibaba-damo/mgp-str-base\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\"]}, \"example_code\": \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", \"performance\": {\"dataset\": \"MJSynth and SynthText\", \"accuracy\": null}, \"description\": \"MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.\"}}", "category": "generic"}
{"question_id": 116, "text": " We want to provide video summaries of news articles on our platform. We would like to turn our article's text into video summaries.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"chavinlo/TempoFunk\", \"api_call\": \"text_to_video = pipeline('text-to-video', model='chavinlo/TempoFunk')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\"}}", "category": "generic"}
{"question_id": 117, "text": " As a children's author, I need my story to be converted to video clips. The text is in English.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"ImRma/Brucelee\", \"api_call\": \"pipeline('text-to-video', model='ImRma/Brucelee')\", \"api_arguments\": [\"your_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Hugging Face model for converting Persian and English text into video.\"}}", "category": "generic"}
{"question_id": 118, "text": " Produce a short movie scene showing the protagonist breaking up with their partner while on a walk in the city under the rain.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"duncan93/video\", \"api_call\": \"N/A (There is no model instantiation provided in the `api_call` field)\", \"api_arguments\": \"\", \"python_environment_requirements\": \"Asteroid\", \"example_code\": \"\", \"performance\": {\"dataset\": \"OpenAssistant/oasst1\", \"accuracy\": \"\"}, \"description\": \"A text-to-video model trained on OpenAssistant/oasst1 dataset.\"}}", "category": "generic"}
{"question_id": 119, "text": " Please generate a code snippet for me to create an AI algorithm that answers questions based on the images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-textvqa\", \"api_call\": \"git_base_textvqa = AutoModel.from_pretrained('microsoft/git-base-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}}", "category": "generic"}
{"question_id": 120, "text": " We are an online study group specializing in history. Our members can upload an image of the text, and they ask questions related to the text. We need a Q&A model to answer these questions automatically.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"frizwankhan/entity-linking-model-final\", \"api_call\": \"pipeline('question-answering', model='frizwankhan/entity-linking-model-final')\", \"api_arguments\": {\"image\": \"path/to/image\", \"question\": \"your question\"}, \"python_environment_requirements\": {\"huggingface\": \"4.12.0\", \"torch\": \"1.9.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Document Question Answering model based on layoutlmv2\"}}", "category": "generic"}
{"question_id": 121, "text": " Our company needs a solution to automatically extract specific information such as invoice numbers and dates from invoices in various formats.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"\"}, \"description\": \"A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}", "category": "generic"}
{"question_id": 122, "text": " I have a document containing information about a company's products, and I want to find answers to my questions about these products.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased-finetuned-infovqa\", \"api_call\": \"layoutlmv2-base-uncased-finetuned-infovqa\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \"4.12.2\", \"pytorch\": \"1.8.0+cu101\", \"datasets\": \"1.14.0\", \"tokenizers\": \"0.10.3\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 2.087}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 123, "text": " I need an electronic document examiner that can answer a question about my document content.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa\", \"api_call\": \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", \"api_arguments\": \"question, context\", \"python_environment_requirements\": [\"transformers==4.15.0\", \"torch==1.8.0+cu101\", \"datasets==1.17.0\", \"tokenizers==0.10.3\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 4.3332}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 124, "text": " I need a molecule prediction model to predict the properties of different molecules in my research.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Graph Machine Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"graphormer-base-pcqm4mv1\", \"api_call\": \"Output: AutoModel.from_pretrained(model_name)\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"See the Graph Classification with Transformers tutorial\", \"performance\": {\"dataset\": \"PCQM4M-LSC\", \"accuracy\": \"1st place on the KDD CUP 2021 (quantum prediction track)\"}, \"description\": \"The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.\"}}", "category": "generic"}
{"question_id": 125, "text": " As an autonomous car company, we want to estimate the depth of objects in front of the car's cameras.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"tiny-random-DPTForDepthEstimation\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random DPT model for depth estimation using Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 126, "text": " I am building a robot that needs to navigate through a room. I need to estimate the depth of the room from an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221122-044810\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-044810')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, torch==1.12.1, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.369, \"Mae\": 0.2909, \"Rmse\": 0.4208, \"Abs Rel\": 0.3635, \"Log Mae\": 0.1224, \"Log Rmse\": 0.1793, \"Delta1\": 0.5323, \"Delta2\": 0.8179, \"Delta3\": 0.9258}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 127, "text": " Our environmental agency wants to estimate the depth of plant cover in a forest from the given dataset of images to understand their growth patterns.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221221-102136\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136').model\", \"api_arguments\": [], \"python_environment_requirements\": [\"Transformers 4.24.0\", \"Pytorch 1.12.1+cu116\", \"Datasets 2.8.0\", \"Tokenizers 0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4222, \"Mae\": 0.411, \"Rmse\": 0.6292, \"Abs Rel\": 0.3778, \"Log Mae\": 0.1636, \"Log Rmse\": 0.224, \"Delta1\": 0.432, \"Delta2\": 0.6806, \"Delta3\": 0.8068}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 128, "text": " A local grocery store is developing an AI application to identify fruits and vegetables from their images. What kind of code could they use for image classification?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"vit_base_patch16_224.augreg2_in21k_ft_in1k\", \"api_call\": \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"timm/vit_base_patch16_224.augreg2_in21k_ft_in1k\"}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 129, "text": " We are developing an app that identifies objects in pictures. In order to classify images, I must find a suitable image classification model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"convnextv2_huge.fcmae_ft_in1k\", \"api_call\": \"Output: timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\"}, \"python_environment_requirements\": [\"timm\"], \"example_code\": \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": 86.256}, \"description\": \"A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 130, "text": " The security team at our company is working on improving camera surveillance in parking lots. We need a model to track objects and cars in the parking lots.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"hustvl/yolos-small\", \"api_call\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", \"api_arguments\": {\"model_name\": \"hustvl/yolos-small\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": {\"import\": [\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\", \"from PIL import Image\", \"import requests\"], \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\", \"image\": \"Image.open(requests.get(url, stream=True).raw)\", \"feature_extractor\": \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", \"model\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", \"inputs\": \"feature_extractor(images=image, return_tensors='pt')\", \"outputs\": \"model(**inputs)\", \"logits\": \"outputs.logits\", \"bboxes\": \"outputs.pred_boxes\"}, \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"36.1 AP\"}, \"description\": \"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\"}}", "category": "generic"}
{"question_id": 131, "text": " Our construction safety management team needs to detect whether workers are using hard hats in a given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-hard-hat-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-hard-hat-detection')\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.24\", \"ultralytics==8.0.23\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"hard-hat-detection\", \"accuracy\": 0.811}, \"description\": \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\"}}", "category": "generic"}
{"question_id": 132, "text": " As a company specialized in smart cities, we need to develop roadway maintenance tools. Recently we got a requirement for detecting potholes in road images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8s-pothole-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8s-pothole-segmentation').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": {\"ultralyticsplus\": \"0.0.23\", \"ultralytics\": \"8.0.21\"}, \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pothole-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.928, \"mAP@0.5(mask)\": 0.928}}, \"description\": \"A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\"}}", "category": "generic"}
{"question_id": 133, "text": " Design a virtual assistant to help users create an effective Instagram Story image. The assistant should modify the input image by generating a stylized version as output.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"lllyasviel/sd-controlnet-canny\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny', torch_dtype=torch.float16)\", \"api_arguments\": {\"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": {\"opencv\": \"pip install opencv-contrib-python\", \"diffusers\": \"pip install diffusers transformers accelerate\"}, \"example_code\": \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", \"performance\": {\"dataset\": \"3M edge-image, caption pairs\", \"accuracy\": \"600 GPU-hours with Nvidia A100 80G\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 134, "text": " How can I insert a text prompt for creating an image which shows the text's meaning?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation model\", \"api_name\": \"lllyasviel/control_v11e_sd15_ip2p\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": [\"checkpoint\", \"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"Stable Diffusion v1-5\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\"}}", "category": "generic"}
{"question_id": 135, "text": " I want to create a new image of a beautiful mountain scenery by providing input of a plain field picture.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"GreeneryScenery/SheepsControlV3\", \"api_call\": \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"api_arguments\": {\"image\": \"Path to image file\", \"text_guidance\": \"Optional text guidance for the model\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": [\"from transformers import pipeline\", \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], \"performance\": {\"dataset\": \"GreeneryScenery/SheepsControlV3\", \"accuracy\": \"Not provided\"}, \"description\": \"GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\"}}", "category": "generic"}
{"question_id": 136, "text": " I am the owner of a photography website and want to automatically generate pictures of cats in order to add content to my website.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ddpm-cat-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.\"}}", "category": "generic"}
{"question_id": 137, "text": " We need to generate a high-quality image for the AI to analyze some human face expression so the algorithms should be improved.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Denoising Diffusion Probabilistic Models (DDPM)\", \"api_name\": \"google/ddpm-ema-celebahq-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": {\"model_id\": \"google/ddpm-ema-celebahq-256\"}, \"python_environment_requirements\": \"diffusers\", \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": {\"CIFAR10\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}, \"LSUN\": {\"sample_quality\": \"similar to ProgressiveGAN\"}}}, \"description\": \"High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.\"}}", "category": "generic"}
{"question_id": 138, "text": " We are an entertainment company specializing in creating realistic virtual characters. We want to generate synthetic images of human faces.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"CompVis/ldm-celebahq-256\", \"api_call\": \"DiffusionPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)\", \"performance\": {\"dataset\": \"CelebA-HQ\", \"accuracy\": \"N/A\"}, \"description\": \"Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.\"}}", "category": "generic"}
{"question_id": 139, "text": " Can you make a butterfly-themed birthday card for my niece?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\", \"api_arguments\": {\"model_id\": \"clp/sd-class-butterflies-32\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies.\"}}", "category": "generic"}
{"question_id": 140, "text": " We need to come up with some creative and innovative facial designs for our new android.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ncsnpp-ffhq-256\", \"api_call\": \"sde_ve = DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-256')\", \"api_arguments\": {\"model_id\": \"google/ncsnpp-ffhq-256\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": [\"!pip install diffusers\", \"from diffusers import DiffusionPipeline\", \"model_id = google/ncsnpp-ffhq-256\", \"sde_ve = DiffusionPipeline.from_pretrained(model_id)\", \"image = sde_ve()[sample]\", \"image[0].save(sde_ve_generated_image.png)\"], \"performance\": {\"dataset\": \"CIFAR-10\", \"accuracy\": {\"Inception score\": 9.89, \"FID\": 2.2, \"Likelihood\": 2.99}}, \"description\": \"Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.\"}}", "category": "generic"}
{"question_id": 141, "text": " Can you make it work to generate an image of a cute butterfly for us?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ocariz/butterfly_200\", \"api_call\": \"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.\"}}", "category": "generic"}
{"question_id": 142, "text": " Animate a video based on the past images we have from the surveillance camera.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-base-finetuned-k600\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\", \"api_arguments\": [\"images\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(8, 3, 224, 224))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-600\", \"accuracy\": null}, \"description\": \"TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 143, "text": " I need help to classify the videos online for my content platform.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"tiny-random-VideoMAEForVideoClassification\", \"api_call\": \"VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\", \"api_arguments\": \"model\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random VideoMAE model for video classification.\"}}", "category": "generic"}
{"question_id": 144, "text": " Look at this visual poster design and find out if it is suitable for use in a bakery.\\n###Input: <url to the poster>\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": 75.3}, \"description\": \"A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model.\"}}", "category": "generic"}
{"question_id": 145, "text": " We are building an app to provide information about objects around us. The user can take a picture of anything in front of them, and the app will tell them what it is.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-large-patch14\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\", \"api_arguments\": {\"text\": [\"a photo of a cat\", \"a photo of a dog\"], \"images\": \"image\", \"return_tensors\": \"pt\", \"padding\": \"True\"}, \"python_environment_requirements\": {\"packages\": [\"PIL\", \"requests\", \"transformers\"]}, \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-large-patch14)\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-large-patch14)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies depending on the dataset\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}", "category": "generic"}
{"question_id": 146, "text": " I am building a machine learning powered app that can classify food into three categories: fruit, vegetable, and junk food just by analyzing images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-g-14-laion2B-s34B-b88K\", \"api_call\": \"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\", \"api_arguments\": {\"image\": \"path/to/image/file\", \"class_names\": \"list_of_class_names\"}, \"python_environment_requirements\": {\"huggingface_hub\": \"0.0.17\", \"transformers\": \"4.11.3\", \"torch\": \"1.9.0\", \"torchvision\": \"0.10.0\"}, \"example_code\": null, \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\"}}", "category": "generic"}
{"question_id": 147, "text": " Classify an image of food and determine if it's a burger or a pizza with a pre-trained model.\\n###Input: \\\"path/to/food_image.jpg\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\", \"api_arguments\": {\"image\": \"path to image file\", \"class_names\": \"list of possible class names (comma-separated)\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; model = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K'); model('path/to/image.jpg', ['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8% to 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. These models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. They can be used for zero-shot image classification, image and text retrieval, and other tasks.\"}}", "category": "generic"}
{"question_id": 148, "text": " We have a box of photos for old family trips, but we can't identify the place. Can you help us estimate which country or region the pictures were taken in?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"tiny-random-CLIPSegModel\", \"api_call\": \"pipeline('zero-shot-image-classification', model='hf-tiny-model-private/tiny-random-CLIPSegModel')\", \"api_arguments\": {\"image\": \"File or URL\", \"class_names\": \"List of strings\"}, \"python_environment_requirements\": {\"transformers\": \">=4.13.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random CLIPSegModel for zero-shot image classification.\"}}", "category": "generic"}
{"question_id": 149, "text": " A client of ours wants to classify images of animals when they upload them to their website to better categorize the content. We are looking for a solution.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\", \"api_arguments\": \"image, class_names\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; clip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'); clip(image, class_names=['cat', 'dog', 'fish'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"79.1 - 79.4\"}, \"description\": \"A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}", "category": "generic"}
{"question_id": 150, "text": " We are a startup focusing on content creation for a Chinese audience. We want to recommend images related to poke categories automatically.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"OFA-Sys/chinese-clip-vit-base-patch16\", \"api_call\": \"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"OFA-Sys/chinese-clip-vit-base-patch16\"}, \"python_environment_requirements\": {\"transformers\": \"ChineseCLIPProcessor, ChineseCLIPModel\"}, \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [\\u6770\\u5c3c\\u9f9f, \\u5999\\u86d9\\u79cd\\u5b50, \\u5c0f\\u706b\\u9f99, \\u76ae\\u5361\\u4e18]\\ninputs = processor(images=image, return_tensors=pt)\\nimage_features = model.get_image_features(**inputs)\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\ntext_features = model.get_text_features(**inputs)\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": {\"MUGE Text-to-Image Retrieval\": {\"accuracy\": {\"Zero-shot R@1\": 63.0, \"Zero-shot R@5\": 84.1, \"Zero-shot R@10\": 89.2, \"Finetune R@1\": 68.9, \"Finetune R@5\": 88.7, \"Finetune R@10\": 93.1}}, \"Flickr30K-CN Retrieval\": {\"accuracy\": {\"Zero-shot Text-to-Image R@1\": 71.2, \"Zero-shot Text-to-Image R@5\": 91.4, \"Zero-shot Text-to-Image R@10\": 95.5, \"Finetune Text-to-Image R@1\": 83.8, \"Finetune Text-to-Image R@5\": 96.9, \"Finetune Text-to-Image R@10\": 98.6, \"Zero-shot Image-to-Text R@1\": 81.6, \"Zero-shot Image-to-Text R@5\": 97.5, \"Zero-shot Image-to-Text R@10\": 98.8, \"Finetune Image-to-Text R@1\": 95.3, \"Finetune Image-to-Text R@5\": 99.7, \"Finetune Image-to-Text R@10\": 100.0}}, \"COCO-CN Retrieval\": {\"accuracy\": {\"Zero-shot Text-to-Image R@1\": 69.2, \"Zero-shot Text-to-Image R@5\": 89.9, \"Zero-shot Text-to-Image R@10\": 96.1, \"Finetune Text-to-Image R@1\": 81.5, \"Finetune Text-to-Image R@5\": 96.9, \"Finetune Text-to-Image R@10\": 99.1, \"Zero-shot Image-to-Text R@1\": 63.0, \"Zero-shot Image-to-Text R@5\": 86.6, \"Zero-shot Image-to-Text R@10\": 92.9, \"Finetune Image-to-Text R@1\": 83.5, \"Finetune Image-to-Text R@5\": 97.3, \"Finetune Image-to-Text R@10\": 99.2}}, \"Zero-shot Image Classification\": {\"accuracy\": {\"CIFAR10\": 96.0, \"CIFAR100\": 79.7, \"DTD\": 51.2, \"EuroSAT\": 52.0, \"FER\": 55.1, \"FGVC\": 26.2, \"KITTI\": 49.9, \"MNIST\": 79.4, \"PC\": 63.5, \"VOC\": 84.9}}}}, \"description\": \"Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder.\"}}", "category": "generic"}
{"question_id": 151, "text": " I run a review website where users submit their reviews of various products. I want to classify positive and negative reviews.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"sentiment_analysis_generic_dataset\", \"api_call\": \"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"sentiment_analysis('I love this product!')\", \"performance\": {\"dataset\": \"generic_dataset\", \"accuracy\": \"Not specified\"}, \"description\": \"This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.\"}}", "category": "generic"}
{"question_id": 152, "text": " We need to keep our platform clean from unsuitable content. Build a system that can identify and flag NSFW (not safe for work) text in user-generated content.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"michellejieli/NSFW_text_classifier\", \"api_call\": \"pipeline('sentiment-analysis', model='michellejieli/NSFW_text_classification')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"classifier(I see youve set aside this special time to humiliate yourself in public.)\", \"performance\": {\"dataset\": \"Reddit posts\", \"accuracy\": \"Not specified\"}, \"description\": \"DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\"}}", "category": "generic"}
{"question_id": 153, "text": " An AI application development company wants to develop an application that can classify emotions from a given text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"michellejieli/emotion_text_classifier\", \"api_call\": \"pipeline('sentiment-analysis', model='michellejieli/emotion_text_classifier')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"classifier(I love this!)\", \"performance\": {\"dataset\": [\"Crowdflower (2016)\", \"Emotion Dataset, Elvis et al. (2018)\", \"GoEmotions, Demszky et al. (2020)\", \"ISEAR, Vikash (2018)\", \"MELD, Poria et al. (2019)\", \"SemEval-2018, EI-reg, Mohammad et al. (2018)\", \"Emotion Lines (Friends)\"], \"accuracy\": \"Not provided\"}, \"description\": \"DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\"}}", "category": "generic"}
{"question_id": 154, "text": " Explain how to find out the date and location of a meeting mentioned in a sentence.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"dslim/bert-base-NER\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(dslim/bert-base-NER)\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\nfrom transformers import pipeline\\ntokenizer = AutoTokenizer.from_pretrained(dslim/bert-base-NER)\\nmodel = AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\nexample = My name is Wolfgang and I live in Berlin\\nner_results = nlp(example)\\nprint(ner_results)\", \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": {\"f1\": 91.3, \"precision\": 90.7, \"recall\": 91.9}}, \"description\": \"bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\"}}", "category": "generic"}
{"question_id": 155, "text": " I am the editor of a political blog, and we often use quotes from different politicians. I want to extract names of politicians from the text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Token Classification\", \"api_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\", \"api_call\": \"Output: AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\\nmodel = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\", \"performance\": {\"dataset\": \"CoNLL-03\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification tasks.\"}}", "category": "generic"}
{"question_id": 156, "text": " Your manager needs a detailed report on a given text. Ensure that you include the part-of-speech tags for each word in the text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Token Classification\", \"api_name\": \"vblagoje/bert-english-uncased-finetuned-pos\", \"api_call\": \"vblagoje/bert-english-uncased-finetuned-pos\", \"api_arguments\": \"model\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"nlp('Hello world!')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A BERT model fine-tuned for Part-of-Speech (POS) tagging in English text.\"}}", "category": "generic"}
{"question_id": 157, "text": " I live in Berlin and I want to develop a multilingual named entity recognition pipeline for processing German, English, and Russian texts.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"Babelscape/wikineural-multilingual-ner\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\nexample = My name is Wolfgang and I live in Berlin\\nner_results = nlp(example)\\nprint(ner_results)\", \"performance\": {\"dataset\": \"Babelscape/wikineural-multilingual-ner\", \"accuracy\": \"span-based F1-score up to 6 points over previous state-of-the-art systems for data creation\"}, \"description\": \"A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.\"}}", "category": "generic"}
{"question_id": 158, "text": " As an online tutor, I need to prepare a list of frequently asked questions about my subjects. I require a tool that can quickly answer questions related to a spreadsheet containing data.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-small-finetuned-wikisql-supervised\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\", \"api_arguments\": \"model = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import TapasTokenizer, TapasForQuestionAnswering\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"Not specified\"}, \"description\": \"TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 159, "text": " I work for a tour agency and I need to answer customer questions using a database of historical Olympic Games. I need to know the host city of the 1904 Olympics.\\n###Input: {\\\"year\\\": [1896, 1900, 1904, 2004, 2008, 2012], \\\"city\\\": [\\\"athens\\\", \\\"paris\\\", \\\"st. louis\\\", \\\"athens\\\", \\\"beijing\\\", \\\"london\\\"]}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"neulab/omnitab-large-1024shot-finetuned-wtq-1024shot\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot-finetuned-wtq-1024shot')\", \"api_arguments\": {\"table\": \"pd.DataFrame\", \"query\": \"string\"}, \"python_environment_requirements\": [\"pandas\", \"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nimport pandas as pd\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot-finetuned-wtq-1024shot)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot-finetuned-wtq-1024shot)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab. neulab/omnitab-large-1024shot-finetuned-wtq-1024shot (based on BART architecture) is initialized with neulab/omnitab-large-1024shot and fine-tuned on WikiTableQuestions in the 1024-shot setting.\"}}", "category": "generic"}
{"question_id": 160, "text": " I want to prepare an AI model for an interactive storybook for kids. It should be able to answer questions related to the story.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/roberta-large-squad2\", \"api_call\": \"pipeline('question-answering', model='deepset/roberta-large-squad2')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": \"Not provided\"}, \"description\": \"A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\"}}", "category": "generic"}
{"question_id": 161, "text": " I want to classify text between three subjects. The subjects are art, science and history.\\n###Input: {\\\"text\\\": \\\"The invention of the telescope allowed astronomers to observe the solar system more effectively and led to the discovery of new planets and celestial objects.\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"typeform/distilbert-base-uncased-mnli\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained(\\\\typeform/distilbert-base-uncased-mnli\\\\)\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(typeform/distilbert-base-uncased-mnli)\", \"model\": \"AutoModelForSequenceClassification.from_pretrained(typeform/distilbert-base-uncased-mnli)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained(typeform/distilbert-base-uncased-mnli)\\nmodel = AutoModelForSequenceClassification.from_pretrained(typeform/distilbert-base-uncased-mnli)\", \"performance\": {\"dataset\": \"multi_nli\", \"accuracy\": 0.8206875508543532}, \"description\": \"This is the uncased DistilBERT model fine-tuned on Multi-Genre Natural Language Inference (MNLI) dataset for the zero-shot classification task.\"}}", "category": "generic"}
{"question_id": 162, "text": " Suppose my company wants to hire a candidate for a technical writer position. How could we determine whether an applicant's cover letter is aligned with the job expectations or not?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"Narsil/deberta-large-mnli-zero-cls\", \"api_call\": \"DebertaModel.from_pretrained('Narsil/deberta-large-mnli-zero-cls')\", \"api_arguments\": \"text, candidate_labels\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": {\"SQuAD 1.1\": {\"F1\": 95.5, \"EM\": 90.1}, \"SQuAD 2.0\": {\"F1\": 90.7, \"EM\": 88.0}, \"MNLI-m/mm\": {\"Accuracy\": 91.3}, \"SST-2\": {\"Accuracy\": 96.5}, \"QNLI\": {\"Accuracy\": 95.3}, \"CoLA\": {\"MCC\": 69.5}, \"RTE\": {\"Accuracy\": 91.0}, \"MRPC\": {\"Accuracy\": 92.6}, \"QQP\": {}, \"STS-B\": {\"P/S\": 92.8}}}, \"description\": \"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on the majority of NLU tasks with 80GB training data. This is the DeBERTa large model fine-tuned with MNLI task.\"}}", "category": "generic"}
{"question_id": 163, "text": " To assist in efficient communication, we need a tool to help us translate comments on our multilingual social media posts to English.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-ROMANCE-en\", \"api_call\": \"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en') and MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\", \"api_arguments\": [\"source languages\", \"target languages\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": 62.2, \"chr-F\": 0.75}}, \"description\": \"A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.\"}}", "category": "generic"}
{"question_id": 164, "text": " I need to translate an English paragraph to Arabic for my website.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-en-ar\", \"api_call\": \"pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\", \"api_arguments\": {\"text\": \"string\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_en_to_ar', model='Helsinki-NLP/opus-mt-en-ar')\\ntranslated_text = translation('Hello World')\", \"performance\": {\"dataset\": \"Tatoeba-test.eng.ara\", \"accuracy\": {\"BLEU\": 14.0, \"chr-F\": 0.437}}, \"description\": \"A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of '>>id<<' (id = valid target language ID).\"}}", "category": "generic"}
{"question_id": 165, "text": " I need a tool that can translate Dutch text to English language for an upcoming project.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-nl-en\", \"api_call\": \"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\ntranslated_text = translation('Hallo, hoe gaat het met je?')[0]['translation_text']\", \"performance\": {\"dataset\": \"Tatoeba.nl.en\", \"accuracy\": {\"BLEU\": 60.9, \"chr-F\": 0.749}}, \"description\": \"A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing.\"}}", "category": "generic"}
{"question_id": 166, "text": " Can you automatically summarize a long article in French language for me?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"moussaKam/barthez-orangesum-abstract\", \"api_call\": \"BarthezModel.from_pretrained('moussaKam/barthez-orangesum-abstract')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"orangeSum\", \"accuracy\": \"\"}, \"description\": \"Barthez model finetuned on orangeSum for abstract generation in French language\"}}", "category": "generic"}
{"question_id": 167, "text": " I am a German management consulting firm. I need to summarize the content of business documents for my clients.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Summarization\", \"api_name\": \"Einmalumdiewelt/T5-Base_GNAD\", \"api_call\": \"pipeline('summarization', model='Einmalumdiewelt/T5-Base_GNAD')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 2.1025, \"Rouge1\": 27.5357, \"Rouge2\": 8.5623, \"Rougel\": 19.1508, \"Rougelsum\": 23.9029, \"Gen Len\": 52.7253}}, \"description\": \"This model is a fine-tuned version of Einmalumdiewelt/T5-Base_GNAD on an unknown dataset. It is intended for German text summarization.\"}}", "category": "generic"}
{"question_id": 168, "text": " Implement a communication method for our employee chatbot for quick and simple responses.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"microsoft/DialoGPT-medium\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium').generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\", \"api_arguments\": [\"bot_input_ids\", \"max_length\", \"pad_token_id\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-medium)\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-medium)\", \"performance\": {\"dataset\": \"Reddit\", \"accuracy\": \"Comparable to human response quality under a single-turn conversation Turing test\"}, \"description\": \"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.\"}}", "category": "generic"}
{"question_id": 169, "text": " Design a text summarizer for a Russian news website that will automatically condense the lengthy articles for the readers.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Abstractive Russian Summarization\", \"api_name\": \"cointegrated/rut5-base-absum\", \"api_call\": \"Output: T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')\", \"api_arguments\": {\"n_words\": \"int\", \"compression\": \"float\", \"max_length\": \"int\", \"num_beams\": \"int\", \"do_sample\": \"bool\", \"repetition_penalty\": \"float\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": \"import torch\\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\\nMODEL_NAME = 'cointegrated/rut5-base-absum'\\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\\nmodel.cuda();\\nmodel.eval();\\ndef summarize(\\n text, n_words=None, compression=None,\\n max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, \\n <strong>kwargs\\n):\\n \\n Summarize the text\\n The following parameters are mutually exclusive:\\n - n_words (int) is an approximate number of words to generate.\\n - compression (float) is an approximate length ratio of summary and original text.\\n \\n if n_words:\\n text = '[{}] '.format(n_words) + text\\n elif compression:\\n text = '[{0:.1g}] '.format(compression) + text\\n x = tokenizer(text, return_tensors='pt', padding=True).to(model.device)\\n with torch.inference_mode():\\n out = model.generate(\\n </strong>x, \\n max_length=max_length, num_beams=num_beams, \\n do_sample=do_sample, repetition_penalty=repetition_penalty, \\n **kwargs\\n )\\n return tokenizer.decode(out[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": [\"csebuetnlp/xlsum\", \"IlyaGusev/gazeta\", \"mlsum\"], \"accuracy\": \"Not provided\"}, \"description\": \"This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets.\"}}", "category": "generic"}
{"question_id": 170, "text": " We are creating a chatbot that can participate in a live Dungeons and Dragons game. Bring Pygmalion-6B into the chat session to play a character.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"pygmalion-6b\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b').generate(input_ids, max_length=100, num_return_sequences=1)\", \"api_arguments\": [\"input_ids\", \"max_length\", \"num_return_sequences\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained('waifu-workshop/pygmalion-6b')\\nmodel = AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b')\\ninput_text = [CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n<START>\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\\noutput = model.generate(input_ids, max_length=100, num_return_sequences=1)\\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"56MB of dialogue data gathered from multiple sources\", \"accuracy\": \"Not specified\"}, \"description\": \"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue.\"}}", "category": "generic"}
{"question_id": 171, "text": " We have an online platform where psychologists chat with anonymous users. We want a virtual agent that will play the role of a psychologist for some of our users.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text-generation\", \"api_name\": \"pygmalion-1.3b\", \"api_call\": \"pipeline('text-generation', 'PygmalionAI/pygmalion-1.3b')\", \"api_arguments\": \"input_prompt\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"[CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\n[DIALOGUE HISTORY]\\nYou: [Your input message here]\\n[CHARACTER]:\", \"performance\": {\"dataset\": \"56MB of dialogue data\", \"accuracy\": \"Not provided\"}, \"description\": \"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}}", "category": "generic"}
{"question_id": 172, "text": " We are designing a conversational AI agent for our customers. We want the agent to respond naturally to users' questions.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Conversational\", \"api_name\": \"allenai/cosmo-xl\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained(\\\\allenai/cosmo-xl\\\\)\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"allenai/cosmo-xl\"}, \"python_environment_requirements\": {\"torch\": \"latest\", \"transformers\": \"latest\"}, \"example_code\": {\"import\": [\"import torch\", \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\"], \"initialize\": [\"device = torch.device(cuda if torch.cuda.is_available() else cpu)\", \"tokenizer = AutoTokenizer.from_pretrained(allenai/cosmo-xl)\", \"model = AutoModelForSeq2SeqLM.from_pretrained(allenai/cosmo-xl).to(device)\"], \"example\": [\"def set_input(situation_narrative, role_instruction, conversation_history):\", \" input_text =  <turn> .join(conversation_history)\", \"if role_instruction != :\", \" input_text = {} &lt;sep&gt; {}'.format(role_instruction, input_text)\", \"if situation_narrative != :\", \" input_text = {} &lt;sep&gt; {}'.format(situation_narrative, input_text)\", \"return input_text\", \"def generate(situation_narrative, role_instruction, conversation_history):\", \" input_text = set_input(situation_narrative, role_instruction, conversation_history)\", \" inputs = tokenizer([input_text], return_tensors=pt).to(device)\", \" outputs = model.generate(inputs[input_ids], max_new_tokens=128, temperature=1.0, top_p=.95, do_sample=True)\", \" response = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\", \" return response\", \"situation = Cosmo had a really fun time participating in the EMNLP conference at Abu Dhabi.\", \"instruction = You are Cosmo and you are talking to a friend.\", \"conversation = [\", \" Hey, how was your trip to Abu Dhabi?\", \"]\", \"response = generate(situation, instruction, conversation)\", \"print(response)\"]}, \"performance\": {\"dataset\": {\"allenai/soda\": \"\", \"allenai/prosocial-dialog\": \"\"}, \"accuracy\": \"\"}, \"description\": \"COSMO is a conversation agent with greater generalizability on both in- and out-of-domain chitchat datasets (e.g., DailyDialog, BlendedSkillTalk). It is trained on two datasets: SODA and ProsocialDialog. COSMO is especially aiming to model natural human conversations. It can accept situation descriptions as well as instructions on what role it should play in the situation.\"}}", "category": "generic"}
{"question_id": 173, "text": " I need help generating a short piece of Python code that calculates the sum of square numbers for any input list.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Program Synthesis\", \"api_name\": \"Salesforce/codegen-2B-multi\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('Salesforce/codegen-2B-multi').generate(input_ids, max_length=128)\", \"api_arguments\": {\"input_ids\": \"input_ids\", \"max_length\": 128}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForCausalLM\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-2B-multi)\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-2B-multi)\\ntext = def hello_world():\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\ngenerated_ids = model.generate(input_ids, max_length=128)\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"HumanEval, MTPB\"}, \"description\": \"CodeGen is a family of autoregressive language models for program synthesis. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B). The checkpoint included in this repository is denoted as CodeGen-Multi 2B, where Multi means the model is initialized with CodeGen-NL 2B and further pre-trained on a dataset of multiple programming languages, and 2B refers to the number of trainable parameters.\"}}", "category": "generic"}
{"question_id": 174, "text": " I own a small apparel store and I need help coming up with a catchy slogan for the clothing.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"decapoda-research/llama-13b-hf\", \"api_call\": \"pipeline('text-generation', model='decapoda-research/llama-13b-hf')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"generator('Once upon a time')\", \"performance\": {\"dataset\": [{\"name\": \"BoolQ\", \"accuracy\": \"85.3\"}, {\"name\": \"PIQA\", \"accuracy\": \"82.8\"}, {\"name\": \"SIQA\", \"accuracy\": \"52.3\"}, {\"name\": \"HellaSwag\", \"accuracy\": \"84.2\"}, {\"name\": \"WinoGrande\", \"accuracy\": \"77\"}, {\"name\": \"ARC-e\", \"accuracy\": \"81.5\"}, {\"name\": \"ARC-c\", \"accuracy\": \"56\"}, {\"name\": \"OBQACOPA\", \"accuracy\": \"60.2\"}]}, \"description\": \"LLaMA-13B is an auto-regressive language model based on the transformer architecture developed by the FAIR team of Meta AI. It is designed for research purposes, such as question answering, natural language understanding, and reading comprehension. The model has been trained on a variety of sources, including web data, GitHub, Wikipedia, and books in 20 languages. It has been evaluated on several benchmarks, including BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, and OpenBookQA.\"}}", "category": "generic"}
{"question_id": 175, "text": " Translate the following English text to Korean: \\\"The weather today is beautiful.\\\"\\n###Input: \\\"The weather today is beautiful.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"kykim/bertshared-kor-base\", \"api_call\": \"EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\", \"api_arguments\": {\"tokenizer\": \"BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import BertTokenizerFast, EncoderDecoderModel\\ntokenizer = BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\\nmodel = EncoderDecoderModel.from_pretrained(kykim/bertshared-kor-base)\", \"performance\": {\"dataset\": \"70GB Korean text dataset\", \"accuracy\": \"42000 lower-cased subwords\"}, \"description\": \"Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks.\"}}", "category": "generic"}
{"question_id": 176, "text": " Our team is working on a search engine, and we require a model that can generate a query from the given document text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text2Text Generation\", \"api_name\": \"castorini/doc2query-t5-base-msmarco\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\", \"api_arguments\": \"text, max_length\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"MS MARCO\", \"accuracy\": \"Not specified\"}, \"description\": \"A T5 model trained on the MS MARCO dataset for generating queries from documents.\"}}", "category": "generic"}
{"question_id": 177, "text": " Our travel agency needs a tool that automatically translates a travel itinerary from English to French for our French-speaking clients.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text-to-Text Transfer Transformer\", \"api_name\": \"google/mt5-base\", \"api_call\": \"MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\", \"api_arguments\": [\"model_name\", \"input_text\", \"generated_text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\\ntokenizer = MT5Tokenizer.from_pretrained('google/mt5-base')\\ninputs = tokenizer.encode('translate English to German: The house is wonderful.', return_tensors='pt')\\noutputs = model.generate(inputs, max_length=40, num_return_sequences=1)\\ndecoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"mc4\", \"accuracy\": \"Not provided\"}, \"description\": \"mT5 is a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. It leverages a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of multilingual NLP tasks.\"}}", "category": "generic"}
{"question_id": 178, "text": " We have a competition going on between our team members. Based on the text, we need to predict the next word.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling, Next Sentence Prediction\", \"api_name\": \"bert-base-uncased\", \"api_call\": \"pipeline('fill-mask', model='bert-base-uncased')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nunmasker = pipeline('fill-mask', model='bert-base-uncased')\\nunmasker(Hello I'm a [MASK] model.)\", \"performance\": {\"dataset\": \"GLUE\", \"accuracy\": 79.6}, \"description\": \"BERT base model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It can be used for masked language modeling, next sentence prediction, and fine-tuning on downstream tasks such as sequence classification, token classification, or question answering.\"}}", "category": "generic"}
{"question_id": 179, "text": " Transform the code snippet for generating a greeting message to a corresponding text summary.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Code Understanding and Generation\", \"api_name\": \"Salesforce/codet5-base\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\", \"api_arguments\": [\"text\", \"return_tensors\", \"input_ids\", \"max_length\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import RobertaTokenizer, T5ForConditionalGeneration\\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\\ntext = def greet(user): print(f'hello <extra_id_0>!')\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\ngenerated_ids = model.generate(input_ids, max_length=8)\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"code_search_net\", \"accuracy\": \"Refer to the paper for evaluation results on several downstream benchmarks\"}, \"description\": \"CodeT5 is a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. It supports both code understanding and generation tasks and allows for multi-task learning. The model can be used for tasks such as code summarization, code generation, code translation, code refinement, code defect detection, and code clone detection.\"}}", "category": "generic"}
{"question_id": 180, "text": " I would like to measure whether two given sentences have similar meanings or not.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Embeddings\", \"api_name\": \"sentence-transformers/paraphrase-MiniLM-L3-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"snli, multi_nli, ms_marco\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 181, "text": " As a part of our educational platform, we need to implement a text-to-speech functionality for visually impaired students to use.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"mio/amadeus\", \"api_call\": \"No changes needed.\", \"api_arguments\": {\"model_name\": \"mio/amadeus\"}, \"python_environment_requirements\": {\"espnet\": \"d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\", \"transformers\": \"latest\"}, \"example_code\": \"cd espnet\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\npip install -e .\\ncd egs2/amadeus/tts1\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus\", \"performance\": {\"dataset\": \"amadeus\", \"accuracy\": \"Not provided\"}, \"description\": \"This model was trained by mio using amadeus recipe in espnet.\"}}", "category": "generic"}
{"question_id": 182, "text": " We are building a virtual assistant that speaks the text we provide as a response.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"SpeechBrain\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"tts-hifigan-ljspeech\", \"api_call\": \"'hifi_gan.decode_batch(mel_specs)'\", \"api_arguments\": {\"mel_specs\": \"torch.Tensor\"}, \"python_environment_requirements\": \"pip install speechbrain\", \"example_code\": [\"import torch\", \"from speechbrain.pretrained import HIFIGAN\", \"hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir)\", \"mel_specs = torch.rand(2, 80,298)\", \"waveforms = hifi_gan.decode_batch(mel_specs)\"], \"performance\": {\"dataset\": \"LJSpeech\", \"accuracy\": \"Not specified\"}, \"description\": \"This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram. The sampling frequency is 22050 Hz.\"}}", "category": "generic"}
{"question_id": 183, "text": " I work for a broadcasting agency, and we need automated text-to-speech for news segments in a more natural way.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"mio/Artoria\", \"api_call\": \"pipeline('text-to-speech', model='mio/Artoria')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; tts = pipeline('text-to-speech', model='mio/Artoria'); tts('\\u3053\\u3093\\u306b\\u3061\\u306f')\", \"performance\": {\"dataset\": \"fate\", \"accuracy\": \"Not provided\"}, \"description\": \"This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\"}}", "category": "generic"}
{"question_id": 184, "text": " Our company is developing an IVR system for the Telugu speaking population. We need a male voice to read out the menu options.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"SYSPIN/Telugu_Male_TTS\", \"api_call\": \"pipeline('text-to-speech', model='SYSPIN/Telugu_Male_TTS')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.\"}}", "category": "generic"}
{"question_id": 185, "text": " As a Chinese language teacher, I am trying to create pronunciation exercises for my students. I need to convert some text into speech.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\", \"api_call\": \"Output: Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"torch\", \"espnet_model_zoo\"], \"example_code\": \"import soundfile\\nfrom espnet2.bin.tts_inference import Text2Speech\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\ntext = \\u6625\\u6c5f\\u6f6e\\u6c34\\u8fde\\u6d77\\u5e73\\uff0c\\u6d77\\u4e0a\\u660e\\u6708\\u5171\\u6f6e\\u751f\\nspeech = text2speech(text)[wav]\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\", \"performance\": {\"dataset\": \"csmsc\", \"accuracy\": \"Not specified\"}, \"description\": \"A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\"}}", "category": "generic"}
{"question_id": 186, "text": " I am running an online course with German content. Can you provide an audio version of the introductory lecture?\\n###Input: \\\"Willkommen zum Einfrungsvortrag! In dieser Online-Kursreihe werden wir verschiedene Themen untersuchen, um Ihnen ein umfassendes Verstdnis der behandelten Konzepte zu ermlichen. Bereiten Sie sich darauf vor, aufregende und interessante Informationen zu erhalten. Viel Spabeim Lernen!\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"speechbrain\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"tts-hifigan-german\", \"api_call\": \"hifi_gan.decode_batch(mel_output)\", \"api_arguments\": [\"mel_output\"], \"python_environment_requirements\": [\"speechbrain\"], \"example_code\": \"import torchaudio\\nfrom speechbrain.pretrained import Tacotron2\\nfrom speechbrain.pretrained import HIFIGAN\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\nwaveforms = hifi_gan.decode_batch(mel_output)\\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\", \"performance\": {\"dataset\": \"custom German dataset\", \"accuracy\": \"Not specified\"}, \"description\": \"A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.\"}}", "category": "generic"}
{"question_id": 187, "text": " Develop a mechanism to monitor and report voice activity in meeting recordings, isolating ongoing conversational moments to improve transcription accuracy.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Voice Activity Detection\", \"framework\": \"pyannote.audio\", \"functionality\": \"Automatic Speech Recognition\", \"api_name\": \"pyannote/voice-activity-detection\", \"api_call\": \"Pipeline.from_pretrained(pyannote/voice-activity-detection)\", \"api_arguments\": [\"audio.wav\"], \"python_environment_requirements\": [\"pyannote.audio 2.1\"], \"example_code\": \"from pyannote.audio import Pipeline\\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\noutput = pipeline(audio.wav)\\nfor speech in output.get_timeline().support():\\n  # active speech between speech.start and speech.end\", \"performance\": {\"dataset\": \"ami\", \"accuracy\": \"Not specified\"}, \"description\": \"A pretrained voice activity detection pipeline that detects active speech in audio files.\"}}", "category": "generic"}
{"question_id": 188, "text": " We are working on a customer service chatbot project. We need to transcribe client voice messages accurately.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Automatic Speech Recognition\", \"api_name\": \"cpierse/wav2vec2-large-xlsr-53-esperanto\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('cpierse/wav2vec2-large-xlsr-53-esperanto')\", \"api_arguments\": [\"path\", \"sentence\", \"speech\", \"sampling_rate\"], \"python_environment_requirements\": [\"torch\", \"torchaudio\", \"datasets\", \"transformers\", \"re\", \"jiwer\"], \"example_code\": \"import torch\\nimport torchaudio\\nfrom datasets import load_dataset\\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\ntest_dataset = load_dataset(common_voice, eo, split=test[:2%]) \\nprocessor = Wav2Vec2Processor.from_pretrained(cpierse/wav2vec2-large-xlsr-53-esperanto) \\nmodel = Wav2Vec2ForCTC.from_pretrained(cpierse/wav2vec2-large-xlsr-53-esperanto) \\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\\ndef speech_file_to_array_fn(batch):\\n speech_array, sampling_rate = torchaudio.load(batch[path])\\n batch[speech] = resampler(speech_array).squeeze().numpy()\\n return batch\\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\\ninputs = processor(test_dataset[speech][:2], sampling_rate=16_000, return_tensors=pt, padding=True)\\nwith torch.no_grad():\\n logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\\npredicted_ids = torch.argmax(logits, dim=-1)\\nprint(Prediction:, processor.batch_decode(predicted_ids))\\nprint(Reference:, test_dataset[sentence][:2])\", \"performance\": {\"dataset\": \"common_voice\", \"accuracy\": \"12.31%\"}, \"description\": \"Fine-tuned facebook/wav2vec2-large-xlsr-53 on esperanto using the Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz.\"}}", "category": "generic"}
{"question_id": 189, "text": " Our team wants to separate the vocals and instrumentals from an audio recording. How do we use a pretrained model to do this?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Asteroid\", \"api_name\": \"JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k\", \"api_call\": \"JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k\", \"api_arguments\": \"audio\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"Libri2Mix\", \"accuracy\": {\"si_sdr\": 10.617130949793383, \"si_sdr_imp\": 12.551811412989263, \"sdr\": 11.231867464482065, \"sdr_imp\": 13.059765009747343, \"sir\": 24.461138352988346, \"sir_imp\": 24.371856452307703, \"sar\": 11.5649982725426, \"sar_imp\": 4.662525705768228, \"stoi\": 0.8701085138712695, \"stoi_imp\": 0.2245418019822898}}, \"description\": \"This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_noisy task of the Libri2Mix dataset.\"}}", "category": "generic"}
{"question_id": 190, "text": " Our client is a radio station and they have many programs in Hokkien language. They want to convert Hokkien programs into English for international audience.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"Speech-to-speech translation\", \"api_name\": \"xm_transformer_s2ut_hk-en\", \"api_call\": \"'S2THubInterface.get_prediction(task, models[0].cpu(), generator, sample)'\", \"api_arguments\": {\"task\": \"speech_to_text\", \"model\": \"facebook/xm_transformer_s2ut_hk-en\", \"generator\": \"task.build_generator([model], cfg)\", \"sample\": \"S2THubInterface.get_model_input(task, audio)\"}, \"python_environment_requirements\": {\"fairseq\": \"latest\", \"torchaudio\": \"latest\", \"huggingface_hub\": \"latest\"}, \"example_code\": \"import json\\nimport os\\nfrom pathlib import Path\\nimport IPython.display as ipd\\nfrom fairseq import hub_utils\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\nfrom huggingface_hub import snapshot_download\\nimport torchaudio\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\n facebook/xm_transformer_s2ut_hk-en,\\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\\n cache_dir=cache_dir,\\n)\\nmodel = models[0].cpu()\\ncfg[task].cpu = True\\ngenerator = task.build_generator([model], cfg)\\naudio, _ = torchaudio.load(/path/to/an/audio/file)\\nsample = S2THubInterface.get_model_input(task, audio)\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\nlibrary_name = fairseq\\ncache_dir = (\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\n)\\ncache_dir = snapshot_download(\\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\\n)\\nx = hub_utils.from_pretrained(\\n cache_dir,\\n model.pt,\\n .,\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\n config_yaml=config.json,\\n fp16=False,\\n is_vocoder=True,\\n)\\nwith open(f{x['args']['data']}/config.json) as f:\\n vocoder_cfg = json.load(f)\\nassert (\\n len(x[args][model_path]) == 1\\n), Too many vocoder models in the input\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\ntts_sample = tts_model.get_model_input(unit)\\nwav, sr = tts_model.get_prediction(tts_sample)\\nipd.Audio(wav, rate=sr)\", \"performance\": {\"dataset\": \"TED, drama, TAT domain\", \"accuracy\": \"Not provided\"}, \"description\": \"Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain.\"}}", "category": "generic"}
{"question_id": 191, "text": " Our team is working on building an application for automating user emotion recognition from audio. \\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Emotion Recognition\", \"api_name\": \"superb/wav2vec2-base-superb-er\", \"api_call\": \"superb/wav2vec2-base-superb-er\", \"api_arguments\": [\"file\", \"top_k\"], \"python_environment_requirements\": [\"datasets\", \"transformers\", \"torch\", \"librosa\"], \"example_code\": \"from datasets import load_dataset\\nfrom transformers import pipeline\\ndataset = load_dataset(anton-l/superb_demo, er, split=session1)\\nclassifier = pipeline(audio-classification, model=superb/wav2vec2-base-superb-er)\\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"IEMOCAP\", \"accuracy\": 0.6258}, \"description\": \"This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Emotion Recognition task. The base model is wav2vec2-base, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\"}}", "category": "generic"}
{"question_id": 192, "text": " A dating app needs to recognize the gender of the user who communicates with other users through voice messages. What model should we use?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"superb/hubert-large-superb-sid\", \"api_call\": \"pipeline('audio-classification', model='superb/hubert-large-superb-sid')\", \"api_arguments\": \"file, top_k\", \"python_environment_requirements\": \"datasets, transformers, librosa\", \"example_code\": \"from datasets import load_dataset\\nfrom transformers import pipeline\\ndataset = load_dataset(anton-l/superb_demo, si, split=test)\\nclassifier = pipeline(audio-classification, model=superb/hubert-large-superb-sid)\\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"VoxCeleb1\", \"accuracy\": 0.9035}, \"description\": \"Hubert-Large for Speaker Identification. This model is pretrained on 16kHz sampled speech audio and should be used with speech input also sampled at 16Khz. It is used for the SUPERB Speaker Identification task and can classify each utterance for its speaker identity as a multi-class classification.\"}}", "category": "generic"}
{"question_id": 193, "text": " We need a voice command ordering assistant ready to parse the numbers 0 to 9, and we need to classify which number is said by the user.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"mazkooleg/0-9up-hubert-base-ls960-ft\", \"api_call\": \"pipeline('audio-classification', model='mazkooleg/0-9up-hubert-base-ls960-ft')\", \"api_arguments\": \"audio file or record from browser\", \"python_environment_requirements\": [\"transformers==4.26.1\", \"torch==1.11.0+cpu\", \"datasets==2.10.1\", \"tokenizers==0.12.1\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"mazkooleg/0-9up_google_speech_commands_augmented_raw\", \"accuracy\": 0.9973}, \"description\": \"This model is a fine-tuned version of facebook/hubert-base-ls960 on the None dataset. It achieves an accuracy of 0.9973 on the evaluation set.\"}}", "category": "generic"}
{"question_id": 194, "text": " Determine the sentiment of a given Spanish audio file.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Classification\", \"api_name\": \"hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\", \"api_call\": \"Output: Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\", \"api_arguments\": {\"model_name\": \"hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\"}, \"python_environment_requirements\": {\"transformers\": \"4.17.0\", \"pytorch\": \"1.10.0+cu111\", \"datasets\": \"2.0.0\", \"tokenizers\": \"0.11.6\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"MESD\", \"accuracy\": 0.9308}, \"description\": \"This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.\"}}", "category": "generic"}
{"question_id": 195, "text": " I want to detect voice activity in Indian languages in my recordings.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Voice Activity Detection\", \"framework\": \"Hugging Face\", \"functionality\": \"Voice Activity Detection\", \"api_name\": \"d4data/Indian-voice-cloning\", \"api_call\": \"pipeline('voice-activity-detection', model='d4data/Indian-voice-cloning').model\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for detecting voice activity in Indian languages.\"}}", "category": "generic"}
{"question_id": 196, "text": " How do I create something to automatically transcribe meetings for me and help me detect who is talking during the meeting?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Voice Activity Detection\", \"framework\": \"Hugging Face\", \"functionality\": \"Voice Activity Detection\", \"api_name\": \"FSMN-VAD\", \"api_call\": \"pipeline('voice-activity-detection', model='funasr/FSMN-VAD')\", \"api_arguments\": {}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"FSMN-VAD model for Voice Activity Detection using Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 197, "text": " We are trying to use this model to predict carbon emissions for our industrial client.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Hugging Face\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"38507101578\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"wangdy/autotrain-data-godaddy2\", \"accuracy\": {\"Loss\": 0.206, \"R2\": 0.997, \"MSE\": 0.042, \"MAE\": 0.081, \"RMSLE\": 0.052}}, \"description\": \"This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\"}}", "category": "generic"}
{"question_id": 198, "text": " Develop a code to predict the amount of tips a waiter can receive based on a certain dataset.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Scikit-learn\", \"functionality\": \"baseline-trainer\", \"api_name\": \"merve/tips5wx_sbh5-tip-regression\", \"api_call\": \"Output: Ridge(alpha=10)\", \"api_arguments\": {\"alpha\": 10}, \"python_environment_requirements\": [\"dabl\"], \"example_code\": \"Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\ntotal_bill True False False ... False False False\\nsex False False False ... False False False\\nsmoker False False False ... False False False\\nday False False False ... False False False\\ntime False False False ... False False False\\nsize False False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))])\", \"performance\": {\"dataset\": \"tips5wx_sbh5\", \"r2\": 0.389363, \"neg_mean_squared_error\": -1.092356}, \"description\": \"Baseline Model trained on tips5wx_sbh5 to apply regression on tip\"}}", "category": "generic"}
{"question_id": 199, "text": " We are building an app to calculate housing prices in California neighborhoods. Can you help us with the prediction?\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Scikit-learn\", \"functionality\": \"Tabular Regression\", \"api_name\": \"rajistics/california_housing\", \"api_call\": \"RandomForestRegressor()\", \"api_arguments\": {\"bootstrap\": \"True\", \"ccp_alpha\": \"0.0\", \"criterion\": \"squared_error\", \"max_depth\": \"\", \"max_features\": \"1.0\", \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": \"0.0\", \"min_samples_leaf\": \"1\", \"min_samples_split\": \"2\", \"min_weight_fraction_leaf\": \"0.0\", \"n_estimators\": \"100\", \"n_jobs\": \"\", \"oob_score\": \"False\", \"random_state\": \"\", \"verbose\": \"0\", \"warm_start\": \"False\"}, \"python_environment_requirements\": \"Scikit-learn\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices.\"}}", "category": "generic"}
{"question_id": 200, "text": " I have a team of AI researchers and we are focusing on playing soccer using artificial agents. I want these agents to perform at a high level in our in-house soccer training environment.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"Raiden-1001/poca-Soccerv7\", \"api_call\": \"No changes needed. The `api_call` field does not include any model instantiation.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\", \"ML-Agents-SoccerTwos\"], \"example_code\": \"Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\\nStep 2: Select your .nn /.onnx file\\nClick on Watch the agent play \\ud83d\\udc40\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 201, "text": " Determine the similarity of sentences from a provided document by obtaining sentence embeddings.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"sup-simcse-roberta-large\", \"api_call\": \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", \"api_arguments\": [\"AutoTokenizer\", \"AutoModel\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\", \"performance\": {\"dataset\": \"STS tasks\", \"accuracy\": \"Spearman's correlation (See associated paper Appendix B)\"}, \"description\": \"A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.\"}}", "category": "generic"}
{"question_id": 202, "text": " I want to create a chatbot with the ability to provide sentence embeddings for a given sentence in Russian.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"sberbank-ai/sbert_large_mt_nlu_ru\", \"api_call\": \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", \"api_arguments\": [\"sentences\", \"padding\", \"truncation\", \"max_length\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n# Sentences we want sentence embeddings for\\nsentences = ['\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442! \\u041a\\u0430\\u043a \\u0442\\u0432\\u043e\\u0438 \\u0434\\u0435\\u043b\\u0430?',\\n             '\\u0410 \\u043f\\u0440\\u0430\\u0432\\u0434\\u0430, \\u0447\\u0442\\u043e 42 \\u0442\\u0432\\u043e\\u0435 \\u043b\\u044e\\u0431\\u0438\\u043c\\u043e\\u0435 \\u0447\\u0438\\u0441\\u043b\\u043e?']\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", \"performance\": {\"dataset\": \"Russian SuperGLUE\", \"accuracy\": \"Not provided\"}, \"description\": \"BERT large model multitask (cased) for Sentence Embeddings in Russian language.\"}}", "category": "generic"}
{"question_id": 203, "text": " I want to build a multilingual content recommendation system using LaBSE. Show me how to handle English, Italian, and Japanese sentences.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"setu4993/LaBSE\", \"api_call\": \"BertModel.from_pretrained('setu4993/LaBSE')\", \"api_arguments\": [\"english_sentences\", \"italian_sentences\", \"japanese_sentences\"], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", \"performance\": {\"dataset\": \"CommonCrawl and Wikipedia\", \"accuracy\": \"Not Specified\"}, \"description\": \"Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.\"}}", "category": "generic"}
{"question_id": 204, "text": " Our online forum has a feature to autocomplete parts of code snippets. We need a model to recognize tokens in the domain of coding languages.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"lanwuwei/BERTOverflow_stackoverflow_github\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"lanwuwei/BERTOverflow_stackoverflow_github\"}, \"python_environment_requirements\": {\"transformers\": \"*\", \"torch\": \"*\"}, \"example_code\": \"from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\", \"performance\": {\"dataset\": \"StackOverflow's 10 year archive\", \"accuracy\": \"Not provided\"}, \"description\": \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\"}}", "category": "generic"}
{"question_id": 205, "text": " We need to create a visual for a character concept from a given description.\\n###Input: A young android girl wearing a blue dress, holding an umbrella in one hand, and a small bird perched on her shoulder.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"hakurei/waifu-diffusion\", \"api_call\": \"pipe(prompt, guidance_scale=6)['sample'][0]\", \"api_arguments\": {\"prompt\": \"text\", \"guidance_scale\": \"number\"}, \"python_environment_requirements\": {\"torch\": \"torch\", \"autocast\": \"from torch\", \"StableDiffusionPipeline\": \"from diffusers\"}, \"example_code\": \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", \"performance\": {\"dataset\": \"high-quality anime images\", \"accuracy\": \"not available\"}, \"description\": \"waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\"}}", "category": "generic"}
{"question_id": 206, "text": " Find a way to generate a high-resolution image of a deep blue car speeding on a highway in the rain.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image generation and modification based on text prompts\", \"api_name\": \"stabilityai/stable-diffusion-2-inpainting\", \"api_call\": \"pipe(prompt=prompt, image=image, mask_image=mask_image)\", \"api_arguments\": [\"prompt\", \"image\", \"mask_image\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-inpainting,\\n torch_dtype=torch.float16,\\n)\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\"}}", "category": "generic"}
{"question_id": 207, "text": " Generate a caption for an image provided from the internet.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Transformers\", \"functionality\": \"Image Captioning\", \"api_name\": \"blip-image-captioning-large\", \"api_call\": \"BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\", \"api_arguments\": {\"raw_image\": \"Image\", \"text\": \"Optional Text\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForConditionalGeneration\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": {\"import_requests\": \"import requests\", \"import_PIL\": \"from PIL import Image\", \"import_transformers\": \"from transformers import BlipProcessor, BlipForConditionalGeneration\", \"load_processor\": \"processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)\", \"load_model\": \"model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\", \"load_image\": \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", \"conditional_captioning\": \"text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"unconditional_captioning\": \"inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\"}, \"performance\": {\"dataset\": \"COCO\", \"accuracy\": {\"image-text retrieval\": \"+2.7% recall@1\", \"image captioning\": \"+2.8% CIDEr\", \"VQA\": \"+1.6% VQA score\"}}, \"description\": \"BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\"}}", "category": "generic"}
{"question_id": 208, "text": " I have an image containing text that is essential for a legal document. Can you extract the text for me?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-large-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", \"api_arguments\": {\"TrOCRProcessor\": \"from_pretrained('microsoft/trocr-large-printed')\", \"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"pip install transformers\", \"PIL\": \"pip install pillow\", \"requests\": \"pip install requests\"}, \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 209, "text": " Use the model to generate captions for images uploaded to the application.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-r-textcaps\", \"api_call\": \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 210, "text": " I have an image that contains text, and I'd like to extract the text from the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"mgp-str\", \"api_call\": \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", \"api_arguments\": {\"model_name\": \"alibaba-damo/mgp-str-base\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\"]}, \"example_code\": \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", \"performance\": {\"dataset\": \"MJSynth and SynthText\", \"accuracy\": null}, \"description\": \"MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.\"}}", "category": "generic"}
{"question_id": 211, "text": " In our next project, we are developing a system that answers questions about the images. Could you help us find the best model to use for this project?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-textvqa\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"See table 11 in the paper for more details.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 212, "text": " Our company prepares an e-commerce website, and we want to be able to automatically answer specific questions about product information and descriptions in our catalogue.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased-finetuned-infovqa\", \"api_call\": \"layoutlmv2-base-uncased-finetuned-infovqa\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \"4.12.2\", \"pytorch\": \"1.8.0+cu101\", \"datasets\": \"1.14.0\", \"tokenizers\": \"0.10.3\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 2.087}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 213, "text": " Invoices for the business are piling up. Find out the due date for each invoice.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_arguments\": \"{'question': 'your_question', 'context': 'your_context'}\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}", "category": "generic"}
{"question_id": 214, "text": " Analyze the given image to find depth information.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\", \"api_call\": \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", \"api_arguments\": [], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 215, "text": " I am a developer programming an autonomous robot. It needs to estimate depth from a single image to navigate successfully.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221215-112116\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"DIODE\", \"accuracy\": \"\"}, \"description\": \"A depth estimation model fine-tuned on the DIODE dataset.\"}}", "category": "generic"}
{"question_id": 216, "text": " Our customer wants to use this model in their factory for an autonomous mobile robot application to estimate depth from images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-kitti-finetuned-diode\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.5845, \"Rmse\": 0.6175}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 217, "text": " We need to create a system that can analyze images and predict their content.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/beit-base-patch16-224-pt22k-ft22k\", \"api_call\": \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"ImageNet-22k\", \"accuracy\": \"Not specified\"}, \"description\": \"BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.\"}}", "category": "generic"}
{"question_id": 218, "text": " Our customer is an animal shelter, and they frequently receive pictures of animals and food. We need to classify these images appropriately.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"abhishek/autotrain-dog-vs-food\", \"api_call\": \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", \"api_arguments\": \"image_path\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"sasha/dog-food\", \"accuracy\": 0.998}, \"description\": \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\"}}", "category": "generic"}
{"question_id": 219, "text": " A grocery store requested a solution to classify beans based on images taken through their inventory management system.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"fxmarty/resnet-tiny-beans\", \"api_call\": \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", \"api_arguments\": {\"model\": \"fxmarty/resnet-tiny-beans\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", \"performance\": {\"dataset\": \"beans\", \"accuracy\": \"Not provided\"}, \"description\": \"A model trained on the beans dataset, just for testing and having a really tiny model.\"}}", "category": "generic"}
{"question_id": 220, "text": " We need to detect the primary object in an image located at: \\\"http://images.cocodataset.org/val2017/000000039769.jpg\\\".\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/swinv2-tiny-patch4-window8-256\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", \"api_arguments\": {\"image\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.\"}}", "category": "generic"}
{"question_id": 221, "text": " We need a classifier to predict the category of items being processed at a food production line.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"timm/mobilenetv3_large_100.ra_in1k\", \"api_call\": \"Output: timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\"}, \"python_environment_requirements\": {\"timm\": \"latest\"}, \"example_code\": \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.\"}}", "category": "generic"}
{"question_id": 222, "text": " Develop a bot with the capability to understand and segment objects and backgrounds within images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-base-coco\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\"}}", "category": "generic"}
{"question_id": 223, "text": " Our team at the architecture firm is trying to analyze the urban design of a city. We need a software that helps us in segmenting urban images into more manageable sections.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"shi-labs/oneformer_ade20k_swin_tiny\", \"api_call\": \"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)'\", \"api_arguments\": {\"images\": \"image\", \"task_inputs\": [\"semantic\", \"instance\", \"panoptic\"], \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\"}}", "category": "generic"}
{"question_id": 224, "text": " Let's build a computer vision application that takes any image and then extracts the segmented objects from the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-tiny-coco-instance\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/mask2former-swin-tiny-coco-instance\"}, \"python_environment_requirements\": [\"torch\", \"transformers\", \"PIL\", \"requests\"], \"example_code\": \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not specified\"}, \"description\": \"Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\"}}", "category": "generic"}
{"question_id": 225, "text": " Our client is a construction firm that need to extract the building plans to annotate their designs.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Segmentation\", \"api_name\": \"lllyasviel/sd-controlnet-seg\", \"api_call\": \"Output: ControlNetModel.from_pretrained(\\\\lllyasviel/sd-controlnet-seg\\\\, torch_dtype=torch.float16)\", \"api_arguments\": [\"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", \"performance\": {\"dataset\": \"ADE20K\", \"accuracy\": \"Trained on 164K segmentation-image, caption pairs\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 226, "text": " I have this idea to use GPT to generate a text while visualizing them at the same time. We already have ControlNet and DDPM separately. We now need to synthesize the image based on the text given by GPT.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation model\", \"api_name\": \"lllyasviel/control_v11e_sd15_ip2p\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": [\"checkpoint\", \"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"Stable Diffusion v1-5\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\"}}", "category": "generic"}
{"question_id": 227, "text": " We need to produce realistic images for a fashion catalog.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"google/ddpm-celebahq-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.\"}}", "category": "generic"}
{"question_id": 228, "text": " Imagine we are a company providing virtual church tours. We need to create a realistic image of a church that can be used for virtual tours.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ddpm-church-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.\"}}", "category": "generic"}
{"question_id": 229, "text": " We are making a visual content generator for a website requiring natural-looking images of buildings. Find a way to create images of churches.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ncsnpp-church-256\", \"api_call\": \"sde_ve = DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\", \"api_arguments\": \"model_id\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-church-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR-10\", \"accuracy\": {\"Inception_score\": 9.89, \"FID\": 2.2, \"likelihood\": 2.99}}, \"description\": \"Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.\"}}", "category": "generic"}
{"question_id": 230, "text": " Generate a new Minecraft character's skin design.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"Minecraft-Skin-Diffusion\", \"api_call\": \"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion')\", \"api_arguments\": {}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion')\\nimage = pipeline().images[0].convert('RGBA')\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods.\"}}", "category": "generic"}
{"question_id": 231, "text": " We are developing a nature educational app. We need to generate images of butterflies while preserving the artistic representation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ntrant7/sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\", \"api_arguments\": [], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies.\"}}", "category": "generic"}
{"question_id": 232, "text": " Detect the type of vehicles in the parking lot.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-large-patch14-336\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\", \"api_arguments\": \"image_path, tokenizer, model\", \"python_environment_requirements\": \"Transformers 4.21.3, TensorFlow 2.8.2, Tokenizers 0.12.1\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": \"N/A\"}, \"description\": \"This model was trained from scratch on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 233, "text": " I want to classify the location of an image by giving it a list of city names.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Geolocalization\", \"api_name\": \"geolocal/StreetCLIP\", \"api_call\": \"CLIPModel.from_pretrained('geolocal/StreetCLIP')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"geolocal/StreetCLIP\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [{\"name\": \"IM2GPS\", \"accuracy\": {\"25km\": 28.3, \"200km\": 45.1, \"750km\": 74.7, \"2500km\": 88.2}}, {\"name\": \"IM2GPS3K\", \"accuracy\": {\"25km\": 22.4, \"200km\": 37.4, \"750km\": 61.3, \"2500km\": 80.4}}]}, \"description\": \"StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\"}}", "category": "generic"}
{"question_id": 234, "text": " We are building a content moderation bot that detects if an article or comment is computer-generated or produced by a human. We need to test it with the text \\\"Hello world! Is this content AI-generated?\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Detect GPT-2 generated text\", \"api_name\": \"roberta-base-openai-detector\", \"api_call\": \"pipeline('text-classification', model='roberta-base-openai-detector')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\nprint(pipe(Hello world! Is this content AI-generated?))\", \"performance\": {\"dataset\": \"WebText\", \"accuracy\": \"95%\"}, \"description\": \"RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.\"}}", "category": "generic"}
{"question_id": 235, "text": " Our company wants to analyze the sentiment of restaurant reviews on Yelp to improve its recommendation system. We need to classify the reviews as positive or negative.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"results-yelp\", \"api_call\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}, \"python_environment_requirements\": {\"Transformers\": \"4.18.0\", \"Pytorch\": \"1.10.0+cu111\", \"Datasets\": \"2.0.0\", \"Tokenizers\": \"0.12.1\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"Yelp\", \"accuracy\": 0.9302}, \"description\": \"This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\"}}", "category": "generic"}
{"question_id": 236, "text": " Help me identify the named entities in a text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Entity Extraction\", \"api_name\": \"904029577\", \"api_call\": \"Output: AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoTrain\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTokenClassification\", \"tokenizer\": \"AutoTokenizer\"}, \"example_code\": \"from transformers import AutoModelForTokenClassification, AutoTokenizer\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"ismail-lucifer011/autotrain-data-name_all\", \"accuracy\": 0.9989316041363876}, \"description\": \"This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams.\"}}", "category": "generic"}
{"question_id": 237, "text": " Being a biology student, I am finding it difficult to get answers to biology-related questions. How can I get those answers?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"sultan/BioM-ELECTRA-Large-SQuAD2\", \"api_call\": \"sultan/BioM-ELECTRA-Large-SQuAD2\", \"api_arguments\": null, \"python_environment_requirements\": [\"transformers\", \"sentencepiece\"], \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\nresult = qa_pipeline({'context': 'your_context', 'question': 'your_question'})\", \"performance\": {\"dataset\": \"SQuAD2.0 Dev\", \"accuracy\": {\"exact\": 84.33420365535248, \"f1\": 87.49354241889522}}, \"description\": \"BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.\"}}", "category": "generic"}
{"question_id": 238, "text": " Create a tool to help users quickly find information in a long document about their favorite sport or hobbies.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/bert-medium-squad2-distilled\", \"api_call\": \"AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\", \"api_arguments\": {\"model\": \"deepset/bert-medium-squad2-distilled\"}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"qa({'context': 'This is an example context.', 'question': 'What is this example about?'})\", \"performance\": {\"dataset\": \"squad_v2\", \"exact\": 68.6431398972458, \"f1\": 72.7637083790805}, \"description\": \"This model is a distilled version of deepset/bert-large-uncased-whole-word-masking-squad2, trained on the SQuAD 2.0 dataset for question answering tasks. It is based on the BERT-medium architecture and uses the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 239, "text": " How can I use a model to understand when a soccer match was played in a paragraph based on the provided information?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"csarron/bert-base-uncased-squad-v1\", \"api_call\": \"pipeline('question-answering', model='csarron/bert-base-uncased-squad-v1', tokenizer='csarron/bert-base-uncased-squad-v1')\", \"api_arguments\": {\"model\": \"csarron/bert-base-uncased-squad-v1\", \"tokenizer\": \"csarron/bert-base-uncased-squad-v1\"}, \"python_environment_requirements\": \"Python 3.7.5\", \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline(\\n question-answering,\\n model=csarron/bert-base-uncased-squad-v1,\\n tokenizer=csarron/bert-base-uncased-squad-v1\\n)\\npredictions = qa_pipeline({\\n 'context': The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.,\\n 'question': What day was the game played on?\\n})\\nprint(predictions)\", \"performance\": {\"dataset\": \"SQuAD1.1\", \"accuracy\": {\"EM\": 80.9, \"F1\": 88.2}}, \"description\": \"BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\"}}", "category": "generic"}
{"question_id": 240, "text": " I need a moderation system for comments in our community app, it should be able to classify comments as harmful, friend, or advertisement.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"typeform/distilbert-base-uncased-mnli\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained(\\\\typeform/distilbert-base-uncased-mnli\\\\)\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(typeform/distilbert-base-uncased-mnli)\", \"model\": \"AutoModelForSequenceClassification.from_pretrained(typeform/distilbert-base-uncased-mnli)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained(typeform/distilbert-base-uncased-mnli)\\nmodel = AutoModelForSequenceClassification.from_pretrained(typeform/distilbert-base-uncased-mnli)\", \"performance\": {\"dataset\": \"multi_nli\", \"accuracy\": 0.8206875508543532}, \"description\": \"This is the uncased DistilBERT model fine-tuned on Multi-Genre Natural Language Inference (MNLI) dataset for the zero-shot classification task.\"}}", "category": "generic"}
{"question_id": 241, "text": " Analyze the given Russian legal document, and output the translated version in English.\\n###Input: \\u0420\\u043e\\u0441\\u0441\\u0438\\u044f \\u0433\\u0430\\u0440\\u0430\\u043d\\u0442\\u0438\\u0440\\u0443\\u0435\\u0442 \\u043f\\u0440\\u0430\\u0432\\u0430 \\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\\u0430 \\u0438 \\u0441\\u0442\\u0440\\u0435\\u043c\\u0438\\u0442\\u0441\\u044f \\u0441\\u043e\\u0437\\u0434\\u0430\\u0442\\u044c \\u0435\\u0434\\u0438\\u043d\\u044b\\u0439 \\u0434\\u043b\\u044f \\u0432\\u0441\\u0435\\u0445 \\u044e\\u0440\\u0438\\u0434\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0444\\u043e\\u043d\\u0434. \\u042d\\u0442\\u043e \\u0431\\u0430\\u0437\\u0438\\u0440\\u0443\\u0435\\u0442\\u0441\\u044f \\u043d\\u0430 \\u043f\\u0440\\u0438\\u043d\\u0446\\u0438\\u043f\\u0430\\u0445 \\u043c\\u0435\\u0436\\u0434\\u0443\\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u043e\\u0433\\u043e \\u0443\\u0440\\u043e\\u0432\\u043d\\u044f. \\u0417\\u0430\\u043a\\u043e\\u043d \\u0434\\u043e\\u043b\\u0436\\u0435\\u043d \\u0431\\u044b\\u0442\\u044c \\u0440\\u0430\\u0432\\u043d\\u044b\\u043c \\u0434\\u043b\\u044f \\u0432\\u0441\\u0435\\u0445 \\u0433\\u0440\\u0430\\u0436\\u0434\\u0430\\u043d.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-ru-en\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-ru-en')\", \"api_arguments\": {\"from_pretrained\": \"Helsinki-NLP/opus-mt-ru-en\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForSeq2SeqLM\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\ntokenizer = AutoTokenizer.from_pretrained(Helsinki-NLP/opus-mt-ru-en)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(Helsinki-NLP/opus-mt-ru-en)\", \"performance\": {\"dataset\": \"newstest2019-ruen.ru.en\", \"accuracy\": 31.4}, \"description\": \"A Russian to English translation model developed by the Language Technology Research Group at the University of Helsinki. It is based on the Transformer-align architecture and trained on the OPUS dataset. The model can be used for translation and text-to-text generation tasks.\"}}", "category": "generic"}
{"question_id": 242, "text": " Operating a media company studio, we gather hundreds of articles daily that need to be summarized.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"google/pegasus-cnn_dailymail\", \"api_call\": \"Output: PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\nmodel_name = 'google/pegasus-cnn_dailymail'\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\ninputs = tokenizer.encode('input_text', return_tensors='pt')\\nsummary_ids = model.generate(inputs)\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"cnn_dailymail\", \"accuracy\": \"44.16/21.56/41.30\"}, \"description\": \"PEGASUS model for abstractive summarization, pretrained on the CNN/DailyMail dataset.\"}}", "category": "generic"}
{"question_id": 243, "text": " Our customer is a busy executive in the entertainment industry. They need to stay informed by regularly receiving news summaries.\\n###Input: \\\"Today, Warner Bros. announced the release date for the upcoming movie in the Harry Potter spinoff series, Fantastic Beasts and Where to Find Them 3. The long-awaited sequel is set to hit theaters on April 15, 2024. Eddie Redmayne, who stars as Newt Scamander, will reprise his role in the upcoming film. Fantastic Beasts 3 will be compared to the original series given its popularity among avid fans. The movie will follow Newt and his friends on a new adventure, with several returning cast members, including Katherine Waterston and Dan Fogler. Director David Yates, who has taken the helm for the previous entries in the series, will return for Fantastic Beasts 3 as well.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Summarization\", \"api_name\": \"google/pegasus-xsum\", \"api_call\": \"pipeline('summarization', model='google/pegasus-xsum')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": [{\"name\": \"xsum\", \"accuracy\": {\"ROUGE-1\": 46.862, \"ROUGE-2\": 24.453, \"ROUGE-L\": 39.055, \"ROUGE-LSUM\": 39.099}}, {\"name\": \"cnn_dailymail\", \"accuracy\": {\"ROUGE-1\": 22.206, \"ROUGE-2\": 7.67, \"ROUGE-L\": 15.405, \"ROUGE-LSUM\": 19.218}}, {\"name\": \"samsum\", \"accuracy\": {\"ROUGE-1\": 21.81, \"ROUGE-2\": 4.253, \"ROUGE-L\": 17.447, \"ROUGE-LSUM\": 18.891}}]}, \"description\": \"PEGASUS is a pre-trained model for abstractive summarization, developed by Google. It is based on the Transformer architecture and trained on both C4 and HugeNews datasets. The model is designed to extract gap sentences and generate summaries by stochastically sampling important sentences.\"}}", "category": "generic"}
{"question_id": 244, "text": " You are designing a chatbot for a taxi booking platform. Generate a response for a user asking about the taxi fare estimation.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Conversational\", \"api_name\": \"ingen51/DialoGPT-medium-GPT4\", \"api_call\": \"pipeline('conversational', model='ingen51/DialoGPT-medium-GPT4')\", \"api_arguments\": [\"message\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"conversational_pipeline('Hello, how are you?')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A GPT-4 model for generating conversational responses in a dialogue setting.\"}}", "category": "generic"}
{"question_id": 245, "text": " We need to write a short story about a lonely astronaut lost in deep space.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"decapoda-research/llama-7b-hf\", \"api_call\": \"AutoModel.from_pretrained('decapoda-research/llama-7b-hf')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\ngen = pipeline('text-generation', model='decapoda-research/llama-7b-hf')\\nresult = gen('Once upon a time')\\nprint(result[0]['generated_text'])\", \"performance\": {\"dataset\": [{\"name\": \"BoolQ\", \"accuracy\": 76.5}, {\"name\": \"PIQA\", \"accuracy\": 79.8}, {\"name\": \"SIQA\", \"accuracy\": 48.9}, {\"name\": \"HellaSwag\", \"accuracy\": 76.1}, {\"name\": \"WinoGrande\", \"accuracy\": 70.1}, {\"name\": \"ARC-e\", \"accuracy\": 76.7}, {\"name\": \"ARC-c\", \"accuracy\": 47.6}, {\"name\": \"OBQAC\", \"accuracy\": 57.2}, {\"name\": \"COPA\", \"accuracy\": 93}]}, \"description\": \"LLaMA-7B is an auto-regressive language model based on the transformer architecture. It is designed for research on large language models, including question answering, natural language understanding, and reading comprehension. The model is trained on various sources, including CCNet, C4, GitHub, Wikipedia, Books, ArXiv, and Stack Exchange, with the majority of the dataset being in English.\"}}", "category": "generic"}
{"question_id": 246, "text": " Can you help me create a model that generates a piece of text about climate change?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"xlnet-base-cased\", \"api_call\": \"XLNetModel.from_pretrained('xlnet-base-cased')\", \"api_arguments\": {\"pretrained_model_name\": \"xlnet-base-cased\"}, \"python_environment_requirements\": {\"library\": \"transformers\", \"version\": \"4.0.0+\"}, \"example_code\": \"from transformers import XLNetTokenizer, XLNetModel\\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\\nmodel = XLNetModel.from_pretrained('xlnet-base-cased')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"bookcorpus, wikipedia\", \"accuracy\": \"state-of-the-art (SOTA) results on various downstream language tasks\"}, \"description\": \"XLNet model pre-trained on English language. It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in this repository. XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context.\"}}", "category": "generic"}
{"question_id": 247, "text": " Our company is working on a project that requires the development of a tool to synthesize code based on natural language instructions.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Program Synthesis\", \"api_name\": \"Salesforce/codegen-350M-multi\", \"api_call\": \"model.generate(input_ids, max_length=128)\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\ntext = def hello_world():\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\ngenerated_ids = model.generate(input_ids, max_length=128)\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"HumanEval and MTPB\", \"accuracy\": \"Refer to the paper for accuracy details\"}, \"description\": \"CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.\"}}", "category": "generic"}
{"question_id": 248, "text": " Write a script to continue the story 'Once upon a time in a small village, a young girl by the name of Alice set out to find the enchanted tree.'\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"facebook/opt-350m\", \"api_call\": \"pipeline('text-generation', model='facebook/opt-350m')\", \"api_arguments\": {\"model\": \"facebook/opt-350m\", \"do_sample\": \"True\", \"num_return_sequences\": 5}, \"python_environment_requirements\": {\"transformers\": \"4.3.0\"}, \"example_code\": \"from transformers import pipeline, set_seed\\nset_seed(32)\\ngenerator = pipeline('text-generation', model='facebook/opt-350m', do_sample=True, num_return_sequences=5)\\ngenerator('The man worked as a')\", \"performance\": {\"dataset\": \"BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit, CCNewsV2\", \"accuracy\": \"Roughly matches GPT-3 performance\"}, \"description\": \"OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, developed by Meta AI. It is designed to enable reproducible and responsible research at scale and bring more voices to the table in studying the impact of large language models. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation. It can also be fine-tuned on a downstream task using the CLM example.\"}}", "category": "generic"}
{"question_id": 249, "text": " Our language chatbot should interpret intents and respond to user commands. First, we want the model to translate the following text from French to English: \\\"Je taime.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"bigscience/bloomz-560m\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m').generate(inputs)\", \"api_arguments\": {\"checkpoint\": \"bigscience/bloomz-560m\", \"inputs\": \"Translate to English: Je taime.\"}, \"python_environment_requirements\": [\"transformers\", \"accelerate\", \"bitsandbytes\"], \"example_code\": {\"CPU\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\\ncheckpoint = bigscience/bloomz-560m\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\ninputs = tokenizer.encode(Translate to English: Je taime., return_tensors=pt)\\noutputs = model.generate(inputs)\\nprint(tokenizer.decode(outputs[0]))\"}, \"performance\": {\"dataset\": \"bigscience/xP3\", \"accuracy\": {\"Winogrande XL (xl) validation set\": 52.41, \"XWinograd (en) test set\": 51.01, \"XWinograd (fr) test set\": 51.81, \"XWinograd (jp) test set\": 52.03, \"XWinograd (pt) test set\": 53.99, \"XWinograd (ru) test set\": 53.97, \"XWinograd (zh) test set\": 54.76, \"ANLI (r1) validation set\": 33.4, \"ANLI (r2) validation set\": 33.4, \"ANLI (r3) validation set\": 33.5}}, \"description\": \"BLOOMZ & mT0 are a family of models capable of following human instructions in dozens of languages zero-shot. Finetuned on the crosslingual task mixture (xP3), these models can generalize to unseen tasks & languages. Useful for tasks expressed in natural language, such as translation, summarization, and question answering.\"}}", "category": "generic"}
{"question_id": 250, "text": " User posted reviews need to be filtered for grammar and correctness before displaying them on our website.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Grammar Synthesis\", \"api_name\": \"pszemraj/flan-t5-large-grammar-synthesis\", \"api_call\": \"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\", \"api_arguments\": [\"raw_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\ncorrector = pipeline(\\n 'text2text-generation',\\n 'pszemraj/flan-t5-large-grammar-synthesis',\\n )\\nraw_text = 'i can has cheezburger'\\nresults = corrector(raw_text)\\nprint(results)\", \"performance\": {\"dataset\": \"jfleg\", \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset.\"}}", "category": "generic"}
{"question_id": 251, "text": " We are developing an AI-based tutor application, please suggest a solution that quickly completes a sentence containing a missing word for helping students improve vocabulary.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling\", \"api_name\": \"bert-base-cased\", \"api_call\": \"pipeline('fill-mask', model='bert-base-cased')\", \"api_arguments\": [\"model\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": [\"from transformers import pipeline\", \"unmasker = pipeline('fill-mask', model='bert-base-cased')\", \"unmasker(Hello I'm a [MASK] model.)\"], \"performance\": {\"dataset\": \"GLUE\", \"accuracy\": 79.6}, \"description\": \"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\"}}", "category": "generic"}
{"question_id": 252, "text": " I am an autonomous vehicle developer, and I often exchange messages with my international colleagues. Can you help me generate a fill-in-the-blank sentence about autonomous vehicles?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"microsoft/deberta-base\", \"api_call\": \"DebertaModel.from_pretrained('microsoft/deberta-base')\", \"api_arguments\": \"text\", \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-base')\\nfill_mask('The capital of France is [MASK].')\", \"performance\": {\"dataset\": {\"SQuAD 1.1\": \"93.1/87.2\", \"SQuAD 2.0\": \"86.2/83.1\", \"MNLI-m\": \"88.8\"}}, \"description\": \"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.\"}}", "category": "generic"}
{"question_id": 253, "text": " Design an AI tool that supports the process of identifying medical terms in a given text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"emilyalsentzer/Bio_ClinicalBERT\", \"api_call\": \"AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\", \"api_arguments\": [\"AutoTokenizer\", \"AutoModel\", \"from_pretrained\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\\nmodel = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\", \"performance\": {\"dataset\": \"MIMIC III\", \"accuracy\": \"Not provided\"}, \"description\": \"Bio_ClinicalBERT is a model initialized with BioBERT and trained on all MIMIC notes. It can be used for various NLP tasks in the clinical domain, such as Named Entity Recognition (NER) and Natural Language Inference (NLI).\"}}", "category": "generic"}
{"question_id": 254, "text": " I want to find the best word to fill the mask in this Japanese text: \\\"\\u4eca\\u65e5\\u306f[MASK]\\u3067\\u3059\\u3002\\\"\\n###Input: <noinput>\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"cl-tohoku/bert-base-japanese-char\", \"api_call\": \"cl-tohoku/bert-base-japanese-char\", \"api_arguments\": \"text\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"fill_mask('\\u4eca\\u65e5\\u306f[MASK]\\u3067\\u3059\\u3002')\", \"performance\": {\"dataset\": \"wikipedia\", \"accuracy\": \"N/A\"}, \"description\": \"This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by character-level tokenization.\"}}", "category": "generic"}
{"question_id": 255, "text": " As a writer, check the dictionary for aandidate word to make the Dutch sentence meaningful.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"GroNLP/bert-base-dutch-cased\", \"api_call\": \"AutoModel.from_pretrained('GroNLP/bert-base-dutch-cased')\", \"api_arguments\": [\"pretrained_model_name_or_path\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModel, TFAutoModel\\ntokenizer = AutoTokenizer.from_pretrained(GroNLP/bert-base-dutch-cased)\\nmodel = AutoModel.from_pretrained(GroNLP/bert-base-dutch-cased)\", \"performance\": {\"dataset\": [{\"name\": \"CoNLL-2002\", \"accuracy\": \"90.24\"}, {\"name\": \"SoNaR-1\", \"accuracy\": \"84.93\"}, {\"name\": \"spaCy UD LassySmall\", \"accuracy\": \"86.10\"}]}, \"description\": \"BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.\"}}", "category": "generic"}
{"question_id": 256, "text": " I want to help my users find similar news articles using the best sentence transformer to convert sentences to a 384-dimensional vector space.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/all-MiniLM-L12-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"1,170,060,424 training pairs\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 257, "text": " Determine if two sentences have similar meanings to create a summary of a long text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"nikcheerla/nooks-amd-detection-v2-full\", \"api_call\": \"SentenceTransformer('{MODEL_NAME}')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": [\"sentence-transformers\", \"transformers\"], \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('{MODEL_NAME}')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 258, "text": " Recommend me a solution to find a solution to semantically group articles into several topics on a multilingual news website.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/distiluse-base-multilingual-cased-v1\", \"api_call\": \"SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"N/A\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 259, "text": " Recommend me a sentence from a given list that is the most similar to a question I am going to provide you.\\n###Input: {\\\"query\\\": \\\"What is the capital city of France?\\\", \\\"docs\\\": [\\\"The capital of France is Paris.\\\", \\\"French cuisine is famous worldwide.\\\", \\\"The Eiffel Tower is an iconic landmark in France.\\\"]}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/multi-qa-mpnet-base-cos-v1\", \"api_call\": \"SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\", \"api_arguments\": {\"query\": \"string\", \"docs\": \"list of strings\"}, \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer, util\\nquery = How many people live in London?\\ndocs = [Around 9 Million people live in London, London is known for its financial district]\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\\nquery_emb = model.encode(query)\\ndoc_emb = model.encode(docs)\\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\\ndoc_score_pairs = list(zip(docs, scores))\\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\\nfor doc, score in doc_score_pairs:\\n print(score, doc)\", \"performance\": {\"dataset\": \"215M (question, answer) pairs from diverse sources\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model that maps sentences and paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.\"}}", "category": "generic"}
{"question_id": 260, "text": " We are building an online language learning platform. Implement a feature to convert phrases to spoken audio.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ONNX\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"NeuML/ljspeech-jets-onnx\", \"api_call\": \"TextToSpeech(NeuML/ljspeech-jets-onnx)\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"soundfile\", \"txtai.pipeline\", \"onnxruntime\", \"yaml\", \"ttstokenizer\"], \"example_code\": \"import soundfile as sf\\nfrom txtai.pipeline import TextToSpeech\\n# Build pipeline\\ntts = TextToSpeech(NeuML/ljspeech-jets-onnx)\\n# Generate speech\\nspeech = tts(Say something here)\\n# Write to file\\nsf.write(out.wav, speech, 22050)\", \"performance\": {\"dataset\": \"ljspeech\", \"accuracy\": null}, \"description\": \"ESPnet JETS Text-to-Speech (TTS) Model for ONNX exported using the espnet_onnx library. Can be used with txtai pipeline or directly with ONNX.\"}}", "category": "generic"}
{"question_id": 261, "text": " A software house needs an AI-powered narrator for narrating their audiobooks. We need a single-speaker model with a female voice.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"Fairseq\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"fastspeech2-en-ljspeech\", \"api_call\": \"'TTSHubInterface.get_prediction(task, models[0], generator, sample)'\", \"api_arguments\": {\"task\": \"task\", \"model\": \"model\", \"generator\": \"generator\", \"sample\": \"sample\"}, \"python_environment_requirements\": [\"fairseq\", \"IPython\"], \"example_code\": \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\nimport IPython.display as ipd\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\n facebook/fastspeech2-en-ljspeech,\\n arg_overrides={vocoder: hifigan, fp16: False}\\n)\\nmodel = models[0]\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\ngenerator = task.build_generator(model, cfg)\\ntext = Hello, this is a test run.\\nsample = TTSHubInterface.get_model_input(task, text)\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\nipd.Audio(wav, rate=rate)\", \"performance\": {\"dataset\": \"LJSpeech\", \"accuracy\": \"N/A\"}, \"description\": \"FastSpeech 2 text-to-speech model from fairseq S^2. English single-speaker female voice trained on LJSpeech.\"}}", "category": "generic"}
{"question_id": 262, "text": " Create a story-based audiobook in the Korean language from a given script.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"lakahaga/novel_reading_tts\", \"api_call\": \"AutoModelForTTS.from_pretrained('lakahaga/novel_reading_tts')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"inputs = processor(text, return_tensors='pt'); generated_audio = model.generate(**inputs);\", \"performance\": {\"dataset\": \"novelspeech\", \"accuracy\": null}, \"description\": \"This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\"}}", "category": "generic"}
{"question_id": 263, "text": " The company is developing an app that reads books to its users. We need to convert text to speech.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"speechbrain\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"tts-hifigan-german\", \"api_call\": \"hifi_gan.decode_batch(mel_output)\", \"api_arguments\": [\"mel_output\"], \"python_environment_requirements\": [\"speechbrain\"], \"example_code\": \"import torchaudio\\nfrom speechbrain.pretrained import Tacotron2\\nfrom speechbrain.pretrained import HIFIGAN\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\nwaveforms = hifi_gan.decode_batch(mel_output)\\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\", \"performance\": {\"dataset\": \"custom German dataset\", \"accuracy\": \"Not specified\"}, \"description\": \"A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.\"}}", "category": "generic"}
{"question_id": 264, "text": " Our customer wants to transcribe recorded speeches given by famous Japanese influencers.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Recognition\", \"api_name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\", \"api_call\": \"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\", \"api_arguments\": [\"audio_paths\"], \"python_environment_requirements\": [\"huggingsound\", \"torch\", \"librosa\", \"datasets\", \"transformers\"], \"example_code\": \"from huggingsound import SpeechRecognitionModel\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\ntranscriptions = model.transcribe(audio_paths)\", \"performance\": {\"dataset\": \"common_voice\", \"accuracy\": {\"WER\": 81.8, \"CER\": 20.16}}, \"description\": \"Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.\"}}", "category": "generic"}
{"question_id": 265, "text": " Extract the Chinese text from the provided audio file to create subtitles for a video.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Recognition\", \"api_name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\", \"api_call\": \"Wav2Vec2Model.from_pretrained('jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn')\", \"api_arguments\": [\"audio_paths\"], \"python_environment_requirements\": [\"huggingsound\", \"torch\", \"librosa\", \"datasets\", \"transformers\"], \"example_code\": \"from huggingsound import SpeechRecognitionModel\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn')\\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\\ntranscriptions = model.transcribe(audio_paths)\", \"performance\": {\"dataset\": \"Common Voice zh-CN\", \"accuracy\": {\"WER\": 82.37, \"CER\": 19.03}}, \"description\": \"Fine-tuned XLSR-53 large model for speech recognition in Chinese. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.\"}}", "category": "generic"}
{"question_id": 266, "text": " I am working on a project that requires transcribing Esperanto speech, and I need a reliable automatic speech recognition model.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Automatic Speech Recognition\", \"api_name\": \"cpierse/wav2vec2-large-xlsr-53-esperanto\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('cpierse/wav2vec2-large-xlsr-53-esperanto')\", \"api_arguments\": [\"path\", \"sentence\", \"speech\", \"sampling_rate\"], \"python_environment_requirements\": [\"torch\", \"torchaudio\", \"datasets\", \"transformers\", \"re\", \"jiwer\"], \"example_code\": \"import torch\\nimport torchaudio\\nfrom datasets import load_dataset\\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\ntest_dataset = load_dataset(common_voice, eo, split=test[:2%]) \\nprocessor = Wav2Vec2Processor.from_pretrained(cpierse/wav2vec2-large-xlsr-53-esperanto) \\nmodel = Wav2Vec2ForCTC.from_pretrained(cpierse/wav2vec2-large-xlsr-53-esperanto) \\nresampler = torchaudio.transforms.Resample(48_000, 16_000)\\ndef speech_file_to_array_fn(batch):\\n speech_array, sampling_rate = torchaudio.load(batch[path])\\n batch[speech] = resampler(speech_array).squeeze().numpy()\\n return batch\\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\\ninputs = processor(test_dataset[speech][:2], sampling_rate=16_000, return_tensors=pt, padding=True)\\nwith torch.no_grad():\\n logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\\npredicted_ids = torch.argmax(logits, dim=-1)\\nprint(Prediction:, processor.batch_decode(predicted_ids))\\nprint(Reference:, test_dataset[sentence][:2])\", \"performance\": {\"dataset\": \"common_voice\", \"accuracy\": \"12.31%\"}, \"description\": \"Fine-tuned facebook/wav2vec2-large-xlsr-53 on esperanto using the Common Voice dataset. When using this model, make sure that your speech input is sampled at 16kHz.\"}}", "category": "generic"}
{"question_id": 267, "text": " Develop a solution to separate the audio of two speakers speaking simultaneously in a conversation.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Asteroid\", \"api_name\": \"Awais/Audio_Source_Separation\", \"api_call\": \"pipeline('audio-source-separation', model='Awais/Audio_Source_Separation')\", \"api_arguments\": \"audio\", \"python_environment_requirements\": \"Asteroid\", \"example_code\": \"\", \"performance\": {\"dataset\": \"Libri2Mix\", \"accuracy\": {\"si_sdr\": 14.764543634468069, \"si_sdr_imp\": 14.764029375607246, \"sdr\": 15.29337970745095, \"sdr_imp\": 15.114146605113111, \"sir\": 24.092904661115366, \"sir_imp\": 23.913669683141528, \"sar\": 16.06055906916849, \"sar_imp\": -51.980784441287454, \"stoi\": 0.9311142440593033, \"stoi_imp\": 0.21817376142710482}}, \"description\": \"This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.\"}}", "category": "generic"}
{"question_id": 268, "text": " We need to enhance the audio quality in our newly developed language translation software.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Enhancement\", \"api_name\": \"speechbrain/sepformer-wham16k-enhancement\", \"api_call\": \"separator.from_hparams(source=speechbrain/sepformer-wham16k-enhancement, savedir='pretrained_models/sepformer-wham16k-enhancement').separate_file(path='speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav')\", \"api_arguments\": {\"path\": \"path to the input audio file\"}, \"python_environment_requirements\": \"pip install speechbrain\", \"example_code\": \"from speechbrain.pretrained import SepformerSeparation as separator\\nimport torchaudio\\nmodel = separator.from_hparams(source=speechbrain/sepformer-wham16k-enhancement, savedir='pretrained_models/sepformer-wham16k-enhancement')\\nest_sources = model.separate_file(path='speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav')\\ntorchaudio.save(enhanced_wham16k.wav, est_sources[:, :, 0].detach().cpu(), 16000)\", \"performance\": {\"dataset\": \"WHAM!\", \"accuracy\": {\"Test-Set SI-SNR\": \"14.3 dB\", \"Test-Set PESQ\": \"2.20\"}}, \"description\": \"This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 16k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.\"}}", "category": "generic"}
{"question_id": 269, "text": " We are developing a call center software and our system needs to process audio files to increase the quality for each call. Can you suggest a model?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"ESPnet\", \"functionality\": \"Audio-to-Audio\", \"api_name\": \"lichenda/wsj0_2mix_skim_noncausal\", \"api_call\": \"No changes needed.\", \"api_arguments\": [], \"python_environment_requirements\": [\"python 3.7.11\", \"espnet 0.10.7a1\", \"pytorch 1.8.1\"], \"example_code\": \"./run.sh --skip_data_prep false --skip_train true --download_model lichenda/wsj0_2mix_skim_noncausal\", \"performance\": {\"dataset\": \"enhanced_cv_min_8k\", \"accuracy\": {\"STOI\": 0.96, \"SAR\": 19.17, \"SDR\": 18.7, \"SIR\": 29.56}}, \"description\": \"This model was trained by LiChenda using wsj0_2mix recipe in espnet.\"}}", "category": "generic"}
{"question_id": 270, "text": " We need to add an ambient noise management system to our efficient call center. Users consistently complain about excessive noise next to the operator on our sales team.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"wav2vec2-random-tiny-classifier\", \"api_call\": \"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('anton-l/wav2vec2-random-tiny-classifier'))\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"An audio classification model based on wav2vec2.\"}}", "category": "generic"}
{"question_id": 271, "text": " One of my clients is from Spain and talks to me in Spanish. I need to classify the sentiment of the audio file that contains his spoken message.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Classification\", \"api_name\": \"hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\", \"api_call\": \"Output: Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\", \"api_arguments\": {\"model_name\": \"hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\"}, \"python_environment_requirements\": {\"transformers\": \"4.17.0\", \"pytorch\": \"1.10.0+cu111\", \"datasets\": \"2.0.0\", \"tokenizers\": \"0.11.6\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"MESD\", \"accuracy\": 0.9308}, \"description\": \"This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.\"}}", "category": "generic"}
{"question_id": 272, "text": " We are working on a project to predict potential customer churn based on customer data. Implement a model that can handle various feature types.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Keras\", \"functionality\": \"Structured data learning with TabTransformer\", \"api_name\": \"keras-io/tab_transformer\", \"api_call\": \"TabTransformer.from_config()\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"Hugging Face\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"United States Census Income Dataset\", \"accuracy\": \"N/A\"}, \"description\": \"This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model's inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model.\"}}", "category": "generic"}
{"question_id": 273, "text": " Please help me prepare a recruitment test where candidates predict if a person makes over 50k a year based on several factors such as age, education, and work experience.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Keras\", \"functionality\": \"Binary Classification\", \"api_name\": \"TF_Decision_Trees\", \"api_call\": \"TF_Decision_Trees(input_features, target)\", \"api_arguments\": [\"input_features\", \"target\"], \"python_environment_requirements\": [\"tensorflow >= 7.0\"], \"example_code\": \"https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\", \"performance\": {\"dataset\": \"Census-Income Data Set\", \"accuracy\": 96.57}, \"description\": \"Use TensorFlow's Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}}", "category": "generic"}
{"question_id": 274, "text": " A real estate developer needs a program to estimate the housing prices based on the data provided in the 'data.csv' file, using the pre-trained model.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Single Column Regression\", \"api_name\": \"1771761512\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\", \"json\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"jwan2021/autotrain-data-us-housing-prices\", \"accuracy\": {\"Loss\": 122809.223, \"R2\": 0.884, \"MSE\": 15082105200.447, \"MAE\": 95586.887, \"RMSLE\": 0.13}}, \"description\": \"A model trained using AutoTrain for predicting US housing prices with single column regression. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a CO2 Emissions of 50.5369 grams.\"}}", "category": "generic"}
{"question_id": 275, "text": " We are working on reducing carbon emissions in our factory, and we need to have a prediction on how much we would save with some changes.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"2311773112\", \"api_call\": \"model\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\", \"json\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"al02783013/autotrain-data-faseiii_diciembre\", \"accuracy\": {\"Loss\": 5487.957, \"R2\": 0.96, \"MSE\": 30117668.0, \"MAE\": 2082.499, \"RMSLE\": 1.918}}, \"description\": \"A tabular regression model trained using AutoTrain to predict carbon emissions based on input features.\"}}", "category": "generic"}
{"question_id": 276, "text": " We are designing an application to predict the carbon emissions of various activities carried out in a city. We need to handle this data and provide the prediction.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"1860863627\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": {\"data\": \"data.csv\"}, \"python_environment_requirements\": {\"import json\": \"\", \"import joblib\": \"\", \"import pandas as pd\": \"\"}, \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"pcoloc/autotrain-data-dragino-7-7-max_495m\", \"accuracy\": {\"Loss\": 72.73, \"R2\": 0.386, \"MSE\": 5289.6, \"MAE\": 60.23, \"RMSLE\": 0.436}}, \"description\": \"A tabular regression model for predicting carbon emissions using the Joblib framework. Trained on the pcoloc/autotrain-data-dragino-7-7-max_495m dataset.\"}}", "category": "generic"}
{"question_id": 277, "text": " The company is building a real estate investment platform where the user will provide information about the real estate, and we need to predict the price of the house.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Scikit-learn\", \"functionality\": \"Tabular Regression\", \"api_name\": \"rajistics/california_housing\", \"api_call\": \"RandomForestRegressor()\", \"api_arguments\": {\"bootstrap\": \"True\", \"ccp_alpha\": \"0.0\", \"criterion\": \"squared_error\", \"max_depth\": \"\", \"max_features\": \"1.0\", \"max_leaf_nodes\": \"\", \"max_samples\": \"\", \"min_impurity_decrease\": \"0.0\", \"min_samples_leaf\": \"1\", \"min_samples_split\": \"2\", \"min_weight_fraction_leaf\": \"0.0\", \"n_estimators\": \"100\", \"n_jobs\": \"\", \"oob_score\": \"False\", \"random_state\": \"\", \"verbose\": \"0\", \"warm_start\": \"False\"}, \"python_environment_requirements\": \"Scikit-learn\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices.\"}}", "category": "generic"}
{"question_id": 278, "text": " I want an artificial intelligence to help me land a simulated lunar lander.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"LunarLander-v2\", \"api_name\": \"araffin/ppo-LunarLander-v2\", \"api_call\": \"Output: model = PPO.load_from_hub('araffin/ppo-LunarLander-v2', 'ppo-LunarLander-v2.zip')\", \"api_arguments\": {\"checkpoint\": \"araffin/ppo-LunarLander-v2.zip\"}, \"python_environment_requirements\": [\"huggingface_sb3\", \"stable_baselines3\"], \"example_code\": {\"load_model\": \"from huggingface_sb3 import load_from_hub\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\nmodel = PPO.load(checkpoint)\", \"evaluate\": \"from stable_baselines3.common.env_util import make_vec_env\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})\"}, \"performance\": {\"dataset\": \"LunarLander-v2\", \"accuracy\": \"283.49 +/- 13.74\"}, \"description\": \"This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.\"}}", "category": "generic"}
{"question_id": 279, "text": " We are trying to build an AI-powered sports management platform. We need to train our agents to play soccer and analyze soccer games.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"Train and play SoccerTwos\", \"api_name\": \"ahmad-alismail/poca-SoccerTwos\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"ml-agents\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"Not specified\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 280, "text": " We're developing a game and we want to train an AI player to perform well in the Gym Hopper environment. Tell me a trained model that can solve this problem.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"edbeeching/decision-transformer-gym-hopper-expert\", \"api_call\": \"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-expert')\", \"api_arguments\": {\"mean\": [1.3490015, -0.11208222, -0.5506444, -0.13188992, -0.00378754, 2.6071432, 0.02322114, -0.01626922, -0.06840388, -0.05183131, 0.04272673], \"std\": [0.15980862, 0.0446214, 0.14307782, 0.17629202, 0.5912333, 0.5899924, 1.5405099, 0.8152689, 2.0173461, 2.4107876, 5.8440027]}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"See our Blog Post, Colab notebook or Example Script for usage.\", \"performance\": {\"dataset\": \"Gym Hopper environment\", \"accuracy\": \"Not provided\"}, \"description\": \"Decision Transformer model trained on expert trajectories sampled from the Gym Hopper environment\"}}", "category": "generic"}
{"question_id": 281, "text": " I want to build a game, where players play football (soccer) as a two-player team. I want to train an agent for this game.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"Train and resume training of poca agent playing SoccerTwos\", \"api_name\": \"LukeSajkowski/SoccerTwos\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\", \"ML-Agents-SoccerTwos\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library\"}}", "category": "generic"}
{"question_id": 282, "text": " In my robotics class, we are learning to play soccer twos. Would you introduce me to a pre-trained model in that game?\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"QYHcrossover/poca-test\", \"api_call\": \"No changes needed. The `api_call` field does not include any model instantiation.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\"], \"example_code\": \"Go to https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos\\nStep 1: Write your model_id: QYHcrossover/poca-test\\nStep 2: Select your .nn /.onnx file\\nClick on Watch the agent play\", \"performance\": {\"dataset\": \"ML-Agents-SoccerTwos\", \"accuracy\": \"Not provided\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 283, "text": " We need to process medical research texts and extract important features to further analyze them.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"dmis-lab/biobert-v1.1\", \"api_call\": \"pipeline('feature-extraction', model='dmis-lab/biobert-v1.1')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.\"}}", "category": "generic"}
{"question_id": 284, "text": " I am writing a children's story where the girl learns how to fly, and I need an illustration for it.\\n###Input: <noinput>\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"prompthero/openjourney-v4\", \"api_call\": \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", \"api_arguments\": {\"text\": \"string\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"generate_image('your text here')\", \"performance\": {\"dataset\": \"Midjourney v4 images\", \"accuracy\": \"Not provided\"}, \"description\": \"Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\"}}", "category": "generic"}
{"question_id": 285, "text": " Can you create a creative image of a snowy forest using the 'wavymulder/Analog-Diffusion' API?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"wavymulder/Analog-Diffusion\", \"api_call\": \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", \"api_arguments\": [\"prompt\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"text_to_image('analog style landscape')\", \"performance\": {\"dataset\": \"analog photographs\", \"accuracy\": \"Not specified\"}, \"description\": \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}}", "category": "generic"}
{"question_id": 286, "text": " We are creating an app for translating Japanese manga to English. We need to extract Japanese text from manga images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"kha-white/manga-ocr-base\", \"api_call\": \"pipeline('ocr', model='kha-white/manga-ocr-base').model\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"manga109s\", \"accuracy\": \"\"}, \"description\": \"Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\"}}", "category": "generic"}
{"question_id": 287, "text": " We need to generate captions for images using both unconditional and conditional captioning methods.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Transformers\", \"functionality\": \"Image Captioning\", \"api_name\": \"blip-image-captioning-large\", \"api_call\": \"BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\", \"api_arguments\": {\"raw_image\": \"Image\", \"text\": \"Optional Text\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForConditionalGeneration\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": {\"import_requests\": \"import requests\", \"import_PIL\": \"from PIL import Image\", \"import_transformers\": \"from transformers import BlipProcessor, BlipForConditionalGeneration\", \"load_processor\": \"processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)\", \"load_model\": \"model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\", \"load_image\": \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", \"conditional_captioning\": \"text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"unconditional_captioning\": \"inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\"}, \"performance\": {\"dataset\": \"COCO\", \"accuracy\": {\"image-text retrieval\": \"+2.7% recall@1\", \"image captioning\": \"+2.8% CIDEr\", \"VQA\": \"+1.6% VQA score\"}}, \"description\": \"BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\"}}", "category": "generic"}
{"question_id": 288, "text": " Our gaming company has a story with great visuals. For marketing, we want some exciting captions for the images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"blip2-opt-6.7b\", \"api_call\": \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", \"api_arguments\": \"image, optional text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"Refer to the documentation\", \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not specified\"}, \"description\": \"BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 289, "text": " A visual designer is building a professional website and wants to know if the website layout has important guidelines missing. Please check using AI technology.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/pix2struct-chartqa-base\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\", \"api_arguments\": [\"t5x_checkpoint_path\", \"pytorch_dump_path\", \"use-large\"], \"python_environment_requirements\": \"transformers\", \"example_code\": \"python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE\", \"performance\": {\"dataset\": \"ChartQA\", \"accuracy\": \"Not provided\"}, \"description\": \"Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.\"}}", "category": "generic"}
{"question_id": 290, "text": " Our team is developing software to extract text from images like a signboard or street signs for an automotive client.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"mgp-str\", \"api_call\": \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", \"api_arguments\": {\"model_name\": \"alibaba-damo/mgp-str-base\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\"]}, \"example_code\": \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", \"performance\": {\"dataset\": \"MJSynth and SynthText\", \"accuracy\": null}, \"description\": \"MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.\"}}", "category": "generic"}
{"question_id": 291, "text": " Create a video of a dog playing on a beach using text-to-video generation.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video Synthesis\", \"api_name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"api_call\": \"DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\", \"api_arguments\": [\"prompt\", \"num_inference_steps\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)\", \"performance\": {\"dataset\": [\"LAION5B\", \"ImageNet\", \"Webvid\"], \"accuracy\": \"Not provided\"}, \"description\": \"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\"}}", "category": "generic"}
{"question_id": 292, "text": " As a pet owner, I recently took a photo of my pets and want to know how many dogs are in the picture I just captured.\\n###Input: {\\\"raw_image\\\": \\\"https://example.com/my_pets_image.jpg\\\", \\\"question\\\": \\\"How many dogs are in the picture?\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"blip-vqa-base\", \"api_call\": \"Output: BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-base').generate(**inputs)\", \"api_arguments\": {\"raw_image\": \"Image\", \"question\": \"String\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForQuestionAnswering\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-base)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"VQA\", \"accuracy\": \"+1.6% in VQA score\"}, \"description\": \"BLIP is a Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is trained on visual question answering with a base architecture (using ViT base backbone).\"}}", "category": "generic"}
{"question_id": 293, "text": " For the new smart home assistant, create a way to receive questions about objects from an image and provide answers based on the image contents.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"sheldonxxxx/OFA_model_weights\", \"api_call\": \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.\"}}", "category": "generic"}
{"question_id": 294, "text": " Suppose I am using an application to process scanned documents to answer questions. How would you recommend a model to help me?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"\"}, \"description\": \"A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}", "category": "generic"}
{"question_id": 295, "text": " A student is reading a textbook and needs help answering questions based on the material. Instruct the model to help by answering the questions.\\n###Input: {\\n  \\\"question\\\": \\\"What is the Pythagorean theorem?\\\",\\n  \\\"context\\\": \\\"The Pythagorean theorem is a fundamental relation in Euclidean geometry between the three sides of a right triangle. It states that the square of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides. The theorem can be written as an equation relating the lengths of the sides a, b, and c, often called the Pythagorean equation: a^2 + b^2 = c^2.\\\"\\n}\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_arguments\": \"{'question': 'your_question', 'context': 'your_context'}\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}", "category": "generic"}
{"question_id": 296, "text": " We need to estimate depth in our self-driving car system. The model should return the depth_map in meters for the given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4359, \"Rmse\": 0.4276}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 297, "text": " You are an AI-powered personal assistant, and your owner asks you: \\\"What are the breeds of these two cats in the pictures?\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/convnext-base-224\", \"api_call\": \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", \"api_arguments\": [\"image\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"datasets\"], \"example_code\": \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": null}, \"description\": \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\"}}", "category": "generic"}
{"question_id": 298, "text": " Develop a model that can predict a person's age based on their facial image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Age Classification\", \"api_name\": \"nateraw/vit-age-classifier\", \"api_call\": \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nateraw/vit-age-classifier\"}, \"python_environment_requirements\": [\"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", \"performance\": {\"dataset\": \"fairface\", \"accuracy\": null}, \"description\": \"A vision transformer finetuned to classify the age of a given person's face.\"}}", "category": "generic"}
{"question_id": 299, "text": " I am building an AI-based photo management software that automatically categorizes images into predefined categories. Can you help me classify them based on their content?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"vit_tiny_patch16_224.augreg_in21k_ft_in1k\", \"api_call\": \"create_model('vit_tiny_patch16_224.augreg_in21k_ft_in1k', pretrained=True)\", \"api_arguments\": \"pretrained\", \"python_environment_requirements\": \"timm\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.\"}}", "category": "generic"}
{"question_id": 300, "text": " Take a picture of a dessert and create a system that can identify the type of dessert it is.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"timm/mobilenetv3_large_100.ra_in1k\", \"api_call\": \"Output: timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\"}, \"python_environment_requirements\": {\"timm\": \"latest\"}, \"example_code\": \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.\"}}", "category": "generic"}
{"question_id": 301, "text": " We are a book scanning service that wants to extract tables from scanned pages. Can you suggest a model that can detect table areas in an image?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Extraction\", \"api_name\": \"keremberke/yolov8m-table-extraction\", \"api_call\": \"YOLO('keremberke/yolov8m-table-extraction')\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"table-extraction\", \"accuracy\": 0.952}, \"description\": \"A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\"}}", "category": "generic"}
{"question_id": 302, "text": " I would like to create a system that can detect and identify objects in images. Please provide assistance in doing so.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"facebook/detr-resnet-101-dc5\", \"api_call\": \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", \"api_arguments\": {\"image\": \"Image.open(requests.get(url, stream=True).raw)\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"AP 44.9\"}, \"description\": \"DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\"}}", "category": "generic"}
{"question_id": 303, "text": " Our company needs a robot to help extract tables from artworks. Please help install this api to let the robot finish the work of extracting tables from artworks.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Extraction\", \"api_name\": \"keremberke/yolov8s-table-extraction\", \"api_call\": \"YOLO('keremberke/yolov8s-table-extraction').predict(image)\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"table-extraction\", \"accuracy\": 0.984}, \"description\": \"A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\"}}", "category": "generic"}
{"question_id": 304, "text": " We have a website streaming eSports, we are building a feature to find the players in the frame and tag the players with the team that they belong to.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8s-csgo-player-detection\", \"api_call\": \"YOLO('keremberke/yolov8s-csgo-player-detection').predict(image)\", \"api_arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"csgo-object-detection\", \"accuracy\": 0.886}, \"description\": \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\"}}", "category": "generic"}
{"question_id": 305, "text": " We are developing an automated drone for monitoring forest areas. We need to detect wild animals in images taken by the drone.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"fcakyon/yolov5s-v7.0\", \"api_call\": \"yolov5.load('fcakyon/yolov5s-v7.0')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic\": false, \"multi_label\": false, \"max_det\": 1000, \"img\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\", \"size\": 640, \"augment\": true}, \"python_environment_requirements\": \"pip install -U yolov5\", \"example_code\": \"import yolov5\\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\\nmodel.conf = 0.25\\nmodel.iou = 0.45\\nmodel.agnostic = False\\nmodel.multi_label = False\\nmodel.max_det = 1000\\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model(img)\\nresults = model(img, size=640)\\nresults = model(img, augment=True)\\npredictions = results.pred[0]\\nboxes = predictions[:, :4]\\nscores = predictions[:, 4]\\ncategories = predictions[:, 5]\\nresults.show()\\nresults.save(save_dir='results/')\", \"performance\": {\"dataset\": \"detection-datasets/coco\", \"accuracy\": null}, \"description\": \"Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\"}}", "category": "generic"}
{"question_id": 306, "text": " Our company is developing an AI-powered photo editor, and we need to integrate a feature that can segment objects in the images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-base-coco\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\"}}", "category": "generic"}
{"question_id": 307, "text": " I need a system that can analyze images of a street and identify different objects or structures in the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-large-cityscapes-semantic\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/mask2former-swin-large-cityscapes-semantic\"}, \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"Cityscapes\", \"accuracy\": \"Not specified\"}, \"description\": \"Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\"}}", "category": "generic"}
{"question_id": 308, "text": " We are a company providing machine control for electronic PCB manufacturers. We want to inspect and detect defects in PCB boards automatically.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8s-pcb-defect-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\", \"api_arguments\": {\"image\": \"URL or local path to image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pcb-defect-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.515, \"mAP@0.5(mask)\": 0.491}}, \"description\": \"YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\"}}", "category": "generic"}
{"question_id": 309, "text": " I need to estimate the pose of a person in a picture with a chef cooking in the kitchen and save it as an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Human Pose Estimation\", \"api_name\": \"lllyasviel/sd-controlnet-openpose\", \"api_call\": \"pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\", \"api_arguments\": {\"text\": \"chef in the kitchen\", \"image\": \"image\", \"num_inference_steps\": 20}, \"python_environment_requirements\": {\"diffusers\": \"pip install diffusers\", \"transformers\": \"pip install transformers\", \"accelerate\": \"pip install accelerate\", \"controlnet_aux\": \"pip install controlnet_aux\"}, \"example_code\": \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", \"performance\": {\"dataset\": \"200k pose-image, caption pairs\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 310, "text": " Please assist me with extracting information about objects in a crowded street scene.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Segmentation\", \"api_name\": \"lllyasviel/sd-controlnet-seg\", \"api_call\": \"Output: ControlNetModel.from_pretrained(\\\\lllyasviel/sd-controlnet-seg\\\\, torch_dtype=torch.float16)\", \"api_arguments\": [\"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", \"performance\": {\"dataset\": \"ADE20K\", \"accuracy\": \"Trained on 164K segmentation-image, caption pairs\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 311, "text": " As a video game developer, our current project involves enhancing the resolution of low-quality images in our game. Help us with this task by using image super-resolution techniques to upscale the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Super-Resolution\", \"api_name\": \"caidas/swin2SR-classical-sr-x2-64\", \"api_call\": \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", \"api_arguments\": \"image, model, feature_extractor\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"Refer to the documentation.\", \"performance\": {\"dataset\": \"arxiv: 2209.11345\", \"accuracy\": \"Not provided\"}, \"description\": \"Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 312, "text": " Design a program to enhance the resolution of an image for our home theater system.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"swin2SR-classical-sr-x4-64\", \"api_call\": \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", \"api_arguments\": [\"input_image\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\"}}", "category": "generic"}
{"question_id": 313, "text": " Our customer has an image of a drawing, and they want the AI to imagine and produce a colorful version of the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Diffusion Models\", \"api_name\": \"lllyasviel/control_v11p_sd15s2_lineart_anime\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15s2_lineart_anime\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"pip install diffusers transformers accelerate\", \"pip install controlnet_aux==0.3.0\"], \"example_code\": [\"import torch\", \"import os\", \"from huggingface_hub import HfApi\", \"from pathlib import Path\", \"from diffusers.utils import load_image\", \"from PIL import Image\", \"import numpy as np\", \"from controlnet_aux import LineartAnimeDetector\", \"from transformers import CLIPTextModel\", \"from diffusers import (\", \" ControlNetModel,\", \" StableDiffusionControlNetPipeline,\", \" UniPCMultistepScheduler,\", \")\", \"checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime\", \"image = load_image(\", \" https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png\", \")\", \"image = image.resize((512, 512))\", \"prompt = A warrior girl in the jungle\", \"processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)\", \"control_image = processor(image)\", \"control_image.save(./images/control.png)\", \"text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)\", \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"pipe = StableDiffusionControlNetPipeline.from_pretrained(\", \" runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16\", \")\", \"pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\", \"pipe.enable_model_cpu_offload()\", \"generator = torch.manual_seed(0)\", \"image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\", \"image.save('images/image_out.png')\"], \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\"}}", "category": "generic"}
{"question_id": 314, "text": " Build a digital artwork generator that produces pictures of butterflies that we can use for our chemistry lab as decoration.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ceyda/butterfly_cropped_uniq1K_512\", \"api_call\": \"LightweightGAN.from_pretrained('ceyda/butterfly_cropped_uniq1K_512')\", \"api_arguments\": [\"pretrained_model_name_or_path\"], \"python_environment_requirements\": [\"torch\", \"huggan.pytorch.lightweight_gan.lightweight_gan\"], \"example_code\": \"import torch\\nfrom huggan.pytorch.lightweight_gan.lightweight_gan import LightweightGAN\\ngan = LightweightGAN.from_pretrained(ceyda/butterfly_cropped_uniq1K_512)\\ngan.eval()\\nbatch_size = 1\\nwith torch.no_grad():\\n ims = gan.G(torch.randn(batch_size, gan.latent_dim)).clamp_(0., 1.)*255\\n ims = ims.permute(0,2,3,1).detach().cpu().numpy().astype(np.uint8)\\n # ims is [BxWxHxC] call Image.fromarray(ims[0])\", \"performance\": {\"dataset\": \"huggan/smithsonian_butterflies_subset\", \"accuracy\": \"FID score on 100 images\"}, \"description\": \"Butterfly GAN model based on the paper 'Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis'. The model is intended for fun and learning purposes. It was trained on 1000 images from the huggan/smithsonian_butterflies_subset dataset, with a focus on low data training as mentioned in the paper. The model generates high-quality butterfly images.\"}}", "category": "generic"}
{"question_id": 315, "text": " As a movie's visual effects designer, we ought to create a futuristic scene for our upcoming film project.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ddpm-cifar10-32\", \"api_call\": \"Output: DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", \"api_arguments\": \"None\", \"python_environment_requirements\": \"!pip install diffusers\", \"example_code\": \"from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception score\": 9.46, \"FID score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.\"}}", "category": "generic"}
{"question_id": 316, "text": " The company wants to build an AI art installation that generates realistic human faces. We need to use a generative model to create new images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Inference\", \"api_name\": \"google/ncsnpp-ffhq-1024\", \"api_call\": \"DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-1024')\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-ffhq-1024\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR-10\", \"accuracy\": {\"Inception_score\": 9.89, \"FID\": 2.2, \"likelihood\": 2.99}}, \"description\": \"Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.\"}}", "category": "generic"}
{"question_id": 317, "text": " Design an application that can recognize activities in videos and provide useful descriptions.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/xclip-base-patch32\", \"api_call\": \"XClipModel.from_pretrained('microsoft/xclip-base-patch32')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"Kinetics 400\", \"accuracy\": {\"top-1\": 80.4, \"top-5\": 95.0}}, \"description\": \"X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\"}}", "category": "generic"}
{"question_id": 318, "text": " We're planning a butterfly-themed event and we need a collection of diverse butterfly images. Generate some images for us.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"utyug1/sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('utyug1/sd-class-butterflies-32')\", \"api_arguments\": {\"pretrained_model\": \"utyug1/sd-class-butterflies-32\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('utyug1/sd-class-butterflies-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies.\"}}", "category": "generic"}
{"question_id": 319, "text": " Our company is developing an advertising platform, and we need a tool to analyze video ads' content to ensure they comply with our guidelines.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"videomae-large\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\", \"api_arguments\": \"pixel_values, bool_masked_pos\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-large)\\npixel_values = processor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not provided\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.\"}}", "category": "generic"}
{"question_id": 320, "text": " Automatically categorize videos in a folder to help organize them.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base-short\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"MCG-NJU/videomae-base-short\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\"]}, \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short)\\npixel_values = processor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not provided\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.\"}}", "category": "generic"}
{"question_id": 321, "text": " As a part of our new physical fitness startup, we need to classify workout videos into different exercise types.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"videomae-base-ssv2\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\", \"api_arguments\": \"video\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": \"\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\"}}", "category": "generic"}
{"question_id": 322, "text": " Our security team wants to classify actions in monitored videos. We are looking for a model pretrained on a large dataset.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2')\", \"api_arguments\": \"video_path\", \"python_environment_requirements\": \"transformers==4.27.4, torch==2.0.0+cu117, datasets==2.11.0, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 0.7212}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-large-finetuned-kinetics on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 323, "text": " I want to find out what species of animal is in a given photograph.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\", \"api_arguments\": [\"image\", \"possible_class_names\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"80.1\"}, \"description\": \"A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 324, "text": " Can you identify objects in the photos on my phone? I would like to categorize the photos into pets, vehicles, and landmarks.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\", \"api_arguments\": \"image_path, class_names\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"results = model(image_path, class_names='cat, dog, bird')\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"76.9\"}, \"description\": \"A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).\"}}", "category": "generic"}
{"question_id": 325, "text": " As a social media monitoring company, we want to analyze images posted by users and classify their content. Identify what is in the given image.\\n###Input: \\\"path_to_image\\\": \\\"/path/to/specific/image.jpg\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg')\", \"api_arguments\": {\"image\": \"path_to_image\", \"candidate_labels\": [\"list\", \"of\", \"candidate\", \"labels\"]}, \"python_environment_requirements\": {\"transformers\": \">=4.13.0\"}, \"example_code\": \"classifier(image='path_to_image', candidate_labels=['cat', 'dog', 'car'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8 - 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Base model (convnext_base) as the image tower, and the same text tower as the RN50x4 (depth 12, embed dim 640) model from OpenAI CLIP. The base models are trained at 256x256 image resolution and roughly match the RN50x4 models on FLOPs and activation counts.\"}}", "category": "generic"}
{"question_id": 326, "text": " A friend of ours is starting a business that categorizes images of toys and wants to know what type of toy is in a specific image.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"OFA-Sys/chinese-clip-vit-large-patch14-336px\", \"api_call\": \"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\", \"api_arguments\": {\"images\": \"image\", \"text\": \"texts\", \"return_tensors\": \"pt\", \"padding\": \"True\"}, \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [\\u6770\\u5c3c\\u9f9f, \\u5999\\u86d9\\u79cd\\u5b50, \\u5c0f\\u706b\\u9f99, \\u76ae\\u5361\\u4e18]\\ninputs = processor(images=image, return_tensors=pt)\\nimage_features = model.get_image_features(**inputs)\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\ntext_features = model.get_text_features(**inputs)\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": {\"CIFAR10\": 96.0, \"CIFAR100\": 79.75, \"DTD\": 51.2, \"EuroSAT\": 52.0, \"FER\": 55.1, \"FGVC\": 26.2, \"KITTI\": 49.9, \"MNIST\": 79.4, \"PC\": 63.5, \"VOC\": 84.9}, \"accuracy\": \"various\"}, \"description\": \"Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder.\"}}", "category": "generic"}
{"question_id": 327, "text": " I would like to classify the sentiments of Twitter users about a stock.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Inferencing for stock-related comments\", \"api_name\": \"zhayunduo/roberta-base-stocktwits-finetuned\", \"api_call\": \"Output: RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\", \"api_arguments\": {\"model\": \"RobertaForSequenceClassification\", \"tokenizer\": \"RobertaTokenizer\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\nfrom transformers import pipeline\\nimport pandas as pd\\nimport emoji\\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\nsentences = pd.Series(['just buy','just sell it','entity rocket to the sky!','go down','even though it is going up, I still think it will not keep this trend in the near future'])\\nsentences = list(sentences)\\nresults = nlp(sentences)\\nprint(results)\", \"performance\": {\"dataset\": \"stocktwits\", \"accuracy\": 0.9343}, \"description\": \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'.\"}}", "category": "generic"}
{"question_id": 328, "text": " We are an online review aggregator and need to analyze the sentiment of user-generated reviews for better product recommendations.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"finiteautomata/bertweet-base-sentiment-analysis\", \"api_call\": \"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"Transformers\", \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\nresult = nlp('I love this movie!')\", \"performance\": {\"dataset\": \"SemEval 2017\", \"accuracy\": null}, \"description\": \"Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is BERTweet, a RoBERTa model trained on English tweets. Uses POS, NEG, NEU labels.\"}}", "category": "generic"}
{"question_id": 329, "text": " Please help me understand names and places in a sentence from an email that I received.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"dslim/bert-base-NER\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(dslim/bert-base-NER)\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\nfrom transformers import pipeline\\ntokenizer = AutoTokenizer.from_pretrained(dslim/bert-base-NER)\\nmodel = AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\nexample = My name is Wolfgang and I live in Berlin\\nner_results = nlp(example)\\nprint(ner_results)\", \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": {\"f1\": 91.3, \"precision\": 90.7, \"recall\": 91.9}}, \"description\": \"bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\"}}", "category": "generic"}
{"question_id": 330, "text": " A French company is searching for a model to identify organizations and individuals mentioned in news articles. We need to present them with the appropriate tool.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"Jean-Baptiste/camembert-ner\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/camembert-ner')\", \"api_arguments\": {\"model\": \"model\", \"tokenizer\": \"tokenizer\", \"aggregation_strategy\": \"simple\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForTokenClassification, pipeline\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\\nfrom transformers import pipeline\\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\\nnlp(Apple est cr le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitu sous forme de socile 3 janvier 1977 l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refler la diversification de ses produits, le mot computer est retirle 9 janvier 2015.)\", \"performance\": {\"dataset\": \"wikiner-fr\", \"accuracy\": {\"overall_f1\": 0.8914, \"PER_f1\": 0.9483, \"ORG_f1\": 0.8181, \"LOC_f1\": 0.8955, \"MISC_f1\": 0.8146}}, \"description\": \"camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\"}}", "category": "generic"}
{"question_id": 331, "text": " In search of a way to identify proper nouns and words from a text in Chinese for my language analyzing company.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Token Classification\", \"api_name\": \"ckiplab/bert-base-chinese-ws\", \"api_call\": \"AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\", \"api_arguments\": {\"pretrained_model\": \"ckiplab/bert-base-chinese-ws\"}, \"python_environment_requirements\": {\"transformers\": \"BertTokenizerFast, AutoModel\"}, \"example_code\": \"from transformers import (\\n BertTokenizerFast,\\n AutoModel,\\n)\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\"}}", "category": "generic"}
{"question_id": 332, "text": " As an urban planner, we need to answer questions about city characteristics based on a table containing urban data.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/tapex-large\", \"api_call\": \"Output: AutoModelForSeq2SeqLM.from_pretrained('microsoft/tapex-large')\", \"api_arguments\": [\"model_name = 'microsoft/tapex-large'\", \"tokenizer = AutoTokenizer.from_pretrained(model_name)\", \"model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.\"}}", "category": "generic"}
{"question_id": 333, "text": " Identify a way to answer the quarterly sales from the available table.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lysandre/tiny-tapas-random-wtq\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('lysandre/tiny-tapas-random-wtq')\", \"api_arguments\": \"model = TapasForQuestionAnswering.from_pretrained('lysandre/tiny-tapas-random-wtq')\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": \"WTQ\", \"accuracy\": \"Not provided\"}, \"description\": \"A tiny TAPAS model trained on the WikiTableQuestions dataset for table question answering tasks.\"}}", "category": "generic"}
{"question_id": 334, "text": " We have a list of employees' names, departments, and salaries in a table format. We need to find the average salary for each department.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-small-finetuned-sqa\", \"api_call\": \"pipeline('table-question-answering', model='google/tapas-small-finetuned-sqa')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.6155}, \"description\": \"TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\"}}", "category": "generic"}
{"question_id": 335, "text": " The principal of a school is analyzing the performance of students in each department. He is interested in finding the department with the highest average marks.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table-based QA\", \"api_name\": \"neulab/omnitab-large\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large')\", \"api_arguments\": {\"table\": \"pd.DataFrame\", \"query\": \"str\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForSeq2SeqLM\", \"pandas\": \"pd\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nimport pandas as pd\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": null}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data.\"}}", "category": "generic"}
{"question_id": 336, "text": " A company executive has requested a report on Olympic host cities for various years. Generate an answer from the Olympiad records.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"neulab/omnitab-large-1024shot-finetuned-wtq-1024shot\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot-finetuned-wtq-1024shot')\", \"api_arguments\": {\"table\": \"pd.DataFrame\", \"query\": \"string\"}, \"python_environment_requirements\": [\"pandas\", \"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nimport pandas as pd\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot-finetuned-wtq-1024shot)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot-finetuned-wtq-1024shot)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab. neulab/omnitab-large-1024shot-finetuned-wtq-1024shot (based on BART architecture) is initialized with neulab/omnitab-large-1024shot and fine-tuned on WikiTableQuestions in the 1024-shot setting.\"}}", "category": "generic"}
{"question_id": 337, "text": " We have data from our last meeting and want to answer a question about a specific value from that data.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"navteca/tapas-large-finetuned-wtq\", \"api_call\": \"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\", \"api_arguments\": {\"table\": \"table_data\", \"query\": \"query\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\"}, \"example_code\": \"from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\n# Load model & tokenizer\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\\n# Get predictions\\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\\nprint(result)\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 338, "text": " We are working on a new product that is a language tool for researchers who need to quickly find answers related to any topic.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/bert-base-cased-squad2\", \"api_call\": \"deepset/bert-base-cased-squad2\", \"api_arguments\": \"context, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can this model be loaded?'})\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"exact_match\": 71.152, \"f1\": 74.671}}, \"description\": \"This is a BERT base cased model trained on SQuAD v2\"}}", "category": "generic"}
{"question_id": 339, "text": " I would like to know when the game was played. Given a context about a game, extract the date it was played.\\n###Input: {\\\"context\\\": \\\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\", \\\"question\\\": \\\"What day was the game played on?\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"csarron/bert-base-uncased-squad-v1\", \"api_call\": \"pipeline('question-answering', model='csarron/bert-base-uncased-squad-v1', tokenizer='csarron/bert-base-uncased-squad-v1')\", \"api_arguments\": {\"model\": \"csarron/bert-base-uncased-squad-v1\", \"tokenizer\": \"csarron/bert-base-uncased-squad-v1\"}, \"python_environment_requirements\": \"Python 3.7.5\", \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline(\\n question-answering,\\n model=csarron/bert-base-uncased-squad-v1,\\n tokenizer=csarron/bert-base-uncased-squad-v1\\n)\\npredictions = qa_pipeline({\\n 'context': The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.,\\n 'question': What day was the game played on?\\n})\\nprint(predictions)\", \"performance\": {\"dataset\": \"SQuAD1.1\", \"accuracy\": {\"EM\": 80.9, \"F1\": 88.2}}, \"description\": \"BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\"}}", "category": "generic"}
{"question_id": 340, "text": " We have a blog post platform, and we want to categorize each blog post under one of the pre-defined categories: \\\"Technology\\\", \\\"Fashion\\\", \\\"Health\\\", or \\\"Travel\\\".\\n###Input: \\\"Google is planning to launch its augmented reality glasses next year. These high-tech glasses are set to revolutionize the way we interact with the world around us, merging the digital world with reality.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"valhalla/distilbart-mnli-12-1\", \"api_call\": \"pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-1')\", \"api_arguments\": {\"model_name_or_path\": \"valhalla/distilbart-mnli-12-1\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"MNLI\", \"matched_accuracy\": 87.08, \"mismatched_accuracy\": 87.5}, \"description\": \"distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.\"}}", "category": "generic"}
{"question_id": 341, "text": " We need to analyze a recent news message to know which category it belongs to: technology, sports or politics.\\n###Input: Apple just announced the newest iPhone X.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"cross-encoder/nli-roberta-base\", \"api_call\": \"CrossEncoder('cross-encoder/nli-roberta-base')\", \"api_arguments\": [\"sentence1\", \"sentence2\"], \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from transformers import pipeline\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-roberta-base')\\nsent = Apple just announced the newest iPhone X\\ncandidate_labels = [technology, sports, politics]\\nres = classifier(sent, candidate_labels)\\nprint(res)\", \"performance\": {\"dataset\": [\"SNLI\", \"MultiNLI\"], \"accuracy\": \"See SBERT.net - Pretrained Cross-Encoder\"}, \"description\": \"Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.\"}}", "category": "generic"}
{"question_id": 342, "text": " We received an email about our new app launch. Classify the sentiment of the email as 'positive', 'negative' or 'neutral'.\\n###Input: \\\"I'm so excited about the new app you just launched! The interface is clean and incredibly user-friendly.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"valhalla/distilbart-mnli-12-6\", \"api_call\": \"pipeline('zero-shot-classification', model=DistilBartForSequenceClassification.from_pretrained('valhalla/distilbart-mnli-12-6'))\", \"api_arguments\": \"text, candidate_labels\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\nresult = nlp('The movie was great!', ['positive', 'negative'])\\nprint(result)\", \"performance\": {\"dataset\": \"MNLI\", \"accuracy\": {\"matched_acc\": \"89.19\", \"mismatched_acc\": \"89.01\"}}, \"description\": \"distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.\"}}", "category": "generic"}
{"question_id": 343, "text": " I need to translate Dutch food recipes for my American friends.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-nl-en\", \"api_call\": \"pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_nl_to_en', model='Helsinki-NLP/opus-mt-nl-en')\\ntranslated_text = translation('Hallo, hoe gaat het met je?')[0]['translation_text']\", \"performance\": {\"dataset\": \"Tatoeba.nl.en\", \"accuracy\": {\"BLEU\": 60.9, \"chr-F\": 0.749}}, \"description\": \"A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing.\"}}", "category": "generic"}
{"question_id": 344, "text": " I need to create a system to quickly translate emails between French and Spanish coworkers in my company.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-fr-es\", \"api_call\": \"Helsinki-NLP/opus-mt-fr-es\", \"api_arguments\": {\"source_languages\": \"fr\", \"target_languages\": \"es\"}, \"python_environment_requirements\": {\"PyTorch\": \"1.0.0\", \"TensorFlow\": \"2.0\", \"Transformers\": \"4.0.0\"}, \"example_code\": \"translation('Bonjour, comment  va?')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newssyscomb2009.fr.es\": 34.3, \"news-test2008.fr.es\": 32.5, \"newstest2009.fr.es\": 31.6, \"newstest2010.fr.es\": 36.5, \"newstest2011.fr.es\": 38.3, \"newstest2012.fr.es\": 38.1, \"newstest2013.fr.es\": 34.0, \"Tatoeba.fr.es\": 53.2}, \"chr-F\": {\"newssyscomb2009.fr.es\": 0.601, \"news-test2008.fr.es\": 0.583, \"newstest2009.fr.es\": 0.586, \"newstest2010.fr.es\": 0.616, \"newstest2011.fr.es\": 0.622, \"newstest2012.fr.es\": 0.619, \"newstest2013.fr.es\": 0.587, \"Tatoeba.fr.es\": 0.709}}}, \"description\": \"A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.\"}}", "category": "generic"}
{"question_id": 345, "text": " We are a news platform that is focusing on multi-lingual. The platform will analyze the conversation between multiple people, so the summarizers should be able to handle this efficiently.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Summarization\", \"api_name\": \"distilbart-cnn-12-6-samsum\", \"api_call\": \"summarizer = pipeline('summarization', model='philschmid/distilbart-cnn-12-6-samsum').model\", \"api_arguments\": {\"model\": \"philschmid/distilbart-cnn-12-6-samsum\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\nconversation = '''Jeff: Can I train a \\ud83e\\udd17 Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\n'''\\nsummarizer(conversation)\", \"performance\": {\"dataset\": \"samsum\", \"accuracy\": {\"ROUGE-1\": 41.09, \"ROUGE-2\": 20.746, \"ROUGE-L\": 31.595, \"ROUGE-LSUM\": 38.339}}, \"description\": \"This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\"}}", "category": "generic"}
{"question_id": 346, "text": " Our team is developing a chatbot for customer service. We need a chatbot that can engage in a conversation.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face\", \"functionality\": \"Conversational\", \"api_name\": \"facebook/blenderbot_small-90M\", \"api_call\": \"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot_small-90M')\", \"api_arguments\": [\"message\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"Input a message to start chatting with facebook/blenderbot_small-90M.\", \"performance\": {\"dataset\": \"blended_skill_talk\", \"accuracy\": \"Not provided\"}, \"description\": \"Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation.\"}}", "category": "generic"}
{"question_id": 347, "text": " Generate a story that starts with \\\"Once upon a time in a small village\\\".\\n###Input: <noinput>\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"facebook/opt-125m\", \"api_call\": \"pipeline('text-generation', model='facebook/opt-125m')\", \"api_arguments\": {\"do_sample\": \"True\"}, \"python_environment_requirements\": \"from transformers import pipeline, set_seed\", \"example_code\": \"generator(Hello, I'm am conscious and)\", \"performance\": {\"dataset\": \"Various\", \"accuracy\": \"Roughly matches GPT-3 performance\"}, \"description\": \"OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. It was predominantly pretrained with English text, but a small amount of non-English data is present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective. OPT can be used for prompting for evaluation of downstream tasks as well as text generation.\"}}", "category": "generic"}
{"question_id": 348, "text": " We are building a translation service that translates user inputs from English to German. Can you suggest an API for this task?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Language model\", \"api_name\": \"google/flan-t5-small\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import T5Tokenizer, T5ForConditionalGeneration\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-small)\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-small)\\ninput_text = translate English to German: How old are you?\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\noutputs = model.generate(input_ids)\\nprint(tokenizer.decode(outputs[0]))\", \"performance\": {\"dataset\": [{\"name\": \"MMLU\", \"accuracy\": \"75.2%\"}]}, \"description\": \"FLAN-T5 small is a fine-tuned version of T5 language model on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. The model is designed for research on language models, including zero-shot and few-shot NLP tasks, reasoning, question answering, fairness, and safety research. It has not been tested in real-world applications and should not be used directly in any application without prior assessment of safety and fairness concerns specific to the application.\"}}", "category": "generic"}
{"question_id": 349, "text": " A user wants to translate a sentence from English to French. Give an example of how the user can do this using the given model.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text-to-Text Transfer Transformer\", \"api_name\": \"google/mt5-base\", \"api_call\": \"MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\", \"api_arguments\": [\"model_name\", \"input_text\", \"generated_text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\\ntokenizer = MT5Tokenizer.from_pretrained('google/mt5-base')\\ninputs = tokenizer.encode('translate English to German: The house is wonderful.', return_tensors='pt')\\noutputs = model.generate(inputs, max_length=40, num_return_sequences=1)\\ndecoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"mc4\", \"accuracy\": \"Not provided\"}, \"description\": \"mT5 is a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. It leverages a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of multilingual NLP tasks.\"}}", "category": "generic"}
{"question_id": 350, "text": " An AI researcher wants to use a new model to automatically complete a sentence. He knows that the model should predict how the company can be perceived.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"microsoft/deberta-v3-base\", \"api_call\": \"DebertaModel.from_pretrained('microsoft/deberta-v3-base')\", \"api_arguments\": [\"model_name_or_path\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-v3-base')\\nresult = fill_mask('Hugging Face is a [MASK] company.')\", \"performance\": {\"dataset\": {\"SQuAD 2.0\": {\"F1\": 88.4, \"EM\": 85.4}, \"MNLI-m/mm\": {\"ACC\": \"90.6/90.7\"}}}, \"description\": \"DeBERTa V3 improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It further improves the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer. This model was trained using the 160GB data as DeBERTa V2.\"}}", "category": "generic"}
{"question_id": 351, "text": " Complete the following sentence in Dutch: \\\"Mijn favoriete eten is [MASK] omdat het erg lekker is.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"GroNLP/bert-base-dutch-cased\", \"api_call\": \"AutoModel.from_pretrained('GroNLP/bert-base-dutch-cased')\", \"api_arguments\": [\"pretrained_model_name_or_path\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModel, TFAutoModel\\ntokenizer = AutoTokenizer.from_pretrained(GroNLP/bert-base-dutch-cased)\\nmodel = AutoModel.from_pretrained(GroNLP/bert-base-dutch-cased)\", \"performance\": {\"dataset\": [{\"name\": \"CoNLL-2002\", \"accuracy\": \"90.24\"}, {\"name\": \"SoNaR-1\", \"accuracy\": \"84.93\"}, {\"name\": \"spaCy UD LassySmall\", \"accuracy\": \"86.10\"}]}, \"description\": \"BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.\"}}", "category": "generic"}
{"question_id": 352, "text": " A law firm wants to automate the completion of some legal texts. They need to fill in the blanks with relevant terms.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"nlpaueb/legal-bert-small-uncased\", \"api_call\": \"AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nlpaueb/legal-bert-small-uncased\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModel\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-small-uncased')\\nmodel = AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\", \"performance\": {\"dataset\": \"Legal Corpora\", \"accuracy\": \"Comparable to larger models\"}, \"description\": \"LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.\"}}", "category": "generic"}
{"question_id": 353, "text": " I have an important presentation in a foreign language but I need the translation to be done fast. Please help me translate my speech.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"Fairseq\", \"functionality\": \"Speech-to-speech translation\", \"api_name\": \"facebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\", \"api_call\": \"tts_model.get_prediction(tts_sample)\", \"api_arguments\": {\"audio\": \"16000Hz mono channel audio\"}, \"python_environment_requirements\": [\"fairseq\", \"hub_utils\", \"huggingface_hub\", \"IPython.display\", \"torchaudio\"], \"example_code\": [\"import json\", \"import os\", \"from pathlib import Path\", \"import IPython.display as ipd\", \"from fairseq import hub_utils\", \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\", \"from fairseq.models.speech_to_text.hub_interface import S2THubInterface\", \"from fairseq.models.text_to_speech import CodeHiFiGANVocoder\", \"from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\", \"from huggingface_hub import snapshot_download\", \"import torchaudio\", \"cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\", \"models, cfg, task = load_model_ensemble_and_task_from_hf_hub(\", \"facebook/xm_transformer_s2ut_800m-es-en-st-asr-bt_h1_2022,\", \"arg_overrides={config_yaml: config.yaml, task: speech_to_text},\", \"cache_dir=cache_dir,\", \")\", \"model = models[0].cpu()\", \"cfg[task].cpu = True\", \"generator = task.build_generator([model], cfg)\", \"# requires 16000Hz mono channel audio\", \"audio, _ = torchaudio.load(/Users/lpw/git/api-inference-community/docker_images/fairseq/tests/samples/sample2.flac)\", \"sample = S2THubInterface.get_model_input(task, audio)\", \"unit = S2THubInterface.get_prediction(task, model, generator, sample)\", \"library_name = fairseq\", \"cache_dir = (\", \" cache_dir or (Path.home() / .cache / library_name).as_posix()\", \")\", \"cache_dir = snapshot_download(\", \" ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\", \")\", \"x = hub_utils.from_pretrained(\", \" cache_dir,\", \" model.pt,\", \" .,\", \" archive_map=CodeHiFiGANVocoder.hub_models(),\", \" config_yaml=config.json,\", \" fp16=False,\", \" is_vocoder=True,\", \")\", \"with open(f{x['args']['data']}/config.json) as f:\", \" vocoder_cfg = json.load(f)\", \"assert (\", \" len(x[args][model_path]) == 1\", \"), Too many vocoder models in the input\", \"vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\", \"tts_model = VocoderHubInterface(vocoder_cfg, vocoder)\", \"tts_sample = tts_model.get_model_input(unit)\", \"wav, sr = tts_model.get_prediction(tts_sample)\", \"ipd.Audio(wav, rate=sr)\"], \"performance\": {\"dataset\": \"covost2\", \"accuracy\": null}, \"description\": \"Speech-to-speech translation model from fairseq S2UT (paper/code) for Spanish-English. Trained on mTEDx, CoVoST 2, Europarl-ST, and VoxPopuli.\"}}", "category": "generic"}
{"question_id": 354, "text": " We are building an audio guide application for tourist attractions in Japan. Provide various useful information about popular tourists spots in Japanese audio.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.\"}}", "category": "generic"}
{"question_id": 355, "text": " Design a tool for the hearing impaired students that will aid in converting their teacher's lectures into a text format.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"speechbrain\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"tts-hifigan-german\", \"api_call\": \"hifi_gan.decode_batch(mel_output)\", \"api_arguments\": [\"mel_output\"], \"python_environment_requirements\": [\"speechbrain\"], \"example_code\": \"import torchaudio\\nfrom speechbrain.pretrained import Tacotron2\\nfrom speechbrain.pretrained import HIFIGAN\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\nwaveforms = hifi_gan.decode_batch(mel_output)\\ntorchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\", \"performance\": {\"dataset\": \"custom German dataset\", \"accuracy\": \"Not specified\"}, \"description\": \"A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.\"}}", "category": "generic"}
{"question_id": 356, "text": " Help me build a Chinese audio transcription tool for people who are deaf, so that they can convert audio to text.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Recognition\", \"api_name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\", \"api_call\": \"Wav2Vec2Model.from_pretrained('jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn')\", \"api_arguments\": [\"audio_paths\"], \"python_environment_requirements\": [\"huggingsound\", \"torch\", \"librosa\", \"datasets\", \"transformers\"], \"example_code\": \"from huggingsound import SpeechRecognitionModel\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn')\\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\\ntranscriptions = model.transcribe(audio_paths)\", \"performance\": {\"dataset\": \"Common Voice zh-CN\", \"accuracy\": {\"WER\": 82.37, \"CER\": 19.03}}, \"description\": \"Fine-tuned XLSR-53 large model for speech recognition in Chinese. Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.\"}}", "category": "generic"}
{"question_id": 357, "text": " Develop a tool to convert one language's speech to another language's speech as a part of a communication program.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"audio\", \"api_name\": \"textless_sm_cs_en\", \"api_call\": \"Wav2Vec2Model.from_pretrained(cached_download('https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt'))\", \"api_arguments\": [], \"python_environment_requirements\": [\"fairseq\", \"huggingface_hub\"], \"example_code\": \"from fairseq.models.wav2vec.wav2vec2_asr import Wav2Vec2Model\\nfrom huggingface_hub import cached_download\\nmodel = Wav2Vec2Model.from_pretrained(cached_download('https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt'))\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A speech-to-speech translation model for converting between languages without using text as an intermediate representation. This model is designed for the task of audio-to-audio translation.\"}}", "category": "generic"}
{"question_id": 358, "text": " A podcast studio wants to correct their audio recordings with overlapping speakers. We need to help them separate the speaker's voices from the mixed audio.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Asteroid\", \"api_name\": \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\", \"api_call\": \"pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')\", \"api_arguments\": {\"model\": \"JorisCos/DPTNet_Libri1Mix_enhsingle_16k\"}, \"python_environment_requirements\": [\"transformers\", \"asteroid\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"Libri1Mix\", \"si_sdr\": 14.829670037349064, \"si_sdr_imp\": 11.379888731489366, \"sdr\": 15.395712644737149, \"sdr_imp\": 11.893049845524112, \"sir\": \"Infinity\", \"sir_imp\": \"NaN\", \"sar\": 15.395712644737149, \"sar_imp\": 11.893049845524112, \"stoi\": 0.9301948391058859, \"stoi_imp\": 0.13427501556534832}, \"description\": \"This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.\"}}", "category": "generic"}
{"question_id": 359, "text": " Need a solution to improve the quality of noisy speech samples with background noise and other disturbances.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Enhancement\", \"api_name\": \"sepformer-wham-enhancement\", \"api_call\": \"model.separate_file(path='speechbrain/sepformer-wham-enhancement/example_wham.wav')\", \"api_arguments\": [\"path\"], \"python_environment_requirements\": [\"pip install speechbrain\"], \"example_code\": \"from speechbrain.pretrained import SepformerSeparation as separator\\nimport torchaudio\\nmodel = separator.from_hparams(source='speechbrain/sepformer-wham-enhancement', savedir='pretrained_models/sepformer-wham-enhancement')\\nest_sources = model.separate_file(path='speechbrain/sepformer-wham-enhancement/example_wham.wav')\\ntorchaudio.save('enhanced_wham.wav', est_sources[:, :, 0].detach().cpu(), 8000)\", \"performance\": {\"dataset\": \"WHAM!\", \"accuracy\": \"14.35 dB SI-SNR\"}, \"description\": \"This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.\"}}", "category": "generic"}
{"question_id": 360, "text": " I have an automatic onboarding app for newcomers in my company. It takes details of each new employee and plays a welcome message in their native language.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Spoken Language Identification\", \"api_name\": \"TalTechNLP/voxlingua107-epaca-tdnn\", \"api_call\": \"'EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)'\", \"api_arguments\": [\"signal\"], \"python_environment_requirements\": [\"speechbrain\", \"torchaudio\"], \"example_code\": \"import torchaudio\\nfrom speechbrain.pretrained import EncoderClassifier\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\nprediction = language_id.classify_batch(signal)\\nprint(prediction)\", \"performance\": {\"dataset\": \"VoxLingua107\", \"accuracy\": \"93%\"}, \"description\": \"This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.\"}}", "category": "generic"}
{"question_id": 361, "text": " We are developing a phone app to evaluate bird species by their vocalizations. We want to find out the species using the sound.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"mazkooleg/0-9up-data2vec-audio-base-960h-ft\", \"api_call\": \"pipeline('audio-classification', model='mazkooleg/0-9up-data2vec-audio-base-960h-ft')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"Transformers 4.26.1, Pytorch 1.11.0+cpu, Datasets 2.10.1, Tokenizers 0.12.1\", \"example_code\": \"\", \"performance\": {\"dataset\": \"None\", \"accuracy\": 0.9967}, \"description\": \"This model is a fine-tuned version of facebook/data2vec-audio-base-960h on the None dataset.\"}}", "category": "generic"}
{"question_id": 362, "text": " The company is aiming to create a product for detecting emotions in voice recordings. We need to classify emotions from voice inputs.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Emotion Recognition\", \"api_name\": \"superb/hubert-large-superb-er\", \"api_call\": \"pipeline('audio-classification', model='superb/hubert-large-superb-er')\", \"api_arguments\": [\"model\"], \"python_environment_requirements\": [\"transformers\", \"datasets\", \"librosa\"], \"example_code\": \"from datasets import load_dataset\\nfrom transformers import pipeline\\ndataset = load_dataset('anton-l/superb_demo', 'er', split='session1')\\nclassifier = pipeline('audio-classification', model='superb/hubert-large-superb-er')\\nlabels = classifier(dataset[0]['file'], top_k=5)\", \"performance\": {\"dataset\": \"IEMOCAP\", \"accuracy\": 0.6762}, \"description\": \"This is a ported version of S3PRL's Hubert for the SUPERB Emotion Recognition task. The base model is hubert-large-ll60k, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\"}}", "category": "generic"}
{"question_id": 363, "text": " As a movie streaming platform, we aim to recommend movies to users by predicting their sentiment towards the movie descriptions.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Joblib\", \"functionality\": \"Multi-class Classification\", \"api_name\": \"1530155186\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"IMDB\", \"accuracy\": 0.487}, \"description\": \"A tabular classification model trained using AutoTrain for sentiment analysis on the IMDB dataset. The model has a CO2 emission of 0.0186 grams and an accuracy of 0.487.\"}}", "category": "generic"}
{"question_id": 364, "text": " We are working on designing an app that will create a regression model to estimate the worth of a home.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Single Column Regression\", \"api_name\": \"1771761513\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"jwan2021/autotrain-data-us-housing-prices\", \"accuracy\": {\"Loss\": 100581.032, \"R2\": 0.922, \"MSE\": 10116543945.03, \"MAE\": 81586.656, \"RMSLE\": 0.101}}, \"description\": \"A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework.\"}}", "category": "generic"}
{"question_id": 365, "text": " Our organization is working towards reducing carbon emissions from vehicles. Develop a solution for estimating carbon emissions given vehicle information.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"2037366889\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data.csv\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv(data.csv)\\ndata = data[features]\\ndata.columns = [feat_ + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"Robertooo/autotrain-data-hmaet\", \"accuracy\": {\"Loss\": 0.003, \"R2\": 0.999, \"MSE\": 0.0, \"MAE\": 0.001, \"RMSLE\": 0.002}}, \"description\": \"A tabular regression model trained using AutoTrain to predict carbon emissions.\"}}", "category": "generic"}
{"question_id": 366, "text": " We are running an environmental campaign, and we want to predict the CO2 emissions in grams for our next project.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Hugging Face\", \"functionality\": \"tabular regression\", \"api_name\": \"farouk97/autotrain-test7-2644pc-linearregr-38619101723\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"farouk97/autotrain-data-test7-2644pc-linearregr\", \"accuracy\": {\"Loss\": 0.145, \"R2\": 0.0, \"MSE\": 0.021, \"MAE\": 0.099, \"RMSLE\": 0.101}}, \"description\": \"A tabular regression model trained using AutoTrain to predict CO2 emissions (in grams).\"}}", "category": "generic"}
{"question_id": 367, "text": " Predict the closing price of a stock using historical data.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Scikit-learn\", \"functionality\": \"baseline-trainer\", \"api_name\": \"srg/outhimar_64-Close-regression\", \"api_call\": \"Output: Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\\\nDate False False False ... True False False\\\\nOpen True False False ... False False False\\\\nHigh True False False ... False False False\\\\nLow True False False ... False False False\\\\nAdj Close True False False ... False False False\\\\nVolume True False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))]).fit(X_train, y_train)\", \"api_arguments\": [\"X_train\", \"y_train\"], \"python_environment_requirements\": [\"scikit-learn\", \"dabl\"], \"example_code\": \"Pipeline(steps=[('easypreprocessor',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\nDate False False False ... True False False\\nOpen True False False ... False False False\\nHigh True False False ... False False False\\nLow True False False ... False False False\\nAdj Close True False False ... False False False\\nVolume True False False ... False False False[6 rows x 7 columns])),('ridge', Ridge(alpha=10))])\", \"performance\": {\"dataset\": \"outhimar_64\", \"accuracy\": {\"r2\": 0.999858, \"neg_mean_squared_error\": -1.067685}}, \"description\": \"Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt.\"}}", "category": "generic"}
{"question_id": 368, "text": " I am coaching a youth soccer team and want to build an AI model to analyze and optimize my players' performance. Can you provide a suggestion?\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"0xid/poca-SoccerTwos\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"N/A\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 369, "text": " An environment is created for controlling Ant-v3 robot. We are providing robotic arm control for a humanoid robot in a virtual environment.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"td3-Ant-v3\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"RL Zoo: https://github.com/DLR-RM/rl-baselines3-zoo\", \"SB3: https://github.com/DLR-RM/stable-baselines3\", \"SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\"], \"example_code\": [\"python -m rl_zoo3.load_from_hub --algo td3 --env Ant-v3 -orga sb3 -f logs/\", \"python enjoy.py --algo td3 --env Ant-v3 -f logs/\", \"python train.py --algo td3 --env Ant-v3 -f logs/\", \"python -m rl_zoo3.push_to_hub --algo td3 --env Ant-v3 -f logs/ -orga sb3\"], \"performance\": {\"dataset\": \"Ant-v3\", \"accuracy\": \"5822.96 +/- 93.33\"}, \"description\": \"This is a trained model of a TD3 agent playing Ant-v3 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 370, "text": " I want to train a soccer player AI to play against other players using the SoccerTwos environment.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"NoNameFound/poca-SoccerTwos\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\", \"ML-Agents-SoccerTwos\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 371, "text": " We are developing a soccer game using Unity. Integrate a trained soccer agent to perform well in SoccerTwos.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"Raiden-1001/poca-Soccerv7\", \"api_call\": \"No changes needed. The `api_call` field does not include any model instantiation.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\", \"ML-Agents-SoccerTwos\"], \"example_code\": \"Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\\nStep 2: Select your .nn /.onnx file\\nClick on Watch the agent play \\ud83d\\udc40\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 372, "text": " Teach me how to make the machine play an arcade game similar to Breakout using reinforcement learning.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"ppo-BreakoutNoFrameskip-v4\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"orga\", \"f\"], \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": [\"python -m rl_zoo3.load_from_hub --algo ppo --env BreakoutNoFrameskip-v4 -orga sb3 -f logs/\", \"python enjoy.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/\", \"python train.py --algo ppo --env BreakoutNoFrameskip-v4 -f logs/\", \"python -m rl_zoo3.push_to_hub --algo ppo --env BreakoutNoFrameskip-v4 -f logs/ -orga sb3\"], \"performance\": {\"dataset\": \"BreakoutNoFrameskip-v4\", \"accuracy\": \"398.00 +/- 16.30\"}, \"description\": \"This is a trained model of a PPO agent playing BreakoutNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 373, "text": " We are a medical research company, and we need to extract features from a complex medical text in English.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"GanjinZero/UMLSBert_ENG\", \"api_call\": \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER\"}}", "category": "generic"}
{"question_id": 374, "text": " Develop a content moderation system for a platform where users upload videos, and it's crucial to identify inappropriate content.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"microsoft/xclip-base-patch16-zero-shot\", \"api_call\": \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": [{\"name\": \"HMDB-51\", \"accuracy\": 44.6}, {\"name\": \"UCF-101\", \"accuracy\": 72.0}, {\"name\": \"Kinetics-600\", \"accuracy\": 65.2}]}, \"description\": \"X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\"}}", "category": "generic"}
{"question_id": 375, "text": " I represent a company creating a virtual spokesperson for an ad campaign, and I desire photo-realistic images of a mascot eating breakfast. Provide generated images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"runwayml/stable-diffusion-v1-5\", \"api_call\": \"'image = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)(prompt).images[0]'\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": {\"diffusers\": \"from diffusers import StableDiffusionPipeline\", \"torch\": \"import torch\"}, \"example_code\": {\"model_id\": \"model_id = runwayml/stable-diffusion-v1-5\", \"pipe\": \"pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\", \"pipe_to_cuda\": \"pipe = pipe.to(cuda)\", \"prompt\": \"prompt = a photo of an astronaut riding a horse on mars\", \"image\": \"image = pipe(prompt).images[0]\", \"save_image\": \"image.save(astronaut_rides_horse.png)\"}, \"performance\": {\"dataset\": \"COCO2017\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\"}}", "category": "generic"}
{"question_id": 376, "text": " A software company needs a new wallpaper for their office. They are interested in an image of people working together in a futuristic office environment.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"stabilityai/stable-diffusion-2\", \"api_call\": \"pipe(prompt).images[0]\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\"}}", "category": "generic"}
{"question_id": 377, "text": " We are a startup creating graphic design software. We need a function to generate artwork in an analog style based on user text prompts.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"wavymulder/Analog-Diffusion\", \"api_call\": \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", \"api_arguments\": [\"prompt\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"text_to_image('analog style landscape')\", \"performance\": {\"dataset\": \"analog photographs\", \"accuracy\": \"Not specified\"}, \"description\": \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}}", "category": "generic"}
{"question_id": 378, "text": " Develop a system that can provide a description of a photo with an optional condition.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Transformers\", \"functionality\": \"Image Captioning\", \"api_name\": \"blip-image-captioning-large\", \"api_call\": \"BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\", \"api_arguments\": {\"raw_image\": \"Image\", \"text\": \"Optional Text\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForConditionalGeneration\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": {\"import_requests\": \"import requests\", \"import_PIL\": \"from PIL import Image\", \"import_transformers\": \"from transformers import BlipProcessor, BlipForConditionalGeneration\", \"load_processor\": \"processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)\", \"load_model\": \"model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\", \"load_image\": \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", \"conditional_captioning\": \"text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"unconditional_captioning\": \"inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\"}, \"performance\": {\"dataset\": \"COCO\", \"accuracy\": {\"image-text retrieval\": \"+2.7% recall@1\", \"image captioning\": \"+2.8% CIDEr\", \"VQA\": \"+1.6% VQA score\"}}, \"description\": \"BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\"}}", "category": "generic"}
{"question_id": 379, "text": " I am an art curator looking to provide engaging descriptions for paintings in our museum. Generate a description for a painting based on an image.\\n###Input: {\\\"url\\\": \\\"https://i.imgur.com/N01U6aN.jpg\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image-to-Text\", \"api_name\": \"ydshieh/vit-gpt2-coco-en\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')\", \"api_arguments\": {\"loc\": \"ydshieh/vit-gpt2-coco-en\"}, \"python_environment_requirements\": [\"torch\", \"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import torch\\nimport requests\\nfrom PIL import Image\\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\\nloc = ydshieh/vit-gpt2-coco-en\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\\ntokenizer = AutoTokenizer.from_pretrained(loc)\\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\\nmodel.eval()\\ndef predict(image):\\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\\n with torch.no_grad():\\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nwith Image.open(requests.get(url, stream=True).raw) as image:\\n preds = predict(image)\\nprint(preds)\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not specified\"}, \"description\": \"A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.\"}}", "category": "generic"}
{"question_id": 380, "text": " For an advertisement, create a video of a new car model based on the following description: \\\"The blue car has sleek design, runs on electric energy, and reaches speeds up to 200mph.\\\"\\n###Input: <noinput>\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"chavinlo/TempoFunk\", \"api_call\": \"text_to_video = pipeline('text-to-video', model='chavinlo/TempoFunk')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\"}}", "category": "generic"}
{"question_id": 381, "text": " Create an application that answers questions about an image's content.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-vqav2\", \"api_call\": \"pipeline('visual-question-answering', model='microsoft/git-base-vqav2')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"vqa(image='path/to/image.jpg', question='What is in the image?')\", \"performance\": {\"dataset\": \"VQAv2\", \"accuracy\": \"Refer to the paper for evaluation results\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 382, "text": " I work with cats rescue organization. I need to analyze an image and tell how many cats are in the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"dandelin/vilt-b32-finetuned-vqa\", \"api_call\": \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", \"api_arguments\": {\"image\": \"Image.open(requests.get(url, stream=True).raw)\", \"text\": \"How many cats are there?\"}, \"python_environment_requirements\": {\"transformers\": \"ViltProcessor, ViltForQuestionAnswering\", \"requests\": \"requests\", \"PIL\": \"Image\"}, \"example_code\": \"from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])\", \"performance\": {\"dataset\": \"VQAv2\", \"accuracy\": \"to do\"}, \"description\": \"Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 383, "text": " We need to develop a chatbot that can answer questions about photos uploaded by users.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"Salesforce/blip-vqa-capfilt-large\", \"api_call\": \"BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\", \"api_arguments\": {\"raw_image\": \"RGB image\", \"question\": \"string\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForQuestionAnswering\"}, \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"VQA\", \"accuracy\": \"+1.6% in VQA score\"}, \"description\": \"BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.\"}}", "category": "generic"}
{"question_id": 384, "text": " I received an image on my phone and I have a question about it, please answer my question and provide the code snippet.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"JosephusCheung/GuanacoVQA\", \"api_call\": \"pipeline('visual-question-answering', model='GuanacoVQA').\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers, torch\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"JosephusCheung/GuanacoVQADataset\", \"accuracy\": \"N/A\"}, \"description\": \"A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.\"}}", "category": "generic"}
{"question_id": 385, "text": " A user wants to get the answers to the questions on an image containing text. Help them to code this question-answering system.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"vision-encoder-decoder\", \"api_name\": \"jinhybr/OCR-DocVQA-Donut\", \"api_call\": \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut').model.vision_encoder\", \"api_arguments\": \"image_path, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\"}}", "category": "generic"}
{"question_id": 386, "text": " We need to perform data analysis on various documents and must answer questions based on different document layouts.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"frizwankhan/entity-linking-model-final\", \"api_call\": \"pipeline('question-answering', model='frizwankhan/entity-linking-model-final')\", \"api_arguments\": {\"image\": \"path/to/image\", \"question\": \"your question\"}, \"python_environment_requirements\": {\"huggingface\": \"4.12.0\", \"torch\": \"1.9.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Document Question Answering model based on layoutlmv2\"}}", "category": "generic"}
{"question_id": 387, "text": " What needs to be done, if we are to use an AI model to answer questions about invoices?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face\", \"functionality\": \"Question Answering\", \"api_name\": \"impira/layoutlm-document-qa\", \"api_call\": \"layoutlm_qa = pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa', return_dict=True))\", \"api_arguments\": [\"image_url\", \"question\"], \"python_environment_requirements\": [\"PIL\", \"pytesseract\", \"PyTorch\", \"transformers\"], \"example_code\": \"nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)\", \"performance\": {\"dataset\": \"SQuAD2.0 and DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.\"}}", "category": "generic"}
{"question_id": 388, "text": " Create a depth estimation model for an autonomous vehicle navigation system.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"tiny-random-DPTForDepthEstimation\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random DPT model for depth estimation using Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 389, "text": " I'm creating an app for hikers to estimate the depth of terrain. Can you build a model to calculate the depth in images?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"dpt-large-redesign\", \"api_call\": \"torch.hub.load('nielsr/dpt-large-redesign')\", \"api_arguments\": [], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A depth estimation model based on the DPT architecture.\"}}", "category": "generic"}
{"question_id": 390, "text": " We are a robotics company and we want to integrate depth estimation into our autonomous navigation system.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-230131-041708\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4425, \"Mae\": 0.427, \"Rmse\": 0.6196, \"Abs_Rel\": 0.4543, \"Log_Mae\": 0.1732, \"Log_Rmse\": 0.2288, \"Delta1\": 0.3787, \"Delta2\": 0.6298, \"Delta3\": 0.8083}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.\"}}", "category": "generic"}
{"question_id": 391, "text": " As an application developer constructing virtual experiences, we need to determine the depth of objects in a given scene.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu\", \"api_call\": \"'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)'\", \"api_arguments\": \"images, return_tensors\", \"python_environment_requirements\": [\"transformers\", \"torch\", \"numpy\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\", \"performance\": {\"dataset\": \"NYUv2\", \"accuracy\": \"Not provided\"}, \"description\": \"Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 392, "text": " We are the development team that creates mods for the GTA5 game. We need to train an AI model that can read and analyze the in-game processes using the MNIST dataset.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Graph Machine Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Train AI model for GTA5 game and save the model\", \"api_name\": \"GTA5_PROCESS_LEARNING_AI\", \"api_call\": \"No changes needed.\", \"api_arguments\": {\"model\": \"NanoCircuit\", \"data_loader\": \"train_loader\", \"criterion\": \"nn.CrossEntropyLoss\", \"optimizer\": \"optim.SGD\", \"device\": \"torch.device\", \"data_cap_gb\": 10}, \"python_environment_requirements\": [\"contextlib\", \"os\", \"matplotlib\", \"numpy\", \"torch\", \"torch.nn\", \"torch.optim\", \"requests\", \"torchvision\", \"psutil\", \"time\", \"subprocess\", \"onnxruntime\", \"numexpr\", \"transformers\"], \"example_code\": {\"import_libraries\": [\"import contextlib\", \"import os\", \"from matplotlib import pyplot as plt\", \"import numpy as np\", \"import torch\", \"import torch.nn as nn\", \"import torch.optim as optim\", \"import requests\", \"from torchvision import datasets, transforms\", \"import psutil\", \"import time\", \"import subprocess\", \"import onnxruntime as ort\", \"import matplotlib.pyplot as plt\", \"import numpy as np\", \"import numexpr as ne\", \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\"], \"define_neural_network\": [\"class NanoCircuit(nn.Module):\", \" def init(self):\", \" super(NanoCircuit, self).init()\", \" self.fc1 = nn.Linear(784, 128)\", \" self.fc2 = nn.Linear(128, 10)\", \"def forward(self, x):\", \" x = x.view(-1, 784)\", \" x = torch.relu(self.fc1(x))\", \" x = self.fc2(x)\", \" return x\"], \"train_with_data_cap\": [\"def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):\", \" data_processed = 0\", \" data_cap_bytes = data_cap_gb * (1024 ** 3)\", \" epoch = 0\", \"while data_processed < data_cap_bytes:\", \" running_loss = 0.0\", \" for i, data in enumerate(data_loader, 0):\", \" inputs, labels = data\", \" inputs, labels = inputs.to(device), labels.to(device)\", \" data_processed += inputs.nelement() * inputs.element_size()\", \" if data_processed >= data_cap_bytes:\", \" break\", \" optimizer.zero_grad()\", \" outputs = model(inputs.view(-1, 28 * 28))\", \" loss = criterion(outputs, labels)\", \" loss.backward()\", \" optimizer.step()\", \" running_loss += loss.item()\", \"epoch += 1\", \"print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})\", \"print(fData processed: {data_processed / (1024 ** 3):.2f} GB)\", \"return model\"]}, \"performance\": {\"dataset\": \"MNIST\", \"accuracy\": \"Not specified\"}, \"description\": \"This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.\"}}", "category": "generic"}
{"question_id": 393, "text": " The company is testing an app for face filters that change the user's appearance. We need an Age Classifier to determine user's age.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Age Classification\", \"api_name\": \"nateraw/vit-age-classifier\", \"api_call\": \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nateraw/vit-age-classifier\"}, \"python_environment_requirements\": [\"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", \"performance\": {\"dataset\": \"fairface\", \"accuracy\": null}, \"description\": \"A vision transformer finetuned to classify the age of a given person's face.\"}}", "category": "generic"}
{"question_id": 394, "text": " A food delivery app needs a tool to identify dishes from images so that it can recommend similar foods to users.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"google/vit-base-patch16-384\", \"api_call\": \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"google/vit-base-patch16-384\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"ImageNet\", \"accuracy\": \"Refer to tables 2 and 5 of the original paper\"}, \"description\": \"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\"}}", "category": "generic"}
{"question_id": 395, "text": " For the grocery store's checkout application, we need to classify various fruits and vegetables.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"google/mobilenet_v2_1.0_224\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v2_1.0_224)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v2_1.0_224)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not specified\"}, \"description\": \"MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.\"}}", "category": "generic"}
{"question_id": 396, "text": " You work for a satellite data company and your goal is to detect planes in satellite imagery.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-plane-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-plane-detection').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"plane-detection\", \"accuracy\": \"0.995\"}, \"description\": \"A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\"}}", "category": "generic"}
{"question_id": 397, "text": " I am a construction engineer, I got drone images of the construction site, and I want to predict the type of material by breaking that large image into a smaller region.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Semantic Segmentation\", \"api_name\": \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"CityScapes\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 398, "text": " We would like to detect and visualize potholes in an image taken by a drone.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8m-pothole-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8m-pothole-segmentation')\", \"api_arguments\": {\"image\": \"URL or local image path\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pothole-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.858, \"mAP@0.5(mask)\": 0.895}}, \"description\": \"A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\"}}", "category": "generic"}
{"question_id": 399, "text": " As a parking lot owner we want to identify the potholes and avoid any issues within a given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8n-pothole-segmentation\", \"api_call\": \"'model.predict(image)'\", \"api_arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\", \"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": {\"ultralyticsplus\": \"0.0.23\", \"ultralytics\": \"8.0.21\"}, \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pothole-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.995, \"mAP@0.5(mask)\": 0.995}}, \"description\": \"A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\"}}", "category": "generic"}
{"question_id": 400, "text": " Our client is a digital artist and wants to convert a text description into an image in his next project.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation model\", \"api_name\": \"lllyasviel/control_v11e_sd15_ip2p\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": [\"checkpoint\", \"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"Stable Diffusion v1-5\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\"}}", "category": "generic"}
{"question_id": 401, "text": " I own a website for photo editing, and I need to provide an image upscaling feature.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"swin2SR-classical-sr-x4-64\", \"api_call\": \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", \"api_arguments\": [\"input_image\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\"}}", "category": "generic"}
{"question_id": 402, "text": " My client needs to create a tool that can transform an input imagery photo into a beautiful art painting.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"GreeneryScenery/SheepsControlV5\", \"api_call\": \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\", \"api_arguments\": {\"input_image\": \"path/to/image/file\"}, \"python_environment_requirements\": {\"huggingface_hub\": \">=0.0.17\", \"transformers\": \">=4.13.0\", \"torch\": \">=1.10.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"poloclub/diffusiondb\", \"accuracy\": \"Not provided\"}, \"description\": \"SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\"}}", "category": "generic"}
{"question_id": 403, "text": " Our company needs to generate high-resolution images of human faces for an upcoming campaign. Please come up with a solution.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"CompVis/ldm-celebahq-256\", \"api_call\": \"DiffusionPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = CompVis/ldm-celebahq-256\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\nimage = pipeline(num_inference_steps=200)[sample]\\nimage[0].save(ldm_generated_image.png)\", \"performance\": {\"dataset\": \"CelebA-HQ\", \"accuracy\": \"N/A\"}, \"description\": \"Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.\"}}", "category": "generic"}
{"question_id": 404, "text": " A pet store needs a unique image of an animal for their marketing campaign. We need to generate the image for them.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"johnowhitaker/sd-class-wikiart-from-bedrooms\", \"api_call\": \"DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"https://huggingface.co/datasets/huggan/wikiart\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.\"}}", "category": "generic"}
{"question_id": 405, "text": " How can I build a model to classify videos of people doing activities?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"sayakpaul/videomae-base-finetuned-ucf101-subset\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')\", \"api_arguments\": {\"learning_rate\": 5e-05, \"train_batch_size\": 8, \"eval_batch_size\": 8, \"seed\": 42, \"optimizer\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\", \"lr_scheduler_type\": \"linear\", \"lr_scheduler_warmup_ratio\": 0.1, \"training_steps\": 148}, \"python_environment_requirements\": {\"Transformers\": \"4.24.0\", \"Pytorch\": \"1.12.1+cu113\", \"Datasets\": \"2.6.1\", \"Tokenizers\": \"0.13.2\"}, \"example_code\": \"from transformers import AutoModelForVideoClassification, AutoTokenizer\\nmodel = AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')\\ntokenizer = AutoTokenizer.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 0.8645}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645.\"}}", "category": "generic"}
{"question_id": 406, "text": " We have set up an online course platform where users can upload images of their paintings. Can you classify those paintings by their style (e.g., baroque, impressionism, cubism)?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-base-patch16\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-base-patch16')\", \"api_arguments\": [\"text\", \"images\", \"return_tensors\", \"padding\"], \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-base-patch16)\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-base-patch16)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies depending on the dataset\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}", "category": "generic"}
{"question_id": 407, "text": " I am developing an online platform where users can upload an image and get instant feedback about the object in the image. We need to evaluate the CLIP model to classify the uploaded images into different categories.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg')\", \"api_arguments\": {\"image\": \"path_to_image\", \"candidate_labels\": [\"list\", \"of\", \"candidate\", \"labels\"]}, \"python_environment_requirements\": {\"transformers\": \">=4.13.0\"}, \"example_code\": \"classifier(image='path_to_image', candidate_labels=['cat', 'dog', 'car'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8 - 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Base model (convnext_base) as the image tower, and the same text tower as the RN50x4 (depth 12, embed dim 640) model from OpenAI CLIP. The base models are trained at 256x256 image resolution and roughly match the RN50x4 models on FLOPs and activation counts.\"}}", "category": "generic"}
{"question_id": 408, "text": " The manager wants an AI-based system that can determine whether a given image is of a cat or a dog.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"kakaobrain/align-base\", \"api_call\": \"AlignModel.from_pretrained('kakaobrain/align-base')\", \"api_arguments\": [\"text\", \"images\", \"return_tensors\"], \"python_environment_requirements\": [\"requests\", \"torch\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AlignProcessor, AlignModel\\nprocessor = AlignProcessor.from_pretrained(kakaobrain/align-base)\\nmodel = AlignModel.from_pretrained(kakaobrain/align-base)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ncandidate_labels = [an image of a cat, an image of a dog]\\ninputs = processor(text=candidate_labels, images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\\nprint(probs)\", \"performance\": {\"dataset\": \"COYO-700M\", \"accuracy\": \"on-par or outperforms Google ALIGN's reported metrics\"}, \"description\": \"The ALIGN model is a dual-encoder architecture with EfficientNet as its vision encoder and BERT as its text encoder. It learns to align visual and text representations with contrastive learning. This implementation is trained on the open source COYO dataset and can be used for zero-shot image classification and multi-modal embedding retrieval.\"}}", "category": "generic"}
{"question_id": 409, "text": " As a biology student, I want to analyze insects in different pictures and categorize them automatically.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\", \"api_arguments\": \"image, class_names\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; clip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'); clip(image, class_names=['cat', 'dog', 'fish'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"79.1 - 79.4\"}, \"description\": \"A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}", "category": "generic"}
{"question_id": 410, "text": " As a financial analyst, to increase efficiency and save time, we need to analyze a financial statement and identify the sentiment.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"financial-sentiment-analysis\", \"api_name\": \"ProsusAI/finbert\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert'); classifier('your_text_here')\", \"performance\": {\"dataset\": \"Financial PhraseBank\", \"accuracy\": \"Not provided\"}, \"description\": \"FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\"}}", "category": "generic"}
{"question_id": 411, "text": " I want to write an AI-powered newsletter platform. The system needs to classify paraphrased text based on how they compare to the original text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"prithivida/parrot_adequacy_model\", \"api_call\": \"pipeline('text-classification', model='prithivida/parrot_adequacy_model')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. This model is an ancillary model for Parrot paraphraser.\"}}", "category": "generic"}
{"question_id": 412, "text": " I am an AI software developing assistant, and I want to analyze a movie review's sentiment.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"lvwerra/distilbert-imdb\", \"api_call\": \"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\", \"pytorch\"], \"example_code\": \"classifier('I love this movie!')\", \"performance\": {\"dataset\": \"imdb\", \"accuracy\": 0.928}, \"description\": \"This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\"}}", "category": "generic"}
{"question_id": 413, "text": " We have a historical document dataset, and we need to find relevant answers quickly to answer questions about World War II.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Information Retrieval\", \"api_name\": \"cross-encoder/ms-marco-MiniLM-L-12-v2\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('model_name')\", \"api_arguments\": {\"padding\": \"True\", \"truncation\": \"True\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\", \"torch\": \"import torch\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\nimport torch\\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\\ntokenizer = AutoTokenizer.from_pretrained('model_name')\\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\\nmodel.eval()\\nwith torch.no_grad():\\n scores = model(**features).logits\\n print(scores)\", \"performance\": {\"dataset\": {\"TREC Deep Learning 2019\": {\"NDCG@10\": 74.31}, \"MS Marco Passage Reranking\": {\"MRR@10\": 39.02, \"accuracy\": \"960 Docs / Sec\"}}}, \"description\": \"This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco\"}}", "category": "generic"}
{"question_id": 414, "text": " Recommend the best solution to examine the conversations taking place in a chat room and classify emotions displayed by different users.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"michellejieli/emotion_text_classifier\", \"api_call\": \"pipeline('sentiment-analysis', model='michellejieli/emotion_text_classifier')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"classifier(I love this!)\", \"performance\": {\"dataset\": [\"Crowdflower (2016)\", \"Emotion Dataset, Elvis et al. (2018)\", \"GoEmotions, Demszky et al. (2020)\", \"ISEAR, Vikash (2018)\", \"MELD, Poria et al. (2019)\", \"SemEval-2018, EI-reg, Mohammad et al. (2018)\", \"Emotion Lines (Friends)\"], \"accuracy\": \"Not provided\"}, \"description\": \"DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\"}}", "category": "generic"}
{"question_id": 415, "text": " We are working on a project and need to find entities (such as person, organization, location) within a given sentence.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Token Classification\", \"api_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\", \"api_call\": \"Output: AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\\nmodel = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\", \"performance\": {\"dataset\": \"CoNLL-03\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification tasks.\"}}", "category": "generic"}
{"question_id": 416, "text": " I need to extract names, locations, and organizations from a broad range of texts in different languages.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"Babelscape/wikineural-multilingual-ner\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\nexample = My name is Wolfgang and I live in Berlin\\nner_results = nlp(example)\\nprint(ner_results)\", \"performance\": {\"dataset\": \"Babelscape/wikineural-multilingual-ner\", \"accuracy\": \"span-based F1-score up to 6 points over previous state-of-the-art systems for data creation\"}, \"description\": \"A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.\"}}", "category": "generic"}
{"question_id": 417, "text": " I am required to build a table for top competitor's sales numbers and I want to answer questions from it like, \\\"Which competitor had the highest sales?\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-base-finetuned-sqa\", \"api_call\": \"TapasTokenizer.from_pretrained('google/tapas-base-finetuned-sqa')\", \"api_arguments\": [\"question\", \"table\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\n# Initialize the pipeline\\ntapas_pipeline = pipeline('table-question-answering', model='google/tapas-base-finetuned-sqa')\\n# Define the table and question\\nquestion = 'How many goals did player A score?'\\ntable = [['Player', 'Goals'], ['Player A', 5], ['Player B', 3]]\\n# Get the answer\\nresult = tapas_pipeline(question=question, table=table)\\nprint(result)\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.6874}, \"description\": \"TAPAS base model fine-tuned on Sequential Question Answering (SQA). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia and fine-tuned on SQA. It can be used for answering questions related to a table in a conversational set-up.\"}}", "category": "generic"}
{"question_id": 418, "text": " Identify the best-selling product for last month from the provided sales data table and provide details about its revenue and total units sold.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-large-finetuned-wikisql-supervised\", \"api_call\": \"pipeline('table-question-answering', model='google/tapas-large-finetuned-wikisql-supervised')\", \"api_arguments\": {\"model\": \"google/tapas-large-finetuned-wikisql-supervised\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"tensorflow\"], \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wikisql-supervised')\\nresult = qa_pipeline(question='What is the capital of France?', table=table)\\nprint(result)\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 419, "text": " A manager wants the assistat to extract the total revenue for COMPANY_A from a given quarterly performance table.\\n###Input: {'table': {'rows': [['Company', 'Q1 Revenue', 'Q2 Revenue', 'Q3 Revenue', 'Q4 Revenue'], ['COMPANY_A', '75000', '83000', '92000', '102000'], ['COMPANY_B', '81000', '88000', '99000', '110000']], 'columns': 5} , 'query': 'What is the total revenue for COMPANY_A?' }\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-medium-finetuned-wtq\", \"api_call\": \"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\", \"api_arguments\": \"table, query\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\n# Initialize the pipeline\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\n# Define the table and the query\\ntable = {...}\\nquery = '...'\\n# Get the answer\\nanswer = table_qa(table=table, query=query)\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": 0.4324}, \"description\": \"TAPAS medium model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia and is used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 420, "text": " Can you help me with building a program to extract answers to legal questions from contracts and documents?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"Rakib/roberta-base-on-cuad\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('Rakib/roberta-base-on-cuad')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": {\"import\": \"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\", \"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\", \"model\": \"model = AutoModelForQuestionAnswering.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"performance\": {\"dataset\": \"cuad\", \"accuracy\": \"46.6%\"}, \"description\": \"This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents.\"}}", "category": "generic"}
{"question_id": 421, "text": " I'm writing an article on the influence of e-commerce on the retail industry. What are the main differences between brick-and-mortar stores and online shops?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/bert-base-cased-squad2\", \"api_call\": \"deepset/bert-base-cased-squad2\", \"api_arguments\": \"context, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can this model be loaded?'})\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"exact_match\": 71.152, \"f1\": 74.671}}, \"description\": \"This is a BERT base cased model trained on SQuAD v2\"}}", "category": "generic"}
{"question_id": 422, "text": " Analyze a scientific statement about the planet Mars and find its logical relationship with the existing knowledge.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Cross-Encoder for Natural Language Inference\", \"api_name\": \"cross-encoder/nli-deberta-v3-base\", \"api_call\": \"CrossEncoder('cross-encoder/nli-deberta-v3-base')\", \"api_arguments\": [\"sentence_pairs\"], \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from sentence_transformers import CrossEncoder\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-base')\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\", \"performance\": {\"dataset\": {\"SNLI-test\": \"92.38\", \"MNLI mismatched set\": \"90.04\"}}, \"description\": \"This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\"}}", "category": "generic"}
{"question_id": 423, "text": " Help me understand a review about a new album release by classifying it as either positive, negative or neutral.\\n###Input: \\\"I've been listening to the album non-stop since it was released. I absolutely love the new sound and creative approach the artist has taken. It feels fresh and innovative.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"valhalla/distilbart-mnli-12-6\", \"api_call\": \"pipeline('zero-shot-classification', model=DistilBartForSequenceClassification.from_pretrained('valhalla/distilbart-mnli-12-6'))\", \"api_arguments\": \"text, candidate_labels\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('zero-shot-classification', model='valhalla/distilbart-mnli-12-6')\\nresult = nlp('The movie was great!', ['positive', 'negative'])\\nprint(result)\", \"performance\": {\"dataset\": \"MNLI\", \"accuracy\": {\"matched_acc\": \"89.19\", \"mismatched_acc\": \"89.01\"}}, \"description\": \"distilbart-mnli is the distilled version of bart-large-mnli created using the No Teacher Distillation technique proposed for BART summarisation by Huggingface. It is designed for zero-shot classification tasks.\"}}", "category": "generic"}
{"question_id": 424, "text": " I need to classify a news headline into the categories of politics, sports, or technology.\\n###Input: \\\"Apple just announced the newest iPhone X\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"cross-encoder/nli-deberta-v3-xsmall\", \"api_call\": \"pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\", \"api_arguments\": [\"sent\", \"candidate_labels\"], \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from transformers import pipeline\\nclassifier = pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\nsent = 'Apple just announced the newest iPhone X'\\ncandidate_labels = ['technology', 'sports', 'politics']\\nres = classifier(sent, candidate_labels)\\nprint(res)\", \"performance\": {\"dataset\": {\"SNLI-test\": \"91.64\", \"MNLI_mismatched\": \"87.77\"}}, \"description\": \"This model is a Cross-Encoder for Natural Language Inference, trained on the SNLI and MultiNLI datasets. It can be used for zero-shot classification tasks.\"}}", "category": "generic"}
{"question_id": 425, "text": " I am working on a project to determine the intent of my customers queries so I can respond to them accordingly.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"Narsil/deberta-large-mnli-zero-cls\", \"api_call\": \"DebertaModel.from_pretrained('Narsil/deberta-large-mnli-zero-cls')\", \"api_arguments\": \"text, candidate_labels\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": {\"SQuAD 1.1\": {\"F1\": 95.5, \"EM\": 90.1}, \"SQuAD 2.0\": {\"F1\": 90.7, \"EM\": 88.0}, \"MNLI-m/mm\": {\"Accuracy\": 91.3}, \"SST-2\": {\"Accuracy\": 96.5}, \"QNLI\": {\"Accuracy\": 95.3}, \"CoLA\": {\"MCC\": 69.5}, \"RTE\": {\"Accuracy\": 91.0}, \"MRPC\": {\"Accuracy\": 92.6}, \"QQP\": {}, \"STS-B\": {\"P/S\": 92.8}}}, \"description\": \"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on the majority of NLU tasks with 80GB training data. This is the DeBERTa large model fine-tuned with MNLI task.\"}}", "category": "generic"}
{"question_id": 426, "text": " What's the main pro and con of companies merging in the same vertical?\\n###Input: \\\"Merging companies in the same vertical often comes with a variety of pros and cons. One of the main benefits is operational efficiency due to the reduction of duplicated efforts and consolidation of resources. On the other hand, the most significant disadvantage is reduced market competition, which can lead to monopolistic behaviors and potentially negative consequences for consumers.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": [\"Translation\", \"Summarization\", \"Question Answering\", \"Text Classification\", \"Text Regression\"], \"api_name\": \"t5-small\", \"api_call\": \"T5Model.from_pretrained('t5-small')\", \"api_arguments\": {\"input_ids\": \"input tokenized text\", \"decoder_input_ids\": \"input tokenized text for decoder\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import T5Tokenizer, T5Model\\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\\nmodel = T5Model.from_pretrained('t5-small')\\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"See research paper, Table 14 for full results\"}, \"description\": \"T5-Small is a Text-To-Text Transfer Transformer (T5) model with 60 million parameters. It is designed to perform a variety of NLP tasks, including machine translation, document summarization, question answering, and classification tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be fine-tuned for specific tasks.\"}}", "category": "generic"}
{"question_id": 427, "text": " I have a text document in English, and I want to generate a summary of it.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation, Summarization, Question Answering, Sentiment Analysis, Regression\", \"api_name\": \"t5-large\", \"api_call\": \"T5Model.from_pretrained('t5-large')\", \"api_arguments\": {\"input_ids\": \"tokenizer(..., return_tensors='pt').input_ids\", \"decoder_input_ids\": \"tokenizer(..., return_tensors='pt').input_ids\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import T5Tokenizer, T5Model\"}, \"example_code\": \"tokenizer = T5Tokenizer.from_pretrained('t5-large')\\nmodel = T5Model.from_pretrained('t5-large')\\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"See research paper, Table 14\"}, \"description\": \"T5-Large is a Text-To-Text Transfer Transformer (T5) model with 770 million parameters. It is designed to handle a variety of NLP tasks, including translation, summarization, question answering, sentiment analysis, and regression. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on various supervised and unsupervised tasks.\"}}", "category": "generic"}
{"question_id": 428, "text": " A company has a marketing team that wants to translate English brochures into German. We need to develop an NLP system to assist in translation.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-en-de\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"Helsinki-NLP/opus-mt-en-de\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\ntokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-de')\", \"performance\": {\"dataset\": \"newstest2018-ende.en.de\", \"accuracy\": {\"BLEU\": 45.2, \"chr-F\": 0.69}}, \"description\": \"The Helsinki-NLP/opus-mt-en-de model is a translation model developed by the Language Technology Research Group at the University of Helsinki. It translates English text to German using the Hugging Face Transformers library. The model is trained on the OPUS dataset and has a BLEU score of 45.2 on the newstest2018-ende.en.de dataset.\"}}", "category": "generic"}
{"question_id": 429, "text": " We are an online educational platform trying to reach international audiences; therefore, we need to translate our content to various languages. Help us to find the right tool to perform the translation.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"facebook/nllb-200-distilled-600M\", \"api_call\": \"pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M')\", \"api_arguments\": [\"model\", \"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline; translator = pipeline('translation_xx_to_yy', model='facebook/nllb-200-distilled-600M'); translator('Hello World')\", \"performance\": {\"dataset\": \"Flores-200\", \"accuracy\": \"BLEU, spBLEU, chrF++\"}, \"description\": \"NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.\"}}", "category": "generic"}
{"question_id": 430, "text": " The company wants to translate product descriptions from Italian to English.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-it-en\", \"api_call\": \"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')('Ciao mondo!')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newssyscomb2009.it.en\": 35.3, \"newstest2009.it.en\": 34.0, \"Tatoeba.it.en\": 70.9}, \"chr-F\": {\"newssyscomb2009.it.en\": 0.6, \"newstest2009.it.en\": 0.594, \"Tatoeba.it.en\": 0.808}}}, \"description\": \"A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\"}}", "category": "generic"}
{"question_id": 431, "text": " We have a series of lengthy conversations that need to be summarized.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text-to-Text Generation\", \"api_name\": \"philschmid/bart-large-cnn-samsum\", \"api_call\": \"summarizer = pipeline('summarization', model=BartForConditionalGeneration.from_pretrained('philschmid/bart-large-cnn-samsum'))\", \"api_arguments\": {\"model\": \"philschmid/bart-large-cnn-samsum\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": \"from transformers import pipeline\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\nconversation = '''Jeff: Can I train a \\ud83e\\udd17 Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\n'''\\nsummarizer(conversation)\", \"performance\": {\"dataset\": \"samsum\", \"accuracy\": {\"eval_rouge1\": 42.621, \"eval_rouge2\": 21.9825, \"eval_rougeL\": 33.034, \"eval_rougeLsum\": 39.6783, \"test_rouge1\": 41.3174, \"test_rouge2\": 20.8716, \"test_rougeL\": 32.1337, \"test_rougeLsum\": 38.4149}}, \"description\": \"philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\"}}", "category": "generic"}
{"question_id": 432, "text": " I want to translate a French email to Spanish to better communicate with my colleagues in Spain.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-fr-es\", \"api_call\": \"Helsinki-NLP/opus-mt-fr-es\", \"api_arguments\": {\"source_languages\": \"fr\", \"target_languages\": \"es\"}, \"python_environment_requirements\": {\"PyTorch\": \"1.0.0\", \"TensorFlow\": \"2.0\", \"Transformers\": \"4.0.0\"}, \"example_code\": \"translation('Bonjour, comment  va?')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newssyscomb2009.fr.es\": 34.3, \"news-test2008.fr.es\": 32.5, \"newstest2009.fr.es\": 31.6, \"newstest2010.fr.es\": 36.5, \"newstest2011.fr.es\": 38.3, \"newstest2012.fr.es\": 38.1, \"newstest2013.fr.es\": 34.0, \"Tatoeba.fr.es\": 53.2}, \"chr-F\": {\"newssyscomb2009.fr.es\": 0.601, \"news-test2008.fr.es\": 0.583, \"newstest2009.fr.es\": 0.586, \"newstest2010.fr.es\": 0.616, \"newstest2011.fr.es\": 0.622, \"newstest2012.fr.es\": 0.619, \"newstest2013.fr.es\": 0.587, \"Tatoeba.fr.es\": 0.709}}}, \"description\": \"A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.\"}}", "category": "generic"}
{"question_id": 433, "text": " Construct a text summarization system to summarize news articles for a mobile application. The input text should be a complete news article.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text2Text Generation\", \"api_name\": \"t5-efficient-large-nl36_fine_tune_sum_V2\", \"api_call\": \"pipeline('summarization', model='Samuel-Fipps/t5-efficient-large-nl36_fine_tune_sum_V2')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": [{\"name\": \"samsum\", \"accuracy\": {\"ROUGE-1\": 54.933, \"ROUGE-2\": 31.797, \"ROUGE-L\": 47.006, \"ROUGE-LSUM\": 51.203, \"loss\": 1.131, \"gen_len\": 23.799}}, {\"name\": \"cnn_dailymail\", \"accuracy\": {\"ROUGE-1\": 34.406, \"ROUGE-2\": 14.127, \"ROUGE-L\": 24.335, \"ROUGE-LSUM\": 31.658, \"loss\": 2.446, \"gen_len\": 45.928}}]}, \"description\": \"A T5-based summarization model trained on the Samsum dataset. This model can be used for text-to-text generation tasks such as summarization without adding 'summarize' to the start of the input string. It has been fine-tuned for 10K steps with a batch size of 10.\"}}", "category": "generic"}
{"question_id": 434, "text": " We need to create a text generator that impersonates Elon Musk for our novelty conversational app.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Conversational\", \"api_name\": \"Pi3141/DialoGPT-medium-elon-3\", \"api_call\": \"pipeline('text-generation', model='Pi3141/DialoGPT-medium-elon-3')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\", \"performance\": {\"dataset\": \"Twitter tweets by Elon Musk\", \"accuracy\": \"N/A\"}, \"description\": \"DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\"}}", "category": "generic"}
{"question_id": 435, "text": " I want to create an engaging chatbot which can interact with users using personalized responses based on personality facts.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Conversational\", \"api_name\": \"af1tang/personaGPT\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('af1tang/personaGPT')\", \"api_arguments\": {\"do_sample\": \"True\", \"top_k\": \"10\", \"top_p\": \".92\", \"max_length\": \"1000\", \"pad_token\": \"tokenizer.eos_token_id\"}, \"python_environment_requirements\": {\"transformers\": \"GPT2Tokenizer, GPT2LMHeadModel\", \"torch\": \"torch\"}, \"example_code\": {\"load_model\": \"tokenizer = AutoTokenizer.from_pretrained(af1tang/personaGPT)\\nmodel = AutoModelForCausalLM.from_pretrained(af1tang/personaGPT)\\nif torch.cuda.is_available():\\n model = model.cuda()\", \"generate_response\": \"bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\\nmsg = generate_next(bot_input_ids)\\ndialog_hx.append(msg)\\nprint(Bot: {}.format(tokenizer.decode(msg, skip_special_tokens=True)))\"}, \"performance\": {\"dataset\": \"Persona-Chat\", \"accuracy\": \"Not provided\"}, \"description\": \"PersonaGPT is an open-domain conversational agent designed to do 2 tasks: decoding personalized responses based on input personality facts (the persona profile of the bot) and incorporating turn-level goals into its responses through action codes (e.g., talk about work, ask about favorite music). It builds on the DialoGPT-medium pretrained model based on the GPT-2 architecture.\"}}", "category": "generic"}
{"question_id": 436, "text": " I have a Russian text and want to generate a continuation of it.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"ruDialoGpt3-medium-finetuned-telegram\", \"api_call\": \"Output: AutoModelForCausalLM.from_pretrained(checkpoint)\", \"api_arguments\": {\"num_return_sequences\": 1, \"max_length\": 512, \"no_repeat_ngram_size\": 3, \"do_sample\": true, \"top_k\": 50, \"top_p\": 0.9, \"temperature\": 0.6, \"mask_token_id\": \"tokenizer.mask_token_id\", \"eos_token_id\": \"tokenizer.eos_token_id\", \"unk_token_id\": \"tokenizer.unk_token_id\", \"pad_token_id\": \"tokenizer.pad_token_id\", \"device\": \"cpu\"}, \"python_environment_requirements\": {\"huggingface_transformers\": \"AutoTokenizer, AutoModelForCausalLM\", \"torch\": \"zeros, cat\"}, \"example_code\": \"checkpoint = Kirili4ik/ruDialoGpt3-medium-finetuned-telegram\\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\\nmodel.eval()\", \"performance\": {\"dataset\": \"Russian forums and Telegram chat\", \"accuracy\": \"Not available\"}, \"description\": \"DialoGPT trained on Russian language and fine tuned on my telegram chat. This model was created by sberbank-ai and trained on Russian forums. It has been fine-tuned on a 30mb json file of exported telegram chat data.\"}}", "category": "generic"}
{"question_id": 437, "text": " We need to create a short story for a children's book based on the theme of friendship.\\n###Input: \\\"<no input>\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"TehVenom/PPO_Pygway-V8p4_Dev-6b\", \"api_call\": \"pipeline('text-generation', model='TehVenom/PPO_Pygway-V8p4_Dev-6b') should be rewritten by looking at the example code and determining the right model instantiation. However, there is not enough information in the example code to determine the correct model instantiation. Therefore, no changes should be made to the value of the `api_call` field.\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"TODO card. Mix of (GPT-J-6B-Janeway + PPO_HH_GPT-J) + Pygmalion-6b-DEV (V8 / Part 4). At a ratio of GPT-J-6B-Janeway - 20%, PPO_HH_GPT-J - 20%, Pygmalion-6b DEV (V8 / Part 4) - 60%.\"}}", "category": "generic"}
{"question_id": 438, "text": " We are building an AI-based writing assistant. We have to create a web-based tool to generate blog titles and outlines.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"cerebras/Cerebras-GPT-111M\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('cerebras/Cerebras-GPT-111M')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"cerebras/Cerebras-GPT-111M\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForCausalLM\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(cerebras/Cerebras-GPT-111M)\\nmodel = AutoModelForCausalLM.from_pretrained(cerebras/Cerebras-GPT-111M)\\ntext = Generative AI is \", \"performance\": {\"dataset\": \"The Pile\", \"accuracy\": {\"PILE_test_xent\": 2.566, \"Hella-Swag\": 0.268, \"PIQA\": 0.594, \"Wino-Grande\": 0.488, \"Lambada\": 0.194, \"ARC-e\": 0.38, \"ARC-c\": 0.166, \"OpenBookQA\": 0.118, \"Downstream_Average\": 0.315}}, \"description\": \"Cerebras-GPT-111M is a transformer-based language model with 111M parameters, trained on the Pile dataset using the GPT-3 style architecture. It is intended for use in research and as a foundation model for NLP applications, ethics, and alignment research. The model can be fine-tuned for various tasks and is licensed under Apache 2.0.\"}}", "category": "generic"}
{"question_id": 439, "text": " A history teacher wants to translate a text from Hindi to French. The text is about \\\"Life is like a box of chocolates.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Multilingual Translation\", \"api_name\": \"facebook/m2m100_418M\", \"api_call\": \"Output: M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\", \"api_arguments\": {\"encoded_input\": \"Encoded input text\", \"target_lang\": \"Target language code\"}, \"python_environment_requirements\": [\"transformers\", \"sentencepiece\"], \"example_code\": [\"from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\", \"hi_text = \\u091c\\u0940\\u0935\\u0928 \\u090f\\u0915 \\u091a\\u0949\\u0915\\u0932\\u0947\\u091f \\u092c\\u0949\\u0915\\u094d\\u0938 \\u0915\\u0940 \\u0924\\u0930\\u0939 \\u0939\\u0948\\u0964\", \"chinese_text = \\u751f\\u6d3b\\u5c31\\u50cf\\u4e00\\u76d2\\u5de7\\u514b\\u529b\\u3002\", \"model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_418M)\", \"tokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_418M)\", \"tokenizer.src_lang = hi\", \"encoded_hi = tokenizer(hi_text, return_tensors=pt)\", \"generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))\", \"tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\"], \"performance\": {\"dataset\": \"WMT\", \"accuracy\": \"Not provided\"}, \"description\": \"M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.\"}}", "category": "generic"}
{"question_id": 440, "text": " We are creating a language-learning app. Provide us with a way to fix grammar mistakes that our users make while learning a new language.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Grammar Synthesis\", \"api_name\": \"pszemraj/flan-t5-large-grammar-synthesis\", \"api_call\": \"pipeline('text2text-generation', 'pszemraj/flan-t5-large-grammar-synthesis')\", \"api_arguments\": [\"raw_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\ncorrector = pipeline(\\n 'text2text-generation',\\n 'pszemraj/flan-t5-large-grammar-synthesis',\\n )\\nraw_text = 'i can has cheezburger'\\nresults = corrector(raw_text)\\nprint(results)\", \"performance\": {\"dataset\": \"jfleg\", \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset.\"}}", "category": "generic"}
{"question_id": 441, "text": " Create an algorithm that converts a Python code snippet into a corresponding text description of its functionality.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"codet5-large-ntp-py\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large-ntp-py')\", \"api_arguments\": [\"text\", \"return_tensors\", \"input_ids\", \"max_length\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, T5ForConditionalGeneration\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codet5-large-ntp-py)\\nmodel = T5ForConditionalGeneration.from_pretrained(Salesforce/codet5-large-ntp-py)\\ntext = def hello_world():\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\ngenerated_ids = model.generate(input_ids, max_length=128)\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"APPS benchmark\", \"accuracy\": \"See Table 5 of the paper\"}, \"description\": \"CodeT5 is a family of encoder-decoder language models for code from the paper: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. The checkpoint included in this repository is denoted as CodeT5-large-ntp-py (770M), which is introduced by the paper: CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning by Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi.\"}}", "category": "generic"}
{"question_id": 442, "text": " Find the appropriate word to complete this sentence: \\\"The sun is the largest [MASK] in the solar system.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling\", \"api_name\": \"distilbert-base-multilingual-cased\", \"api_call\": \"pipeline('fill-mask', model='distilbert-base-multilingual-cased')\", \"api_arguments\": [\"pipeline\", \"fill-mask\", \"model\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nunmasker = pipeline('fill-mask', model='distilbert-base-multilingual-cased')\\nunmasker(Hello I'm a [MASK] model.)\", \"performance\": {\"dataset\": [{\"name\": \"XNLI\", \"accuracy\": {\"English\": 78.2, \"Spanish\": 69.1, \"Chinese\": 64.0, \"German\": 66.3, \"Arabic\": 59.1, \"Urdu\": 54.7}}]}, \"description\": \"This model is a distilled version of the BERT base multilingual model. It is trained on the concatenation of Wikipedia in 104 different languages. The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters. On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.\"}}", "category": "generic"}
{"question_id": 443, "text": " To improve the language quality of my essay, I want to spot words or sentences that can be replaced with better alternatives.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling and Next Sentence Prediction\", \"api_name\": \"bert-large-uncased\", \"api_call\": \"pipeline('fill-mask', model='bert-large-uncased')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nunmasker = pipeline('fill-mask', model='bert-large-uncased')\\nunmasker(Hello I'm a [MASK] model.)\", \"performance\": {\"dataset\": {\"SQUAD 1.1 F1/EM\": \"91.0/84.3\", \"Multi NLI Accuracy\": \"86.05\"}}, \"description\": \"BERT large model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters. The model is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.\"}}", "category": "generic"}
{"question_id": 444, "text": " We would like to develop an AI chatbot in Portuguese language that is able to fill in the blanks in sentences, based on the context of the conversation.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"neuralmind/bert-base-portuguese-cased\", \"api_call\": \"AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"neuralmind/bert-base-portuguese-cased\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\npipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\\npipe('Tinha uma [MASK] no meio do caminho.')\", \"performance\": {\"dataset\": \"brWaC\", \"accuracy\": \"state-of-the-art\"}, \"description\": \"BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.\"}}", "category": "generic"}
{"question_id": 445, "text": " We want to know the most relevant word to fill in the blank space in the given sentence.\\n###Input: Hugging Face is a [MASK] company.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"microsoft/deberta-v3-base\", \"api_call\": \"DebertaModel.from_pretrained('microsoft/deberta-v3-base')\", \"api_arguments\": [\"model_name_or_path\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-v3-base')\\nresult = fill_mask('Hugging Face is a [MASK] company.')\", \"performance\": {\"dataset\": {\"SQuAD 2.0\": {\"F1\": 88.4, \"EM\": 85.4}, \"MNLI-m/mm\": {\"ACC\": \"90.6/90.7\"}}}, \"description\": \"DeBERTa V3 improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It further improves the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer. This model was trained using the 160GB data as DeBERTa V2.\"}}", "category": "generic"}
{"question_id": 446, "text": " I'm a Japanese learner studying abroad. Please help me complete the following sentence: \\\"\\u4eca\\u65e5\\u306f[MASK]\\u3067\\u3059\\u3002\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"cl-tohoku/bert-base-japanese\", \"api_call\": \"cl-tohoku/bert-base-japanese\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"fill_mask('\\u4eca\\u65e5\\u306f[MASK]\\u3067\\u3059\\u3002')\", \"performance\": {\"dataset\": \"wikipedia\", \"accuracy\": \"N/A\"}, \"description\": \"This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.\"}}", "category": "generic"}
{"question_id": 447, "text": " I work at a company dealing with customer complaints. I need to compare the similarity between a customer's complaint and a list of known issues in our database.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Embeddings\", \"api_name\": \"sentence-transformers/bert-base-nli-mean-tokens\", \"api_call\": \"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 448, "text": " Our company is developing an AI customer service system. We need to check if the customer's query is similar to the existing ones.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"nikcheerla/nooks-amd-detection-v2-full\", \"api_call\": \"SentenceTransformer('{MODEL_NAME}')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": [\"sentence-transformers\", \"transformers\"], \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('{MODEL_NAME}')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 449, "text": " Our company is developing a virtual assistant to help users with their daily tasks. We need a text-to-speech functionality for the assistant.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"mio/amadeus\", \"api_call\": \"No changes needed.\", \"api_arguments\": {\"model_name\": \"mio/amadeus\"}, \"python_environment_requirements\": {\"espnet\": \"d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\", \"transformers\": \"latest\"}, \"example_code\": \"cd espnet\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\npip install -e .\\ncd egs2/amadeus/tts1\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus\", \"performance\": {\"dataset\": \"amadeus\", \"accuracy\": \"Not provided\"}, \"description\": \"This model was trained by mio using amadeus recipe in espnet.\"}}", "category": "generic"}
{"question_id": 450, "text": " I want to build a voice assistant that can read text out loud for me in Spanish.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"Fairseq\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"facebook/tts_transformer-es-css10\", \"api_call\": \"load_model_ensemble_and_task_from_hf_hub('facebook/tts_transformer-es-css10')\", \"api_arguments\": {\"arg_overrides\": {\"vocoder\": \"hifigan\", \"fp16\": false}}, \"python_environment_requirements\": [\"fairseq\", \"IPython\"], \"example_code\": \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\nimport IPython.display as ipd\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\n facebook/tts_transformer-es-css10,\\n arg_overrides={vocoder: hifigan, fp16: False}\\n)\\nmodel = models[0]\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\ngenerator = task.build_generator(model, cfg)\\ntext = Hola, esta es una prueba.\\nsample = TTSHubInterface.get_model_input(task, text)\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\nipd.Audio(wav, rate=rate)\", \"performance\": {\"dataset\": \"CSS10\", \"accuracy\": null}, \"description\": \"Transformer text-to-speech model from fairseq S^2. Spanish single-speaker male voice trained on CSS10.\"}}", "category": "generic"}
{"question_id": 451, "text": " Suppose you want to create speech audio for the phrase \\\"We are now launching our newest project \\\" in Chinese audio.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\", \"api_call\": \"Output: Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"torch\", \"espnet_model_zoo\"], \"example_code\": \"import soundfile\\nfrom espnet2.bin.tts_inference import Text2Speech\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\ntext = \\u6625\\u6c5f\\u6f6e\\u6c34\\u8fde\\u6d77\\u5e73\\uff0c\\u6d77\\u4e0a\\u660e\\u6708\\u5171\\u6f6e\\u751f\\nspeech = text2speech(text)[wav]\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\", \"performance\": {\"dataset\": \"csmsc\", \"accuracy\": \"Not specified\"}, \"description\": \"A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\"}}", "category": "generic"}
{"question_id": 452, "text": " Our company needs to transcribe a podcast that we recently recorded.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Automatic Speech Recognition and Speech Translation\", \"api_name\": \"openai/whisper-large-v2\", \"api_call\": \"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\", \"api_arguments\": {\"forced_decoder_ids\": \"WhisperProcessor.get_decoder_prompt_ids(language='english', task='transcribe')\"}, \"python_environment_requirements\": [\"transformers\", \"datasets\"], \"example_code\": [\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\", \"from datasets import load_dataset\", \"processor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\", \"model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v2')\", \"model.config.forced_decoder_ids = None\", \"ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\", \"sample = ds[0]['audio']\", \"input_features = processor(sample['array'], sampling_rate=sample['sampling_rate'], return_tensors='pt').input_features\", \"predicted_ids = model.generate(input_features)\", \"transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\"], \"performance\": {\"dataset\": \"LibriSpeech test-clean\", \"accuracy\": 3.0003583080317573}, \"description\": \"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning.\"}}", "category": "generic"}
{"question_id": 453, "text": " I am building a smart conference recording system, and now I need to transcript the meeting minutes from the audio.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech to Text\", \"api_name\": \"facebook/s2t-medium-librispeech-asr\", \"api_call\": \"'Speech2TextForConditionalGeneration.from_pretrained(facebook/s2t-medium-librispeech-asr)'\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/s2t-medium-librispeech-asr\"}, \"python_environment_requirements\": [\"torchaudio\", \"sentencepiece\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\\nfrom datasets import load_dataset\\nimport soundfile as sf\\nmodel = Speech2TextForConditionalGeneration.from_pretrained(facebook/s2t-medium-librispeech-asr)\\nprocessor = Speech2Textprocessor.from_pretrained(facebook/s2t-medium-librispeech-asr)\\ndef map_to_array(batch):\\n speech, _ = sf.read(batch[file])\\n batch[speech] = speech\\n return batch\\nds = load_dataset(\\n patrickvonplaten/librispeech_asr_dummy,\\n clean,\\n split=validation\\n)\\nds = ds.map(map_to_array)\\ninput_features = processor(\\n ds[speech][0],\\n sampling_rate=16_000,\\n return_tensors=pt\\n).input_features # Batch size 1\\ngenerated_ids = model.generate(input_features=input_features)\\ntranscription = processor.batch_decode(generated_ids)\", \"performance\": {\"dataset\": \"LibriSpeech\", \"accuracy\": {\"clean\": 3.5, \"other\": 7.8}}, \"description\": \"s2t-medium-librispeech-asr is a Speech to Text Transformer (S2T) model trained for automatic speech recognition (ASR). The S2T model was proposed in this paper and released in this repository.\"}}", "category": "generic"}
{"question_id": 454, "text": " Develop an app to convert a noisy audio recording to cleaner audio.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Asteroid\", \"api_name\": \"DCCRNet_Libri1Mix_enhsingle_16k\", \"api_call\": \"AutoModelForAudioToAudio.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')\", \"api_arguments\": \"pretrained_model_name_or_path\", \"python_environment_requirements\": [\"transformers\", \"asteroid\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"Libri1Mix\", \"accuracy\": {\"si_sdr\": 13.329767398333798, \"si_sdr_imp\": 9.879986092474098, \"sdr\": 13.87279932997016, \"sdr_imp\": 10.370136530757103, \"sir\": \"Infinity\", \"sir_imp\": \"NaN\", \"sar\": 13.87279932997016, \"sar_imp\": 10.370136530757103, \"stoi\": 0.9140907015623948, \"stoi_imp\": 0.11817087802185405}}, \"description\": \"This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.\"}}", "category": "generic"}
{"question_id": 455, "text": " A user is interested in enhancing the audio quality of their home recordings using a pre-trained model. Distortions such as room echoes exist, and they want to know which API will be useful in this situation.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"ESPnet\", \"functionality\": \"chime4\", \"api_name\": \"espnet/Wangyou_Zhang_chime4_enh_train_enh_conv_tasnet_raw\", \"api_call\": \"Not available\", \"api_arguments\": \"Not available\", \"python_environment_requirements\": \"espnet\", \"example_code\": \"./run.sh --skip_data_prep false --skip_train true --download_model espnet/Wangyou_Zhang_chime4_enh_train_enh_conv_tasnet_raw\", \"performance\": {\"dataset\": \"chime4\", \"accuracy\": \"Not available\"}, \"description\": \"This model was trained by Wangyou Zhang using chime4 recipe in espnet.\"}}", "category": "generic"}
{"question_id": 456, "text": " We have noisy recordings, and when we review the recordings, the background noise disturbs us. Please give an enhancement method.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Enhancement\", \"api_name\": \"speechbrain/sepformer-wham16k-enhancement\", \"api_call\": \"separator.from_hparams(source=speechbrain/sepformer-wham16k-enhancement, savedir='pretrained_models/sepformer-wham16k-enhancement').separate_file(path='speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav')\", \"api_arguments\": {\"path\": \"path to the input audio file\"}, \"python_environment_requirements\": \"pip install speechbrain\", \"example_code\": \"from speechbrain.pretrained import SepformerSeparation as separator\\nimport torchaudio\\nmodel = separator.from_hparams(source=speechbrain/sepformer-wham16k-enhancement, savedir='pretrained_models/sepformer-wham16k-enhancement')\\nest_sources = model.separate_file(path='speechbrain/sepformer-wham16k-enhancement/example_wham16k.wav')\\ntorchaudio.save(enhanced_wham16k.wav, est_sources[:, :, 0].detach().cpu(), 16000)\", \"performance\": {\"dataset\": \"WHAM!\", \"accuracy\": {\"Test-Set SI-SNR\": \"14.3 dB\", \"Test-Set PESQ\": \"2.20\"}}, \"description\": \"This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 16k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.\"}}", "category": "generic"}
{"question_id": 457, "text": " We are working on a project to clean up and enhance audio recordings from various sources. How can we utilize pretrained models for this?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Asteroid\", \"api_name\": \"DCUNet_Libri1Mix_enhsingle_16k\", \"api_call\": \"JorisCos/DCUNet_Libri1Mix_enhsingle_16k\", \"api_arguments\": \"audio\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"Libri1Mix\", \"accuracy\": {\"si_sdr\": 13.154035391645971, \"si_sdr_imp\": 9.704254085786271, \"sdr\": 13.568058873121435, \"sdr_imp\": 10.065396073908367, \"sar\": 13.568058873121435, \"sar_imp\": 10.065396073908367, \"stoi\": 0.9199373340235417, \"stoi_imp\": 0.12401751048300132}}, \"description\": \"This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset.\"}}", "category": "generic"}
{"question_id": 458, "text": " Our tourism business wants an automatic translator for customer support. The translator must be able to understand the speech in English and respond in French.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"facebook/textless_sm_en_fr\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"input_file\"], \"python_environment_requirements\": [\"huggingface_hub\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is a speech-to-speech translation model trained by Facebook. It is designed for translating English speech to French speech.\"}}", "category": "generic"}
{"question_id": 459, "text": "\\nDemonstrate an English to Hokkien speech-to-speech translation approach to test the audio translation quality of our audio processing system.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"xm_transformer_s2ut_en-hk\", \"api_call\": \"load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk')\", \"api_arguments\": {\"arg_overrides\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\"}, \"cache_dir\": \"cache_dir\"}, \"python_environment_requirements\": [\"fairseq\", \"huggingface_hub\", \"torchaudio\"], \"example_code\": {\"import_modules\": [\"import json\", \"import os\", \"from pathlib import Path\", \"import IPython.display as ipd\", \"from fairseq import hub_utils\", \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\", \"from fairseq.models.speech_to_text.hub_interface import S2THubInterface\", \"from fairseq.models.text_to_speech import CodeHiFiGANVocoder\", \"from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\", \"from huggingface_hub import snapshot_download\", \"import torchaudio\"], \"load_model\": [\"cache_dir = os.getenv('HUGGINGFACE_HUB_CACHE')\", \"models, cfg, task = load_model_ensemble_and_task_from_hf_hub('facebook/xm_transformer_s2ut_en-hk', arg_overrides={'config_yaml': 'config.yaml', 'task': 'speech_to_text'}, cache_dir=cache_dir)\", \"model = models[0].cpu()\", \"cfg['task'].cpu = True\"], \"generate_prediction\": [\"generator = task.build_generator([model], cfg)\", \"audio, _ = torchaudio.load('/path/to/an/audio/file')\", \"sample = S2THubInterface.get_model_input(task, audio)\", \"unit = S2THubInterface.get_prediction(task, model, generator, sample)\"], \"speech_synthesis\": [\"library_name = 'fairseq'\", \"cache_dir = (cache_dir or (Path.home() / '.cache' / library_name).as_posix())\", \"cache_dir = snapshot_download('facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS', cache_dir=cache_dir, library_name=library_name)\", \"x = hub_utils.from_pretrained(cache_dir, 'model.pt', '.', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml='config.json', fp16=False, is_vocoder=True)\", \"with open(f'{x['args']['data']}/config.json') as f:\", \"  vocoder_cfg = json.load(f)\", \"assert (len(x['args']['model_path']) == 1), 'Too many vocoder models in the input'\", \"vocoder = CodeHiFiGANVocoder(x['args']['model_path'][0], vocoder_cfg)\", \"tts_model = VocoderHubInterface(vocoder_cfg, vocoder)\", \"tts_sample = tts_model.get_model_input(unit)\", \"wav, sr = tts_model.get_prediction(tts_sample)\", \"ipd.Audio(wav, rate=sr)\"]}, \"performance\": {\"dataset\": \"MuST-C\", \"accuracy\": \"Not specified\"}, \"description\": \"Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.\"}}", "category": "generic"}
{"question_id": 460, "text": " We are conducting an analysis of Spanish voice samples in our call center. We want to classify the sentiment of the callers.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Classification\", \"api_name\": \"hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\", \"api_call\": \"Output: Wav2Vec2ForSequenceClassification.from_pretrained('hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD')\", \"api_arguments\": {\"model_name\": \"hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\"}, \"python_environment_requirements\": {\"transformers\": \"4.17.0\", \"pytorch\": \"1.10.0+cu111\", \"datasets\": \"2.0.0\", \"tokenizers\": \"0.11.6\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"MESD\", \"accuracy\": 0.9308}, \"description\": \"This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.\"}}", "category": "generic"}
{"question_id": 461, "text": " In our podcast platform, we want to detect voice activity so that we can separate speakers in the audio files.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Voice Activity Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Voice Activity Detection\", \"api_name\": \"popcornell/pyannote-segmentation-chime6-mixer6\", \"api_call\": \"Model.from_pretrained('popcornell/pyannote-segmentation-chime6-mixer6')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"pyannote.audio 2.1.1\", \"example_code\": \"from pyannote.audio import Model\\nmodel = Model.from_pretrained(popcornell/pyannote-segmentation-chime6-mixer6)\", \"performance\": {\"dataset\": \"ami\", \"accuracy\": \"N/A\"}, \"description\": \"Pyannote Segmentation model fine-tuned on data from CHiME-7 DASR Challenge. Used to perform diarization in the CHiME-7 DASR diarization baseline.\"}}", "category": "generic"}
{"question_id": 462, "text": " An environmental organization needs our help to estimate CO2 emissions using a dataset with characteristics of different plants and trees. \\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Joblib\", \"functionality\": \"Binary Classification\", \"api_name\": \"934630783\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\", \"json\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"rajistics/autotrain-data-Adult\", \"accuracy\": 0.8628221244500315}, \"description\": \"This model is trained for binary classification on the Adult dataset using AutoTrain. It is designed to predict CO2 emissions based on input features.\"}}", "category": "generic"}
{"question_id": 463, "text": " I want to estimate the amount of CO2 emission generated for different cars. Please provide me the code to analyze the data.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Tabular Classification\", \"api_name\": \"44534112235\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"datadmg/autotrain-data-test-news\", \"accuracy\": 0.333}, \"description\": \"This model is trained for Multi-class Classification on CO2 Emissions dataset. It uses the Hugging Face Transformers framework and is based on the extra_trees algorithm. The model is trained with AutoTrain and has a tabular classification functionality.\"}}", "category": "generic"}
{"question_id": 464, "text": " Predict the carbon emissions of a dataset of vehicles.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Hugging Face\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"49741119788\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"kochetkovIT/autotrain-data-ironhack\", \"accuracy\": {\"Loss\": 2.603, \"R2\": 0.013, \"MSE\": 6.776, \"MAE\": 1.666, \"RMSLE\": 0.502}}, \"description\": \"A tabular regression model trained using AutoTrain to predict carbon emissions (in grams) with an R2 score of 0.013.\"}}", "category": "generic"}
{"question_id": 465, "text": " Our vehicle production unit wants to predict CO2 emissions based on vehicle specifications, which regression model should we employ?\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Hugging Face\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"38507101578\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"wangdy/autotrain-data-godaddy2\", \"accuracy\": {\"Loss\": 0.206, \"R2\": 0.997, \"MSE\": 0.042, \"MAE\": 0.081, \"RMSLE\": 0.052}}, \"description\": \"This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\"}}", "category": "generic"}
{"question_id": 466, "text": " We are building an application to make the factory more eco-friendly. Estimate the carbon emissions of the factory using the given data.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"1860863627\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": {\"data\": \"data.csv\"}, \"python_environment_requirements\": {\"import json\": \"\", \"import joblib\": \"\", \"import pandas as pd\": \"\"}, \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"pcoloc/autotrain-data-dragino-7-7-max_495m\", \"accuracy\": {\"Loss\": 72.73, \"R2\": 0.386, \"MSE\": 5289.6, \"MAE\": 60.23, \"RMSLE\": 0.436}}, \"description\": \"A tabular regression model for predicting carbon emissions using the Joblib framework. Trained on the pcoloc/autotrain-data-dragino-7-7-max_495m dataset.\"}}", "category": "generic"}
{"question_id": 467, "text": " Identifying the best strategies for a game, we need a pre-trained reinforcement learning model with great performance.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"ppo-seals-CartPole-v0\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": \"python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/\", \"performance\": {\"dataset\": \"seals/CartPole-v0\", \"accuracy\": \"500.00 +/- 0.00\"}, \"description\": \"This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 468, "text": " We want to automate the process of learning a robotic arm to balance an inverted pendulum.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"Acrobot-v1\", \"api_name\": \"sb3/dqn-Acrobot-v1\", \"api_call\": \"No changes needed.\", \"api_arguments\": {\"algo\": \"dqn\", \"env\": \"Acrobot-v1\", \"logs_folder\": \"logs/\"}, \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": [\"python -m rl_zoo3.load_from_hub --algo dqn --env Acrobot-v1 -orga sb3 -f logs/\", \"python enjoy.py --algo dqn --env Acrobot-v1 -f logs/\", \"python train.py --algo dqn --env Acrobot-v1 -f logs/\", \"python -m rl_zoo3.push_to_hub --algo dqn --env Acrobot-v1 -f logs/ -orga sb3\"], \"performance\": {\"dataset\": \"Acrobot-v1\", \"accuracy\": \"-72.10 +/- 6.44\"}, \"description\": \"This is a trained model of a DQN agent playing Acrobot-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 469, "text": " A gaming company requires a trained AI model to effectively control game characters in a soccer match. Provide the suitable Reinforcement Learning API.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"QYHcrossover/poca-test\", \"api_call\": \"No changes needed. The `api_call` field does not include any model instantiation.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\"], \"example_code\": \"Go to https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos\\nStep 1: Write your model_id: QYHcrossover/poca-test\\nStep 2: Select your .nn /.onnx file\\nClick on Watch the agent play\", \"performance\": {\"dataset\": \"ML-Agents-SoccerTwos\", \"accuracy\": \"Not provided\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 470, "text": " Create a research aid tool that summarizes the research papers in a selected field for users.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", \"api_call\": \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", \"api_arguments\": \"input_ids, attention_mask\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", \"performance\": {\"dataset\": \"UMLS\", \"accuracy\": \"N/A\"}, \"description\": \"SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.\"}}", "category": "generic"}
{"question_id": 471, "text": " Analyze an image for the clothing style of the person in the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"facebook/dino-vitb16\", \"api_call\": \"ViTModel.from_pretrained('facebook/dino-vitb16')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/dino-vitb16\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"PIL\": \"latest\", \"requests\": \"latest\"}, \"example_code\": \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.\"}}", "category": "generic"}
{"question_id": 472, "text": " I am working on medical terms normalization project. Please find me an appropriate model for the task in English.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"GanjinZero/UMLSBert_ENG\", \"api_call\": \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER\"}}", "category": "generic"}
{"question_id": 473, "text": " We are working on designing some science fiction themed digital illustrations from text for an upcoming graphic novel.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"CompVis/stable-diffusion-v1-4\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"prompt\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"scipy\"], \"example_code\": \"import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\"}}", "category": "generic"}
{"question_id": 474, "text": " As a writer, I would like to have an AI tool to create an illustration for my recent sci-fi story that involves a futuristic city with flying cars.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"stabilityai/stable-diffusion-2-1\", \"api_call\": \"pipe(prompt).images[0]\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", \"performance\": {\"dataset\": \"COCO2017\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\"}}", "category": "generic"}
{"question_id": 475, "text": " Provide a smart tool that can generate a high-quality image of a lion in its natural habitat in the Serengeti, using natural light and high resolution.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"Realistic_Vision_V1.4\", \"api_call\": \"pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)\", \"api_arguments\": {\"prompt\": \"string\", \"negative_prompt\": \"string\"}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\\nresult = model(prompt, negative_prompt=negative_prompt)\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\"}}", "category": "generic"}
{"question_id": 476, "text": " A visually impaired user would like the AI to generate an image based on a written description. Implement a multimodal text-to-image pipeline to help the user.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"Linaqruf/anything-v3.0\", \"api_call\": \"pipeline('text-to-image', model='Linaqruf/anything-v3.0') should be rewritten as Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A text-to-image model that generates images from text descriptions.\"}}", "category": "generic"}
{"question_id": 477, "text": " Design a tool that can create an artistic image based on a provided description.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"Lykon/DreamShaper\", \"api_call\": \"pipeline('text-to-image', model=Lykon/DreamShaper)\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers, torch\", \"example_code\": \"https://huggingface.co/spaces/Lykon/DreamShaper-webui\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\"}}", "category": "generic"}
{"question_id": 478, "text": " I would like to create a tool that can convert Japanese manga text into English.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"kha-white/manga-ocr-base\", \"api_call\": \"pipeline('ocr', model='kha-white/manga-ocr-base').model\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"manga109s\", \"accuracy\": \"\"}, \"description\": \"Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\"}}", "category": "generic"}
{"question_id": 479, "text": " I am organizing a conference and need help with reading documents such as flyers, posters, and reports. The assistant should be capable of converting the text in the images to readable content.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-base-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 480, "text": " A tourist organization is using our software to give information about different places. We need to generate text descriptions based on the images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"blip2-opt-6.7b\", \"api_call\": \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", \"api_arguments\": \"image, optional text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"Refer to the documentation\", \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not specified\"}, \"description\": \"BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 481, "text": " Our company works with social media and we require a tool to describe images in a textual manner.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-textcaps\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", \"api_arguments\": \"image, text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 482, "text": " We have a video-based teaching platform. Come up with a way to create video content from written text.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"ImRma/Brucelee\", \"api_call\": \"pipeline('text-to-video', model='ImRma/Brucelee')\", \"api_arguments\": [\"your_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Hugging Face model for converting Persian and English text into video.\"}}", "category": "generic"}
{"question_id": 483, "text": " Develop a solution for a video advertising agency to generate short videos based on text descriptions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video Synthesis\", \"api_name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"api_call\": \"DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\", \"api_arguments\": [\"prompt\", \"num_inference_steps\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)\", \"performance\": {\"dataset\": [\"LAION5B\", \"ImageNet\", \"Webvid\"], \"accuracy\": \"Not provided\"}, \"description\": \"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\"}}", "category": "generic"}
{"question_id": 484, "text": " We have a large dataset of images and their descriptions, and we want to identify objects and actions within the images by asking questions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-textvqa\", \"api_call\": \"git_base_textvqa = AutoModel.from_pretrained('microsoft/git-base-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}}", "category": "generic"}
{"question_id": 485, "text": " Build an application combining computer vision and natural language processing to answer questions based on a given document.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"CZ_DVQA_layoutxlm-base\", \"api_call\": \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Document Question Answering model based on LayoutXLM.\"}}", "category": "generic"}
{"question_id": 486, "text": " Please generate possible questions and answers based on a legal document about carbon emissions.\\n###Input: \\\"<noinput>\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face\", \"functionality\": \"Document Question Answering\", \"api_name\": \"davanstrien/testwebhook\", \"api_call\": \"pipeline('question-answering')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"pile-of-law/pile-of-law\", \"accuracy\": \"\"}, \"description\": \"A model trained for answering questions related to legal documents and carbon emissions.\"}}", "category": "generic"}
{"question_id": 487, "text": " Explain how I can estimate the depth of an given image while processing it.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Monocular Depth Estimation\", \"api_name\": \"Intel/dpt-large\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"Intel/dpt-large\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\", \"performance\": {\"dataset\": \"MIX 6\", \"accuracy\": \"10.82\"}, \"description\": \"Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\"}}", "category": "generic"}
{"question_id": 488, "text": " We have a hotel management app that scans rooms with cameras. We need to figure out the depth of each room to organize 3D layouts.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu\", \"api_call\": \"'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)'\", \"api_arguments\": \"images, return_tensors\", \"python_environment_requirements\": [\"transformers\", \"torch\", \"numpy\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\", \"performance\": {\"dataset\": \"NYUv2\", \"accuracy\": \"Not provided\"}, \"description\": \"Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 489, "text": " We are working in the field of computer vision and need a model for depth estimation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221116-110652\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4018, \"Mae\": 0.3272, \"Rmse\": 0.4546, \"Abs Rel\": 0.3934, \"Log Mae\": 0.138, \"Log Rmse\": 0.1907, \"Delta1\": 0.4598, \"Delta2\": 0.7659, \"Delta3\": 0.9082}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.\"}}", "category": "generic"}
{"question_id": 490, "text": " Implement a depth estimation pipeline to analyze a given 3D image for potential health hazards.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221122-044810\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-044810')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, torch==1.12.1, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.369, \"Mae\": 0.2909, \"Rmse\": 0.4208, \"Abs Rel\": 0.3635, \"Log Mae\": 0.1224, \"Log Rmse\": 0.1793, \"Delta1\": 0.5323, \"Delta2\": 0.8179, \"Delta3\": 0.9258}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 491, "text": " As a clothing company, we need a system for identifying the types of clothing in images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/convnext-base-224\", \"api_call\": \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", \"api_arguments\": [\"image\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"datasets\"], \"example_code\": \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": null}, \"description\": \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\"}}", "category": "generic"}
{"question_id": 492, "text": " In our delivery business, we need assistance to categorize the items in the delivery pictures.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/beit-base-patch16-224\", \"api_call\": \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"microsoft/beit-base-patch16-224\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"ImageNet\", \"accuracy\": \"Refer to tables 1 and 2 of the original paper\"}, \"description\": \"BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224.\"}}", "category": "generic"}
{"question_id": 493, "text": " In our project, we want to create an AI system that could help us recognize different types of beans according to the images provided by our drone.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"fxmarty/resnet-tiny-beans\", \"api_call\": \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", \"api_arguments\": {\"model\": \"fxmarty/resnet-tiny-beans\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", \"performance\": {\"dataset\": \"beans\", \"accuracy\": \"Not provided\"}, \"description\": \"A model trained on the beans dataset, just for testing and having a really tiny model.\"}}", "category": "generic"}
{"question_id": 494, "text": " Write some code to help me process obtained tables through my mobile device and obtain the respective layout.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/table-transformer-structure-recognition\", \"api_call\": \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"PubTables1M\", \"accuracy\": \"\"}, \"description\": \"Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\"}}", "category": "generic"}
{"question_id": 495, "text": " A logistics company needs to build a system for detecting different types of packages in their warehouse. We need to train a system for object detection.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"hustvl/yolos-tiny\", \"api_call\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"28.7 AP\"}, \"description\": \"YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\"}}", "category": "generic"}
{"question_id": 496, "text": " We are developing a traffic monitoring system. We need to recognize traffic signs and vehicles in images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"facebook/detr-resnet-101-dc5\", \"api_call\": \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", \"api_arguments\": {\"image\": \"Image.open(requests.get(url, stream=True).raw)\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"AP 44.9\"}, \"description\": \"DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\"}}", "category": "generic"}
{"question_id": 497, "text": " A company specializing in creating virtual environments needs to segment an image to understand the different objects within it.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Semantic Segmentation\", \"api_name\": \"nvidia/segformer-b0-finetuned-ade-512-512\", \"api_call\": \"'SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)'\", \"api_arguments\": {\"images\": \"Image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"SegformerImageProcessor, SegformerForSemanticSegmentation\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": \"from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nprocessor = SegformerImageProcessor.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 498, "text": " I have an image of a park, and I want to know the type of vegetation that covers the largest area.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"nvidia/segformer-b5-finetuned-ade-640-640\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"ADE20K\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 499, "text": " Our team is working on a city planning project and needs a tool to segment the different components of some aerial images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"shi-labs/oneformer_ade20k_swin_large\", \"api_call\": \"OneFormerForUniversalSegmentation.from_pretrained(\\\\shi-labs/oneformer_ade20k_swin_large\\\\)\", \"api_arguments\": [\"images\", \"task_inputs\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"scene_parse_150\", \"accuracy\": null}, \"description\": \"OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\"}}", "category": "generic"}
{"question_id": 500, "text": " A photography client has requested some variations of their image. You need to generate enhancements.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Variations\", \"api_name\": \"lambdalabs/sd-image-variations-diffusers\", \"api_call\": \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", \"api_arguments\": {\"revision\": \"v2.0\"}, \"python_environment_requirements\": \"Diffusers >=0.8.0\", \"example_code\": \"from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)\", \"performance\": {\"dataset\": \"ChristophSchuhmann/improved_aesthetics_6plus\", \"accuracy\": \"N/A\"}, \"description\": \"This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 501, "text": " I'm an artist, wanting to sketch something based on a word or phrase I provide. Generate an image based on my input.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Diffusion Models\", \"api_name\": \"lllyasviel/sd-controlnet-scribble\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\", \"api_arguments\": [\"image\", \"text\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux\"], \"example_code\": \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", \"performance\": {\"dataset\": \"500k scribble-image, caption pairs\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 502, "text": " I am an artist, I need a tool to generate a picture of sheep in a new artistic style.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"GreeneryScenery/SheepsControlV5\", \"api_call\": \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\", \"api_arguments\": {\"input_image\": \"path/to/image/file\"}, \"python_environment_requirements\": {\"huggingface_hub\": \">=0.0.17\", \"transformers\": \">=4.13.0\", \"torch\": \">=1.10.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"poloclub/diffusiondb\", \"accuracy\": \"Not provided\"}, \"description\": \"SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\"}}", "category": "generic"}
{"question_id": 503, "text": " Our design team is working on a bathroom wall picture and they need the picture without the previously used watermark.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Inpainting\", \"api_name\": \"lllyasviel/control_v11p_sd15_inpaint\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_inpaint\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": \"pip install diffusers transformers accelerate\", \"example_code\": \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", \"performance\": {\"dataset\": \"Stable Diffusion v1-5\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\"}}", "category": "generic"}
{"question_id": 504, "text": " A game environment company has requested images of randomly generated churches. Please provide these images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ddpm-church-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.\"}}", "category": "generic"}
{"question_id": 505, "text": " Create a computer-generated artwork of a cosmic landscape using unconditional image generation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ocariz/universe_1400\", \"api_call\": \"DDPMPipeline.from_pretrained('ocariz/universe_1400')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs.\"}}", "category": "generic"}
{"question_id": 506, "text": " Our team is working on a project to create an app that generates random images of cute animals as avatars for new users without providing any conditions.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"sd-class-pandas-32\", \"api_call\": \"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\", \"api_arguments\": {\"pretrained_model\": \"schdoel/sd-class-AFHQ-32\"}, \"python_environment_requirements\": {\"package\": \"diffusers\", \"import\": \"from diffusers import DDPMPipeline\"}, \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"AFHQ\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute \\ud83e\\udd8b.\"}}", "category": "generic"}
{"question_id": 507, "text": " We need to process videos for our new AI video content generation platform. Please provide the necessary tools for video classification.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base)\\npixel_values = processor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"To be provided\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.\"}}", "category": "generic"}
{"question_id": 508, "text": " We are building a fitness app that can recognize human actions by processing short video clips.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-hr-finetuned-k400\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 448, 448))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\n  logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not specified\"}, \"description\": \"TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al.\"}}", "category": "generic"}
{"question_id": 509, "text": " The advertising team needs a tool to automatically recognize different types of video content for ad placements. \\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-hr-finetuned-ssv2\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 448, 448))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-ssv2)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-ssv2)\\ninputs = feature_extractor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something Something v2\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 510, "text": " Our team received a video and we didn't understand its content. We would like to classify the video by checking if it belongs to any known categories or has any hidden meanings.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base-finetuned-ssv2\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\", \"api_arguments\": \"video\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n    logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": {\"top-1\": 70.6, \"top-5\": 92.6}}, \"description\": \"VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 511, "text": " Please classify an accident in a given video clip.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset')\", \"api_arguments\": {\"learning_rate\": 5e-05, \"train_batch_size\": 8, \"eval_batch_size\": 8, \"seed\": 42, \"optimizer\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\", \"lr_scheduler_type\": \"linear\", \"lr_scheduler_warmup_ratio\": 0.1, \"training_steps\": 111}, \"python_environment_requirements\": {\"transformers\": \"4.24.0\", \"pytorch\": \"1.12.1+cu113\", \"datasets\": \"2.6.1\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 1.0}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 512, "text": " As a pharmaceutical company, we get thousands of drug images every day. We need a model to classify those images into different drug categories.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": 75.3}, \"description\": \"A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model.\"}}", "category": "generic"}
{"question_id": 513, "text": " I would like to know which category my dog belongs to by analyzing its image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k\", \"api_call\": \"clip.load('timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k')\", \"api_arguments\": \"image, class_names\", \"python_environment_requirements\": \"huggingface_hub, openai, transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"This model is a zero-shot image classification model based on OpenCLIP. It can be used for classifying images into various categories without any additional training.\"}}", "category": "generic"}
{"question_id": 514, "text": " We have a picture and want to sort it into the potentially relevant categories: cat, dog, or fish.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\", \"api_arguments\": \"image, class_names\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; clip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'); clip(image, class_names=['cat', 'dog', 'fish'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"79.1 - 79.4\"}, \"description\": \"A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}", "category": "generic"}
{"question_id": 515, "text": " Analyze an image for a maintenance company to identify possible issues with equipment in the facility.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\", \"api_arguments\": {\"image_path\": \"Path to the image file\", \"class_names\": \"List of comma-separated class names\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'); classifier('path/to/image.jpg', ['class1', 'class2'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"75.9-76.9%\"}, \"description\": \"A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 516, "text": " I am developing an application in Korean for a local fashion store where customers upload their pictures of fashion items and the app identifies them.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"clip-vit-base-patch32-ko\", \"api_call\": \"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\", \"api_arguments\": {\"images\": \"url\", \"candidate_labels\": \"Array of strings\", \"hypothesis_template\": \"String\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import pipeline\\nrepo = 'Bingsu/clip-vit-base-patch32-ko'\\npipe = pipeline('zero-shot-image-classification', model=repo)\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nresult = pipe(images=url, candidate_labels=['\\uace0\\uc591\\uc774 \\ud55c \\ub9c8\\ub9ac', '\\uace0\\uc591\\uc774 \\ub450 \\ub9c8\\ub9ac', '\\ubd84\\ud64d\\uc0c9 \\uc18c\\ud30c\\uc5d0 \\ub4dc\\ub7ec\\ub204\\uc6b4 \\uace0\\uc591\\uc774 \\uce5c\\uad6c\\ub4e4'], hypothesis_template='{}')\\nresult\", \"performance\": {\"dataset\": \"AIHUB\", \"accuracy\": \"Not provided\"}, \"description\": \"Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data.\"}}", "category": "generic"}
{"question_id": 517, "text": " As a stock trader, I receive many stock-related comments every day. I need a model to help me understand the sentiment for these comments.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Inferencing for stock-related comments\", \"api_name\": \"zhayunduo/roberta-base-stocktwits-finetuned\", \"api_call\": \"Output: RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\", \"api_arguments\": {\"model\": \"RobertaForSequenceClassification\", \"tokenizer\": \"RobertaTokenizer\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\nfrom transformers import pipeline\\nimport pandas as pd\\nimport emoji\\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\nsentences = pd.Series(['just buy','just sell it','entity rocket to the sky!','go down','even though it is going up, I still think it will not keep this trend in the near future'])\\nsentences = list(sentences)\\nresults = nlp(sentences)\\nprint(results)\", \"performance\": {\"dataset\": \"stocktwits\", \"accuracy\": 0.9343}, \"description\": \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'.\"}}", "category": "generic"}
{"question_id": 518, "text": " Recommend to me the most relevant search result from a list of passages for my research question about the population of Berlin.\\n###Input: {\\\"question\\\": \\\"What is the population of Berlin?\\\", \\\"possible_passages\\\": [\\\"Berlin is the capital and the largest city of Germany by both area and population. Its population is approximately 3.7 million.\\\", \\\"Berlin enjoys a very rich cultural scene, featuring numerous museums, art galleries, and historical monuments.\\\", \\\"New York City is one of the most populous cities in the United States, with over 8 million residents.\\\"]}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Information Retrieval\", \"api_name\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\", \"api_arguments\": {\"model_name\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\nimport torch\\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\\ntokenizer = AutoTokenizer.from_pretrained('model_name')\\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\\nmodel.eval()\\nwith torch.no_grad():\\n scores = model(**features).logits\\n print(scores)\", \"performance\": {\"dataset\": \"MS Marco Passage Reranking\", \"accuracy\": \"MRR@10: 39.01%\"}, \"description\": \"This model was trained on the MS Marco Passage Ranking task and can be used for Information Retrieval. Given a query, encode the query with all possible passages, then sort the passages in a decreasing order.\"}}", "category": "generic"}
{"question_id": 519, "text": " A company analyses online reviews of its products, and they need an auto-generated sentiment score ranging from Positive, Negative to Neutral to understand their customers' feedback.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"finiteautomata/beto-sentiment-analysis\", \"api_call\": \"pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"Hugging Face Transformers library\", \"example_code\": \"\", \"performance\": {\"dataset\": \"TASS 2020 corpus\", \"accuracy\": \"\"}, \"description\": \"Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\"}}", "category": "generic"}
{"question_id": 520, "text": " Our company is a pharmaceutical firm. We want to analyze the keywords in a research articles about cancer drugs.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"d4data/biomedical-ner-all\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)\", \"aggregation_strategy\": \"simple\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\"}, \"example_code\": \"pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)\", \"performance\": {\"dataset\": \"Maccrobat\", \"accuracy\": \"Not provided\"}, \"description\": \"An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.\"}}", "category": "generic"}
{"question_id": 521, "text": " Our company is interested in answering queries related to a table, and we need to select a suitable Table Question Answering model to use.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-mini-finetuned-wtq\", \"api_call\": \"AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\", \"api_arguments\": \"tokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq'); model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('table-question-answering', model='google/tapas-mini-finetuned-wtq', tokenizer='google/tapas-mini-finetuned-wtq')\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": 0.2854}, \"description\": \"TAPAS mini model fine-tuned on WikiTable Questions (WTQ). It is pretrained on a large corpus of English data from Wikipedia and can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 522, "text": " We have a CSV file available with sales data for last year. We want to query sales information for multiple products.\\n###Input: [{\\\"Product\\\":\\\"Monitor\\\", \\\"Region\\\":\\\"Europe\\\", \\\"Units Sold\\\":\\\"550\\\", \\\"Revenue\\\":\\\"180000\\\"}, {\\\"Product\\\":\\\"Laptop\\\", \\\"Region\\\":\\\"Asia\\\", \\\"Units Sold\\\":\\\"750\\\", \\\"Revenue\\\":\\\"240000\\\"}, {\\\"Product\\\":\\\"Keyboard\\\", \\\"Region\\\":\\\"North America\\\", \\\"Units Sold\\\":\\\"1200\\\", \\\"Revenue\\\":\\\"60000\\\"}]\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"lysandre/tapas-temporary-repo\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\", \"api_arguments\": [\"model_name\", \"question\", \"table\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\noutputs = model(**inputs)\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\", \"performance\": {\"dataset\": \"SQA\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.\"}}", "category": "generic"}
{"question_id": 523, "text": " I am designing an application to boost productivity. I want to include a feature that can answer questions based on text given by the user.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"distilbert-base-cased-distilled-squad\", \"api_call\": \"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nquestion_answerer = pipeline(question-answering, model='distilbert-base-cased-distilled-squad')\\ncontext = r\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\n... \\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\nprint(\\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\\n...)\", \"performance\": {\"dataset\": \"SQuAD v1.1\", \"accuracy\": {\"Exact Match\": 79.6, \"F1\": 86.996}}, \"description\": \"DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.\"}}", "category": "generic"}
{"question_id": 524, "text": " We'd like to find answers in a Korean language document by a querist who needs information about policies and programs of the government.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"monologg/koelectra-small-v2-distilled-korquad-384\", \"api_call\": \"pipeline('question-answering', model='monologg/koelectra-small-v2-distilled-korquad-384')\", \"api_arguments\": {\"model\": \"monologg/koelectra-small-v2-distilled-korquad-384\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"nlp(question='your_question', context='your_context')\", \"performance\": {\"dataset\": \"KorQuAD\", \"accuracy\": \"Not provided\"}, \"description\": \"A Korean Question Answering model based on Electra and trained on the KorQuAD dataset.\"}}", "category": "generic"}
{"question_id": 525, "text": " The customer requires a question answer pipeline that works on an example in which context and questions are given.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"philschmid/distilbert-onnx\", \"api_call\": \"pipeline('question-answering', model='philschmid/distilbert-onnx')\", \"api_arguments\": {\"model\": \"philschmid/distilbert-onnx\"}, \"python_environment_requirements\": [\"transformers\", \"onnx\"], \"example_code\": {\"Compute\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\\nqa_pipeline({'context': 'This is a context', 'question': 'What is this?'})\"}, \"performance\": {\"dataset\": \"squad\", \"accuracy\": \"F1 score: 87.1\"}, \"description\": \"This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.\"}}", "category": "generic"}
{"question_id": 526, "text": " I am looking for a personal assistant for helping me making a quote in investing in China stock market. First, we need to find how many companies are listed in Shanghai Stock Exchange.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\", \"api_call\": \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large.from_pretrained()\", \"api_arguments\": \"context, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\", \"performance\": {\"dataset\": \"Dureader-2021\", \"accuracy\": \"83.1\"}, \"description\": \"A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition.\"}}", "category": "generic"}
{"question_id": 527, "text": " Develop a recommendation engine using a model to understand if a given sentence is similar to another sentence provided (semantic similarity).\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"cross-encoder/nli-deberta-v3-small\", \"api_call\": \"CrossEncoder('cross-encoder/nli-deberta-v3-small')\", \"api_arguments\": [\"sentence1\", \"sentence2\"], \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from sentence_transformers import CrossEncoder\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-small')\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\", \"performance\": {\"dataset\": {\"SNLI-test\": \"91.65\", \"MNLI-mismatched\": \"87.55\"}, \"accuracy\": {\"SNLI-test\": \"91.65\", \"MNLI-mismatched\": \"87.55\"}}, \"description\": \"Cross-Encoder for Natural Language Inference based on microsoft/deberta-v3-small, trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\"}}", "category": "generic"}
{"question_id": 528, "text": " We need to evaluate the similarity of the following pair of sentences: \\\"A child is sitting on a swing in the park\\\" and \\\"A person is standing in a park putting their hands on their hips\\\".\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Cross-Encoder for Natural Language Inference\", \"api_name\": \"cross-encoder/nli-deberta-v3-base\", \"api_call\": \"CrossEncoder('cross-encoder/nli-deberta-v3-base')\", \"api_arguments\": [\"sentence_pairs\"], \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from sentence_transformers import CrossEncoder\\nmodel = CrossEncoder('cross-encoder/nli-deberta-v3-base')\\nscores = model.predict([('A man is eating pizza', 'A man eats something'), ('A black race car starts up in front of a crowd of people.', 'A man is driving down a lonely road.')])\", \"performance\": {\"dataset\": {\"SNLI-test\": \"92.38\", \"MNLI mismatched set\": \"90.04\"}}, \"description\": \"This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\"}}", "category": "generic"}
{"question_id": 529, "text": " We are designing a translator software for tourists. They will be able to translate their English sentences into French.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-en-fr\", \"api_call\": \"translate('input_text', model='Helsinki-NLP/opus-mt-en-fr')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newsdiscussdev2015-enfr.en.fr\": 33.8, \"newsdiscusstest2015-enfr.en.fr\": 40.0, \"newssyscomb2009.en.fr\": 29.8, \"news-test2008.en.fr\": 27.5, \"newstest2009.en.fr\": 29.4, \"newstest2010.en.fr\": 32.7, \"newstest2011.en.fr\": 34.3, \"newstest2012.en.fr\": 31.8, \"newstest2013.en.fr\": 33.2, \"Tatoeba.en.fr\": 50.5}}}, \"description\": \"Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\"}}", "category": "generic"}
{"question_id": 530, "text": " The team is researching a new diet called \\\"intermittent fasting\\\". They need a summary of the latest article about intermittent fasting they just found.\\n###Input: \\\"Intermittent fasting is a popular dietary trend that involves cycling between periods of fasting and eating. The most common methods include the 16:8 method, which involves fasting for 16 hours a day and eating during an eight-hour window, and the 5:2 method, which involves eating normally for five days a week and restricting calorie intake to 500-600 calories for the remaining two days. Advocates of intermittent fasting claim that it can lead to weight loss, improved metabolism, and increased energy levels. Some research supports these claims, with studies showing that intermittent fasting can help individuals lose weight, improve insulin sensitivity, and lower inflammation levels. However, not all experts agree on the benefits of intermittent fasting, as some argue that it can be difficult to adhere to for extended periods of time and may lead to disordered eating patterns or nutritional deficiencies. It is essential for individuals considering intermittent fasting to consult with a healthcare professional before making significant changes to their diet.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation, Summarization, Question Answering, Sentiment Analysis, Regression\", \"api_name\": \"t5-large\", \"api_call\": \"T5Model.from_pretrained('t5-large')\", \"api_arguments\": {\"input_ids\": \"tokenizer(..., return_tensors='pt').input_ids\", \"decoder_input_ids\": \"tokenizer(..., return_tensors='pt').input_ids\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import T5Tokenizer, T5Model\"}, \"example_code\": \"tokenizer = T5Tokenizer.from_pretrained('t5-large')\\nmodel = T5Model.from_pretrained('t5-large')\\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"See research paper, Table 14\"}, \"description\": \"T5-Large is a Text-To-Text Transfer Transformer (T5) model with 770 million parameters. It is designed to handle a variety of NLP tasks, including translation, summarization, question answering, sentiment analysis, and regression. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on various supervised and unsupervised tasks.\"}}", "category": "generic"}
{"question_id": 531, "text": " Provide a method to translate a given Spanish text to English.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-es-en\", \"api_call\": \"pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en')('Hola, o est?')\", \"performance\": {\"dataset\": [{\"name\": \"newssyscomb2009-spaeng.spa.eng\", \"accuracy\": {\"BLEU\": 30.6, \"chr-F\": 0.57}}, {\"name\": \"news-test2008-spaeng.spa.eng\", \"accuracy\": {\"BLEU\": 27.9, \"chr-F\": 0.553}}, {\"name\": \"newstest2009-spaeng.spa.eng\", \"accuracy\": {\"BLEU\": 30.4, \"chr-F\": 0.572}}, {\"name\": \"newstest2010-spaeng.spa.eng\", \"accuracy\": {\"BLEU\": 36.1, \"chr-F\": 0.614}}, {\"name\": \"newstest2011-spaeng.spa.eng\", \"accuracy\": {\"BLEU\": 34.2, \"chr-F\": 0.599}}, {\"name\": \"newstest2012-spaeng.spa.eng\", \"accuracy\": {\"BLEU\": 37.9, \"chr-F\": 0.624}}, {\"name\": \"newstest2013-spaeng.spa.eng\", \"accuracy\": {\"BLEU\": 35.3, \"chr-F\": 0.609}}, {\"name\": \"Tatoeba-test.spa.eng\", \"accuracy\": {\"BLEU\": 59.6, \"chr-F\": 0.739}}]}, \"description\": \"Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset.\"}}", "category": "generic"}
{"question_id": 532, "text": " Please convert a message from Italian to English.\\n###Input: \\\"Ciao! Come stai? Ho sentito che il tuo viaggio stato fantastico.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-it-en\", \"api_call\": \"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')('Ciao mondo!')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newssyscomb2009.it.en\": 35.3, \"newstest2009.it.en\": 34.0, \"Tatoeba.it.en\": 70.9}, \"chr-F\": {\"newssyscomb2009.it.en\": 0.6, \"newstest2009.it.en\": 0.594, \"Tatoeba.it.en\": 0.808}}}, \"description\": \"A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\"}}", "category": "generic"}
{"question_id": 533, "text": " Our mobile app includes a chat feature, and we want to translate user messages from German to Spanish.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-de-es\", \"api_call\": \"Helsinki-NLP/opus-mt-de-es\", \"api_arguments\": {\"text\": \"string\"}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_de_to_es', model='Helsinki-NLP/opus-mt-de-es')\\ntranslated_text = translation('Guten Tag')\", \"performance\": {\"dataset\": \"Tatoeba.de.es\", \"accuracy\": {\"BLEU\": 48.5, \"chr-F\": 0.676}}, \"description\": \"A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization.\"}}", "category": "generic"}
{"question_id": 534, "text": " We have a Swedish company and for our international expansion, we are in need of a solution that can help us to send English translated emails to our clients.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-sv-en\", \"api_call\": \"AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').\", \"api_arguments\": [\"inputs\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"Tatoeba.sv.en\", \"accuracy\": \"BLEU: 64.5, chr-F: 0.763\"}, \"description\": \"A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece.\"}}", "category": "generic"}
{"question_id": 535, "text": " Can you help us come up with a brief summary of an input passage for our busy editor?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"sshleifer/distilbart-cnn-12-6\", \"api_call\": \"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"huggingface/transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": [{\"name\": \"cnn_dailymail\", \"accuracy\": {\"Rouge 2\": \"22.12\", \"Rouge-L\": \"36.99\"}}]}, \"description\": \"DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, 'sshleifer/distilbart-cnn-12-6', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 536, "text": " We are building a chatting product that translates the user's feed written in French into Spanish to reach users in Latin America.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-fr-es\", \"api_call\": \"Helsinki-NLP/opus-mt-fr-es\", \"api_arguments\": {\"source_languages\": \"fr\", \"target_languages\": \"es\"}, \"python_environment_requirements\": {\"PyTorch\": \"1.0.0\", \"TensorFlow\": \"2.0\", \"Transformers\": \"4.0.0\"}, \"example_code\": \"translation('Bonjour, comment  va?')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newssyscomb2009.fr.es\": 34.3, \"news-test2008.fr.es\": 32.5, \"newstest2009.fr.es\": 31.6, \"newstest2010.fr.es\": 36.5, \"newstest2011.fr.es\": 38.3, \"newstest2012.fr.es\": 38.1, \"newstest2013.fr.es\": 34.0, \"Tatoeba.fr.es\": 53.2}, \"chr-F\": {\"newssyscomb2009.fr.es\": 0.601, \"news-test2008.fr.es\": 0.583, \"newstest2009.fr.es\": 0.586, \"newstest2010.fr.es\": 0.616, \"newstest2011.fr.es\": 0.622, \"newstest2012.fr.es\": 0.619, \"newstest2013.fr.es\": 0.587, \"Tatoeba.fr.es\": 0.709}}}, \"description\": \"A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.\"}}", "category": "generic"}
{"question_id": 537, "text": " Our client is looking for an AI chatbot. Can you create one for us?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Conversational\", \"api_name\": \"facebook/blenderbot-400M-distill\", \"api_call\": \"BlenderbotForConditionalGeneration.from_pretrained('facebook/blenderbot-400M-distill')\", \"api_arguments\": \"['message']\", \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"Input a message to start chatting with facebook/blenderbot-400M-distill.\", \"performance\": {\"dataset\": \"blended_skill_talk\", \"accuracy\": \"Not specified\"}, \"description\": \"BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.\"}}", "category": "generic"}
{"question_id": 538, "text": " The company needs a way to improve customer support. We want a conversational agent to talk to the users and provide them with helpful information.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face\", \"functionality\": \"Dialogue Response Generation\", \"api_name\": \"microsoft/DialoGPT-small\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small').generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\", \"api_arguments\": [\"bot_input_ids\", \"max_length\", \"pad_token_id\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-small)\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-small)\\nfor step in range(5):\\n new_user_input_ids = tokenizer.encode(input(>> User:) + tokenizer.eos_token, return_tensors='pt')\\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\", \"performance\": {\"dataset\": \"Reddit discussion thread\", \"accuracy\": \"Comparable to human response quality under a single-turn conversation Turing test\"}, \"description\": \"DialoGPT is a state-of-the-art large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.\"}}", "category": "generic"}
{"question_id": 539, "text": " We want to build a virtual travel advisor assistant that would participate in a conversation with its client.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"DialoGPT-large\", \"api_call\": \"model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\", \"api_arguments\": [\"bot_input_ids\", \"max_length\", \"pad_token_id\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-large)\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-large)\\nfor step in range(5):\\n new_user_input_ids = tokenizer.encode(input(&gt;&gt; User:) + tokenizer.eos_token, return_tensors='pt')\\n bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids\\n chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\n print(DialoGPT: {}.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\", \"performance\": {\"dataset\": \"Reddit discussion thread\", \"accuracy\": \"Comparable to human response quality under a single-turn conversation Turing test\"}, \"description\": \"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.\"}}", "category": "generic"}
{"question_id": 540, "text": " Our customer needs a brief overview of how artificial intelligence has evolved over the years.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"bigscience/bloom-560m\", \"api_call\": \"pipeline('text-generation', model='bigscience/bloom-560m')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\nmodel_name = 'bigscience/bloom-560m'\\napi = pipeline('text-generation', model=model_name)\\ntext = 'The history of artificial intelligence began in the '\\noutput = api(text)\\nprint(output[0]['generated_text'])\", \"performance\": {\"dataset\": \"Validation\", \"accuracy\": {\"Training Loss\": 2.0, \"Validation Loss\": 2.2, \"Perplexity\": 8.9}}, \"description\": \"BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads.\"}}", "category": "generic"}
{"question_id": 541, "text": " We are developing a content-creation tool which can be used by users to generate academic articles on various topics.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"EleutherAI/gpt-j-6B\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-j-6B')\", \"api_arguments\": {\"pretrained_model\": \"EleutherAI/gpt-j-6B\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForCausalLM\"}, \"example_code\": {\"loading_model\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(EleutherAI/gpt-j-6B)\\nmodel = AutoModelForCausalLM.from_pretrained(EleutherAI/gpt-j-6B)\"}, \"performance\": {\"dataset\": \"the_pile\", \"accuracy\": {\"LAMBADA_PPL\": 3.99, \"LAMBADA_Acc\": \"69.7%\", \"Winogrande\": \"65.3%\", \"Hellaswag\": \"66.1%\", \"PIQA\": \"76.5%\"}}, \"description\": \"GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. It consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3. GPT-J 6B was trained on the Pile, a large-scale curated dataset created by EleutherAI.\"}}", "category": "generic"}
{"question_id": 542, "text": " Consdier the type of job you have. Generate a fictional story about a language model that comes to life.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"sshleifer/tiny-gpt2\", \"api_call\": \"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\", \"api_arguments\": {\"model\": \"sshleifer/tiny-gpt2\"}, \"python_environment_requirements\": {\"huggingface_transformers\": \">=4.0.0\"}, \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\nresult = nlp('Once upon a time')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.\"}}", "category": "generic"}
{"question_id": 543, "text": " Our company aims to generate conversational text based on provided prompts.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"facebook/opt-13b\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('facebook/opt-13b', torch_dtype=torch.float16).cuda().generate(input_ids)\", \"api_arguments\": [\"input_ids\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-13b, torch_dtype=torch.float16).cuda()\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-13b, use_fast=False)\\nprompt = Hello, I'm am conscious and\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\ngenerated_ids = model.generate(input_ids)\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\", \"performance\": {\"dataset\": \"GPT-3\", \"accuracy\": \"roughly match the performance and sizes of the GPT-3 class of models\"}, \"description\": \"OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The models are trained to match the performance and sizes of the GPT-3 class of models. The primary goal is to enable reproducible and responsible research at scale and to bring more voices to the table in studying the impact of large language models. OPT-13B is a 13-billion-parameter model trained predominantly with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\"}}", "category": "generic"}
{"question_id": 544, "text": " As a language learning platform, we want to offer a feature where users can ask for translations of sentences from their native language into the language they are trying to learn.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text-to-Text Transfer Transformer\", \"api_name\": \"google/mt5-base\", \"api_call\": \"MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\", \"api_arguments\": [\"model_name\", \"input_text\", \"generated_text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\\ntokenizer = MT5Tokenizer.from_pretrained('google/mt5-base')\\ninputs = tokenizer.encode('translate English to German: The house is wonderful.', return_tensors='pt')\\noutputs = model.generate(inputs, max_length=40, num_return_sequences=1)\\ndecoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"mc4\", \"accuracy\": \"Not provided\"}, \"description\": \"mT5 is a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. It leverages a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of multilingual NLP tasks.\"}}", "category": "generic"}
{"question_id": 545, "text": " I want a ready-to-use NLP model to complete this sentence: \\\"Today, I went to the [MASK] to buy some groceries.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling\", \"api_name\": \"bert-base-cased\", \"api_call\": \"pipeline('fill-mask', model='bert-base-cased')\", \"api_arguments\": [\"model\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": [\"from transformers import pipeline\", \"unmasker = pipeline('fill-mask', model='bert-base-cased')\", \"unmasker(Hello I'm a [MASK] model.)\"], \"performance\": {\"dataset\": \"GLUE\", \"accuracy\": 79.6}, \"description\": \"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\"}}", "category": "generic"}
{"question_id": 546, "text": " I am working on a chatbot that is supposed to have knowledge about geography. One question it should be able to answer is, what is the capital city of France?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"microsoft/deberta-base\", \"api_call\": \"DebertaModel.from_pretrained('microsoft/deberta-base')\", \"api_arguments\": \"text\", \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\nfill_mask = pipeline('fill-mask', model='microsoft/deberta-base')\\nfill_mask('The capital of France is [MASK].')\", \"performance\": {\"dataset\": {\"SQuAD 1.1\": \"93.1/87.2\", \"SQuAD 2.0\": \"86.2/83.1\", \"MNLI-m\": \"88.8\"}}, \"description\": \"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data.\"}}", "category": "generic"}
{"question_id": 547, "text": " Analyze the following user comments and compare their similarity to find out which comments have the closest meaning.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Embeddings\", \"api_name\": \"sentence-transformers/bert-base-nli-mean-tokens\", \"api_call\": \"SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 548, "text": " I want to assess the similarity between various sentences to find related content.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"nikcheerla/nooks-amd-detection-realtime\", \"api_call\": \"Output: SentenceTransformer('nikcheerla/nooks-amd-detection-realtime')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('{MODEL_NAME}')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Automated evaluation\"}, \"description\": \"This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 549, "text": " Can you help me find the most relevant document among these three Chinese sentences about my business proposal?\\n###Input: {\\\"source_sentence\\\": \\\"\\u6211\\u6b63\\u5728\\u4e3a\\u4e00\\u5bb6\\u65b0\\u7684\\u79d1\\u6280\\u516c\\u53f8\\u5236\\u5b9a\\u4e1a\\u52a1\\u8ba1\\u5212\\u3002\\\",  \\\"sentences_to_compare\\\": [\\\"\\u5173\\u4e8e\\u65b0\\u516c\\u53f8\\u7684\\u4e1a\\u52a1\\u8ba1\\u5212\\u3002\\\", \\\"\\u79d1\\u6280\\u884c\\u4e1a\\u7684\\u53d1\\u5c55\\u8d8b\\u52bf\\u3002\\\", \\\"\\u7b80\\u4ecb\\u53ca\\u4e86\\u89e3\\u516c\\u53f8\\u3002\\\"]}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"text2vec-large-chinese\", \"api_call\": \"AutoModel.from_pretrained('GanymedeNil/text2vec-large-chinese')\", \"api_arguments\": \"source_sentence, sentences_to_compare\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import AutoModel, AutoTokenizer\\ntokenizer = AutoTokenizer.from_pretrained('GanymedeNil/text2vec-large-chinese')\\nmodel = AutoModel.from_pretrained('GanymedeNil/text2vec-large-chinese')\", \"performance\": {\"dataset\": \"https://huggingface.co/shibing624/text2vec-base-chinese\", \"accuracy\": \"Not provided\"}, \"description\": \"A Chinese sentence similarity model based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replacing MacBERT with LERT, and keeping other training conditions unchanged.\"}}", "category": "generic"}
{"question_id": 550, "text": " We are a library, and we want to find the similarity between two sentences about books.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/all-roberta-large-v1\", \"api_call\": \"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Automated evaluation\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 551, "text": " An AI recommended me an Indian male voice for reading Marathi text loudly. I will use Text-to-Speech conversion for this purpose.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"SYSPIN/Marathi_Male_TTS\", \"api_call\": \"api.load('ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS').\", \"api_arguments\": [], \"python_environment_requirements\": [\"huggingface_hub\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Marathi Male Text-to-Speech model using ESPnet framework.\"}}", "category": "generic"}
{"question_id": 552, "text": " Convert a text written in the Telugu language to an audio file, spoken by a male voice.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"SYSPIN/Telugu_Male_TTS\", \"api_call\": \"pipeline('text-to-speech', model='SYSPIN/Telugu_Male_TTS')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.\"}}", "category": "generic"}
{"question_id": 553, "text": " Our team builds a mobile application that reads out news articles. We want to include audio output from text read by a text-to-speech engine.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan\", \"api_call\": \"espnet/kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"LJSpeech\", \"accuracy\": \"\"}, \"description\": \"A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset. This model is capable of converting text input into synthesized speech.\"}}", "category": "generic"}
{"question_id": 554, "text": " Write an email for a marketing campaign and create an audio file to check its effectiveness.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"Fairseq\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"fastspeech2-en-ljspeech\", \"api_call\": \"Output: models[0]\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"fairseq\", \"IPython\"], \"example_code\": \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\nimport IPython.display as ipd\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\n facebook/fastspeech2-en-ljspeech,\\n arg_overrides={vocoder: hifigan, fp16: False}\\n)\\nmodel = models[0]\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\ngenerator = task.build_generator(model, cfg)\\ntext = Hello, this is a test run.\\nsample = TTSHubInterface.get_model_input(task, text)\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\nipd.Audio(wav, rate=rate)\", \"performance\": {\"dataset\": \"LJSpeech\", \"accuracy\": \"Not provided\"}, \"description\": \"FastSpeech 2 text-to-speech model from fairseq S^2. English single-speaker female voice trained on LJSpeech dataset.\"}}", "category": "generic"}
{"question_id": 555, "text": " Our company is developing an app for learning Dutch. We would like to implement a feature to auto-convert the user's speech into Dutch text.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Recognition\", \"api_name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-dutch\", \"api_call\": \"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\", \"api_arguments\": [\"audio_paths\"], \"python_environment_requirements\": [\"huggingsound\", \"torch\", \"librosa\", \"datasets\", \"transformers\"], \"example_code\": \"from huggingsound import SpeechRecognitionModel\\nmodel = SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-dutch')\\naudio_paths = ['/path/to/file.mp3', '/path/to/another_file.wav']\\ntranscriptions = model.transcribe(audio_paths)\", \"performance\": {\"dataset\": \"Common Voice nl\", \"accuracy\": {\"Test WER\": 15.72, \"Test CER\": 5.35, \"Test WER (+LM)\": 12.84, \"Test CER (+LM)\": 4.64}}, \"description\": \"Fine-tuned XLSR-53 large model for speech recognition in Dutch. Fine-tuned on Dutch using the train and validation splits of Common Voice 6.1 and CSS10.\"}}", "category": "generic"}
{"question_id": 556, "text": " Our company is developing an AI assistant to perform live transcription of meetings. We need to convert recorded speech into text.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Automatic Speech Recognition and Speech Translation\", \"api_name\": \"openai/whisper-large\", \"api_call\": \"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\", \"api_arguments\": [\"audio\", \"sampling_rate\"], \"python_environment_requirements\": [\"transformers\", \"datasets\"], \"example_code\": \"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\nfrom datasets import load_dataset\\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-large)\\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-large)\\nmodel.config.forced_decoder_ids = None\\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\nsample = ds[0][audio]\\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\npredicted_ids = model.generate(input_features)\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\", \"performance\": {\"dataset\": [{\"name\": \"LibriSpeech (clean)\", \"accuracy\": 3.0}, {\"name\": \"LibriSpeech (other)\", \"accuracy\": 5.4}, {\"name\": \"Common Voice 11.0\", \"accuracy\": 54.8}]}, \"description\": \"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.\"}}", "category": "generic"}
{"question_id": 557, "text": " A web browser extension automatically transcribes meetings in real-time to help people with hearing disabilities or those who join late to understand what is being discussed.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech to Text\", \"api_name\": \"facebook/s2t-medium-librispeech-asr\", \"api_call\": \"'Speech2TextForConditionalGeneration.from_pretrained(facebook/s2t-medium-librispeech-asr)'\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/s2t-medium-librispeech-asr\"}, \"python_environment_requirements\": [\"torchaudio\", \"sentencepiece\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\\nfrom datasets import load_dataset\\nimport soundfile as sf\\nmodel = Speech2TextForConditionalGeneration.from_pretrained(facebook/s2t-medium-librispeech-asr)\\nprocessor = Speech2Textprocessor.from_pretrained(facebook/s2t-medium-librispeech-asr)\\ndef map_to_array(batch):\\n speech, _ = sf.read(batch[file])\\n batch[speech] = speech\\n return batch\\nds = load_dataset(\\n patrickvonplaten/librispeech_asr_dummy,\\n clean,\\n split=validation\\n)\\nds = ds.map(map_to_array)\\ninput_features = processor(\\n ds[speech][0],\\n sampling_rate=16_000,\\n return_tensors=pt\\n).input_features # Batch size 1\\ngenerated_ids = model.generate(input_features=input_features)\\ntranscription = processor.batch_decode(generated_ids)\", \"performance\": {\"dataset\": \"LibriSpeech\", \"accuracy\": {\"clean\": 3.5, \"other\": 7.8}}, \"description\": \"s2t-medium-librispeech-asr is a Speech to Text Transformer (S2T) model trained for automatic speech recognition (ASR). The S2T model was proposed in this paper and released in this repository.\"}}", "category": "generic"}
{"question_id": 558, "text": " As an international company, our phone conference calls happen in multiple languages. We want to create a tool to convert English speech input to French speech output for our meetings.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"facebook/textless_sm_en_fr\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"input_file\"], \"python_environment_requirements\": [\"huggingface_hub\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is a speech-to-speech translation model trained by Facebook. It is designed for translating English speech to French speech.\"}}", "category": "generic"}
{"question_id": 559, "text": " Create a tourist-oriented mobile app that translates spoken statements from a Romanian-speaking tour guide to English.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"facebook/textless_sm_ro_en\", \"api_call\": \"pipeline('audio-to-audio', model='facebook/textless_sm_ro_en')\", \"api_arguments\": \"audio file or recording\", \"python_environment_requirements\": \"fairseq, huggingface_hub\", \"example_code\": \"https://huggingface.co/facebook/textless_sm_cs_en\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": \"unknown\"}, \"description\": \"A speech-to-speech translation model for Romanian to English developed by Facebook AI\"}}", "category": "generic"}
{"question_id": 560, "text": " We would like to improve the customer support quality of our call center by understanding the emotions of our clients in realtime.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Emotion Recognition\", \"api_name\": \"harshit345/xlsr-wav2vec-speech-emotion-recognition\", \"api_call\": \"Wav2Vec2ForSpeechClassification.from_pretrained('harshit345/xlsr-wav2vec-speech-emotion-recognition')\", \"api_arguments\": {\"model_name_or_path\": \"harshit345/xlsr-wav2vec-speech-emotion-recognition\"}, \"python_environment_requirements\": [\"pip install git+https://github.com/huggingface/datasets.git\", \"pip install git+https://github.com/huggingface/transformers.git\", \"pip install torchaudio\", \"pip install librosa\"], \"example_code\": \"path = '/data/jtes_v1.1/wav/f01/ang/f01_ang_01.wav'\\noutputs = predict(path, sampling_rate)\", \"performance\": {\"dataset\": \"JTES v1.1\", \"accuracy\": {\"anger\": 0.82, \"disgust\": 0.85, \"fear\": 0.78, \"happiness\": 0.84, \"sadness\": 0.86, \"overall\": 0.806}}, \"description\": \"This model is trained on the JTES v1.1 dataset for speech emotion recognition. It uses the Wav2Vec2 architecture for audio classification and can recognize emotions like anger, disgust, fear, happiness, and sadness.\"}}", "category": "generic"}
{"question_id": 561, "text": " Develop a software for classifying keywords spoken by users. It should predict the top five keywords.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"superb/hubert-base-superb-ks\", \"api_call\": \"pipeline('audio-classification', model='superb/hubert-base-superb-ks')\", \"api_arguments\": [\"file\", \"top_k\"], \"python_environment_requirements\": [\"datasets\", \"transformers\", \"torchaudio\"], \"example_code\": \"from datasets import load_dataset\\nfrom transformers import pipeline\\ndataset = load_dataset(anton-l/superb_demo, ks, split=test)\\nclassifier = pipeline(audio-classification, model=superb/hubert-base-superb-ks)\\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"Speech Commands dataset v1.0\", \"accuracy\": 0.9672}, \"description\": \"This is a ported version of S3PRL's Hubert for the SUPERB Keyword Spotting task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\"}}", "category": "generic"}
{"question_id": 562, "text": " Automate the process of categorizing noises on a blog with video content, analyzing and describing the type of sound in the video.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Audio Classification\", \"api_name\": \"distil-ast-audioset\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('bookbot/distil-ast-audioset')\", \"api_arguments\": [\"input_audio\"], \"python_environment_requirements\": [\"transformers==4.27.0.dev0\", \"pytorch==1.13.1+cu117\", \"datasets==2.10.0\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"AudioSet\", \"accuracy\": 0.0714}, \"description\": \"Distil Audio Spectrogram Transformer AudioSet is an audio classification model based on the Audio Spectrogram Transformer architecture. This model is a distilled version of MIT/ast-finetuned-audioset-10-10-0.4593 on the AudioSet dataset.\"}}", "category": "generic"}
{"question_id": 563, "text": " We are an e-Learning platform providing language lessons. Detect the speaker from a given speech segment.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"superb/hubert-large-superb-sid\", \"api_call\": \"pipeline('audio-classification', model='superb/hubert-large-superb-sid')\", \"api_arguments\": \"file, top_k\", \"python_environment_requirements\": \"datasets, transformers, librosa\", \"example_code\": \"from datasets import load_dataset\\nfrom transformers import pipeline\\ndataset = load_dataset(anton-l/superb_demo, si, split=test)\\nclassifier = pipeline(audio-classification, model=superb/hubert-large-superb-sid)\\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"VoxCeleb1\", \"accuracy\": 0.9035}, \"description\": \"Hubert-Large for Speaker Identification. This model is pretrained on 16kHz sampled speech audio and should be used with speech input also sampled at 16Khz. It is used for the SUPERB Speaker Identification task and can classify each utterance for its speaker identity as a multi-class classification.\"}}", "category": "generic"}
{"question_id": 564, "text": " An audio application is developed which requires speaker verification. Can you include the appropriate API call and example code?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"wav2vec2-base-superb-sv\", \"api_call\": \"Output: AutoModelForAudioXVector.from_pretrained('anton-l/wav2vec2-base-superb-sv')\", \"api_arguments\": \"anton-l/wav2vec2-base-superb-sv\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import AutoProcessor, AutoModelForAudioXVector\\nprocessor = AutoProcessor.from_pretrained(anton-l/wav2vec2-base-superb-sv)\\nmodel = AutoModelForAudioXVector.from_pretrained(anton-l/wav2vec2-base-superb-sv)\", \"performance\": {\"dataset\": \"superb\", \"accuracy\": \"More information needed\"}, \"description\": \"This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Verification task. The base model is wav2vec2-large-lv60, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\"}}", "category": "generic"}
{"question_id": 565, "text": " I am a software engineer, and I want to build an application that can control smart home devices using voice commands. The model must identify which command has been given by the user.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"ast-finetuned-speech-commands-v2\", \"api_call\": \"MIT/ast-finetuned-speech-commands-v2\", \"api_arguments\": \"audio file\", \"python_environment_requirements\": \"transformers library\", \"example_code\": \"result = audio_classifier('path/to/audio/file.wav')\", \"performance\": {\"dataset\": \"Speech Commands v2\", \"accuracy\": \"98.120\"}, \"description\": \"Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.\"}}", "category": "generic"}
{"question_id": 566, "text": " I am building a web application to predict a person's income based on adult census data, and I need to know how to use the XGBoost model to make the predictions. \\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Joblib\", \"functionality\": \"Transformers\", \"api_name\": \"abhishek/autotrain-adult-census-xgboost\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"scikit-learn/adult-census-income\", \"accuracy\": 0.8750191923844618}, \"description\": \"A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual's income is above or below $50,000 per year.\"}}", "category": "generic"}
{"question_id": 567, "text": " We need an AI model that can predict the carbon emission levels of different products based on certain factors.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"1659958767\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"omarques/autotrain-data-in-class-test-demo\", \"accuracy\": 0.983}, \"description\": \"A model trained for binary classification of carbon emissions using AutoTrain.\"}}", "category": "generic"}
{"question_id": 568, "text": " As a speaker factory, we want to know the correlation beteween components weight and carbon emmisions during production.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"1839063122\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"pcoloc/autotrain-data-600-dragino\", \"accuracy\": {\"Loss\": 93.595, \"R2\": 0.502, \"MSE\": 8760.052, \"MAE\": 77.527, \"RMSLE\": 0.445}}, \"description\": \"This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\"}}", "category": "generic"}
{"question_id": 569, "text": " Generate the amount of carbon emissions for given building data in order to predict the efficiency at which it produces.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"pcoloc/autotrain-mikrotik-7-7-1860563590\", \"api_call\": \"Output: model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"pcoloc/autotrain-data-mikrotik-7-7\", \"accuracy\": {\"Loss\": 52.881, \"R2\": 0.584, \"MSE\": 2796.357, \"MAE\": 37.116, \"RMSLE\": 0.518}}, \"description\": \"A tabular regression model for predicting carbon emissions using the Joblib framework.\"}}", "category": "generic"}
{"question_id": 570, "text": " I oversee management of a company with warehouse robots. I want our robots to move in a more natural gait. Can you provide a suitable model for this purpose?\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"edbeeching/decision-transformer-gym-walker2d-expert\", \"api_call\": \"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-walker2d-expert')\", \"api_arguments\": {\"mean\": [1.2384834, 0.19578537, -0.10475016, -0.18579608, 0.23003316, 0.022800924, -0.37383768, 0.337791, 3.925096, -0.0047428459, 0.025267061, -0.0039287535, -0.01736751, -0.48212224, 0.00035432147, -0.0037124525, 0.0026285544], \"std\": [0.06664903, 0.16980624, 0.17309439, 0.21843709, 0.74599105, 0.02410989, 0.3729872, 0.6226182, 0.9708009, 0.72936815, 1.504065, 2.495893, 3.511518, 5.3656907, 0.79503316, 4.317483, 6.1784487]}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"See our Blog Post, Colab notebook or Example Script for usage.\", \"performance\": {\"dataset\": \"Gym Walker2d environment\", \"accuracy\": \"Not provided\"}, \"description\": \"Decision Transformer model trained on expert trajectories sampled from the Gym Walker2d environment.\"}}", "category": "generic"}
{"question_id": 571, "text": " Our software team wants to implement an AI-driven multi-agent soccer game. Identify an existing model to use.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Unity ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"JYC333/poca-SoccerTwos-v1\", \"api_call\": \"No changes needed. The `api_call` field does not include any model instantiation.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"ml-agents\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"N/A\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 572, "text": " We'd like to analyze a customer review in Korean and help identify the most relevant parts for potential improvements.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"kobart-base-v2\", \"api_call\": \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", \"api_arguments\": {\"tokenizer\": \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"tokenizers\": \"latest\"}, \"example_code\": \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", \"performance\": {\"dataset\": \"NSMC\", \"accuracy\": 0.901}, \"description\": \"KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.\"}}", "category": "generic"}
{"question_id": 573, "text": " Extract linguistic features from an audio sample.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"hubert-large-ll60k\", \"api_call\": \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", \"api_arguments\": \"pretrained model name\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", \"performance\": {\"dataset\": \"Libri-Light\", \"accuracy\": \"matches or improves upon the state-of-the-art wav2vec 2.0 performance\"}, \"description\": \"Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.\"}}", "category": "generic"}
{"question_id": 574, "text": " Analyze the trend of people's fashion style based on the designer's description and generate related images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"Linaqruf/anything-v3.0\", \"api_call\": \"pipeline('text-to-image', model='Linaqruf/anything-v3.0') should be rewritten as Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A text-to-image model that generates images from text descriptions.\"}}", "category": "generic"}
{"question_id": 575, "text": " An online manga translation platform needs to recognize text in Japanese manga and want to convert it into plain text.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"kha-white/manga-ocr-base\", \"api_call\": \"pipeline('ocr', model='kha-white/manga-ocr-base').model\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"manga109s\", \"accuracy\": \"\"}, \"description\": \"Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\"}}", "category": "generic"}
{"question_id": 576, "text": " Recognize and extract text from an image that contains a printed sample.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-base-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 577, "text": " A magazine editor is interested in extracting text from a printed image of a handwritten manuscript to include in the publication.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-large-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", \"api_arguments\": {\"TrOCRProcessor\": \"from_pretrained('microsoft/trocr-large-printed')\", \"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"pip install transformers\", \"PIL\": \"pip install pillow\", \"requests\": \"pip install requests\"}, \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 578, "text": " I want to create a security system based on text identification in my company. It will be used for reading texts from images captured by the security system.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"mgp-str\", \"api_call\": \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", \"api_arguments\": {\"model_name\": \"alibaba-damo/mgp-str-base\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\"]}, \"example_code\": \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", \"performance\": {\"dataset\": \"MJSynth and SynthText\", \"accuracy\": null}, \"description\": \"MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.\"}}", "category": "generic"}
{"question_id": 579, "text": " A marketing team needs to create high-quality content for product promotion. They want to turn text descriptions into short video clips to engage their audience on social media.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"ImRma/Brucelee\", \"api_call\": \"pipeline('text-to-video', model='ImRma/Brucelee')\", \"api_arguments\": [\"your_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Hugging Face model for converting Persian and English text into video.\"}}", "category": "generic"}
{"question_id": 580, "text": " As part of a social-aware project, we are analyzing images and answering questions based on those images using pretrained models.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-textvqa\", \"api_call\": \"git_base_textvqa = AutoModel.from_pretrained('microsoft/git-base-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}}", "category": "generic"}
{"question_id": 581, "text": " We want to build a program that would answer questions based on visuals.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"blip-vqa-base\", \"api_call\": \"Output: BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-base').generate(**inputs)\", \"api_arguments\": {\"raw_image\": \"Image\", \"question\": \"String\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForQuestionAnswering\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-base)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"VQA\", \"accuracy\": \"+1.6% in VQA score\"}, \"description\": \"BLIP is a Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is trained on visual question answering with a base architecture (using ViT base backbone).\"}}", "category": "generic"}
{"question_id": 582, "text": " Create an AI tool that answers simple questions related to an image provided as input.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"hf-tiny-model-private/tiny-random-ViltForQuestionAnswering\", \"api_call\": \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", \"api_arguments\": {\"image\": \"path/to/image/file\", \"question\": \"your_question\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random model for Visual Question Answering using the VILT framework.\"}}", "category": "generic"}
{"question_id": 583, "text": " Someone is sending me pictures of famous places, but they want me to guess the place by asking questions. Can you help me respond to the vital question?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"temp_vilt_vqa\", \"api_call\": \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", \"api_arguments\": {\"model\": \"Bingsu/temp_vilt_vqa\", \"tokenizer\": \"Bingsu/temp_vilt_vqa\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A visual question answering model for answering questions related to images using the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 584, "text": " An accounting firm would like to extract specific information, such as invoice numbers, from scanned images of documents.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"impira/layoutlm-document-qa\", \"api_call\": \"layoutlm_document_qa = pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa', return_dict=True))\", \"api_arguments\": [\"image_url\", \"question\"], \"python_environment_requirements\": [\"PIL\", \"pytesseract\", \"PyTorch\", \"transformers\"], \"example_code\": \"nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)\", \"performance\": {\"dataset\": [\"SQuAD2.0\", \"DocVQA\"], \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.\"}}", "category": "generic"}
{"question_id": 585, "text": " Can you explain how I can extract answers from a document image with a specific question?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"layoutlmv3-base-mpdocvqa\", \"api_call\": \"'LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)'\", \"api_arguments\": [\"image\", \"question\", \"context\", \"boxes\"], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()\", \"performance\": {\"dataset\": \"rubentito/mp-docvqa\", \"accuracy\": {\"ANLS\": 0.4538, \"APPA\": 51.9426}}, \"description\": \"This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.\"}}", "category": "generic"}
{"question_id": 586, "text": " I have some scanned documents, and I want a tool that extracts information from these images to answer questions related to the content.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\", \"api_call\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \">=4.11.0\"}, \"example_code\": {}, \"performance\": {\"dataset\": {}, \"accuracy\": {}}, \"description\": \"A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.\"}}", "category": "generic"}
{"question_id": 587, "text": " We would like to extract important information from an invoice for bookkeeping and accounting purposes.\\n###Input: {\\\"url\\\": \\\"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\\", \\\"question\\\": \\\"What is the total amount due?\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"CQI_Visual_Question_Awnser_PT_v0\", \"api_call\": \"layoutlm_document_qa = pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", \"api_arguments\": [\"url\", \"question\"], \"python_environment_requirements\": [\"PIL\", \"pytesseract\", \"PyTorch\", \"transformers\"], \"example_code\": [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], \"performance\": {\"dataset\": [{\"accuracy\": 0.9943977}, {\"accuracy\": 0.9912159}, {\"accuracy\": 0.59147286}]}, \"description\": \"A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\"}}", "category": "generic"}
{"question_id": 588, "text": " A robot company wants to create a model to predict the distance of objects from the camera. They want to know which API would be best suitable for this purpose.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221121-113853\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.24.0\", \"torch==1.12.1\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3384, \"Mae\": 0.2739, \"Rmse\": 0.3959, \"Abs Rel\": 0.323, \"Log Mae\": 0.1148, \"Log Rmse\": 0.1651, \"Delta1\": 0.5576, \"Delta2\": 0.8345, \"Delta3\": 0.9398}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 589, "text": " We need to classify images of animals as a part of our game development process. How can we use an ML model to achieve this?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/convnext-base-224\", \"api_call\": \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", \"api_arguments\": [\"image\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"datasets\"], \"example_code\": \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": null}, \"description\": \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\"}}", "category": "generic"}
{"question_id": 590, "text": " For an article about popular street foods, we need to find out if the given image is a hotdog.\\n###Input: path/to/image\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"julien-c/hotdog-not-hotdog\", \"api_call\": \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": 0.825}, \"description\": \"A model that classifies images as hotdog or not hotdog.\"}}", "category": "generic"}
{"question_id": 591, "text": " Our company sells custom-made aprons. We want to use an AI tool to identify images of aprons and analyze the customer feedback.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"timm/mobilenetv3_large_100.ra_in1k\", \"api_call\": \"Output: timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\"}, \"python_environment_requirements\": {\"timm\": \"latest\"}, \"example_code\": \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.\"}}", "category": "generic"}
{"question_id": 592, "text": " We need to automatically segment items in images to improve our warehouse management system. Help us to detect objects in the images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"deformable-detr\", \"api_call\": \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"COCO 2017\", \"accuracy\": \"Not provided\"}, \"description\": \"Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 593, "text": " I want a chat product to classify whether an image is healthy by checking the presence of red blood cells, white blood cells and platelets in the bloodstream.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-blood-cell-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-blood-cell-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.24\", \"ultralytics==8.0.23\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"blood-cell-object-detection\", \"accuracy\": 0.927}, \"description\": \"A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\"}}", "category": "generic"}
{"question_id": 594, "text": " My client manages a real estate company, they want to segment the rooms in their apartment pictures to showcase them to their potential customers.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"openmmlab/upernet-convnext-small\", \"api_call\": \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\"}}", "category": "generic"}
{"question_id": 595, "text": " Our city management wants to analyze urban landscape images to identify various elements such as buildings, roads, and trees.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"CityScapes\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 596, "text": " We need to detect potholes in a provided image to help the city government take necessary actions in fixing damaged roads.\\n###Input: {\\\"image\\\": \\\"https://example.com/pothole_image.jpg\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8s-pothole-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8s-pothole-segmentation').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": {\"ultralyticsplus\": \"0.0.23\", \"ultralytics\": \"8.0.21\"}, \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pothole-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.928, \"mAP@0.5(mask)\": 0.928}}, \"description\": \"A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\"}}", "category": "generic"}
{"question_id": 597, "text": " You are working with a fashion magazine that needs a tool to generate a variation of a given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Variations\", \"api_name\": \"lambdalabs/sd-image-variations-diffusers\", \"api_call\": \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", \"api_arguments\": {\"revision\": \"v2.0\"}, \"python_environment_requirements\": \"Diffusers >=0.8.0\", \"example_code\": \"from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)\", \"performance\": {\"dataset\": \"ChristophSchuhmann/improved_aesthetics_6plus\", \"accuracy\": \"N/A\"}, \"description\": \"This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 598, "text": " A travel agency wants to use computer vision for generating virtual tours of tourist attractions based on the provided text description.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"lllyasviel/sd-controlnet-hed\", \"api_call\": \"ControlNetModel.from_pretrained(\\\\lllyasviel/sd-controlnet-hed\\\\, torch_dtype=torch.float16)\", \"api_arguments\": [\"image\", \"text\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux\"], \"example_code\": \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", \"performance\": {\"dataset\": \"3M edge-image, caption pairs\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 599, "text": " Generate an image for marketing purposes depicting a concert of a famous pop star based on the line art as input.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"ControlNet\", \"api_name\": \"lllyasviel/control_v11p_sd15_lineart\", \"api_call\": \"Output: ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"ControlNet-1-1-preview/control_v11p_sd15_lineart\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": \"pip install diffusers transformers accelerate controlnet_aux==0.3.0\", \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import LineartDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = ControlNet-1-1-preview/control_v11p_sd15_lineart\\nimage = load_image(\\n https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart/resolve/main/images/input.png\\n)\\nimage = image.resize((512, 512))\\nprompt = michael jackson concert\\nprocessor = LineartDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"ControlNet-1-1-preview\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\"}}", "category": "generic"}
{"question_id": 600, "text": " We are aiming to help a designer to come up with a poster visual for their new home decor product line inspired by a royal chamber with a fancy bed.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Diffusers\", \"functionality\": \"Text-to-Image\", \"api_name\": \"lllyasviel/control_v11p_sd15_scribble\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_scribble\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux==0.3.0\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"Stable Diffusion v1-5\", \"accuracy\": \"Not specified\"}, \"description\": \"Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}", "category": "generic"}
{"question_id": 601, "text": " We are building a website to showcase various churches. Write a Python script that generates a high-quality image of a church using Denoising Diffusion Probabilistic Models (DDPM).\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"google/ddpm-ema-church-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception score\": 9.46, \"FID score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.\"}}", "category": "generic"}
{"question_id": 602, "text": " How can I generate an image using the provided API for a creative project?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Inference\", \"api_name\": \"google/ncsnpp-ffhq-1024\", \"api_call\": \"DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-1024')\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-ffhq-1024\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR-10\", \"accuracy\": {\"Inception_score\": 9.89, \"FID\": 2.2, \"likelihood\": 2.99}}, \"description\": \"Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.\"}}", "category": "generic"}
{"question_id": 603, "text": " We want to prepare some Minecraft skin images for a gaming event. Use an API to generate some unique skins.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"WiNE-iNEFF/Minecraft-Skin-Diffusion-V2\", \"api_call\": \"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\", \"api_arguments\": [], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\nimage = pipeline().images[0].convert('RGBA')\\nimage\", \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"An unconditional image generation model for generating Minecraft skin images using the diffusion model.\"}}", "category": "generic"}
{"question_id": 604, "text": " Generate an image of a galaxy using a pre-trained diffusion model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"myunus1/diffmodels_galaxies_scratchbook\", \"api_call\": \"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\", \"api_arguments\": {\"from_pretrained\": \"myunus1/diffmodels_galaxies_scratchbook\"}, \"python_environment_requirements\": {\"package\": \"diffusers\", \"import\": \"from diffusers import DDPMPipeline\"}, \"example_code\": {\"initialize_pipeline\": \"pipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\", \"generate_image\": \"image = pipeline().images[0]\", \"display_image\": \"image\"}, \"performance\": {\"dataset\": \"Not provided\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute \\ud83e\\udd8b.\"}}", "category": "generic"}
{"question_id": 605, "text": " Develop a method for us to categorize videos according to their contents.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-base-finetuned-k400\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k400')\", \"api_arguments\": \"video, return_tensors\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(8, 3, 224, 224))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k400)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k400)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\n  logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.\"}}", "category": "generic"}
{"question_id": 606, "text": " An application that can identify human actions in short video clips is needed. For this task, select a model suitable for video classification.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base-finetuned-ssv2\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\", \"api_arguments\": \"video\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n    logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": {\"top-1\": 70.6, \"top-5\": 92.6}}, \"description\": \"VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 607, "text": " Our content provides physical fitness videos to customers. Identifying and classifying the type of exercise in each video would help provide personalized recommendations.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset')\", \"api_arguments\": {\"learning_rate\": 5e-05, \"train_batch_size\": 8, \"eval_batch_size\": 8, \"seed\": 42, \"optimizer\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\", \"lr_scheduler_type\": \"linear\", \"lr_scheduler_warmup_ratio\": 0.1, \"training_steps\": 111}, \"python_environment_requirements\": {\"transformers\": \"4.24.0\", \"pytorch\": \"1.12.1+cu113\", \"datasets\": \"2.6.1\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 1.0}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 608, "text": " We need to classify some video clips for the exercise videos' application. Use a suitable video model to do this.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"videomae-base-ssv2\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\", \"api_arguments\": \"video\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": \"\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\"}}", "category": "generic"}
{"question_id": 609, "text": " I'm creating a security system for my company. I want to analyze video feeds and decide the main action occurring in the video.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Action Recognition\", \"api_name\": \"videomae-base-finetuned-ucf101\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}, \"python_environment_requirements\": [\"transformers\", \"decord\", \"huggingface_hub\"], \"example_code\": \"from decord import VideoReader, cpu\\nimport torch\\nimport numpy as np\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\nfrom huggingface_hub import hf_hub_download\\nnp.random.seed(0)\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\n converted_len = int(clip_len * frame_sample_rate)\\n end_idx = np.random.randint(converted_len, seg_len)\\n start_idx = end_idx - converted_len\\n indices = np.linspace(start_idx, end_idx, num=clip_len)\\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\n return indices\\nfile_path = hf_hub_download(\\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\\n)\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\nvideoreader.seek(0)\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\nvideo = videoreader.get_batch(indices).asnumpy()\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\ninputs = feature_extractor(list(video), return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"UCF101\", \"accuracy\": 0.758209764957428}, \"description\": \"VideoMAE Base model fine tuned on UCF101 for Video Action Recognition\"}}", "category": "generic"}
{"question_id": 610, "text": " A member of a museum curatorial team needs to categorize new art pieces recently acquired for their online database. They want to classify them automatically.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": 75.3}, \"description\": \"A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model.\"}}", "category": "generic"}
{"question_id": 611, "text": " Imagine you are building an app that identifies dog breed from a photo. We need to know the breed when user uploads a new photo.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-g-14-laion2B-s34B-b88K\", \"api_call\": \"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\", \"api_arguments\": {\"image\": \"path/to/image/file\", \"class_names\": \"list_of_class_names\"}, \"python_environment_requirements\": {\"huggingface_hub\": \"0.0.17\", \"transformers\": \"4.11.3\", \"torch\": \"1.9.0\", \"torchvision\": \"0.10.0\"}, \"example_code\": null, \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\"}}", "category": "generic"}
{"question_id": 612, "text": " We have a finance startup working on making investing easy. We want to analyze sentiment of financial news.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"financial-sentiment-analysis\", \"api_name\": \"ProsusAI/finbert\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert'); classifier('your_text_here')\", \"performance\": {\"dataset\": \"Financial PhraseBank\", \"accuracy\": \"Not provided\"}, \"description\": \"FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\"}}", "category": "generic"}
{"question_id": 613, "text": " I would like to create a tool for investors to analyze stock-related comments and classify them as \\\"Bullish\\\" or \\\"Bearish\\\".\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Inferencing for stock-related comments\", \"api_name\": \"zhayunduo/roberta-base-stocktwits-finetuned\", \"api_call\": \"Output: RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\", \"api_arguments\": {\"model\": \"RobertaForSequenceClassification\", \"tokenizer\": \"RobertaTokenizer\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\nfrom transformers import pipeline\\nimport pandas as pd\\nimport emoji\\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\nsentences = pd.Series(['just buy','just sell it','entity rocket to the sky!','go down','even though it is going up, I still think it will not keep this trend in the near future'])\\nsentences = list(sentences)\\nresults = nlp(sentences)\\nprint(results)\", \"performance\": {\"dataset\": \"stocktwits\", \"accuracy\": 0.9343}, \"description\": \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'.\"}}", "category": "generic"}
{"question_id": 614, "text": " In my department store, analyze the customers' reviews in the German language for better feedback.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"German Sentiment Classification\", \"api_name\": \"oliverguhr/german-sentiment-bert\", \"api_call\": \"Output: SentimentModel()\", \"api_arguments\": [\"texts\"], \"python_environment_requirements\": \"pip install germansentiment\", \"example_code\": [\"from germansentiment import SentimentModel\", \"model = SentimentModel()\", \"texts = [\", \" Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,\", \" Total awesome!,nicht so schlecht wie erwartet,\", \" Der Test verlief positiv.,Sie frt ein gres Auto.]\", \"result = model.predict_sentiment(texts)\", \"print(result)\"], \"performance\": {\"dataset\": [\"holidaycheck\", \"scare\", \"filmstarts\", \"germeval\", \"PotTS\", \"emotions\", \"sb10k\", \"Leipzig Wikipedia Corpus 2016\", \"all\"], \"accuracy\": [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, \"description\": \"This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.\"}}", "category": "generic"}
{"question_id": 615, "text": " Our software system has communication interactions with users. We need to identify whether the received message is gibberish or not.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"madhurjindal/autonlp-Gibberish-Detector-492513457\", \"api_call\": \"Output: AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoNLP\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForSequenceClassification\", \"AutoTokenizer\": \"from_pretrained\"}, \"example_code\": \"from transformers import AutoModelForSequenceClassification, AutoTokenizer\\nmodel = AutoModelForSequenceClassification.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\ninputs = tokenizer(I love AutoNLP, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"madhurjindal/autonlp-data-Gibberish-Detector\", \"accuracy\": 0.9735624586913417}, \"description\": \"A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT.\"}}", "category": "generic"}
{"question_id": 616, "text": " We have online medical records and we want to remove sensitive information like names, dates, addresses, etc. Is there any API available to simplify this process?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"De-identification\", \"api_name\": \"StanfordAIMI/stanford-deidentifier-base\", \"api_call\": \"pipeline('ner', model='StanfordAIMI/stanford-deidentifier-base')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"deidentifier('Your input text here')\", \"performance\": {\"dataset\": \"radreports\", \"accuracy\": {\"known_institution_F1\": 97.9, \"new_institution_F1\": 99.6, \"i2b2_2006_F1\": 99.5, \"i2b2_2014_F1\": 98.9}}, \"description\": \"Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.\"}}", "category": "generic"}
{"question_id": 617, "text": " I have a list of information on the Olympic Games and I need to find the cities and years they were held in. Can you generate a table for me?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/tapex-base-finetuned-wtq\", \"api_call\": \"BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')\", \"api_arguments\": {\"tokenizer\": \"TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)\", \"model\": \"BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)\", \"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"query\"}, \"python_environment_requirements\": [\"transformers\", \"pandas\"], \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.\"}}", "category": "generic"}
{"question_id": 618, "text": " The company we are working with is trying to get information about the medical industry. They want to get answers to their questions based on information from tables.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"lysandre/tapas-temporary-repo\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\", \"api_arguments\": [\"model_name\", \"question\", \"table\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\noutputs = model(**inputs)\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\", \"performance\": {\"dataset\": \"SQA\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.\"}}", "category": "generic"}
{"question_id": 619, "text": " A medical research team needs an AI model to extract answers from medical articles for their research project.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"sultan/BioM-ELECTRA-Large-SQuAD2\", \"api_call\": \"sultan/BioM-ELECTRA-Large-SQuAD2\", \"api_arguments\": null, \"python_environment_requirements\": [\"transformers\", \"sentencepiece\"], \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\nresult = qa_pipeline({'context': 'your_context', 'question': 'your_question'})\", \"performance\": {\"dataset\": \"SQuAD2.0 Dev\", \"accuracy\": {\"exact\": 84.33420365535248, \"f1\": 87.49354241889522}}, \"description\": \"BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.\"}}", "category": "generic"}
{"question_id": 620, "text": " I need information about Mount Everest in Chinese. Please tell me how high it is.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\", \"api_call\": \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large.from_pretrained()\", \"api_arguments\": \"context, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\", \"performance\": {\"dataset\": \"Dureader-2021\", \"accuracy\": \"83.1\"}, \"description\": \"A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition.\"}}", "category": "generic"}
{"question_id": 621, "text": " I'm a retinal surgeon, and I want to detect which diseases have been cured or quite stable based on a given clinical note.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"cross-encoder/nli-deberta-v3-xsmall\", \"api_call\": \"pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\", \"api_arguments\": [\"sent\", \"candidate_labels\"], \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from transformers import pipeline\\nclassifier = pipeline('zero-shot-classification', model='cross-encoder/nli-deberta-v3-xsmall')\\nsent = 'Apple just announced the newest iPhone X'\\ncandidate_labels = ['technology', 'sports', 'politics']\\nres = classifier(sent, candidate_labels)\\nprint(res)\", \"performance\": {\"dataset\": {\"SNLI-test\": \"91.64\", \"MNLI_mismatched\": \"87.77\"}}, \"description\": \"This model is a Cross-Encoder for Natural Language Inference, trained on the SNLI and MultiNLI datasets. It can be used for zero-shot classification tasks.\"}}", "category": "generic"}
{"question_id": 622, "text": " Assess the compatibility of two Russian sentences you are given.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Natural Language Inference\", \"api_name\": \"cointegrated/rubert-base-cased-nli-threeway\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\", \"api_arguments\": [\"text1\", \"text2\"], \"python_environment_requirements\": [\"transformers\", \"sentencepiece\"], \"example_code\": \"import torch\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\nmodel_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\nif torch.cuda.is_available():\\n model.cuda()\\ntext1 = '\\u0421\\u043e\\u043a\\u0440\\u0430\\u0442 - \\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a, \\u0430 \\u0432\\u0441\\u0435 \\u043b\\u044e\\u0434\\u0438 \\u0441\\u043c\\u0435\\u0440\\u0442\\u043d\\u044b.'\\ntext2 = '\\u0421\\u043e\\u043a\\u0440\\u0430\\u0442 \\u043d\\u0438\\u043a\\u043e\\u0433\\u0434\\u0430 \\u043d\\u0435 \\u0443\\u043c\\u0440\\u0451\\u0442.'\\nwith torch.inference_mode():\\n out = model(**tokenizer(text1, text2, return_tensors='pt').to(model.device))\\n proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\\nprint({v: proba[k] for k, v in model.config.id2label.items()})\", \"performance\": {\"dataset\": [\"JOCI\", \"MNLI\", \"MPE\", \"SICK\", \"SNLI\", \"ANLI\", \"NLI-style FEVER\", \"IMPPRES\"], \"accuracy\": {\"ROC AUC\": {\"entailment\": 0.91, \"contradiction\": 0.71, \"neutral\": 0.79}}}, \"description\": \"This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral.\"}}", "category": "generic"}
{"question_id": 623, "text": " I need to display this phrase \\\"Bonjour, comment  va?\\\" in English.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-fr-en\", \"api_call\": \"pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"translation_pipeline('Bonjour, comment  va?')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newsdiscussdev2015-enfr.fr.en\": 33.1, \"newsdiscusstest2015-enfr.fr.en\": 38.7, \"newssyscomb2009.fr.en\": 30.3, \"news-test2008.fr.en\": 26.2, \"newstest2009.fr.en\": 30.2, \"newstest2010.fr.en\": 32.2, \"newstest2011.fr.en\": 33.0, \"newstest2012.fr.en\": 32.8, \"newstest2013.fr.en\": 33.9, \"newstest2014-fren.fr.en\": 37.8, \"Tatoeba.fr.en\": 57.5}}}, \"description\": \"Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.\"}}", "category": "generic"}
{"question_id": 624, "text": " We need to translate a recipe from our Italian cookbook to English for international customers.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-it-en\", \"api_call\": \"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')('Ciao mondo!')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newssyscomb2009.it.en\": 35.3, \"newstest2009.it.en\": 34.0, \"Tatoeba.it.en\": 70.9}, \"chr-F\": {\"newssyscomb2009.it.en\": 0.6, \"newstest2009.it.en\": 0.594, \"Tatoeba.it.en\": 0.808}}}, \"description\": \"A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\"}}", "category": "generic"}
{"question_id": 625, "text": " Can you help me to create an implementation to translate English text to French text?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Text-to-Text Generation\", \"api_name\": \"optimum/t5-small\", \"api_call\": \"ORTModelForSeq2SeqLM.from_pretrained('optimum/t5-small')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"optimum.onnxruntime\"], \"example_code\": \"from transformers import AutoTokenizer, pipeline\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\nresults = translator(My name is Eustache and I have a pet raccoon)\\nprint(results)\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"N/A\"}, \"description\": \"T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\"}}", "category": "generic"}
{"question_id": 626, "text": " A tourist company requested that for each destination, they need to include a detailed brief written in different languages. They want you to create a system that will handle this.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-en-ROMANCE\", \"api_call\": \"pipeline('translation_en_to_ROMANCE', model='Helsinki-NLP/opus-mt-en-ROMANCE')\", \"api_arguments\": \"source languages, target languages\", \"python_environment_requirements\": \"transformers, torch\", \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_en_to_ROMANCE', model='Helsinki-NLP/opus-mt-en-ROMANCE')\\ntranslated_text = translation('Hello, how are you?', tgt_lang='es')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": 50.1, \"chr-F\": 0.693}}, \"description\": \"A translation model trained on the OPUS dataset that supports translation between English and various Romance languages. It uses a transformer architecture and requires a sentence initial language token in the form of >>id<< (id = valid target language ID).\"}}", "category": "generic"}
{"question_id": 627, "text": " As a content-manager in the field of computer AI, I want to generate summaries for blog posts.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text-to-Text Generation\", \"api_name\": \"philschmid/bart-large-cnn-samsum\", \"api_call\": \"summarizer = pipeline('summarization', model=BartForConditionalGeneration.from_pretrained('philschmid/bart-large-cnn-samsum'))\", \"api_arguments\": {\"model\": \"philschmid/bart-large-cnn-samsum\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": \"from transformers import pipeline\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\nconversation = '''Jeff: Can I train a \\ud83e\\udd17 Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\n'''\\nsummarizer(conversation)\", \"performance\": {\"dataset\": \"samsum\", \"accuracy\": {\"eval_rouge1\": 42.621, \"eval_rouge2\": 21.9825, \"eval_rougeL\": 33.034, \"eval_rougeLsum\": 39.6783, \"test_rouge1\": 41.3174, \"test_rouge2\": 20.8716, \"test_rougeL\": 32.1337, \"test_rougeLsum\": 38.4149}}, \"description\": \"philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\"}}", "category": "generic"}
{"question_id": 628, "text": " My company receives many lengthy reports, and we need a tool to automatically generate summaries.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Transformers\", \"functionality\": \"Text Summarization\", \"api_name\": \"sshleifer/distilbart-cnn-6-6\", \"api_call\": \"BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-6-6')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": {\"cnn_dailymail\": {\"Rouge 2\": 20.17, \"Rouge-L\": 29.7}, \"xsum\": {\"Rouge 2\": 20.92, \"Rouge-L\": 35.73}}}, \"description\": \"DistilBART model for text summarization, trained on the CNN/Daily Mail and XSum datasets. It is a smaller and faster version of BART, suitable for summarizing English text.\"}}", "category": "generic"}
{"question_id": 629, "text": " I'm working on a research project about patents in the tech industry. I need help summarizing lengthy patent documents.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"google/bigbird-pegasus-large-bigpatent\", \"api_call\": \"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-bigpatent')\", \"api_arguments\": {\"attention_type\": \"original_full\", \"block_size\": 16, \"num_random_blocks\": 2}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-bigpatent)\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-bigpatent)\\ntext = Replace me by any text you'd like.\\ninputs = tokenizer(text, return_tensors='pt')\\nprediction = model.generate(**inputs)\\nprediction = tokenizer.batch_decode(prediction)\", \"performance\": {\"dataset\": \"big_patent\", \"accuracy\": \"Not provided\"}, \"description\": \"BigBird, a sparse-attention based transformer, extends Transformer-based models like BERT to much longer sequences. It can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. BigBird has achieved state-of-the-art results on various tasks involving very long sequences such as long documents summarization and question-answering with long contexts.\"}}", "category": "generic"}
{"question_id": 630, "text": " Our client, a news website, is looking for an automated system to provide summaries of long articles in Russian. Generate a summary for a given text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Abstractive Russian Summarization\", \"api_name\": \"cointegrated/rut5-base-absum\", \"api_call\": \"Output: T5ForConditionalGeneration.from_pretrained('cointegrated/rut5-base-absum')\", \"api_arguments\": {\"n_words\": \"int\", \"compression\": \"float\", \"max_length\": \"int\", \"num_beams\": \"int\", \"do_sample\": \"bool\", \"repetition_penalty\": \"float\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": \"import torch\\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\\nMODEL_NAME = 'cointegrated/rut5-base-absum'\\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\\nmodel.cuda();\\nmodel.eval();\\ndef summarize(\\n text, n_words=None, compression=None,\\n max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, \\n <strong>kwargs\\n):\\n \\n Summarize the text\\n The following parameters are mutually exclusive:\\n - n_words (int) is an approximate number of words to generate.\\n - compression (float) is an approximate length ratio of summary and original text.\\n \\n if n_words:\\n text = '[{}] '.format(n_words) + text\\n elif compression:\\n text = '[{0:.1g}] '.format(compression) + text\\n x = tokenizer(text, return_tensors='pt', padding=True).to(model.device)\\n with torch.inference_mode():\\n out = model.generate(\\n </strong>x, \\n max_length=max_length, num_beams=num_beams, \\n do_sample=do_sample, repetition_penalty=repetition_penalty, \\n **kwargs\\n )\\n return tokenizer.decode(out[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": [\"csebuetnlp/xlsum\", \"IlyaGusev/gazeta\", \"mlsum\"], \"accuracy\": \"Not provided\"}, \"description\": \"This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets.\"}}", "category": "generic"}
{"question_id": 631, "text": " I want to create a conversation with a fictional character I am designing. Can you help me generate text in the character's voice?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"pygmalion-6b\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b').generate(input_ids, max_length=100, num_return_sequences=1)\", \"api_arguments\": [\"input_ids\", \"max_length\", \"num_return_sequences\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained('waifu-workshop/pygmalion-6b')\\nmodel = AutoModelForCausalLM.from_pretrained('waifu-workshop/pygmalion-6b')\\ninput_text = [CHARACTER]'s Persona: [A few sentences about the character you want the model to play]\\\\n<START>\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\\noutput = model.generate(input_ids, max_length=100, num_return_sequences=1)\\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"56MB of dialogue data gathered from multiple sources\", \"accuracy\": \"Not specified\"}, \"description\": \"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue.\"}}", "category": "generic"}
{"question_id": 632, "text": " We are looking to implement a chatbot that can engage in open-domain conversations with our customers.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"hyunwoongko/blenderbot-9B\", \"api_call\": \"pipeline('conversational', model='hyunwoongko/blenderbot-9B')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"Input a message to start chatting with hyunwoongko/blenderbot-9B.\", \"performance\": {\"dataset\": \"blended_skill_talk\", \"accuracy\": \"Not provided\"}, \"description\": \"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\"}}", "category": "generic"}
{"question_id": 633, "text": " Generate a short summary of the highlighted benefits of a low-carb diet and the possible drawbacks.\\n###Input: A low-carb diet can bring numerous benefits, including weight loss, improved blood sugar control, enhanced mental focus, lower blood pressure, and reduced hunger. However, there are potential drawbacks, such as nutrient deficiencies, increased risk of heart disease due to higher saturated fat consumption, and an initial period of low energy and mood swings.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"bigscience/test-bloomd-6b3\", \"api_call\": \"pipeline('text-generation', model='bigscience/test-bloomd-6b3')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; generator = pipeline('text-generation', model='bigscience/test-bloomd-6b3'); generator('Once upon a time')\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"A text generation model from Hugging Face, using the bigscience/test-bloomd-6b3 architecture. It can be used for generating text based on a given input.\"}}", "category": "generic"}
{"question_id": 634, "text": " I have limited computational resources. Design an AI that can complete short stories from given prompts.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"sshleifer/tiny-gpt2\", \"api_call\": \"TinyGPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\", \"api_arguments\": {\"model\": \"sshleifer/tiny-gpt2\"}, \"python_environment_requirements\": {\"huggingface_transformers\": \">=4.0.0\"}, \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('text-generation', model='sshleifer/tiny-gpt2')\\nresult = nlp('Once upon a time')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A tiny GPT-2 model for text generation, suitable for low-resource environments and faster inference. This model is part of the Hugging Face Transformers library and can be used for generating text given a prompt.\"}}", "category": "generic"}
{"question_id": 635, "text": " Greg desires a translation of the following sentence \\\"What is your favorite color?\\\" from English to German.\\n###Input: What is your favorite color?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Language model\", \"api_name\": \"google/flan-t5-base\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\", \"api_arguments\": \"input_text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import T5Tokenizer, T5ForConditionalGeneration\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-base)\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-base)\\ninput_text = translate English to German: How old are you?\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\noutputs = model.generate(input_ids)\\nprint(tokenizer.decode(outputs[0]))\", \"performance\": {\"dataset\": [{\"name\": \"MMLU\", \"accuracy\": \"75.2%\"}]}, \"description\": \"FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering.\"}}", "category": "generic"}
{"question_id": 636, "text": " Give me a brief summary of a long conversation between five people discussing the future of technology and privacy issues.\\n###Input: '<long_conversation>'\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text2Text Generation\", \"api_name\": \"DialogLED-base-16384\", \"api_call\": \"LEDForConditionalGeneration.from_pretrained('MingZhong/DialogLED-base-16384')\", \"api_arguments\": \"input_text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"arxiv\", \"accuracy\": \"2109.02492\"}, \"description\": \"DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.\"}}", "category": "generic"}
{"question_id": 637, "text": " Create a simple text-based calculator that uses a language model for arithmetic operations.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Language model\", \"api_name\": \"google/flan-t5-small\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import T5Tokenizer, T5ForConditionalGeneration\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-small)\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-small)\\ninput_text = translate English to German: How old are you?\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\noutputs = model.generate(input_ids)\\nprint(tokenizer.decode(outputs[0]))\", \"performance\": {\"dataset\": [{\"name\": \"MMLU\", \"accuracy\": \"75.2%\"}]}, \"description\": \"FLAN-T5 small is a fine-tuned version of T5 language model on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. The model is designed for research on language models, including zero-shot and few-shot NLP tasks, reasoning, question answering, fairness, and safety research. It has not been tested in real-world applications and should not be used directly in any application without prior assessment of safety and fairness concerns specific to the application.\"}}", "category": "generic"}
{"question_id": 638, "text": " Create a program that will predict what word is needed to fill in the blanks in the following incomplete sentence. \\\"She decided to ____ a movie tonight because she was tired.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling\", \"api_name\": \"albert-base-v2\", \"api_call\": \"pipeline('fill-mask', model='albert-base-v2')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nunmasker = pipeline('fill-mask', model='albert-base-v2')\\nunmasker(Hello I'm a [MASK] model.)\", \"performance\": {\"dataset\": {\"SQuAD1.1\": \"90.2/83.2\", \"SQuAD2.0\": \"82.1/79.3\", \"MNLI\": \"84.6\", \"SST-2\": \"92.9\", \"RACE\": \"66.8\"}, \"accuracy\": \"82.3\"}, \"description\": \"ALBERT Base v2 is a transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model, as all ALBERT models, is uncased: it does not make a difference between english and English.\"}}", "category": "generic"}
{"question_id": 639, "text": " I have a sentence about my cat. The cat is a missing word in Spanish. Help me complete the sentence.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling\", \"api_name\": \"distilbert-base-multilingual-cased\", \"api_call\": \"pipeline('fill-mask', model='distilbert-base-multilingual-cased')\", \"api_arguments\": [\"pipeline\", \"fill-mask\", \"model\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nunmasker = pipeline('fill-mask', model='distilbert-base-multilingual-cased')\\nunmasker(Hello I'm a [MASK] model.)\", \"performance\": {\"dataset\": [{\"name\": \"XNLI\", \"accuracy\": {\"English\": 78.2, \"Spanish\": 69.1, \"Chinese\": 64.0, \"German\": 66.3, \"Arabic\": 59.1, \"Urdu\": 54.7}}]}, \"description\": \"This model is a distilled version of the BERT base multilingual model. It is trained on the concatenation of Wikipedia in 104 different languages. The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters. On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.\"}}", "category": "generic"}
{"question_id": 640, "text": " We need a model to evaluate the similarity between two sentences for a text-analysis application.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/all-MiniLM-L12-v1\", \"api_call\": \"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v1')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v1')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": [{\"name\": \"Sentence Embeddings Benchmark\", \"url\": \"https://seb.sbert.net\"}], \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model that maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 641, "text": " Can you generate Japanese audio files of the given text?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"mio/amadeus\", \"api_call\": \"No changes needed.\", \"api_arguments\": {\"model_name\": \"mio/amadeus\"}, \"python_environment_requirements\": {\"espnet\": \"d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\", \"transformers\": \"latest\"}, \"example_code\": \"cd espnet\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\npip install -e .\\ncd egs2/amadeus/tts1\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus\", \"performance\": {\"dataset\": \"amadeus\", \"accuracy\": \"Not provided\"}, \"description\": \"This model was trained by mio using amadeus recipe in espnet.\"}}", "category": "generic"}
{"question_id": 642, "text": " Setup a voice assistant to play Korean novel audiobooks.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"lakahaga/novel_reading_tts\", \"api_call\": \"AutoModelForTTS.from_pretrained('lakahaga/novel_reading_tts')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"inputs = processor(text, return_tensors='pt'); generated_audio = model.generate(**inputs);\", \"performance\": {\"dataset\": \"novelspeech\", \"accuracy\": null}, \"description\": \"This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\"}}", "category": "generic"}
{"question_id": 643, "text": " The language learning app developers have requested that we implement TTS functionality for their Chinese language study materials.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\", \"api_call\": \"Output: Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"torch\", \"espnet_model_zoo\"], \"example_code\": \"import soundfile\\nfrom espnet2.bin.tts_inference import Text2Speech\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\ntext = \\u6625\\u6c5f\\u6f6e\\u6c34\\u8fde\\u6d77\\u5e73\\uff0c\\u6d77\\u4e0a\\u660e\\u6708\\u5171\\u6f6e\\u751f\\nspeech = text2speech(text)[wav]\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\", \"performance\": {\"dataset\": \"csmsc\", \"accuracy\": \"Not specified\"}, \"description\": \"A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\"}}", "category": "generic"}
{"question_id": 644, "text": " I need a solution to transcribe voice memos from my phone into written text.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Transformers\", \"functionality\": \"Transcription\", \"api_name\": \"facebook/wav2vec2-base-960h\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\", \"api_arguments\": [\"input_values\"], \"python_environment_requirements\": [\"transformers\", \"datasets\", \"torch\", \"jiwer\"], \"example_code\": \"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\nfrom datasets import load_dataset\\nimport torch\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\\nlogits = model(input_values).logits\\npredicted_ids = torch.argmax(logits, dim=-1)\\ntranscription = processor.batch_decode(predicted_ids)\", \"performance\": {\"dataset\": \"LibriSpeech\", \"accuracy\": {\"clean\": 3.4, \"other\": 8.6}}, \"description\": \"Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.\"}}", "category": "generic"}
{"question_id": 645, "text": " Develop an automatic speech recognition system for a customer service hotline.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transcription and Translation\", \"api_name\": \"openai/whisper-small\", \"api_call\": \"WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')\", \"api_arguments\": {\"language\": \"english\", \"task\": \"transcribe\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"datasets\": \"latest\"}, \"example_code\": [\"from transformers import WhisperProcessor, WhisperForConditionalGeneration\", \"from datasets import load_dataset\", \"processor = WhisperProcessor.from_pretrained(openai/whisper-small)\", \"model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-small)\", \"model.config.forced_decoder_ids = None\", \"ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\", \"sample = ds[0][audio]\", \"input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\", \"predicted_ids = model.generate(input_features)\", \"transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\", \"print(transcription)\"], \"performance\": {\"dataset\": \"LibriSpeech (clean) test set\", \"accuracy\": \"3.432 WER\"}, \"description\": \"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages.\"}}", "category": "generic"}
{"question_id": 646, "text": " I would like to clean up some noisy audio recordings so they can be played at a conference.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"ESPnet\", \"functionality\": \"Audio-to-Audio\", \"api_name\": \"lichenda/wsj0_2mix_skim_noncausal\", \"api_call\": \"No changes needed.\", \"api_arguments\": [], \"python_environment_requirements\": [\"python 3.7.11\", \"espnet 0.10.7a1\", \"pytorch 1.8.1\"], \"example_code\": \"./run.sh --skip_data_prep false --skip_train true --download_model lichenda/wsj0_2mix_skim_noncausal\", \"performance\": {\"dataset\": \"enhanced_cv_min_8k\", \"accuracy\": {\"STOI\": 0.96, \"SAR\": 19.17, \"SDR\": 18.7, \"SIR\": 29.56}}, \"description\": \"This model was trained by LiChenda using wsj0_2mix recipe in espnet.\"}}", "category": "generic"}
{"question_id": 647, "text": " Our company is developing an app that separates different speakers in a podcast recording. We need to find a transformer model that can help us achieve this.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Asteroid\", \"api_name\": \"ConvTasNet_Libri2Mix_sepclean_8k\", \"api_call\": \"Output: hf_hub_download(repo_id=JorisCos/ConvTasNet_Libri2Mix_sepclean_8k, filename=pretrained_model.pth)\", \"api_arguments\": [\"repo_id\", \"filename\"], \"python_environment_requirements\": [\"huggingface_hub\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"Libri2Mix\", \"accuracy\": {\"si_sdr\": 14.764543634468069, \"si_sdr_imp\": 14.764029375607246, \"sdr\": 15.29337970745095, \"sdr_imp\": 15.114146605113111, \"sir\": 24.092904661115366, \"sir_imp\": 23.913669683141528, \"sar\": 16.06055906916849, \"sar_imp\": -51.980784441287454, \"stoi\": 0.9311142440593033, \"stoi_imp\": 0.21817376142710482}}, \"description\": \"This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.\"}}", "category": "generic"}
{"question_id": 648, "text": " Convert the Hokkien speech into English speech so that everyone can understand it.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"Speech-to-speech translation\", \"api_name\": \"xm_transformer_s2ut_hk-en\", \"api_call\": \"'S2THubInterface.get_prediction(task, models[0].cpu(), generator, sample)'\", \"api_arguments\": {\"task\": \"speech_to_text\", \"model\": \"facebook/xm_transformer_s2ut_hk-en\", \"generator\": \"task.build_generator([model], cfg)\", \"sample\": \"S2THubInterface.get_model_input(task, audio)\"}, \"python_environment_requirements\": {\"fairseq\": \"latest\", \"torchaudio\": \"latest\", \"huggingface_hub\": \"latest\"}, \"example_code\": \"import json\\nimport os\\nfrom pathlib import Path\\nimport IPython.display as ipd\\nfrom fairseq import hub_utils\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\nfrom huggingface_hub import snapshot_download\\nimport torchaudio\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\n facebook/xm_transformer_s2ut_hk-en,\\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\\n cache_dir=cache_dir,\\n)\\nmodel = models[0].cpu()\\ncfg[task].cpu = True\\ngenerator = task.build_generator([model], cfg)\\naudio, _ = torchaudio.load(/path/to/an/audio/file)\\nsample = S2THubInterface.get_model_input(task, audio)\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\nlibrary_name = fairseq\\ncache_dir = (\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\n)\\ncache_dir = snapshot_download(\\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\\n)\\nx = hub_utils.from_pretrained(\\n cache_dir,\\n model.pt,\\n .,\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\n config_yaml=config.json,\\n fp16=False,\\n is_vocoder=True,\\n)\\nwith open(f{x['args']['data']}/config.json) as f:\\n vocoder_cfg = json.load(f)\\nassert (\\n len(x[args][model_path]) == 1\\n), Too many vocoder models in the input\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\ntts_sample = tts_model.get_model_input(unit)\\nwav, sr = tts_model.get_prediction(tts_sample)\\nipd.Audio(wav, rate=sr)\", \"performance\": {\"dataset\": \"TED, drama, TAT domain\", \"accuracy\": \"Not provided\"}, \"description\": \"Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain.\"}}", "category": "generic"}
{"question_id": 649, "text": " Develop an audio-to-audio translation script that converts a short audio file from one language to another without using text.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"textless_sm_sl_es\", \"api_call\": \"textless_sm_sl_es()\", \"api_arguments\": null, \"python_environment_requirements\": \"fairseq\", \"example_code\": \"https://huggingface.co/facebook/textless_sm_cs_en\", \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"A Fairseq model for audio-to-audio speech-to-speech translation.\"}}", "category": "generic"}
{"question_id": 650, "text": " The company's call center needs a solution to enhance the quality of noisy recordings for better customer interactions.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Enhancement\", \"api_name\": \"sepformer-wham-enhancement\", \"api_call\": \"model.separate_file(path='speechbrain/sepformer-wham-enhancement/example_wham.wav')\", \"api_arguments\": [\"path\"], \"python_environment_requirements\": [\"pip install speechbrain\"], \"example_code\": \"from speechbrain.pretrained import SepformerSeparation as separator\\nimport torchaudio\\nmodel = separator.from_hparams(source='speechbrain/sepformer-wham-enhancement', savedir='pretrained_models/sepformer-wham-enhancement')\\nest_sources = model.separate_file(path='speechbrain/sepformer-wham-enhancement/example_wham.wav')\\ntorchaudio.save('enhanced_wham.wav', est_sources[:, :, 0].detach().cpu(), 8000)\", \"performance\": {\"dataset\": \"WHAM!\", \"accuracy\": \"14.35 dB SI-SNR\"}, \"description\": \"This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.\"}}", "category": "generic"}
{"question_id": 651, "text": " I want to create an app for emotion recognition, which can detect emotions in people's voices based on a given audio file.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"Rajaram1996/Hubert_emotion\", \"api_call\": \"HubertForSpeechClassification.from_pretrained('Rajaram1996/Hubert_emotion')\", \"api_arguments\": {\"audio_file\": \"string\"}, \"python_environment_requirements\": [\"audio_models\", \"transformers\", \"torch\", \"numpy\", \"pydub\"], \"example_code\": \"def predict_emotion_hubert(audio_file):\\n from audio_models import HubertForSpeechClassification\\n from transformers import Wav2Vec2FeatureExtractor, AutoConfig\\n import torch.nn.functional as F\\n import torch\\n import numpy as np\\n from pydub import AudioSegment\\nmodel = HubertForSpeechClassification.from_pretrained(Rajaram1996/Hubert_emotion)\\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(facebook/hubert-base-ls960)\\nsampling_rate=16000\\nconfig = AutoConfig.from_pretrained(Rajaram1996/Hubert_emotion)\\ndef speech_file_to_array(path, sampling_rate):\\n sound = AudioSegment.from_file(path)\\n sound = sound.set_frame_rate(sampling_rate)\\n sound_array = np.array(sound.get_array_of_samples())\\n return sound_array\\nsound_array = speech_file_to_array(audio_file, sampling_rate)\\ninputs = feature_extractor(sound_array, sampling_rate=sampling_rate, return_tensors=pt, padding=True)\\ninputs = {key: inputs[key].to(cpu).float() for key in inputs}\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\nscores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\\noutputs = [{\\n emo: config.id2label[i],\\n score: round(score * 100, 1)}\\n for i, score in enumerate(scores)\\n]\\nreturn [row for row in sorted(outputs, key=lambda x:x[score], reverse=True) if row['score'] != '0.0%'][:2]\\nresult = predict_emotion_hubert(male-crying.mp3)\\nresult\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": \"unknown\"}, \"description\": \"A pretrained model for predicting emotion in local audio files using Hubert.\"}}", "category": "generic"}
{"question_id": 652, "text": " Our company provides a global voice assistant. We need to identify the language used in incoming calls to provide the right service to callers in their language. \\n \n Use this API documentation for reference:  {\"domain\": \"Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Language Identification\", \"api_name\": \"sanchit-gandhi/whisper-medium-fleurs-lang-id\", \"api_call\": \"AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')\", \"api_arguments\": [\"model = AutoModelForSpeechClassification.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')\", \"processor = Wav2Vec2Processor.from_pretrained('sanchit-gandhi/whisper-medium-fleurs-lang-id')\"], \"python_environment_requirements\": [\"transformers==4.27.0.dev0\", \"pytorch==1.13.1\", \"datasets==2.9.0\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"google/xtreme_s\", \"accuracy\": 0.8805}, \"description\": \"This model is a fine-tuned version of openai/whisper-medium on the FLEURS subset of the google/xtreme_s dataset. It is used for language identification in audio classification tasks.\"}}", "category": "generic"}
{"question_id": 653, "text": " Analyze a recording of a Russian customer service representative to determine their emotion during the conversation.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"wav2vec2-xlsr-53-russian-emotion-recognition\", \"api_call\": \"Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-xlsr-53') .predict('/path/to/russian_audio_speech.wav', sampling_rate=16000)\", \"api_arguments\": {\"path\": \"/path/to/russian_audio_speech.wav\", \"sampling_rate\": 16000}, \"python_environment_requirements\": [\"torch\", \"torchaudio\", \"transformers\", \"librosa\", \"numpy\"], \"example_code\": \"result = predict('/path/to/russian_audio_speech.wav', 16000)\\nprint(result)\", \"performance\": {\"dataset\": \"Russian Emotional Speech Dialogs\", \"accuracy\": \"72%\"}, \"description\": \"A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.\"}}", "category": "generic"}
{"question_id": 654, "text": " Create a system that can detect voice activity in audio files, estimate the speech-to-noise ratio, and measure the C50 room acoustics.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Voice Activity Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\", \"api_name\": \"pyannote/brouhaha\", \"api_call\": \"Output: Model.from_pretrained('pyannote/brouhaha', use_auth_token='ACCESS_TOKEN_GOES_HERE')\", \"api_arguments\": [\"audio.wav\"], \"python_environment_requirements\": [\"pyannote-audio\", \"brouhaha-vad\"], \"example_code\": [\"from pyannote.audio import Model\", \"model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\", \"from pyannote.audio import Inference\", \"inference = Inference(model)\", \"output = inference(audio.wav)\", \"for frame, (vad, snr, c50) in output:\", \"  t = frame.middle\", \"  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\"], \"performance\": {\"dataset\": \"LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\", \"accuracy\": \"Not provided\"}, \"description\": \"Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\"}}", "category": "generic"}
{"question_id": 655, "text": " I have a set of plant observations with various features, and I need to categorize them into different species. What classifier can you suggest and what do I need to implement it?\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Joblib\", \"functionality\": \"Multi-class Classification\", \"api_name\": \"9705278\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"scikit-learn/iris\", \"accuracy\": 0.8666666666666667}, \"description\": \"A tabular classification model trained on the Iris dataset using XGBoost and AutoTrain. The model is capable of multi-class classification and has an accuracy of 86.67%.\"}}", "category": "generic"}
{"question_id": 656, "text": " Generate predictions for carbon emissions from a dataset provided as a .csv file to make our factory more eco-friendly.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"pcoloc/autotrain-mikrotik-7-7-1860563588\", \"api_call\": \"Output: model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"pcoloc/autotrain-data-mikrotik-7-7\", \"accuracy\": {\"Loss\": 48.213, \"R2\": 0.654, \"MSE\": 2324.518, \"MAE\": 32.634, \"RMSLE\": 0.586}}, \"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.\"}}", "category": "generic"}
{"question_id": 657, "text": " I want to predict future carbon emissions within a location. Develop an automation tool that provides continuous data analysis and estimates future changes.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"2037366891\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"json\", \"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"Robertooo/autotrain-data-hmaet\", \"accuracy\": {\"Loss\": 0.067, \"R2\": 0.486, \"MSE\": 0.005, \"MAE\": 0.055, \"RMSLE\": 0.036}}, \"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions.\"}}", "category": "generic"}
{"question_id": 658, "text": " I am an environmental expert, and my job is to predict carbon emissions given a dataset. The dataset has columns for different factors affecting emissions.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"1860863627\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": {\"data\": \"data.csv\"}, \"python_environment_requirements\": {\"import json\": \"\", \"import joblib\": \"\", \"import pandas as pd\": \"\"}, \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"pcoloc/autotrain-data-dragino-7-7-max_495m\", \"accuracy\": {\"Loss\": 72.73, \"R2\": 0.386, \"MSE\": 5289.6, \"MAE\": 60.23, \"RMSLE\": 0.436}}, \"description\": \"A tabular regression model for predicting carbon emissions using the Joblib framework. Trained on the pcoloc/autotrain-data-dragino-7-7-max_495m dataset.\"}}", "category": "generic"}
{"question_id": 659, "text": " Can you analyze the dataset containing information about electric cars and predict the carbon emissions for specific vehicles?\\n###Input: {'data': 'data.csv'}\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Hugging Face\", \"functionality\": \"Carbon Emissions Prediction\", \"api_name\": \"bibekbehera/autotrain-numeric_prediction-40376105019\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"bibekbehera/autotrain-data-numeric_prediction\", \"accuracy\": {\"Loss\": 0.152, \"R2\": 0.659, \"MSE\": 0.023, \"MAE\": 0.062, \"RMSLE\": 0.105}}, \"description\": \"A tabular regression model trained with AutoTrain to predict carbon emissions based on input features.\"}}", "category": "generic"}
{"question_id": 660, "text": " You are designing a gaming AI and you want to use the existing sb3/dqn-Acrobot-v1 model as your starting point. Train your gaming AI using this model.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"Acrobot-v1\", \"api_name\": \"sb3/dqn-Acrobot-v1\", \"api_call\": \"No changes needed.\", \"api_arguments\": {\"algo\": \"dqn\", \"env\": \"Acrobot-v1\", \"logs_folder\": \"logs/\"}, \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": [\"python -m rl_zoo3.load_from_hub --algo dqn --env Acrobot-v1 -orga sb3 -f logs/\", \"python enjoy.py --algo dqn --env Acrobot-v1 -f logs/\", \"python train.py --algo dqn --env Acrobot-v1 -f logs/\", \"python -m rl_zoo3.push_to_hub --algo dqn --env Acrobot-v1 -f logs/ -orga sb3\"], \"performance\": {\"dataset\": \"Acrobot-v1\", \"accuracy\": \"-72.10 +/- 6.44\"}, \"description\": \"This is a trained model of a DQN agent playing Acrobot-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 661, "text": " A gaming company is eager to develop a new reinforcement learning game based on ant locomotion. We need to use the pre-trained model.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"td3-Ant-v3\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"RL Zoo: https://github.com/DLR-RM/rl-baselines3-zoo\", \"SB3: https://github.com/DLR-RM/stable-baselines3\", \"SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\"], \"example_code\": [\"python -m rl_zoo3.load_from_hub --algo td3 --env Ant-v3 -orga sb3 -f logs/\", \"python enjoy.py --algo td3 --env Ant-v3 -f logs/\", \"python train.py --algo td3 --env Ant-v3 -f logs/\", \"python -m rl_zoo3.push_to_hub --algo td3 --env Ant-v3 -f logs/ -orga sb3\"], \"performance\": {\"dataset\": \"Ant-v3\", \"accuracy\": \"5822.96 +/- 93.33\"}, \"description\": \"This is a trained model of a TD3 agent playing Ant-v3 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 662, "text": " I have a Soccer simulation with two teams, and I would like to train agents to play effectively in the simulation.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"Train and resume training of poca agent playing SoccerTwos\", \"api_name\": \"LukeSajkowski/SoccerTwos\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\", \"ML-Agents-SoccerTwos\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library\"}}", "category": "generic"}
{"question_id": 663, "text": " The factory needs an intelligent robot to perform complex tasks with real-time interaction. The robot needs to have a visual model that can manipulate objects and navigate in an indoor environment.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning Robotics\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"EmbodiedAI tasks, such as object manipulation and indoor navigation\", \"api_name\": \"facebook/vc1-large\", \"api_call\": \"Output: model_utils.load_model(model_utils.VC1_BASE_NAME)(transformed_img)\", \"api_arguments\": \"img\", \"python_environment_requirements\": \"from vc_models.models.vit import model_utils\", \"example_code\": \"model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\nimg = your_function_here ...\\ntransformed_img = model_transforms(img)\\nembedding = model(transformed_img)\", \"performance\": {\"dataset\": \"CortexBench\", \"accuracy\": \"68.7 (Mean Success)\"}, \"description\": \"The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.\"}}", "category": "generic"}
{"question_id": 664, "text": " Please generate a representation of an image from the URL: 'http://images.cocodataset.org/val2017/000000039769.jpg'\\n###Input: http://images.cocodataset.org/val2017/000000039769.jpg\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"facebook/dino-vits8\", \"api_call\": \"ViTModel.from_pretrained('facebook/dino-vits8')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": null}, \"description\": \"Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, HervJou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.\"}}", "category": "generic"}
{"question_id": 665, "text": " Classify the content of the video clip.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"microsoft/xclip-base-patch16-zero-shot\", \"api_call\": \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": [{\"name\": \"HMDB-51\", \"accuracy\": 44.6}, {\"name\": \"UCF-101\", \"accuracy\": 72.0}, {\"name\": \"Kinetics-600\", \"accuracy\": 65.2}]}, \"description\": \"X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\"}}", "category": "generic"}
{"question_id": 666, "text": " Our client wants to create an image from a text description, and also needs to inpaint a specific part of the image using a mask.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Generation\", \"api_name\": \"runwayml/stable-diffusion-inpainting\", \"api_call\": \"pipe(prompt=prompt, image=image, mask_image=mask_image)\", \"api_arguments\": {\"prompt\": \"Text prompt\", \"image\": \"PIL image\", \"mask_image\": \"PIL image (mask)\"}, \"python_environment_requirements\": {\"diffusers\": \"from diffusers import StableDiffusionInpaintPipeline\"}, \"example_code\": {\"import_code\": \"from diffusers import StableDiffusionInpaintPipeline\", \"instantiate_code\": \"pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)\", \"generate_image_code\": \"image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\", \"save_image_code\": \"image.save(./yellow_cat_on_park_bench.png)\"}, \"performance\": {\"dataset\": {\"name\": \"LAION-2B (en)\", \"accuracy\": \"Not optimized for FID scores\"}}, \"description\": \"Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\"}}", "category": "generic"}
{"question_id": 667, "text": " Develop an AI service that generates pictures of people in the context of various professions, such as a doctor, a teacher, or a chef.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"vintedois-diffusion-v0-1\", \"api_call\": \"text2img = pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')\", \"api_arguments\": [\"prompt\", \"CFG Scale\", \"Scheduler\", \"Steps\", \"Seed\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)\", \"performance\": {\"dataset\": \"large amount of high quality images\", \"accuracy\": \"not specified\"}, \"description\": \"Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\"}}", "category": "generic"}
{"question_id": 668, "text": " I need the GPT model to generate a higher-resolution image of a small simple yellow bird in a forest.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image generation and modification based on text prompts\", \"api_name\": \"stabilityai/stable-diffusion-x4-upscaler\", \"api_call\": \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler', torch_dtype=torch.float16)\", \"api_arguments\": {\"model_id\": \"stabilityai/stable-diffusion-x4-upscaler\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\", \"xformers (optional, for memory efficient attention)\"], \"example_code\": \"pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\"}}", "category": "generic"}
{"question_id": 669, "text": " I would like to recognize the handwritten text in an image in the future with a given URL.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-base-handwritten\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"microsoft/trocr-base-handwritten\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"IAM\", \"accuracy\": \"Not specified\"}, \"description\": \"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 670, "text": " At a party, people want to know what is in the image displayed in the living room. Could you help them answer some questions related to the image?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-textvqa\", \"api_call\": \"git_base_textvqa = AutoModel.from_pretrained('microsoft/git-base-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}}", "category": "generic"}
{"question_id": 671, "text": " Our clothes store client wants to build a webpage that suggests the right combination or matching colors for various types of fashion items.\\n###Input: {\\\"image_path\\\": \\\"path/to/fashion_item_image.jpg\\\", \\\"question\\\": \\\"What colors should be combined with the color in this fashion item?\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"api_call\": \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", \"api_arguments\": {\"model\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"tokenizer\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": \"vqa(image_path, question)\", \"performance\": {\"dataset\": \"JosephusCheung/GuanacoVQADataset\", \"accuracy\": \"unknown\"}, \"description\": \"A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\"}}", "category": "generic"}
{"question_id": 672, "text": " Can you guide me on how to extract specific information from a document using a model that can answer questions based on the document layout?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"layoutlm-vqa\", \"api_call\": \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", \"api_arguments\": {\"question\": \"string\", \"context\": \"string\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for document question answering using the LayoutLM architecture.\"}}", "category": "generic"}
{"question_id": 673, "text": " We are an educational institution experiencing so many forms getting filled out. For our research purpose we want to pick a particular answer \\\"parent's name\\\" given a picture of form filled out.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"vision-encoder-decoder\", \"api_name\": \"jinhybr/OCR-DocVQA-Donut\", \"api_call\": \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut').model.vision_encoder\", \"api_arguments\": \"image_path, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\"}}", "category": "generic"}
{"question_id": 674, "text": " I am an accountant in a company, and I need to extract information from invoices. Can you please assist me in designing a solution for it?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"DataIntelligenceTeam/eurocorpV4\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('DataIntelligenceTeam/eurocorpV4')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers>=4.26.0.dev0, torch>=1.12.1+cu113, datasets>=2.2.2, tokenizers>=0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"sroie\", \"accuracy\": 0.982}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv3-large on the sroie dataset. It achieves the following results on the evaluation set: Loss: 0.1239, Precision: 0.9548, Recall: 0.9602, F1: 0.9575, Accuracy: 0.9819\"}}", "category": "generic"}
{"question_id": 675, "text": " Design a system that takes document images and user questions to provide answers from the document.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"\"}, \"description\": \"A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}", "category": "generic"}
{"question_id": 676, "text": " I have a document containing a college transcript. I'd like to know the student's overall GPA displayed on the transcript.\\n###Input: \\\"your_question\\\": \\\"What is the overall GPA on the transcript?\\\", \\\"your_context\\\": \\\"Multimodal Document Question Answer\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_arguments\": \"{'question': 'your_question', 'context': 'your_context'}\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}", "category": "generic"}
{"question_id": 677, "text": " Our client wants to categorize their product images into different categories. Prepare a solution to classify those images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lysandre/tiny-vit-random\", \"api_call\": \"ViTForImageClassification.from_pretrained('lysandre/tiny-vit-random')\", \"api_arguments\": \"image_path\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny-vit-random model for image classification using Hugging Face Transformers.\"}}", "category": "generic"}
{"question_id": 678, "text": " I need to find a way to automatically detect objects in images, ideally using a model suited towards smaller images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"hustvl/yolos-tiny\", \"api_call\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"28.7 AP\"}, \"description\": \"YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\"}}", "category": "generic"}
{"question_id": 679, "text": " Our e-sports team needs to detect objects like enemies, teammates, and dropped spike in the Valorant game.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-valorant-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"valorant-object-detection\", \"accuracy\": 0.965}, \"description\": \"A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\"}}", "category": "generic"}
{"question_id": 680, "text": " Our company is responsible for the safety of workers in a warehouse. We need to detect forklifts and persons in the surveillance images to prevent accidents.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-forklift-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-forklift-detection').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"forklift-object-detection\", \"accuracy\": 0.846}, \"description\": \"A YOLOv8 model for detecting forklifts and persons in images.\"}}", "category": "generic"}
{"question_id": 681, "text": " We want to extract the outline and label each element in an image of a factory.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"facebook/detr-resnet-50-panoptic\", \"api_call\": \"DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"torch\", \"numpy\", \"transformers\", \"PIL\", \"requests\", \"io\"], \"example_code\": [\"import io\", \"import requests\", \"from PIL import Image\", \"import torch\", \"import numpy\", \"from transformers import DetrFeatureExtractor, DetrForSegmentation\", \"from transformers.models.detr.feature_extraction_detr import rgb_to_id\", \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", \"image = Image.open(requests.get(url, stream=True).raw)\", \"feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"model = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"inputs = feature_extractor(images=image, return_tensors='pt')\", \"outputs = model(**inputs)\", \"processed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\", \"result = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]\", \"panoptic_seg = Image.open(io.BytesIO(result['png_string']))\", \"panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8)\", \"panoptic_seg_id = rgb_to_id(panoptic_seg)\"], \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": {\"box_AP\": 38.8, \"segmentation_AP\": 31.1, \"PQ\": 43.4}}, \"description\": \"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 682, "text": " A client needs to generate handwritten texts using controlnets with diffusion. Implement the controlnet model with canny edges and generate a colorized image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"lllyasviel/sd-controlnet-canny\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny', torch_dtype=torch.float16)\", \"api_arguments\": {\"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": {\"opencv\": \"pip install opencv-contrib-python\", \"diffusers\": \"pip install diffusers transformers accelerate\"}, \"example_code\": \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", \"performance\": {\"dataset\": \"3M edge-image, caption pairs\", \"accuracy\": \"600 GPU-hours with Nvidia A100 80G\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 683, "text": " For my online art gallery, I need to create an image of a \\\"mystical forest with colorful animals\\\" based on textual description.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation\", \"api_name\": \"lllyasviel/control_v11p_sd15_softedge\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_softedge\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux==0.3.0\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"ControlNet\", \"accuracy\": \"Not provided\"}, \"description\": \"Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}", "category": "generic"}
{"question_id": 684, "text": " As a photographer, I would like to generate an image of \\\"a tropical beach at sunset\\\" using text-to-image generation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation model\", \"api_name\": \"lllyasviel/control_v11p_sd15_normalbae\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": [\"checkpoint\", \"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}", "category": "generic"}
{"question_id": 685, "text": " A group of gamers needs to generate unique Minecraft skins for their Minecraft characters.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"Minecraft-Skin-Diffusion\", \"api_call\": \"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion')\", \"api_arguments\": {}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion')\\nimage = pipeline().images[0].convert('RGBA')\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods.\"}}", "category": "generic"}
{"question_id": 686, "text": " I have video frames and I am going to build a tool to classify their activities.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-base-finetuned-ssv2\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-ssv2')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(8, 3, 224, 224))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-ssv2)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-ssv2)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something Something v2\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 687, "text": " Create a smart security system that can detect any suspicious person inside an indoor building captured in a video.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base-finetuned-ssv2\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\", \"api_arguments\": \"video\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n    logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": {\"top-1\": 70.6, \"top-5\": 92.6}}, \"description\": \"VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 688, "text": " An art gallery requires a tool for automatically classifying pictures based on their content. Use an appropriate API to predict the content of an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", \"api_call\": \"pipeline('zero-shot-classification', model='laion/CLIP-ViT-B-32-laion2B-s34B-b79K')\", \"api_arguments\": {\"image\": \"path/to/image\", \"class_names\": [\"class1\", \"class2\", \"class3\"]}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"from transformers import pipeline; classifier = pipeline('zero-shot-classification', model='laion/CLIP-ViT-B-32-laion2B-s34B-b79K'); classifier(image='path/to/image', class_names=['class1', 'class2', 'class3'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": 66.6}, \"description\": \"A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. It enables researchers to better understand and explore zero-shot, arbitrary image classification. The model can be used for zero-shot image classification, image and text retrieval, among others.\"}}", "category": "generic"}
{"question_id": 689, "text": " I have an image and want to know whether it is a picture of a car, a bicycle, or a person. How can I do this?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-large-patch14\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\", \"api_arguments\": {\"text\": [\"a photo of a cat\", \"a photo of a dog\"], \"images\": \"image\", \"return_tensors\": \"pt\", \"padding\": \"True\"}, \"python_environment_requirements\": {\"packages\": [\"PIL\", \"requests\", \"transformers\"]}, \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-large-patch14)\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-large-patch14)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies depending on the dataset\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}", "category": "generic"}
{"question_id": 690, "text": " Implement a program that can identify if an image is showing animals, food, or vehicles.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-base-patch32\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\", \"api_arguments\": [\"text\", \"images\", \"return_tensors\", \"padding\"], \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-base-patch32)\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}", "category": "generic"}
{"question_id": 691, "text": " Can you create a function to identify the presence of animals in photos, like cats, dogs, and birds? The model must also classify the animal type.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\", \"api_arguments\": {\"image\": \"path to image file\", \"class_names\": \"list of possible class names (comma-separated)\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; model = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K'); model('path/to/image.jpg', ['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8% to 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. These models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. They can be used for zero-shot image classification, image and text retrieval, and other tasks.\"}}", "category": "generic"}
{"question_id": 692, "text": " Develop a simple text box for users to test the language detection model.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Language Detection\", \"api_name\": \"papluca/xlm-roberta-base-language-detection\", \"api_call\": \"pipeline('text-classification', model='papluca/xlm-roberta-base-language-detection')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"language_detection('Hello, how are you?')\", \"performance\": {\"dataset\": \"Language Identification\", \"accuracy\": 0.996}, \"description\": \"This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset. It is an XLM-RoBERTa transformer model with a classification head on top, and can be used as a language detector for sequence classification tasks. It supports 20 languages including Arabic, Bulgarian, German, Greek, English, Spanish, French, Hindi, Italian, Japanese, Dutch, Polish, Portuguese, Russian, Swahili, Thai, Turkish, Urdu, Vietnamese, and Chinese.\"}}", "category": "generic"}
{"question_id": 693, "text": " As a support agent for a streaming service, we want to know how the users feel about their content.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"finiteautomata/bertweet-base-sentiment-analysis\", \"api_call\": \"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"Transformers\", \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\nresult = nlp('I love this movie!')\", \"performance\": {\"dataset\": \"SemEval 2017\", \"accuracy\": null}, \"description\": \"Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is BERTweet, a RoBERTa model trained on English tweets. Uses POS, NEG, NEU labels.\"}}", "category": "generic"}
{"question_id": 694, "text": " A manager of an e-commerce store needs to automatically classify customer reviews based on their emotional content.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"joeddav/distilbert-base-uncased-go-emotions-student\", \"api_call\": \"pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\", \"api_arguments\": \"text\", \"python_environment_requirements\": [\"transformers\", \"torch\", \"tensorflow\"], \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\nresult = nlp('I am so happy today!')\", \"performance\": {\"dataset\": \"go_emotions\"}, \"description\": \"This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data.\"}}", "category": "generic"}
{"question_id": 695, "text": " I have recently authored an online article, and I need to make sure it's safe for a work environment. Can you check if it contains explicit content?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"michellejieli/NSFW_text_classifier\", \"api_call\": \"pipeline('sentiment-analysis', model='michellejieli/NSFW_text_classification')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"classifier(I see youve set aside this special time to humiliate yourself in public.)\", \"performance\": {\"dataset\": \"Reddit posts\", \"accuracy\": \"Not specified\"}, \"description\": \"DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\"}}", "category": "generic"}
{"question_id": 696, "text": " The company wants to build a product that answers user queries based on provided tables. How to approach this?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-base-finetuned-wtq\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\", \"api_arguments\": {\"model_name\": \"google/tapas-base-finetuned-wtq\"}, \"python_environment_requirements\": {\"transformers\": \"4.12.0\"}, \"example_code\": \"from transformers import TapasTokenizer, TapasForQuestionAnswering\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-base-finetuned-wtq')\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": 0.4638}, \"description\": \"TAPAS base model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion, and then fine-tuned on SQA, WikiSQL, and finally WTQ. It can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 697, "text": " A news agency wants to extract names of people, organizations, and locations mentioned in a given article.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Flair\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-ontonotes-large\", \"api_call\": \"SequenceTagger.load('flair/ner-english-ontonotes-large')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\n# load tagger\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\\n# make example sentence\\nsentence = Sentence('On September 1st George won 1 dollar while watching Game of Thrones.')\\n# predict NER tags\\ntagger.predict(sentence)\\n# print sentence\\nprint(sentence)\\n# print predicted NER spans\\nprint('The following NER tags are found:')\\n# iterate over entities and print\\nfor entity in sentence.get_spans('ner'):\\n    print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": 90.93}, \"description\": \"English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.\"}}", "category": "generic"}
{"question_id": 698, "text": " Our users need to answer questions based on the given data in a table format. We need a model to assist them.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-base-finetuned-wikisql-supervised\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wikisql-supervised')\", \"api_arguments\": [\"question\", \"table\"], \"python_environment_requirements\": [\"PyTorch\", \"TensorFlow\"], \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It was pretrained with two objectives: Masked language modeling (MLM) and Intermediate pre-training. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQA and WikiSQL.\"}}", "category": "generic"}
{"question_id": 699, "text": " We are a sports analytics company. I need to know in which year Beijing hosted the Olympic Games from a given table of Olympic years and cities.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/tapex-large-finetuned-wtq\", \"api_call\": \"BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wtq')\", \"api_arguments\": {\"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"str\"}, \"python_environment_requirements\": [\"transformers\", \"pandas\"], \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-finetuned-wtq)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-finetuned-wtq)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiTableQuestions dataset.\"}}", "category": "generic"}
{"question_id": 700, "text": " We are designing a website that offers a question and answer service for users to provide information based on tables of data. Help us generate answers from given tables.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"microsoft/tapex-large-finetuned-wikisql\", \"api_call\": \"Output: BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\", \"api_arguments\": {\"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"query\"}, \"python_environment_requirements\": {\"transformers\": \"TapexTokenizer, BartForConditionalGeneration\", \"pandas\": \"pd\"}, \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"N/A\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiSQL dataset.\"}}", "category": "generic"}
{"question_id": 701, "text": " I have tables in my app and I want to create a functionality when users can ask questions and the system can pick the correct value from the table.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-small-finetuned-wikisql-supervised\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\", \"api_arguments\": \"model = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import TapasTokenizer, TapasForQuestionAnswering\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"Not specified\"}, \"description\": \"TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 702, "text": " We need to create a table-based assistant to respond to queries related to a specific dataset.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-mini-finetuned-sqa\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')\", \"api_arguments\": [\"model_name\", \"table\", \"queries\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.5148}, \"description\": \"TAPAS mini model fine-tuned on Sequential Question Answering (SQA)\"}}", "category": "generic"}
{"question_id": 703, "text": " We need an AI-based voice assistant for answering questions from a given text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"distilbert-base-uncased-distilled-squad\", \"api_call\": \"pipeline(\\\\question-answering\\\\, model='distilbert-base-uncased-distilled-squad')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nquestion_answerer = pipeline(question-answering, model='distilbert-base-uncased-distilled-squad')\\ncontext = r\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\n... \\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\nprint(\\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\\n...)\", \"performance\": {\"dataset\": \"SQuAD v1.1\", \"accuracy\": \"86.9 F1 score\"}, \"description\": \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\"}}", "category": "generic"}
{"question_id": 704, "text": " A student wants help identifying the main subject of a given text passage. Assist the student with the information they need.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"bert-large-uncased-whole-word-masking-finetuned-squad\", \"api_call\": \"AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\", \"api_arguments\": {\"model_name_or_path\": \"bert-large-uncased-whole-word-masking\", \"dataset_name\": \"squad\", \"do_train\": true, \"do_eval\": true, \"learning_rate\": 3e-05, \"num_train_epochs\": 2, \"max_seq_length\": 384, \"doc_stride\": 128, \"output_dir\": \"./examples/models/wwm_uncased_finetuned_squad/\", \"per_device_eval_batch_size\": 3, \"per_device_train_batch_size\": 3}, \"python_environment_requirements\": {\"torch\": \"1.9.0\", \"transformers\": \"4.9.2\"}, \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\\nresult = qa_pipeline({'question': 'What is the capital of France?', 'context': 'Paris is the capital of France.'})\\nprint(result)\", \"performance\": {\"dataset\": \"SQuAD\", \"accuracy\": {\"f1\": 93.15, \"exact_match\": 86.91}}, \"description\": \"BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model.\"}}", "category": "generic"}
{"question_id": 705, "text": " Review this article on cancer treatment, identify the types of therapies mentioned, and answer any questions I might have.\\n###Input: Cancer therapy has come a long way and has given rise to multiple treatment options. Some common cancer treatments include surgery, chemotherapy, and radiation-based treatments. Immunotherapy, an area gaining more attention, uses the body's immune system to fight cancer cells. Targeted therapies are another option, which selectively block the growth and spread of cancer cells. Finally, hormonal therapy helps regulate hormone levels to slow the growth of hormone-sensitive cancers.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"bigwiz83/sapbert-from-pubmedbert-squad2\", \"api_call\": \"pipeline('question-answering', model='bigwiz83/sapbert-from-pubmedbert-squad2')\", \"api_arguments\": [\"context\", \"question\"], \"python_environment_requirements\": [\"transformers==4.7.0\", \"torch==1.8.0\", \"datasets==1.4.1\", \"tokenizers==0.10.2\"], \"example_code\": \"qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can the model be loaded?'})\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": \"1.2582\"}, \"description\": \"This model is a fine-tuned version of cambridgeltl/SapBERT-from-PubMedBERT-fulltext on the squad_v2 dataset.\"}}", "category": "generic"}
{"question_id": 706, "text": " Determine whether the following statement is true, false, or unrelated: \\\"The earth is flat.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Cross-Encoder for Natural Language Inference\", \"api_name\": \"cross-encoder/nli-distilroberta-base\", \"api_call\": \"CrossEncoder('cross-encoder/nli-distilroberta-base')\", \"api_arguments\": \"('A man is eating pizza', 'A man eats something')\", \"python_environment_requirements\": [\"sentence_transformers\", \"transformers\"], \"example_code\": \"from transformers import pipeline\\nclassifier = pipeline(zero-shot-classification, model='cross-encoder/nli-distilroberta-base')\\nsent = Apple just announced the newest iPhone X\\ncandidate_labels = [technology, sports, politics]\\nres = classifier(sent, candidate_labels)\\nprint(res)\", \"performance\": {\"dataset\": \"SNLI and MultiNLI\", \"accuracy\": \"See SBERT.net - Pretrained Cross-Encoder for evaluation results\"}, \"description\": \"This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\"}}", "category": "generic"}
{"question_id": 707, "text": " Please create an AI that reads deployment meeting schedule requests and classifies them into high, medium, or low priority.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"typeform/mobilebert-uncased-mnli\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('typeform/mobilebert-uncased-mnli')\", \"api_arguments\": {\"pretrained_model\": \"typeform/mobilebert-uncased-mnli\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForSequenceClassification\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\ntokenizer = AutoTokenizer.from_pretrained(typeform/mobilebert-uncased-mnli)\\nmodel = AutoModelForSequenceClassification.from_pretrained(typeform/mobilebert-uncased-mnli)\", \"performance\": {\"dataset\": \"multi_nli\", \"accuracy\": \"More information needed\"}, \"description\": \"This model is the Multi-Genre Natural Language Inference (MNLI) fine-turned version of the uncased MobileBERT model. It can be used for the task of zero-shot classification.\"}}", "category": "generic"}
{"question_id": 708, "text": " I'm working on a text completion software that helps users finish their sentences. The model needs to generate the rest of given incomplete sentence.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": [\"Translation\", \"Summarization\", \"Question Answering\", \"Text Classification\", \"Text Regression\"], \"api_name\": \"t5-small\", \"api_call\": \"T5Model.from_pretrained('t5-small')\", \"api_arguments\": {\"input_ids\": \"input tokenized text\", \"decoder_input_ids\": \"input tokenized text for decoder\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import T5Tokenizer, T5Model\\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\\nmodel = T5Model.from_pretrained('t5-small')\\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"See research paper, Table 14 for full results\"}, \"description\": \"T5-Small is a Text-To-Text Transfer Transformer (T5) model with 60 million parameters. It is designed to perform a variety of NLP tasks, including machine translation, document summarization, question answering, and classification tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be fine-tuned for specific tasks.\"}}", "category": "generic"}
{"question_id": 709, "text": " Can you please help me communicate to my workers in Germany? My message to them is \\\"Gute Arbeit, weiter so!\\\"\\n###Input: Gute Arbeit, weiter so!\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-de-en\", \"api_call\": \"translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')\", \"api_arguments\": [\"source languages: de\", \"target languages: en\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"newssyscomb2009.de.en\": 29.4, \"news-test2008.de.en\": 27.8, \"newstest2009.de.en\": 26.8, \"newstest2010.de.en\": 30.2, \"newstest2011.de.en\": 27.4, \"newstest2012.de.en\": 29.1, \"newstest2013.de.en\": 32.1, \"newstest2014-deen.de.en\": 34.0, \"newstest2015-ende.de.en\": 34.2, \"newstest2016-ende.de.en\": 40.4, \"newstest2017-ende.de.en\": 35.7, \"newstest2018-ende.de.en\": 43.7, \"newstest2019-deen.de.en\": 40.1, \"Tatoeba.de.en\": 55.4}}, \"description\": \"A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 710, "text": " One of our customers submitted a request in French, and we need to translate it into English.\\n###Input: Bonjour, j'aimerais connare les dimensions de ce produit, s'il vous pla.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-fr-en\", \"api_call\": \"pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"translation_pipeline('Bonjour, comment  va?')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newsdiscussdev2015-enfr.fr.en\": 33.1, \"newsdiscusstest2015-enfr.fr.en\": 38.7, \"newssyscomb2009.fr.en\": 30.3, \"news-test2008.fr.en\": 26.2, \"newstest2009.fr.en\": 30.2, \"newstest2010.fr.en\": 32.2, \"newstest2011.fr.en\": 33.0, \"newstest2012.fr.en\": 32.8, \"newstest2013.fr.en\": 33.9, \"newstest2014-fren.fr.en\": 37.8, \"Tatoeba.fr.en\": 57.5}}}, \"description\": \"Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.\"}}", "category": "generic"}
{"question_id": 711, "text": " Your manager requested you to automate sending replies to Brazilian wholesalers' emails reporting the availability of goods.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Neural machine translation\", \"api_name\": \"opus-mt-tc-big-en-pt\", \"api_call\": \"Output: MarianMTModel.from_pretrained(model_name).generate(**tokenizer(src_text, return_tensors='pt', padding=True))\", \"api_arguments\": {\"src_text\": \"list of text strings with language tokens\"}, \"python_environment_requirements\": {\"transformers\": \"4.16.2\"}, \"example_code\": \"from transformers import MarianMTModel, MarianTokenizer\\nsrc_text = [\\n &gt;&gt;por&lt;&lt; Tom tried to stab me.,\\n &gt;&gt;por&lt;&lt; He has been to Hawaii several times.\\n]\\nmodel_name = pytorch-models/opus-mt-tc-big-en-pt\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\nmodel = MarianMTModel.from_pretrained(model_name)\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=pt, padding=True))\\nfor t in translated:\\n print( tokenizer.decode(t, skip_special_tokens=True) )\", \"performance\": {\"dataset\": [{\"name\": \"flores101-devtest\", \"accuracy\": 50.4}, {\"name\": \"tatoeba-test-v2021-08-07\", \"accuracy\": 49.6}]}, \"description\": \"Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world.\"}}", "category": "generic"}
{"question_id": 712, "text": " Translate the following Italian sentence into English: \\\"Mi chiamo Marco e lavoro come ingegnere.\\\"\\n###Input:  Mi chiamo Marco e lavoro come ingegnere.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Transformers\", \"functionality\": \"Translation\", \"api_name\": \"Helsinki-NLP/opus-mt-it-en\", \"api_call\": \"pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline\\ntranslation = pipeline('translation_it_to_en', model='Helsinki-NLP/opus-mt-it-en')('Ciao mondo!')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newssyscomb2009.it.en\": 35.3, \"newstest2009.it.en\": 34.0, \"Tatoeba.it.en\": 70.9}, \"chr-F\": {\"newssyscomb2009.it.en\": 0.6, \"newstest2009.it.en\": 0.594, \"Tatoeba.it.en\": 0.808}}}, \"description\": \"A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\"}}", "category": "generic"}
{"question_id": 713, "text": " A team working on news articles requests an AI tool to summarize articles automatically so they can save time digesting.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Summarization\", \"api_name\": \"facebook/bart-large-cnn\", \"api_call\": \"pipeline('summarization', model='facebook/bart-large-cnn')\", \"api_arguments\": [\"ARTICLE\", \"max_length\", \"min_length\", \"do_sample\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\\nARTICLE = ...\\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\", \"performance\": {\"dataset\": \"cnn_dailymail\", \"accuracy\": {\"ROUGE-1\": 42.949, \"ROUGE-2\": 20.815, \"ROUGE-L\": 30.619, \"ROUGE-LSUM\": 40.038}}, \"description\": \"BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\"}}", "category": "generic"}
{"question_id": 714, "text": " As a news agency, we need a summary of this long article: \\\"In many cities around the world, urban parks provide an opportunity for residents to escape the hustle and bustle of daily life, allowing them to relax and recharge. Whether walking the dog or playing with the kids, city dwellers can forget about the stresses of work and enjoy the simple pleasures nature provides. London's eight Royal Parks, for example, are a true urban oasis, covering over 5,000 acres of parkland and attracting millions of visitors each year. From Hyde Park to Kensington Gardens, these green spaces offer a range of activities and experiences, including boating on the Serpentine, picnicking in the sun or visiting numerous cultural attractions such as art galleries and historical landmarks.\\\"\\n###Input: \\\"In many cities around the world, urban parks provide an opportunity for residents to escape the hustle and bustle of daily life, allowing them to relax and recharge. Whether walking the dog or playing with the kids, city dwellers can forget about the stresses of work and enjoy the simple pleasures nature provides. London's eight Royal Parks, for example, are a true urban oasis, covering over 5,000 acres of parkland and attracting millions of visitors each year. From Hyde Park to Kensington Gardens, these green spaces offer a range of activities and experiences, including boating on the Serpentine, picnicking in the sun or visiting numerous cultural attractions such as art galleries and historical landmarks.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"google/pegasus-cnn_dailymail\", \"api_call\": \"Output: PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\nmodel_name = 'google/pegasus-cnn_dailymail'\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\ninputs = tokenizer.encode('input_text', return_tensors='pt')\\nsummary_ids = model.generate(inputs)\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"cnn_dailymail\", \"accuracy\": \"44.16/21.56/41.30\"}, \"description\": \"PEGASUS model for abstractive summarization, pretrained on the CNN/DailyMail dataset.\"}}", "category": "generic"}
{"question_id": 715, "text": " Translate a sentence from French to Spanish.\\n###Input: Bonjour, comment  va?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-fr-es\", \"api_call\": \"Helsinki-NLP/opus-mt-fr-es\", \"api_arguments\": {\"source_languages\": \"fr\", \"target_languages\": \"es\"}, \"python_environment_requirements\": {\"PyTorch\": \"1.0.0\", \"TensorFlow\": \"2.0\", \"Transformers\": \"4.0.0\"}, \"example_code\": \"translation('Bonjour, comment  va?')\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": {\"newssyscomb2009.fr.es\": 34.3, \"news-test2008.fr.es\": 32.5, \"newstest2009.fr.es\": 31.6, \"newstest2010.fr.es\": 36.5, \"newstest2011.fr.es\": 38.3, \"newstest2012.fr.es\": 38.1, \"newstest2013.fr.es\": 34.0, \"Tatoeba.fr.es\": 53.2}, \"chr-F\": {\"newssyscomb2009.fr.es\": 0.601, \"news-test2008.fr.es\": 0.583, \"newstest2009.fr.es\": 0.586, \"newstest2010.fr.es\": 0.616, \"newstest2011.fr.es\": 0.622, \"newstest2012.fr.es\": 0.619, \"newstest2013.fr.es\": 0.587, \"Tatoeba.fr.es\": 0.709}}}, \"description\": \"A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.\"}}", "category": "generic"}
{"question_id": 716, "text": " Give me the summary of the following news article: IBM has announced its plan to acquire Bluestacks, a leading mobile virtualization software, to integrate with its Bigfix endpoint management platform. The goal of this acquisition is to strengthen IBM's mobile device management capabilities and expand the services offered to commercial and government clients.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Summarization\", \"api_name\": \"it5-base-news-summarization\", \"api_call\": \"pipeline('summarization', model='it5/it5-base-news-summarization')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"newsum(Dal 31 maggio infine partita la piattaforma ITsART, a pidi un anno da quando \\u2013 durante il primo lockdown \\u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di na sorta di Netflix della cultura pensata per ffrire a tutto il mondo la cultura italiana a pagamento presto per dare giudizi definitivi sulla piattaforma, e di certo sardifficile farlo anche piavanti senza numeri precisi. Al momento, lunica cosa che si pufare guardare comfatto il sito, contare quanti contenuti ci sono (circa 700 titoli, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet Intanto, una cosa notata da piparti che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\", \"performance\": {\"dataset\": \"NewsSum-IT\", \"accuracy\": {\"Rouge1\": 0.339, \"Rouge2\": 0.16, \"RougeL\": 0.263}}, \"description\": \"IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\"}}", "category": "generic"}
{"question_id": 717, "text": " I have a long academic article due tomorrow, and it's too long. Help me summarize it into a more digestible format.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Summarization\", \"api_name\": \"pszemraj/long-t5-tglobal-base-16384-book-summary\", \"api_call\": \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained('pszemraj/long-t5-tglobal-base-16384-book-summary')\\\\nsummarizer = pipeline(\\\\n    summarization,\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nsummarizer(long_text)\", \"api_arguments\": [\"long_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nimport torch\\nsummarizer = pipeline(\\n summarization,\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\n device=0 if torch.cuda.is_available() else -1,\\n)\\nlong_text = Here is a lot of text I don't want to read. Replace me\\nresult = summarizer(long_text)\\nprint(result[0][summary_text])\", \"performance\": {\"dataset\": \"kmfoda/booksum\", \"accuracy\": {\"ROUGE-1\": 36.408, \"ROUGE-2\": 6.065, \"ROUGE-L\": 16.721, \"ROUGE-LSUM\": 33.34}}, \"description\": \"A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\"}}", "category": "generic"}
{"question_id": 718, "text": " I need to find inspiration for a speech I will give at my company's annual meeting. Create a conversation that starts with \\\"Our company is future driven.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Conversational\", \"api_name\": \"DialoGPT-medium-PALPATINE2\", \"api_call\": \"pipeline('text-generation', model='Filosofas/DialoGPT-medium-PALPATINE2')\", \"api_arguments\": \"message\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"api('Input a message to start chatting with Filosofas/DialoGPT-medium-PALPATINE2.')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A DialoGPT model trained for generating human-like conversational responses.\"}}", "category": "generic"}
{"question_id": 719, "text": " I want a model that can help me generate text to finish my sentences on topics related to artificial intelligence, particularly generative AI.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"cerebras/Cerebras-GPT-111M\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('cerebras/Cerebras-GPT-111M')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"cerebras/Cerebras-GPT-111M\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForCausalLM\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(cerebras/Cerebras-GPT-111M)\\nmodel = AutoModelForCausalLM.from_pretrained(cerebras/Cerebras-GPT-111M)\\ntext = Generative AI is \", \"performance\": {\"dataset\": \"The Pile\", \"accuracy\": {\"PILE_test_xent\": 2.566, \"Hella-Swag\": 0.268, \"PIQA\": 0.594, \"Wino-Grande\": 0.488, \"Lambada\": 0.194, \"ARC-e\": 0.38, \"ARC-c\": 0.166, \"OpenBookQA\": 0.118, \"Downstream_Average\": 0.315}}, \"description\": \"Cerebras-GPT-111M is a transformer-based language model with 111M parameters, trained on the Pile dataset using the GPT-3 style architecture. It is intended for use in research and as a foundation model for NLP applications, ethics, and alignment research. The model can be fine-tuned for various tasks and is licensed under Apache 2.0.\"}}", "category": "generic"}
{"question_id": 720, "text": " Write a Python function that takes in a text and generates 10 lines of different programming languages code snippets.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Program Synthesis\", \"api_name\": \"Salesforce/codegen-350M-multi\", \"api_call\": \"model.generate(input_ids, max_length=128)\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForCausalLM\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\ntext = def hello_world():\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\ngenerated_ids = model.generate(input_ids, max_length=128)\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"HumanEval and MTPB\", \"accuracy\": \"Refer to the paper for accuracy details\"}, \"description\": \"CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.\"}}", "category": "generic"}
{"question_id": 721, "text": " Generate a paragraph describing the benefits of using renewable energy sources.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"facebook/opt-66b\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('facebook/opt-66b', torch_dtype=torch.float16).generate(input_ids)\", \"api_arguments\": [\"input_ids\", \"do_sample\", \"num_return_sequences\", \"max_length\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\nimport torch\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\nprompt = Hello, I am conscious and\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\nset_seed(32)\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\", \"performance\": {\"dataset\": \"GPT-3\", \"accuracy\": \"roughly matched\"}, \"description\": \"OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.\"}}", "category": "generic"}
{"question_id": 722, "text": " The company plans to build a text-to-text translation system using artificial intelligence, focusing on Korean language conversion. How do we accomplish that?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"kykim/bertshared-kor-base\", \"api_call\": \"EncoderDecoderModel.from_pretrained('kykim/bertshared-kor-base')\", \"api_arguments\": {\"tokenizer\": \"BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import BertTokenizerFast, EncoderDecoderModel\\ntokenizer = BertTokenizerFast.from_pretrained(kykim/bertshared-kor-base)\\nmodel = EncoderDecoderModel.from_pretrained(kykim/bertshared-kor-base)\", \"performance\": {\"dataset\": \"70GB Korean text dataset\", \"accuracy\": \"42000 lower-cased subwords\"}, \"description\": \"Bert base model for Korean, trained on a 70GB Korean text dataset and 42000 lower-cased subwords. Can be used for Text2Text Generation tasks.\"}}", "category": "generic"}
{"question_id": 723, "text": " Generate a list of questions from a document. The document describes the history of the company Apple Inc.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text2Text Generation\", \"api_name\": \"castorini/doc2query-t5-base-msmarco\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-base-msmarco')\", \"api_arguments\": \"text, max_length\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"MS MARCO\", \"accuracy\": \"Not specified\"}, \"description\": \"A T5 model trained on the MS MARCO dataset for generating queries from documents.\"}}", "category": "generic"}
{"question_id": 724, "text": " I am a high school student and want to train a language model that can complete my sentences. For instance, when I say \\\"I am a\\\", the model should be able to predict the next word.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling\", \"api_name\": \"bert-base-cased\", \"api_call\": \"pipeline('fill-mask', model='bert-base-cased')\", \"api_arguments\": [\"model\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": [\"from transformers import pipeline\", \"unmasker = pipeline('fill-mask', model='bert-base-cased')\", \"unmasker(Hello I'm a [MASK] model.)\"], \"performance\": {\"dataset\": \"GLUE\", \"accuracy\": 79.6}, \"description\": \"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between 'english' and 'English'. The model can be used for masked language modeling or next sentence prediction, but it's mainly intended to be fine-tuned on a downstream task.\"}}", "category": "generic"}
{"question_id": 725, "text": " I work in a pharmaceutical company and have a text with a missing word related to our field. Please help me find the most suitable word for the context.\\n###Input: \\\"report showed an increase in [MASK] levels after exposure to the new drug.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"dmis-lab/biobert-base-cased-v1.2\", \"api_call\": \"pipeline('fill-mask', model='dmis-lab/biobert-base-cased-v1.2')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"fill_mask('Hugging Face is a [MASK] company.')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.\"}}", "category": "generic"}
{"question_id": 726, "text": " I need a software tool to help me complete and format code snippets in various programming languages like Python, Java, and Ruby.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling Prediction\", \"api_name\": \"CodeBERTa-small-v1\", \"api_call\": \"pipeline('fill-mask', model='huggingface/CodeBERTa-small-v1')\", \"api_arguments\": [\"task\", \"model\", \"tokenizer\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"fill_mask(PHP_CODE)\", \"performance\": {\"dataset\": \"code_search_net\", \"accuracy\": null}, \"description\": \"CodeBERTa is a RoBERTa-like model trained on the CodeSearchNet dataset from GitHub. It supports languages like Go, Java, JavaScript, PHP, Python, and Ruby. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face tokenizers. The small model is a 6-layer, 84M parameters, RoBERTa-like Transformer model.\"}}", "category": "generic"}
{"question_id": 727, "text": " We need a powerful AI to predict the missing words in this statement: \\\"Bob went to the ___ to buy some groceries.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"microsoft/deberta-v2-xlarge\", \"api_call\": \"DebertaModel.from_pretrained('microsoft/deberta-v2-xlarge')\", \"api_arguments\": \"Mask token: [MASK]\", \"python_environment_requirements\": \"PyTorch, TensorFlow\", \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": [{\"name\": \"SQuAD 1.1\", \"accuracy\": \"95.8/90.8\"}, {\"name\": \"SQuAD 2.0\", \"accuracy\": \"91.4/88.9\"}, {\"name\": \"MNLI-m/mm\", \"accuracy\": \"91.7/91.6\"}, {\"name\": \"SST-2\", \"accuracy\": \"97.5\"}, {\"name\": \"QNLI\", \"accuracy\": \"95.8\"}, {\"name\": \"CoLA\", \"accuracy\": \"71.1\"}, {\"name\": \"RTE\", \"accuracy\": \"93.9\"}, {\"name\": \"MRPC\", \"accuracy\": \"92.0/94.2\"}, {\"name\": \"QQP\", \"accuracy\": \"92.3/89.8\"}, {\"name\": \"STS-B\", \"accuracy\": \"92.9/92.9\"}]}, \"description\": \"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on majority of NLU tasks with 80GB training data. This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.\"}}", "category": "generic"}
{"question_id": 728, "text": " I want to find the most similar sentences among a list of sentences.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/all-distilroberta-v1\", \"api_call\": \"SentenceTransformer('sentence-transformers/all-distilroberta-v1')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": [{\"name\": \"s2orc\", \"accuracy\": \"Not provided\"}, {\"name\": \"MS Marco\", \"accuracy\": \"Not provided\"}, {\"name\": \"yahoo_answers_topics\", \"accuracy\": \"Not provided\"}]}, \"description\": \"This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 729, "text": " We need to find the similarity between multiple sentences for use in our article summarization tool.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/all-MiniLM-L12-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"1,170,060,424 training pairs\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 730, "text": " Our organization is creating a search filter to find relevant tweets about a specific topic on Twitter.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"nikcheerla/nooks-amd-detection-v2-full\", \"api_call\": \"SentenceTransformer('{MODEL_NAME}')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": [\"sentence-transformers\", \"transformers\"], \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('{MODEL_NAME}')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 731, "text": " Help us implement a semantic search model to find relevant information in our internal document repository.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\", \"api_call\": \"SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\", \"api_arguments\": [\"query\", \"docs\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer, util\\nquery = How many people live in London?\\ndocs = [Around 9 Million people live in London, London is known for its financial district]\\nmodel = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\\nquery_emb = model.encode(query)\\ndoc_emb = model.encode(docs)\\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\\ndoc_score_pairs = list(zip(docs, scores))\\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\\nfor doc, score in doc_score_pairs:\\n print(score, doc)\", \"performance\": {\"dataset\": [{\"name\": \"WikiAnswers\", \"accuracy\": \"77,427,422\"}, {\"name\": \"PAQ\", \"accuracy\": \"64,371,441\"}, {\"name\": \"Stack Exchange\", \"accuracy\": \"25,316,456\"}, {\"name\": \"MS MARCO\", \"accuracy\": \"17,579,773\"}, {\"name\": \"GOOAQ\", \"accuracy\": \"3,012,496\"}, {\"name\": \"Amazon-QA\", \"accuracy\": \"2,448,839\"}, {\"name\": \"Yahoo Answers\", \"accuracy\": \"1,198,260\"}, {\"name\": \"SearchQA\", \"accuracy\": \"582,261\"}, {\"name\": \"ELI5\", \"accuracy\": \"325,475\"}, {\"name\": \"Quora\", \"accuracy\": \"103,663\"}, {\"name\": \"Natural Questions (NQ)\", \"accuracy\": \"100,231\"}, {\"name\": \"SQuAD2.0\", \"accuracy\": \"87,599\"}, {\"name\": \"TriviaQA\", \"accuracy\": \"73,346\"}]}, \"description\": \"This is a sentence-transformers model that maps sentences & paragraphs to a 384-dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.\"}}", "category": "generic"}
{"question_id": 732, "text": " You are running an e-commerce, you need to cluster similar customer questions together to enhance the product accuracy.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Embeddings\", \"api_name\": \"sentence-transformers/nli-mpnet-base-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/nli-mpnet-base-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Automated evaluation\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 733, "text": " Our company is creating a virtual assistant, and we want to integrate text-to-speech functionality into our product for better user experience.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"mio/amadeus\", \"api_call\": \"No changes needed.\", \"api_arguments\": {\"model_name\": \"mio/amadeus\"}, \"python_environment_requirements\": {\"espnet\": \"d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\", \"transformers\": \"latest\"}, \"example_code\": \"cd espnet\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\npip install -e .\\ncd egs2/amadeus/tts1\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus\", \"performance\": {\"dataset\": \"amadeus\", \"accuracy\": \"Not provided\"}, \"description\": \"This model was trained by mio using amadeus recipe in espnet.\"}}", "category": "generic"}
{"question_id": 734, "text": " I need an AI that would convert written words into spoken words for audiobooks.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"mio/tokiwa_midori\", \"api_call\": \"Text2Speech(config, model_path)\", \"api_arguments\": [\"espnet\", \"mio/tokiwa_midori\", \"Text2Speech\", \"config\", \"model_path\"], \"python_environment_requirements\": [\"espnet2\", \"huggingface_hub\"], \"example_code\": \"./run.sh --skip_data_prep false --skip_train true --download_model mio/tokiwa_midori\", \"performance\": {\"dataset\": \"amadeus\", \"accuracy\": \"Not provided\"}, \"description\": \"This model was trained by mio using amadeus recipe in espnet.\"}}", "category": "generic"}
{"question_id": 735, "text": " We are adding a feature for converting speech to text in meetings to create transcripts automatically. How can we implement this?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech to Text\", \"api_name\": \"facebook/s2t-medium-librispeech-asr\", \"api_call\": \"'Speech2TextForConditionalGeneration.from_pretrained(facebook/s2t-medium-librispeech-asr)'\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/s2t-medium-librispeech-asr\"}, \"python_environment_requirements\": [\"torchaudio\", \"sentencepiece\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\\nfrom datasets import load_dataset\\nimport soundfile as sf\\nmodel = Speech2TextForConditionalGeneration.from_pretrained(facebook/s2t-medium-librispeech-asr)\\nprocessor = Speech2Textprocessor.from_pretrained(facebook/s2t-medium-librispeech-asr)\\ndef map_to_array(batch):\\n speech, _ = sf.read(batch[file])\\n batch[speech] = speech\\n return batch\\nds = load_dataset(\\n patrickvonplaten/librispeech_asr_dummy,\\n clean,\\n split=validation\\n)\\nds = ds.map(map_to_array)\\ninput_features = processor(\\n ds[speech][0],\\n sampling_rate=16_000,\\n return_tensors=pt\\n).input_features # Batch size 1\\ngenerated_ids = model.generate(input_features=input_features)\\ntranscription = processor.batch_decode(generated_ids)\", \"performance\": {\"dataset\": \"LibriSpeech\", \"accuracy\": {\"clean\": 3.5, \"other\": 7.8}}, \"description\": \"s2t-medium-librispeech-asr is a Speech to Text Transformer (S2T) model trained for automatic speech recognition (ASR). The S2T model was proposed in this paper and released in this repository.\"}}", "category": "generic"}
{"question_id": 736, "text": " We aim to develop a tool to remove background noise from voice recordings. Let's access the available enhancement model to preprocess the recordings.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"ESPnet\", \"functionality\": \"chime4\", \"api_name\": \"espnet/Wangyou_Zhang_chime4_enh_train_enh_conv_tasnet_raw\", \"api_call\": \"Not available\", \"api_arguments\": \"Not available\", \"python_environment_requirements\": \"espnet\", \"example_code\": \"./run.sh --skip_data_prep false --skip_train true --download_model espnet/Wangyou_Zhang_chime4_enh_train_enh_conv_tasnet_raw\", \"performance\": {\"dataset\": \"chime4\", \"accuracy\": \"Not available\"}, \"description\": \"This model was trained by Wangyou Zhang using chime4 recipe in espnet.\"}}", "category": "generic"}
{"question_id": 737, "text": " I would like to create an application that allows me to translate someone's speech in Hokkien into English audio.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"Speech-to-speech translation\", \"api_name\": \"xm_transformer_s2ut_hk-en\", \"api_call\": \"'S2THubInterface.get_prediction(task, models[0].cpu(), generator, sample)'\", \"api_arguments\": {\"task\": \"speech_to_text\", \"model\": \"facebook/xm_transformer_s2ut_hk-en\", \"generator\": \"task.build_generator([model], cfg)\", \"sample\": \"S2THubInterface.get_model_input(task, audio)\"}, \"python_environment_requirements\": {\"fairseq\": \"latest\", \"torchaudio\": \"latest\", \"huggingface_hub\": \"latest\"}, \"example_code\": \"import json\\nimport os\\nfrom pathlib import Path\\nimport IPython.display as ipd\\nfrom fairseq import hub_utils\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\nfrom huggingface_hub import snapshot_download\\nimport torchaudio\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\n facebook/xm_transformer_s2ut_hk-en,\\n arg_overrides={config_yaml: config.yaml, task: speech_to_text},\\n cache_dir=cache_dir,\\n)\\nmodel = models[0].cpu()\\ncfg[task].cpu = True\\ngenerator = task.build_generator([model], cfg)\\naudio, _ = torchaudio.load(/path/to/an/audio/file)\\nsample = S2THubInterface.get_model_input(task, audio)\\nunit = S2THubInterface.get_prediction(task, model, generator, sample)\\nlibrary_name = fairseq\\ncache_dir = (\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\n)\\ncache_dir = snapshot_download(\\n ffacebook/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur, cache_dir=cache_dir, library_name=library_name\\n)\\nx = hub_utils.from_pretrained(\\n cache_dir,\\n model.pt,\\n .,\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\n config_yaml=config.json,\\n fp16=False,\\n is_vocoder=True,\\n)\\nwith open(f{x['args']['data']}/config.json) as f:\\n vocoder_cfg = json.load(f)\\nassert (\\n len(x[args][model_path]) == 1\\n), Too many vocoder models in the input\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\ntts_sample = tts_model.get_model_input(unit)\\nwav, sr = tts_model.get_prediction(tts_sample)\\nipd.Audio(wav, rate=sr)\", \"performance\": {\"dataset\": \"TED, drama, TAT domain\", \"accuracy\": \"Not provided\"}, \"description\": \"Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq for Hokkien-English. Trained with supervised data in TED, drama, TAT domain, and weakly supervised data in drama domain.\"}}", "category": "generic"}
{"question_id": 738, "text": " We are a live streaming company with a lot of environmental noises. Find a way to classify the noises present.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Audio Classification\", \"api_name\": \"ast-finetuned-audioset-10-10-0.4593\", \"api_call\": \"pipeline('audio-classification', model='MIT/ast-finetuned-audioset-10-10-0.4593')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"AudioSet\", \"accuracy\": \"\"}, \"description\": \"Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.\"}}", "category": "generic"}
{"question_id": 739, "text": " We need a way to classify audio tracks from smartphones or smart audio devices to recognize if the audio is coming from a particular speaker or not.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"wav2vec2-base-superb-sv\", \"api_call\": \"Output: AutoModelForAudioXVector.from_pretrained('anton-l/wav2vec2-base-superb-sv')\", \"api_arguments\": \"anton-l/wav2vec2-base-superb-sv\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import AutoProcessor, AutoModelForAudioXVector\\nprocessor = AutoProcessor.from_pretrained(anton-l/wav2vec2-base-superb-sv)\\nmodel = AutoModelForAudioXVector.from_pretrained(anton-l/wav2vec2-base-superb-sv)\", \"performance\": {\"dataset\": \"superb\", \"accuracy\": \"More information needed\"}, \"description\": \"This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Speaker Verification task. The base model is wav2vec2-large-lv60, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\"}}", "category": "generic"}
{"question_id": 740, "text": " Analyze audio recordings from company meetings and identify the segments where multiple people are talking at the same time.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Voice Activity Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speaker segmentation, Voice activity detection, Overlapped speech detection, Resegmentation, Raw scores\", \"api_name\": \"pyannote/segmentation\", \"api_call\": \"Output: Model.from_pretrained('pyannote/segmentation', use_auth_token='ACCESS_TOKEN_GOES_HERE')\", \"api_arguments\": {\"use_auth_token\": \"ACCESS_TOKEN_GOES_HERE\"}, \"python_environment_requirements\": \"pyannote.audio 2.1.1\", \"example_code\": {\"voice_activity_detection\": \"from pyannote.audio.pipelines import VoiceActivityDetection\\npipeline = VoiceActivityDetection(segmentation=model)\\nHYPER_PARAMETERS = {\\n onset: 0.5, offset: 0.5,\\n min_duration_on: 0.0,\\n min_duration_off: 0.0\\n}\\npipeline.instantiate(HYPER_PARAMETERS)\\nvad = pipeline(audio.wav)\", \"overlapped_speech_detection\": \"from pyannote.audio.pipelines import OverlappedSpeechDetection\\npipeline = OverlappedSpeechDetection(segmentation=model)\\npipeline.instantiate(HYPER_PARAMETERS)\\nosd = pipeline(audio.wav)\", \"resegmentation\": \"from pyannote.audio.pipelines import Resegmentation\\npipeline = Resegmentation(segmentation=model, diarization=baseline)\\npipeline.instantiate(HYPER_PARAMETERS)\\nresegmented_baseline = pipeline({audio: audio.wav, baseline: baseline})\"}, \"performance\": {\"dataset\": {\"AMI Mix-Headset\": {\"voice_activity_detection_accuracy\": {\"onset\": 0.684, \"offset\": 0.577, \"min_duration_on\": 0.181, \"min_duration_off\": 0.037}, \"overlapped_speech_detection_accuracy\": {\"onset\": 0.448, \"offset\": 0.362, \"min_duration_on\": 0.116, \"min_duration_off\": 0.187}, \"resegmentation_accuracy\": {\"onset\": 0.542, \"offset\": 0.527, \"min_duration_on\": 0.044, \"min_duration_off\": 0.705}}, \"DIHARD3\": {\"voice_activity_detection_accuracy\": {\"onset\": 0.767, \"offset\": 0.377, \"min_duration_on\": 0.136, \"min_duration_off\": 0.067}, \"overlapped_speech_detection_accuracy\": {\"onset\": 0.43, \"offset\": 0.32, \"min_duration_on\": 0.091, \"min_duration_off\": 0.144}, \"resegmentation_accuracy\": {\"onset\": 0.592, \"offset\": 0.489, \"min_duration_on\": 0.163, \"min_duration_off\": 0.182}}, \"VoxConverse\": {\"voice_activity_detection_accuracy\": {\"onset\": 0.767, \"offset\": 0.713, \"min_duration_on\": 0.182, \"min_duration_off\": 0.501}, \"overlapped_speech_detection_accuracy\": {\"onset\": 0.587, \"offset\": 0.426, \"min_duration_on\": 0.337, \"min_duration_off\": 0.112}, \"resegmentation_accuracy\": {\"onset\": 0.537, \"offset\": 0.724, \"min_duration_on\": 0.41, \"min_duration_off\": 0.563}}}}, \"description\": \"A pre-trained model for speaker segmentation, voice activity detection, overlapped speech detection, and resegmentation using the pyannote.audio framework.\"}}", "category": "generic"}
{"question_id": 741, "text": " We are developing an application to help wine enthusiasts determine the quality of wines. Use the provided API to classify the input wine samples based on their properties.\\n###Input: {\\\"samples\\\": [{\\\"fixed_acidity\\\": 7.4, \\\"volatile_acidity\\\": 0.7, \\\"citric_acid\\\": 0.0, \\\"residual_sugar\\\": 1.9, \\\"chlorides\\\": 0.076, \\\"free_sulfur_dioxide\\\": 11.0, \\\"total_sulfur_dioxide\\\": 34.0, \\\"density\\\": 0.9978, \\\"pH\\\": 3.51, \\\"sulphates\\\": 0.56, \\\"alcohol\\\": 9.4}, {\\\"fixed_acidity\\\": 7.8, \\\"volatile_acidity\\\": 0.88, \\\"citric_acid\\\": 0.0, \\\"residual_sugar\\\": 2.6, \\\"chlorides\\\": 0.098, \\\"free_sulfur_dioxide\\\": 25.0, \\\"total_sulfur_dioxide\\\": 67.0, \\\"density\\\": 0.9968, \\\"pH\\\": 3.2, \\\"sulphates\\\": 0.68, \\\"alcohol\\\": 9.8}]}\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Scikit-learn\", \"functionality\": \"Wine Quality classification\", \"api_name\": \"osanseviero/wine-quality\", \"api_call\": \"model.predict(X[:3])\", \"api_arguments\": \"X\", \"python_environment_requirements\": [\"huggingface_hub\", \"joblib\", \"pandas\"], \"example_code\": \"from huggingface_hub import hf_hub_url, cached_download\\nimport joblib\\nimport pandas as pd\\nREPO_ID = julien-c/wine-quality\\nFILENAME = sklearn_model.joblib\\nmodel = joblib.load(cached_download(\\n hf_hub_url(REPO_ID, FILENAME)\\n))\\ndata_file = cached_download(\\n hf_hub_url(REPO_ID, winequality-red.csv)\\n)\\nwinedf = pd.read_csv(data_file, sep=;)\\nX = winedf.drop([quality], axis=1)\\nY = winedf[quality]\\nprint(X[:3])\\nlabels = model.predict(X[:3])\", \"performance\": {\"dataset\": \"winequality-red.csv\", \"accuracy\": 0.6616635397123202}, \"description\": \"A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.\"}}", "category": "generic"}
{"question_id": 742, "text": " Estimate the carbon emissions of the factory from the provided data.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"pcoloc/autotrain-only-rssi-1813762559\", \"api_call\": \"Output: model.predict(data)\", \"api_arguments\": {\"data\": \"data.csv\"}, \"python_environment_requirements\": {\"joblib\": \"latest\", \"pandas\": \"latest\"}, \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": {\"Loss\": 83.432, \"R2\": 0.312, \"MSE\": 6960.888, \"MAE\": 60.449, \"RMSLE\": 0.532}}, \"description\": \"A tabular regression model trained using AutoTrain for estimating carbon emissions from given features.\"}}", "category": "generic"}
{"question_id": 743, "text": " We need to estimate the carbon emissions of a fleet of vehicles based on the dataset containing various features.\\n###Input: data = pd.read_csv('data.csv')\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"1860863627\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": {\"data\": \"data.csv\"}, \"python_environment_requirements\": {\"import json\": \"\", \"import joblib\": \"\", \"import pandas as pd\": \"\"}, \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"pcoloc/autotrain-data-dragino-7-7-max_495m\", \"accuracy\": {\"Loss\": 72.73, \"R2\": 0.386, \"MSE\": 5289.6, \"MAE\": 60.23, \"RMSLE\": 0.436}}, \"description\": \"A tabular regression model for predicting carbon emissions using the Joblib framework. Trained on the pcoloc/autotrain-data-dragino-7-7-max_495m dataset.\"}}", "category": "generic"}
{"question_id": 744, "text": " I am building a game and I would like to train an AI character with reinforcement learning in the CartPole environment using PPO.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"ppo-seals-CartPole-v0\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": \"python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/\", \"performance\": {\"dataset\": \"seals/CartPole-v0\", \"accuracy\": \"500.00 +/- 0.00\"}, \"description\": \"This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 745, "text": " I'm building a game based on Lunar Lander. I want to use your model for testing the game performance.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"LunarLander-v2\", \"api_name\": \"araffin/dqn-LunarLander-v2\", \"api_call\": \"Output: DQN.load(checkpoint, **kwargs)\", \"api_arguments\": {\"checkpoint\": \"araffin/dqn-LunarLander-v2\", \"kwargs\": {\"target_update_interval\": 30}}, \"python_environment_requirements\": [\"huggingface_sb3\", \"stable_baselines3\"], \"example_code\": {\"load_model\": \"from huggingface_sb3 import load_from_hub\\nfrom stable_baselines3 import DQN\\nfrom stable_baselines3.common.env_util import make_vec_env\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\ncheckpoint = load_from_hub(araffin/dqn-LunarLander-v2, dqn-LunarLander-v2.zip)\\nkwargs = dict(target_update_interval=30)\\nmodel = DQN.load(checkpoint, **kwargs)\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\", \"evaluate\": \"mean_reward, std_reward = evaluate_policy(\\n model,\\n env,\\n n_eval_episodes=20,\\n deterministic=True,\\n)\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})\"}, \"performance\": {\"dataset\": \"LunarLander-v2\", \"accuracy\": \"280.22 +/- 13.03\"}, \"description\": \"This is a trained model of a DQN agent playing LunarLander-v2 using the stable-baselines3 library.\"}}", "category": "generic"}
{"question_id": 746, "text": " We need a program that will be able to learn how to control a robotic arm for pick and place operations.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"CartPole-v1\", \"api_name\": \"dqn-CartPole-v1\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"logs\"], \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": \"python train.py --algo dqn --env CartPole-v1 -f logs/\", \"performance\": {\"dataset\": \"CartPole-v1\", \"accuracy\": \"500.00 +/- 0.00\"}, \"description\": \"This is a trained model of a DQN agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 747, "text": " An AI company is developing a soccer game. They want to train AI for playing Soccer Twos using your model.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"Train and play SoccerTwos\", \"api_name\": \"ahmad-alismail/poca-SoccerTwos\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"ml-agents\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"Not specified\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 748, "text": " I'm looking for a useful agent for Ant-v3 environment to use the agent, train it, evaluate it, and add it to my hub.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"td3-Ant-v3\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"RL Zoo: https://github.com/DLR-RM/rl-baselines3-zoo\", \"SB3: https://github.com/DLR-RM/stable-baselines3\", \"SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\"], \"example_code\": [\"python -m rl_zoo3.load_from_hub --algo td3 --env Ant-v3 -orga sb3 -f logs/\", \"python enjoy.py --algo td3 --env Ant-v3 -f logs/\", \"python train.py --algo td3 --env Ant-v3 -f logs/\", \"python -m rl_zoo3.push_to_hub --algo td3 --env Ant-v3 -f logs/ -orga sb3\"], \"performance\": {\"dataset\": \"Ant-v3\", \"accuracy\": \"5822.96 +/- 93.33\"}, \"description\": \"This is a trained model of a TD3 agent playing Ant-v3 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 749, "text": " Our company is developing a soccer training application. We need an agent that can learn and play soccer effectively.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"SoccerTwos\", \"api_name\": \"Raiden-1001/poca-Soccerv7\", \"api_call\": \"No changes needed. The `api_call` field does not include any model instantiation.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"unity-ml-agents\", \"deep-reinforcement-learning\", \"ML-Agents-SoccerTwos\"], \"example_code\": \"Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\\nStep 2: Select your .nn /.onnx file\\nClick on Watch the agent play \\ud83d\\udc40\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 750, "text": " Please find a method to identify the similarity of English phrases using a feature extraction model.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"sup-simcse-roberta-large\", \"api_call\": \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", \"api_arguments\": [\"AutoTokenizer\", \"AutoModel\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\", \"performance\": {\"dataset\": \"STS tasks\", \"accuracy\": \"Spearman's correlation (See associated paper Appendix B)\"}, \"description\": \"A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.\"}}", "category": "generic"}
{"question_id": 751, "text": " Can you please generate an image of a mysterious knight with a shining armor in a gothic artstyle?\\n###Input: \\\"mysterious knight with shining armor in gothic artstyle\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"nitrosocke/nitro-diffusion\", \"api_call\": \"pipe(prompt).images[0]\", \"api_arguments\": [\"prompt\"], \"python_environment_requirements\": [\"torch\", \"diffusers\"], \"example_code\": \"from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)\", \"performance\": {\"dataset\": \"Stable Diffusion\", \"accuracy\": \"N/A\"}, \"description\": \"Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\"}}", "category": "generic"}
{"question_id": 752, "text": " We need to extract the text from the image URL for an OCR application.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-base-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 753, "text": " Imagine you are designing an app that would allow the user to take a photo and ask questions about it. They are going to take a picture of their living room and have questions about the objects in the room.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"blip2-flan-t5-xxl\", \"api_call\": \"Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\", \"api_arguments\": {\"raw_image\": \"Image\", \"question\": \"Text\"}, \"python_environment_requirements\": [\"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not provided\"}, \"description\": \"BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 754, "text": " Design an interface to provide the user with the best image captions and interactive responses to questions considering a given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"blip2-opt-6.7b\", \"api_call\": \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", \"api_arguments\": \"image, optional text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"Refer to the documentation\", \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not specified\"}, \"description\": \"BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 755, "text": " We want to build a system that takes input as a rendered image and generates nice looking textual descriptions for the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/pix2struct-base\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\", \"api_arguments\": {\"t5x_checkpoint_path\": \"PATH_TO_T5X_CHECKPOINTS\", \"pytorch_dump_path\": \"PATH_TO_SAVE\"}, \"python_environment_requirements\": {\"transformers\": \"4.15.0\", \"torch\": \"1.10.1\"}, \"example_code\": \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)\", \"performance\": {\"dataset\": [{\"name\": \"Documents\", \"accuracy\": \"N/A\"}, {\"name\": \"Illustrations\", \"accuracy\": \"N/A\"}, {\"name\": \"User Interfaces\", \"accuracy\": \"N/A\"}, {\"name\": \"Natural Images\", \"accuracy\": \"N/A\"}]}, \"description\": \"Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.\"}}", "category": "generic"}
{"question_id": 756, "text": " Let's create a Python function that will take an image URL as an input and returns the recognized text in the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-small-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", \"api_arguments\": {\"image\": \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", \"processor\": \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not specified\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.\"}}", "category": "generic"}
{"question_id": 757, "text": " Design a tool for hearing-impaired people where they can visualize spoken words.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"ImRma/Brucelee\", \"api_call\": \"pipeline('text-to-video', model='ImRma/Brucelee')\", \"api_arguments\": [\"your_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Hugging Face model for converting Persian and English text into video.\"}}", "category": "generic"}
{"question_id": 758, "text": " I have a small company making movies. I want to generate different movie scenes from text input describing the scenes.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"camenduru/text2-video-zero\", \"api_call\": \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\"}}", "category": "generic"}
{"question_id": 759, "text": " I want to design a tool that will take an image and a question as inputs and provide an answer to the question based on the contents of the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-textvqa\", \"api_call\": \"git_base_textvqa = AutoModel.from_pretrained('microsoft/git-base-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}}", "category": "generic"}
{"question_id": 760, "text": " Develop software that answers questions based on an input image for an educational app.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-textvqa\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"See table 11 in the paper for more details.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 761, "text": " Using a Visual Question Answering model, provide me with details on where I can find the price of a product on a webpage.\\n###Input: <<<image>>>: image_path, <<<question>>>: \\\"Where can I find the price of a product on this webpage?\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"api_call\": \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", \"api_arguments\": {\"model\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"tokenizer\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": \"vqa(image_path, question)\", \"performance\": {\"dataset\": \"JosephusCheung/GuanacoVQADataset\", \"accuracy\": \"unknown\"}, \"description\": \"A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\"}}", "category": "generic"}
{"question_id": 762, "text": " I want to develop an AI-powered program that can scan images of documents with questions and automatically answer them.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased-finetuned-docvqa\", \"api_call\": \"layoutlmv2-base-uncased-finetuned-docvqa\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.12.2\", \"torch==1.8.0+cu101\", \"datasets==1.14.0\", \"tokenizers==0.10.3\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 1.194}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 763, "text": " The HR department receives a large number of documents every day. They need to find answers to specific questions from these documents.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_arguments\": {\"model\": \"hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa\"}, \"python_environment_requirements\": {\"transformers\": \"4.27.4\", \"pytorch\": \"2.0.0+cu117\", \"datasets\": \"2.11.0\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", \"performance\": {\"dataset\": \"None\", \"accuracy\": {\"Loss\": 4.843}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.\"}}", "category": "generic"}
{"question_id": 764, "text": " We are working on a drug discovery application, and we need to classify molecules based on their molecular structure.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Graph Machine Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Graph Classification\", \"api_name\": \"graphormer-base-pcqm4mv2\", \"api_call\": \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", \"api_arguments\": \"pretrained_model_name\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"See the Graph Classification with Transformers tutorial.\", \"performance\": {\"dataset\": \"PCQM4M-LSCv2\", \"accuracy\": \"Not provided\"}, \"description\": \"The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.\"}}", "category": "generic"}
{"question_id": 765, "text": " While creating virtual reality games, I need to estimate the depth of objects in 3D space.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\", \"api_call\": \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", \"api_arguments\": [], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 766, "text": " Add a feature to our drone that allows it to estimate the depth of different elements in its field of view.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4359, \"Rmse\": 0.4276}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 767, "text": " I need an AI system that can predict the depth map of a single color image. Can you suggest any models?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221215-095508\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", \"api_arguments\": null, \"python_environment_requirements\": [\"transformers\"], \"example_code\": null, \"performance\": {\"dataset\": \"DIODE\", \"accuracy\": null}, \"description\": \"A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.\"}}", "category": "generic"}
{"question_id": 768, "text": " I want to analyze the depth of objects in an image using AI.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221122-030603\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-030603')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.24.0\", \"torch==1.12.1\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3597, \"Mae\": 0.3054, \"Rmse\": 0.4481, \"Abs Rel\": 0.3462, \"Log Mae\": 0.1256, \"Log Rmse\": 0.1798, \"Delta1\": 0.5278, \"Delta2\": 0.8055, \"Delta3\": 0.9191}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 769, "text": " We are building a robot to follow a person while maintaining a specific distance. We need to estimate the depth of each object in the scene.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221221-102136\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136').model\", \"api_arguments\": [], \"python_environment_requirements\": [\"Transformers 4.24.0\", \"Pytorch 1.12.1+cu116\", \"Datasets 2.8.0\", \"Tokenizers 0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4222, \"Mae\": 0.411, \"Rmse\": 0.6292, \"Abs Rel\": 0.3778, \"Log Mae\": 0.1636, \"Log Rmse\": 0.224, \"Delta1\": 0.432, \"Delta2\": 0.6806, \"Delta3\": 0.8068}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 770, "text": " I am building an app that can classify the type of animal in a given image. Can you help me configure the code for this?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/beit-base-patch16-224-pt22k-ft22k\", \"api_call\": \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"ImageNet-22k\", \"accuracy\": \"Not specified\"}, \"description\": \"BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.\"}}", "category": "generic"}
{"question_id": 771, "text": " Detect the severity of Diabetic Retinopathy in the eye images of patients.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"martinezomg/vit-base-patch16-224-diabetic-retinopathy\", \"api_call\": \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy').model\", \"api_arguments\": {\"model_name\": \"martinezomg/vit-base-patch16-224-diabetic-retinopathy\"}, \"python_environment_requirements\": {\"transformers\": \"4.28.1\", \"pytorch\": \"2.0.0+cu118\", \"datasets\": \"2.11.0\", \"tokenizers\": \"0.13.3\"}, \"example_code\": \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", \"performance\": {\"dataset\": \"None\", \"accuracy\": 0.7744}, \"description\": \"This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.\"}}", "category": "generic"}
{"question_id": 772, "text": " We want to create a machine learning model that can identify objects in photographs.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"google/vit-base-patch16-384\", \"api_call\": \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"google/vit-base-patch16-384\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"ImageNet\", \"accuracy\": \"Refer to tables 2 and 5 of the original paper\"}, \"description\": \"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\"}}", "category": "generic"}
{"question_id": 773, "text": " I need to create a tool that identifies objects in an image to help blind people see the world more effectively.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/swinv2-tiny-patch4-window8-256\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", \"api_arguments\": {\"image\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.\"}}", "category": "generic"}
{"question_id": 774, "text": " I'm developing a mobile app that classifies whether an image contains a cat or not. I need to use an API that can provide excellent accuracy.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/regnet-y-008\", \"api_call\": \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"zuppif/regnet-y-040\"}, \"python_environment_requirements\": {\"transformers\": \"AutoFeatureExtractor, RegNetForImageClassification\", \"torch\": \"torch\", \"datasets\": \"load_dataset\"}, \"example_code\": \"from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.\"}}", "category": "generic"}
{"question_id": 775, "text": " There is a security breach in the company's server room. We must detect any unauthorized personnel in the CCTV footage.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-valorant-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"valorant-object-detection\", \"accuracy\": 0.965}, \"description\": \"A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\"}}", "category": "generic"}
{"question_id": 776, "text": " Our company works with processing various types of documents. We need a solution to detect tables in documents, including bordered and borderless ones.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Extraction\", \"api_name\": \"keremberke/yolov8s-table-extraction\", \"api_call\": \"YOLO('keremberke/yolov8s-table-extraction').predict(image)\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"table-extraction\", \"accuracy\": 0.984}, \"description\": \"A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\"}}", "category": "generic"}
{"question_id": 777, "text": " Monitor the work of your warehouse employees by detecting forklifts and persons on your security camera images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-forklift-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-forklift-detection').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"forklift-object-detection\", \"accuracy\": 0.846}, \"description\": \"A YOLOv8 model for detecting forklifts and persons in images.\"}}", "category": "generic"}
{"question_id": 778, "text": " There is a need to segment specific objects in a landscape image for better visual processing.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Semantic Segmentation\", \"api_name\": \"nvidia/segformer-b0-finetuned-ade-512-512\", \"api_call\": \"'SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)'\", \"api_arguments\": {\"images\": \"Image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"SegformerImageProcessor, SegformerForSemanticSegmentation\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": \"from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nprocessor = SegformerImageProcessor.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 779, "text": " We are developing an autonomous car and we need to segment the road and surrounding environment in real-time.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"CityScapes\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 780, "text": " Generate a high-quality image of a human face using a Denoising Diffusion Probabilistic Model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Denoising Diffusion Probabilistic Models (DDPM)\", \"api_name\": \"google/ddpm-ema-celebahq-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": {\"model_id\": \"google/ddpm-ema-celebahq-256\"}, \"python_environment_requirements\": \"diffusers\", \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": {\"CIFAR10\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}, \"LSUN\": {\"sample_quality\": \"similar to ProgressiveGAN\"}}}, \"description\": \"High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.\"}}", "category": "generic"}
{"question_id": 781, "text": " A e-commerce platform wants to generate images of butterflies to boost sales of butterfly related products. Please provide suitable recommendations.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ocariz/butterfly_200\", \"api_call\": \"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.\"}}", "category": "generic"}
{"question_id": 782, "text": " I want to classify an online video about traveling without any label and find out if it's about nature, food, or city life.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/xclip-base-patch32\", \"api_call\": \"XClipModel.from_pretrained('microsoft/xclip-base-patch32')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"Kinetics 400\", \"accuracy\": {\"top-1\": 80.4, \"top-5\": 95.0}}, \"description\": \"X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\"}}", "category": "generic"}
{"question_id": 783, "text": " Create a program that can classify a given video file into various categories like sports, music, etc.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-large-finetuned-kinetics\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": {\"top-1\": 84.7, \"top-5\": 96.5}}, \"description\": \"VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 784, "text": " Create a model to classify short videos based on their content for our video-sharing application.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"videomae-base-ssv2\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\", \"api_arguments\": \"video\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": \"\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\"}}", "category": "generic"}
{"question_id": 785, "text": " I want to build an app that can recognize images and classify them into categories I choose. The content of the image is like a tool for work, a handheld device, or an instrument for entertainment.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-large-patch14-336\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\", \"api_arguments\": \"image_path, tokenizer, model\", \"python_environment_requirements\": \"Transformers 4.21.3, TensorFlow 2.8.2, Tokenizers 0.12.1\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": \"N/A\"}, \"description\": \"This model was trained from scratch on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 786, "text": " I am an art collector and want to create a recommendation system for categorizing and recognizing diverse art styles. I want to distinguish between different styles, such as impressionism, cubism, and abstract art.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-g-14-laion2B-s34B-b88K\", \"api_call\": \"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\", \"api_arguments\": {\"image\": \"path/to/image/file\", \"class_names\": \"list_of_class_names\"}, \"python_environment_requirements\": {\"huggingface_hub\": \"0.0.17\", \"transformers\": \"4.11.3\", \"torch\": \"1.9.0\", \"torchvision\": \"0.10.0\"}, \"example_code\": null, \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\"}}", "category": "generic"}
{"question_id": 787, "text": " I have a photo archive that I need to label with different collectibles, such as stamps, coins, and antiques. Automate the process of identifying what's in each picture using a zero-shot image classification model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w-laion2B-s13B-b82K\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"api_arguments\": {\"image_path\": \"path to the image file\", \"labels\": \"list of possible class names\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8 - 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}", "category": "generic"}
{"question_id": 788, "text": " Design a system to identify the category of the given image from a collection of categories including animals, vehicles, and nature.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')\", \"api_arguments\": {\"image_path\": \"./path/to/image.jpg\", \"class_names\": \"class1,class2,class3\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nclip = pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')\\nclip('./path/to/image.jpg', 'class1,class2,class3')\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"75.9%\"}, \"description\": \"A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on LAION-2B (english), a subset of LAION-5B, using OpenCLIP. The models are trained at 256x256 image resolution and achieve a 75.9 top-1 zero-shot accuracy on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 789, "text": " I am building a food recommendation service from images. For that, I would like you to determine what the dish is supposed to be based on the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"OFA-Sys/chinese-clip-vit-base-patch16\", \"api_call\": \"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"OFA-Sys/chinese-clip-vit-base-patch16\"}, \"python_environment_requirements\": {\"transformers\": \"ChineseCLIPProcessor, ChineseCLIPModel\"}, \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [\\u6770\\u5c3c\\u9f9f, \\u5999\\u86d9\\u79cd\\u5b50, \\u5c0f\\u706b\\u9f99, \\u76ae\\u5361\\u4e18]\\ninputs = processor(images=image, return_tensors=pt)\\nimage_features = model.get_image_features(**inputs)\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\ntext_features = model.get_text_features(**inputs)\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": {\"MUGE Text-to-Image Retrieval\": {\"accuracy\": {\"Zero-shot R@1\": 63.0, \"Zero-shot R@5\": 84.1, \"Zero-shot R@10\": 89.2, \"Finetune R@1\": 68.9, \"Finetune R@5\": 88.7, \"Finetune R@10\": 93.1}}, \"Flickr30K-CN Retrieval\": {\"accuracy\": {\"Zero-shot Text-to-Image R@1\": 71.2, \"Zero-shot Text-to-Image R@5\": 91.4, \"Zero-shot Text-to-Image R@10\": 95.5, \"Finetune Text-to-Image R@1\": 83.8, \"Finetune Text-to-Image R@5\": 96.9, \"Finetune Text-to-Image R@10\": 98.6, \"Zero-shot Image-to-Text R@1\": 81.6, \"Zero-shot Image-to-Text R@5\": 97.5, \"Zero-shot Image-to-Text R@10\": 98.8, \"Finetune Image-to-Text R@1\": 95.3, \"Finetune Image-to-Text R@5\": 99.7, \"Finetune Image-to-Text R@10\": 100.0}}, \"COCO-CN Retrieval\": {\"accuracy\": {\"Zero-shot Text-to-Image R@1\": 69.2, \"Zero-shot Text-to-Image R@5\": 89.9, \"Zero-shot Text-to-Image R@10\": 96.1, \"Finetune Text-to-Image R@1\": 81.5, \"Finetune Text-to-Image R@5\": 96.9, \"Finetune Text-to-Image R@10\": 99.1, \"Zero-shot Image-to-Text R@1\": 63.0, \"Zero-shot Image-to-Text R@5\": 86.6, \"Zero-shot Image-to-Text R@10\": 92.9, \"Finetune Image-to-Text R@1\": 83.5, \"Finetune Image-to-Text R@5\": 97.3, \"Finetune Image-to-Text R@10\": 99.2}}, \"Zero-shot Image Classification\": {\"accuracy\": {\"CIFAR10\": 96.0, \"CIFAR100\": 79.7, \"DTD\": 51.2, \"EuroSAT\": 52.0, \"FER\": 55.1, \"FGVC\": 26.2, \"KITTI\": 49.9, \"MNIST\": 79.4, \"PC\": 63.5, \"VOC\": 84.9}}}}, \"description\": \"Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder.\"}}", "category": "generic"}
{"question_id": 790, "text": " We need to analyze multiple financial news headlines to conclude the sentiment of each headline.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"financial-sentiment-analysis\", \"api_name\": \"yiyanghkust/finbert-tone\", \"api_call\": \"Output: BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import BertTokenizer, BertForSequenceClassification\\nfrom transformers import pipeline\\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\nsentences = [there is a shortage of capital, and we need extra financing,\\n growth is strong and we have plenty of liquidity,\\n there are doubts about our finances,\\n profits are flat]\\nresults = nlp(sentences)\\nprint(results)\", \"performance\": {\"dataset\": \"10,000 manually annotated sentences from analyst reports\", \"accuracy\": \"superior performance on financial tone analysis task\"}, \"description\": \"FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\"}}", "category": "generic"}
{"question_id": 791, "text": " Could you please analyze the sentiment of the provided customer review text?\\n###Input: \\\"El producto llegen perfectas condiciones, pero tardmucho en llegar.\\\" \\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"finiteautomata/beto-sentiment-analysis\", \"api_call\": \"pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"Hugging Face Transformers library\", \"example_code\": \"\", \"performance\": {\"dataset\": \"TASS 2020 corpus\", \"accuracy\": \"\"}, \"description\": \"Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\"}}", "category": "generic"}
{"question_id": 792, "text": " I am building an app to read tweets and help users understand the general sentiment of those tweets. We want to know if tweets are positive, negative, or neutral.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"finiteautomata/bertweet-base-sentiment-analysis\", \"api_call\": \"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"Transformers\", \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\nresult = nlp('I love this movie!')\", \"performance\": {\"dataset\": \"SemEval 2017\", \"accuracy\": null}, \"description\": \"Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is BERTweet, a RoBERTa model trained on English tweets. Uses POS, NEG, NEU labels.\"}}", "category": "generic"}
{"question_id": 793, "text": " Categorize this input comment as toxic or non-toxic.\\n###Input: You're an absolute idiot and I hate everything about you.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"martin-ha/toxic-comment-model\", \"api_call\": \"pipeline(model=AutoModelForSequenceClassification.from_pretrained('martin-ha/toxic-comment-model'), tokenizer=AutoTokenizer.from_pretrained('martin-ha/toxic-comment-model')).__call__('This is a test text.')\", \"api_arguments\": {\"model_path\": \"martin-ha/toxic-comment-model\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\nmodel_path = martin-ha/toxic-comment-model\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\nprint(pipeline('This is a test text.'))\", \"performance\": {\"dataset\": \"held-out test set\", \"accuracy\": 0.94, \"f1-score\": 0.59}, \"description\": \"This model is a fine-tuned version of the DistilBERT model to classify toxic comments.\"}}", "category": "generic"}
{"question_id": 794, "text": " The company received emails from customers. We need to classify them into questions or statements.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"shahrukhx01/question-vs-statement-classifier\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\\nmodel = AutoModelForSequenceClassification.from_pretrained(shahrukhx01/question-vs-statement-classifier)\", \"performance\": {\"dataset\": \"Haystack\", \"accuracy\": \"Not provided\"}, \"description\": \"Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack\"}}", "category": "generic"}
{"question_id": 795, "text": " As a company focusing on cybersecurity, we would like to integrate a model into our email filter to detect gibberish text for better scanning.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"madhurjindal/autonlp-Gibberish-Detector-492513457\", \"api_call\": \"Output: AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoNLP\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForSequenceClassification\", \"AutoTokenizer\": \"from_pretrained\"}, \"example_code\": \"from transformers import AutoModelForSequenceClassification, AutoTokenizer\\nmodel = AutoModelForSequenceClassification.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\ninputs = tokenizer(I love AutoNLP, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"madhurjindal/autonlp-data-Gibberish-Detector\", \"accuracy\": 0.9735624586913417}, \"description\": \"A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT.\"}}", "category": "generic"}
{"question_id": 796, "text": " A content moderation company requires an efficient way to detect inappropriate text in the chat rooms. They need help.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"michellejieli/NSFW_text_classifier\", \"api_call\": \"pipeline('sentiment-analysis', model='michellejieli/NSFW_text_classification')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"classifier(I see youve set aside this special time to humiliate yourself in public.)\", \"performance\": {\"dataset\": \"Reddit posts\", \"accuracy\": \"Not specified\"}, \"description\": \"DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\"}}", "category": "generic"}
{"question_id": 797, "text": " Clients ask us to identify persons, businesses and regions in a text to provide data-driven analytics.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"dslim/bert-base-NER-uncased\", \"api_call\": \"pipeline('ner', model='dslim/bert-base-NER-uncased')\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"nlp('My name is John and I live in New York.')\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.\"}}", "category": "generic"}
{"question_id": 798, "text": " As part of our customer support platform, we need to develop a service that can detect companies mentioned in users' questions.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Entity Extraction\", \"api_name\": \"903429548\", \"api_call\": \"Output: AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoTrain\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTokenClassification, AutoTokenizer\"}, \"example_code\": \"from transformers import AutoModelForTokenClassification, AutoTokenizer\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"ismail-lucifer011/autotrain-data-company_all\", \"accuracy\": 0.9979930566588805}, \"description\": \"A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.\"}}", "category": "generic"}
{"question_id": 799, "text": " Our company is planning to analyze customer feedback from users in China. We want a system to perform part-of-speech tagging on the Chinese text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Part-of-speech tagging\", \"api_name\": \"ckiplab/bert-base-chinese-pos\", \"api_call\": \"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\", \"api_arguments\": {\"tokenizer\": \"BertTokenizerFast.from_pretrained('bert-base-chinese')\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import (\\n  BertTokenizerFast,\\n  AutoModel,\\n)\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\"}}", "category": "generic"}
{"question_id": 800, "text": " We are developing a grammar-checking application. We need to identify the part-of-speech tags of input text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Part-of-Speech Tagging\", \"api_name\": \"flair/pos-english\", \"api_call\": \"'tagger = SequenceTagger.load(flair/pos-english)'\", \"api_arguments\": \"sentence\", \"python_environment_requirements\": \"flair (pip install flair)\", \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\ntagger = SequenceTagger.load(flair/pos-english)\\nsentence = Sentence(I love Berlin.)\\ntagger.predict(sentence)\\nprint(sentence)\\nfor entity in sentence.get_spans('pos'):\\n print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": \"98.19\"}, \"description\": \"This is the standard part-of-speech tagging model for English that ships with Flair. It predicts fine-grained POS tags based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 801, "text": " Identify a solution for classifying user's question and their corresponding data in a given table.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lysandre/tiny-tapas-random-wtq\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('lysandre/tiny-tapas-random-wtq')\", \"api_arguments\": \"model = TapasForQuestionAnswering.from_pretrained('lysandre/tiny-tapas-random-wtq')\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": \"WTQ\", \"accuracy\": \"Not provided\"}, \"description\": \"A tiny TAPAS model trained on the WikiTableQuestions dataset for table question answering tasks.\"}}", "category": "generic"}
{"question_id": 802, "text": " Our research team needs assistance in answering questions from a given table.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-mini-finetuned-sqa\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')\", \"api_arguments\": [\"model_name\", \"table\", \"queries\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.5148}, \"description\": \"TAPAS mini model fine-tuned on Sequential Question Answering (SQA)\"}}", "category": "generic"}
{"question_id": 803, "text": " Our task is to quickly analyze various datasets for an upcoming business presentation. We need answers for several questions related to the dataset.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Table-based QA\", \"api_name\": \"neulab/omnitab-large-1024shot\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')\", \"api_arguments\": {\"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"str\"}, \"python_environment_requirements\": [\"transformers\", \"pandas\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nimport pandas as pd\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting).\"}}", "category": "generic"}
{"question_id": 804, "text": " We are a translation agency focusing on various languages. We have a document containing Korean content, and we need to answer questions based on the context.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"monologg/koelectra-small-v2-distilled-korquad-384\", \"api_call\": \"pipeline('question-answering', model='monologg/koelectra-small-v2-distilled-korquad-384')\", \"api_arguments\": {\"model\": \"monologg/koelectra-small-v2-distilled-korquad-384\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"nlp(question='your_question', context='your_context')\", \"performance\": {\"dataset\": \"KorQuAD\", \"accuracy\": \"Not provided\"}, \"description\": \"A Korean Question Answering model based on Electra and trained on the KorQuAD dataset.\"}}", "category": "generic"}
{"question_id": 805, "text": " Create a legal-question answering system to extract information from contracts.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"Rakib/roberta-base-on-cuad\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('Rakib/roberta-base-on-cuad')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": {\"import\": \"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\", \"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\", \"model\": \"model = AutoModelForQuestionAnswering.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"performance\": {\"dataset\": \"cuad\", \"accuracy\": \"46.6%\"}, \"description\": \"This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents.\"}}", "category": "generic"}
{"question_id": 806, "text": " I am an AI scientist checking COVID-19 impacts on different industries. I have some documents that I want to analyze whether the text mentions the impact of the virus on a specific industry.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/minilm-uncased-squad2\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\", \"api_arguments\": {\"model_name\": \"deepset/minilm-uncased-squad2\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": {\"a\": {\"code\": \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\nmodel_name = deepset/minilm-uncased-squad2\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\nQA_input = {\\n 'question': 'Why is model conversion important?',\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\n}\\nres = nlp(QA_input)\"}, \"b\": {\"code\": \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\nmodel_name = deepset/minilm-uncased-squad2\\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\"}}, \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"exact\": 76.13071675229513, \"f1\": 79.49786500219953}}, \"description\": \"MiniLM-L12-H384-uncased is a language model fine-tuned for extractive question answering on the SQuAD 2.0 dataset. It is based on the microsoft/MiniLM-L12-H384-uncased model and can be used with the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 807, "text": " I am a Spanish editor, I need to figure out the best category for a news article for subsequent publishing and promotion.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"Recognai/bert-base-spanish-wwm-cased-xnli\", \"api_call\": \"Recognai/bert-base-spanish-wwm-cased-xnli.from_pretrained()\", \"api_arguments\": [\"sequence\", \"candidate_labels\", \"hypothesis_template\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nclassifier = pipeline(zero-shot-classification, model=Recognai/bert-base-spanish-wwm-cased-xnli)\\nclassifier(\\nEl autor se perfila, a los 50 as de su muerte, como uno de los grandes de su siglo,\\ncandidate_labels=[cultura, sociedad, economia, salud, deportes],\\nhypothesis_template=Este ejemplo es {}. \\n)\", \"performance\": {\"dataset\": \"XNLI-es\", \"accuracy\": \"79.9%\"}, \"description\": \"This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training.\"}}", "category": "generic"}
{"question_id": 808, "text": " Our team is working on a new app to encourage people to recycle more. We would like to use AI to classify user submissions in categories like : 'plastic', 'paper', 'glass', 'metal', 'electronics'.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"typeform/squeezebert-mnli\", \"api_call\": \"squeezebert-mnli\", \"api_arguments\": \"text, candidate_labels, multi_label\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('zero-shot-classification', model='typeform/squeezebert-mnli')\\nresult = nlp('The quick brown fox jumps over the lazy dog', candidate_labels=['sports', 'language', 'animals'])\", \"performance\": {\"dataset\": \"mulit_nli\", \"accuracy\": \"not provided\"}, \"description\": \"SqueezeBERT is a transformer model designed for efficient inference on edge devices. This specific model, typeform/squeezebert-mnli, is fine-tuned on the MultiNLI dataset for zero-shot classification tasks.\"}}", "category": "generic"}
{"question_id": 809, "text": " Our customer is an international law firm, we need to translate contracts from English to French. Make sure the translations are accurate and context-based.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Translation, Summarization, Question Answering, Text Classification\", \"api_name\": \"t5-base\", \"api_call\": \"T5Model.from_pretrained('t5-base')\", \"api_arguments\": [\"input_ids\", \"decoder_input_ids\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import T5Tokenizer, T5Model\\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\\nmodel = T5Model.from_pretrained('t5-base')\\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"See research paper, Table 14\"}, \"description\": \"T5-Base is a Text-To-Text Transfer Transformer (T5) model with 220 million parameters. It is designed to perform various NLP tasks, including machine translation, document summarization, question answering, and text classification. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be used with the Transformers library.\"}}", "category": "generic"}
{"question_id": 810, "text": " I have a customer inquiry asking about a refund policy. I need the answer in my native language, French. Can you help me get the details from the company policy?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation, Summarization, Question Answering, Sentiment Analysis, Regression\", \"api_name\": \"t5-large\", \"api_call\": \"T5Model.from_pretrained('t5-large')\", \"api_arguments\": {\"input_ids\": \"tokenizer(..., return_tensors='pt').input_ids\", \"decoder_input_ids\": \"tokenizer(..., return_tensors='pt').input_ids\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import T5Tokenizer, T5Model\"}, \"example_code\": \"tokenizer = T5Tokenizer.from_pretrained('t5-large')\\nmodel = T5Model.from_pretrained('t5-large')\\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"See research paper, Table 14\"}, \"description\": \"T5-Large is a Text-To-Text Transfer Transformer (T5) model with 770 million parameters. It is designed to handle a variety of NLP tasks, including translation, summarization, question answering, sentiment analysis, and regression. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on various supervised and unsupervised tasks.\"}}", "category": "generic"}
{"question_id": 811, "text": " Our Team needs a group translator to convert all project documents from listed Romance languages (French, Spanish, Portuguese, Italian, Romanian) to English.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Translation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation\", \"api_name\": \"opus-mt-ROMANCE-en\", \"api_call\": \"MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en') and MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ROMANCE-en')\", \"api_arguments\": [\"source languages\", \"target languages\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"opus\", \"accuracy\": {\"BLEU\": 62.2, \"chr-F\": 0.75}}, \"description\": \"A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.\"}}", "category": "generic"}
{"question_id": 812, "text": " As a news organization, we require a system to summarize long articles for our readers.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"google/pegasus-cnn_dailymail\", \"api_call\": \"Output: PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\\nmodel_name = 'google/pegasus-cnn_dailymail'\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\ninputs = tokenizer.encode('input_text', return_tensors='pt')\\nsummary_ids = model.generate(inputs)\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\", \"performance\": {\"dataset\": \"cnn_dailymail\", \"accuracy\": \"44.16/21.56/41.30\"}, \"description\": \"PEGASUS model for abstractive summarization, pretrained on the CNN/DailyMail dataset.\"}}", "category": "generic"}
{"question_id": 813, "text": " You are helping a journalist to generate a concise news summary to be posted on social media.\\n###Input: <<<ARTICLE>>>: \\\"Tech giant Apple Inc. announced today that the all-new MacBook Pro will feature the company's latest in-house ARM-based M1 chip, dramatically improving performance and battery life. The new MacBook Pro is expected to be up to 3.5 times faster in processing power, and up to six times faster in graphics, compared to its predecessors powered by Intel processors. Apple's shift from Intel processors to its own custom-designed chips marks a significant change in the company's approach to hardware manufacturing. The move is set to make production more efficient, cost-effective, and environmentally friendly. However, the transition is not without its challenges, as software developers will need to adapt their applications to run smoothly on the new hardware. Apple plans to complete the two-year transition period by the end of 2022, with more products featuring the M1 chip expected to be unveiled in the coming months.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Summarization\", \"api_name\": \"facebook/bart-large-cnn\", \"api_call\": \"pipeline('summarization', model='facebook/bart-large-cnn')\", \"api_arguments\": [\"ARTICLE\", \"max_length\", \"min_length\", \"do_sample\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\\nARTICLE = ...\\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\", \"performance\": {\"dataset\": \"cnn_dailymail\", \"accuracy\": {\"ROUGE-1\": 42.949, \"ROUGE-2\": 20.815, \"ROUGE-L\": 30.619, \"ROUGE-LSUM\": 40.038}}, \"description\": \"BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\"}}", "category": "generic"}
{"question_id": 814, "text": " We are a news agency looking to automatically summarize lengthy news articles while maintaining the original meaning.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"google/pegasus-newsroom\", \"api_call\": \"pipeline('summarization', model='google/pegasus-newsroom')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": \"newsroom\", \"accuracy\": \"45.98/34.20/42.18\"}, \"description\": \"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.\"}}", "category": "generic"}
{"question_id": 815, "text": " Suppose we are building a friendly chatbot to keep clients engaged while waiting for our service, and we want to generate suggestions for their next vacations.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"microsoft/DialoGPT-medium\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium').generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\", \"api_arguments\": [\"bot_input_ids\", \"max_length\", \"pad_token_id\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/DialoGPT-medium)\\nmodel = AutoModelForCausalLM.from_pretrained(microsoft/DialoGPT-medium)\", \"performance\": {\"dataset\": \"Reddit\", \"accuracy\": \"Comparable to human response quality under a single-turn conversation Turing test\"}, \"description\": \"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.\"}}", "category": "generic"}
{"question_id": 816, "text": " The company wants an AI bot that replies to its employees to answer their questions without human intervention.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"Zixtrauce/BDBot4Epoch\", \"api_call\": \"pipeline('text-generation', model='Zixtrauce/BDBot4Epoch')\", \"api_arguments\": {\"input\": \"message\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": {\"input\": \"Input a message to start chatting with Zixtrauce/BDBot4Epoch.\"}, \"performance\": {\"dataset\": \"unknown\", \"accuracy\": \"unknown\"}, \"description\": \"BrandonBot4Epochs is a conversational model trained on the GPT-2 architecture for text generation. It can be used to generate responses in a chatbot-like interface.\"}}", "category": "generic"}
{"question_id": 817, "text": " Help me build a solution that recognizes historical figures, and retrieves interesting details on their achievements. Write a python code as an example.\\n###Input: instruction=\\\"Who was Albert Einstein?\\\", knowledge=\\\"Albert Einstein was a theoretical physicist who developed the theory of relativity. His work is also known for its influence on the philosophy of science. He received the Nobel Prize in Physics in 1921 for his services to theoretical physics, and especially for his discovery of the photoelectric effect, a pivotal step in the development of quantum theory.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Conversational\", \"api_name\": \"microsoft/GODEL-v1_1-base-seq2seq\", \"api_call\": \"model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\", \"api_arguments\": [\"instruction\", \"knowledge\", \"dialog\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\\ndef generate(instruction, knowledge, dialog):\\n if knowledge != '':\\n knowledge = '[KNOWLEDGE] ' + knowledge\\n dialog = ' EOS '.join(dialog)\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n return output\", \"performance\": {\"dataset\": \"Reddit discussion thread, instruction and knowledge grounded dialogs\", \"accuracy\": \"N/A\"}, \"description\": \"GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\"}}", "category": "generic"}
{"question_id": 818, "text": " Let's make some interesting stories for young kids.Get the beginning lines of 5 stories.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"bigscience/test-bloomd-6b3\", \"api_call\": \"pipeline('text-generation', model='bigscience/test-bloomd-6b3')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; generator = pipeline('text-generation', model='bigscience/test-bloomd-6b3'); generator('Once upon a time')\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"A text generation model from Hugging Face, using the bigscience/test-bloomd-6b3 architecture. It can be used for generating text based on a given input.\"}}", "category": "generic"}
{"question_id": 819, "text": " The author can't continue their story. Help them with writing the next few lines.\\n###Input: Hello, I'm am conscious and\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"facebook/opt-1.3b\", \"api_call\": \"pipeline('text-generation', model='facebook/opt-1.3b')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\ngenerator = pipeline('text-generation', model='facebook/opt-1.3b')\\ngenerator(Hello, I'm am conscious and)\", \"performance\": {\"dataset\": \"BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit, CCNewsV2\", \"accuracy\": \"Not provided\"}, \"description\": \"OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, trained to roughly match the performance and sizes of the GPT-3 class of models. It can be used for prompting for evaluation of downstream tasks as well as text generation.\"}}", "category": "generic"}
{"question_id": 820, "text": " We are developing an AI model to communicate with humans, give me an example based on the prompt \\\"What is your purpose?\\\"\\n###Input: What is your purpose?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"facebook/opt-13b\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('facebook/opt-13b', torch_dtype=torch.float16).cuda().generate(input_ids)\", \"api_arguments\": [\"input_ids\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-13b, torch_dtype=torch.float16).cuda()\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-13b, use_fast=False)\\nprompt = Hello, I'm am conscious and\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\ngenerated_ids = model.generate(input_ids)\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\", \"performance\": {\"dataset\": \"GPT-3\", \"accuracy\": \"roughly match the performance and sizes of the GPT-3 class of models\"}, \"description\": \"OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The models are trained to match the performance and sizes of the GPT-3 class of models. The primary goal is to enable reproducible and responsible research at scale and to bring more voices to the table in studying the impact of large language models. OPT-13B is a 13-billion-parameter model trained predominantly with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\"}}", "category": "generic"}
{"question_id": 821, "text": " My son has a difficult time translating English to German, and he needs help with his homework. Develop a translator which can assist.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Language model\", \"api_name\": \"google/flan-t5-base\", \"api_call\": \"T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\", \"api_arguments\": \"input_text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import T5Tokenizer, T5ForConditionalGeneration\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-base)\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-base)\\ninput_text = translate English to German: How old are you?\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\noutputs = model.generate(input_ids)\\nprint(tokenizer.decode(outputs[0]))\", \"performance\": {\"dataset\": [{\"name\": \"MMLU\", \"accuracy\": \"75.2%\"}]}, \"description\": \"FLAN-T5 is a language model fine-tuned on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks and is designed for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering.\"}}", "category": "generic"}
{"question_id": 822, "text": " We are supporting French language learning. The students should fill the blank in the sentence: \\\"Le camembert est ____\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Fill-Mask\", \"api_name\": \"camembert-base\", \"api_call\": \"pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base')\", \"api_arguments\": [\"model\", \"tokenizer\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import pipeline; camembert_fill_mask = pipeline('fill-mask', model='camembert-base', tokenizer='camembert-base'); results = camembert_fill_mask('Le camembert est <mask> :)')\", \"performance\": {\"dataset\": \"oscar\", \"accuracy\": \"N/A\"}, \"description\": \"CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.\"}}", "category": "generic"}
{"question_id": 823, "text": " Predict the missing word in the following text: \\\"I am an AI who can perform various NLP [MASK].\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Transformers\", \"functionality\": \"Masked Language Modeling\", \"api_name\": \"bert-large-cased\", \"api_call\": \"pipeline('fill-mask', model='bert-large-cased')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"unmasker(Hello I'm a [MASK] model.)\", \"performance\": {\"dataset\": {\"SQUAD 1.1\": {\"F1\": 91.5, \"EM\": 84.8}, \"Multi NLI\": {\"accuracy\": 86.09}}}, \"description\": \"BERT large model (cased) pretrained on English language using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters.\"}}", "category": "generic"}
{"question_id": 824, "text": " Calculate the similarity between two movie reviews to find out if two users have similar opinions on the film.\\n###Input: \\nReview 1: \\\"I thoroughly enjoyed this movie - the acting was great, the storyline was interesting, and the cinematography was stunning.\\\"\\nReview 2: \\\"The film was a delightful experience, with outstanding performances, an engaging plot, and exceptional visual effects.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Embeddings\", \"api_name\": \"sentence-transformers/paraphrase-mpnet-base-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Automated evaluation\"}, \"description\": \"This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 825, "text": " We are building a chatbot and we need to be able to compare which response is the most suitable match for the input message from the user.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/paraphrase-distilroberta-base-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Automated evaluation\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 826, "text": " I am building transcript summaries of meeting audio recordings. I need to extract only the most important sentences.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Embeddings\", \"api_name\": \"sentence-transformers/paraphrase-albert-small-v2\", \"api_call\": \"SentenceTransformer('sentence-transformers/paraphrase-albert-small-v2')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/paraphrase-albert-small-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": [\"snli\", \"multi_nli\", \"ms_marco\"], \"accuracy\": \"https://seb.sbert.net\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 827, "text": " I have several paragraphs of text from different sources. I want to analyze their similarity in content.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\", \"api_call\": \"model.encode(text)\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"sentence-transformers library\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nmodel = SentenceTransformer('flax-sentence-embeddings/all_datasets_v4_MiniLM-L6')\\ntext = Replace me by any text you'd like.\\ntext_embbedding = model.encode(text)\", \"performance\": {\"dataset\": \"1,097,953,922\", \"accuracy\": \"N/A\"}, \"description\": \"The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\"}}", "category": "generic"}
{"question_id": 828, "text": " Create a prototype artificial voice for the novel character that we described in the product description. It will be used in an audiobook.\\n###Input: \\\"<noinput>\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"lakahaga/novel_reading_tts\", \"api_call\": \"AutoModelForTTS.from_pretrained('lakahaga/novel_reading_tts')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"inputs = processor(text, return_tensors='pt'); generated_audio = model.generate(**inputs);\", \"performance\": {\"dataset\": \"novelspeech\", \"accuracy\": null}, \"description\": \"This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\"}}", "category": "generic"}
{"question_id": 829, "text": " A client is launching their website in Arabic and wants a voice-over for their promotional video.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"Fairseq\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"tts_transformer-ar-cv7\", \"api_call\": \"'TTSHubInterface.get_model_input(task, text)'\", \"api_arguments\": {\"text\": \"input text\"}, \"python_environment_requirements\": [\"fairseq\", \"IPython\"], \"example_code\": \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\nimport IPython.display as ipd\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\n facebook/tts_transformer-ar-cv7,\\n arg_overrides={vocoder: hifigan, fp16: False}\\n)\\nmodel = models[0]\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\ngenerator = task.build_generator(model, cfg)\\ntext = \\u0645\\u0631\\u062d\\u0628\\u064b\\u0627 \\u060c \\u0647\\u0630\\u0627 \\u0627\\u062e\\u062a\\u0628\\u0627\\u0631 \\u062a\\u0634\\u063a\\u064a\\u0644.\\nsample = TTSHubInterface.get_model_input(task, text)\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\nipd.Audio(wav, rate=rate)\", \"performance\": {\"dataset\": \"common_voice\", \"accuracy\": \"Not specified\"}, \"description\": \"Transformer text-to-speech model for Arabic language with a single-speaker male voice, trained on Common Voice v7 dataset.\"}}", "category": "generic"}
{"question_id": 830, "text": " In a podcast with 3 speakers, we want to separate the speakers by segmenting the audio.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"pyannote.audio\", \"functionality\": \"Speaker Diarization\", \"api_name\": \"pyannote/speaker-diarization\", \"api_call\": \"Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', use_auth_token='ACCESS_TOKEN_GOES_HERE')\", \"api_arguments\": {\"num_speakers\": \"int (optional)\", \"min_speakers\": \"int (optional)\", \"max_speakers\": \"int (optional)\"}, \"python_environment_requirements\": \"pyannote.audio 2.1.1\", \"example_code\": [\"from pyannote.audio import Pipeline\", \"pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2.1, use_auth_token=ACCESS_TOKEN_GOES_HERE)\", \"diarization = pipeline(audio.wav)\", \"with open(audio.rttm, w) as rttm:\", \"  diarization.write_rttm(rttm)\"], \"performance\": {\"dataset\": \"ami\", \"accuracy\": {\"DER%\": \"18.91\", \"FA%\": \"4.48\", \"Miss%\": \"9.51\", \"Conf%\": \"4.91\"}}, \"description\": \"This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers.\"}}", "category": "generic"}
{"question_id": 831, "text": " Design a system to transcribe a podcast episode in a format that can be used for closed captions, including accurate punctuation.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"https://github.com/neonbjb/ocotillo\", \"performance\": {\"dataset\": \"librispeech validation set\", \"accuracy\": \"4.45%\"}, \"description\": \"This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.\"}}", "category": "generic"}
{"question_id": 832, "text": " A company wants to develop a voice assistant to be integrated into their web-based services. The main objective of this voice assistant is to transcribe user's voice commands and generate appropriate text inputs.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/wav2vec2-xlsr-53-espeak-cv-ft\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-xlsr-53-espeak-cv-ft')\", \"api_arguments\": {\"model_name\": \"facebook/wav2vec2-xlsr-53-espeak-cv-ft\"}, \"python_environment_requirements\": {\"transformers\": \"4.13.0\", \"torch\": \"1.10.0\", \"datasets\": \"1.14.0\"}, \"example_code\": \"processor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\ninput_values = processor(ds[0][audio][array], return_tensors=pt).input_values\\nwith torch.no_grad():\\n    logits = model(input_values).logits\\npredicted_ids = torch.argmax(logits, dim=-1)\\ntranscription = processor.batch_decode(predicted_ids)\", \"performance\": {\"dataset\": \"common_voice\", \"accuracy\": \"Not specified\"}, \"description\": \"Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words.\"}}", "category": "generic"}
{"question_id": 833, "text": " Analyze customer calls and identify the spoken words in various languages from an audio file.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"CTranslate2\", \"functionality\": \"Automatic Speech Recognition\", \"api_name\": \"guillaumekln/faster-whisper-large-v2\", \"api_call\": \"WhisperModel(large-v2)\", \"api_arguments\": [\"audio.mp3\"], \"python_environment_requirements\": [\"faster_whisper\"], \"example_code\": \"from faster_whisper import WhisperModel\\nmodel = WhisperModel(large-v2)\\nsegments, info = model.transcribe(audio.mp3)\\nfor segment in segments:\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))\", \"performance\": {\"dataset\": \"99 languages\", \"accuracy\": \"Not provided\"}, \"description\": \"Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.\"}}", "category": "generic"}
{"question_id": 834, "text": " My team is building a customer support chatbot that can detect customer's emotions from their voice messages. We need to evaluate emotions from an audio file.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Emotion Recognition\", \"api_name\": \"superb/wav2vec2-base-superb-er\", \"api_call\": \"superb/wav2vec2-base-superb-er\", \"api_arguments\": [\"file\", \"top_k\"], \"python_environment_requirements\": [\"datasets\", \"transformers\", \"torch\", \"librosa\"], \"example_code\": \"from datasets import load_dataset\\nfrom transformers import pipeline\\ndataset = load_dataset(anton-l/superb_demo, er, split=session1)\\nclassifier = pipeline(audio-classification, model=superb/wav2vec2-base-superb-er)\\nlabels = classifier(dataset[0][file], top_k=5)\", \"performance\": {\"dataset\": \"IEMOCAP\", \"accuracy\": 0.6258}, \"description\": \"This is a ported version of S3PRL's Wav2Vec2 for the SUPERB Emotion Recognition task. The base model is wav2vec2-base, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\"}}", "category": "generic"}
{"question_id": 835, "text": " Develop a voice interface for our robotic vacuum. The robot should recognize different commands from pre-recorded audio files.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"mazkooleg/0-9up-wavlm-base-plus-ft\", \"api_call\": \"pipeline('audio-classification', model='mazkooleg/0-9up-wavlm-base-plus-ft')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.27.3, torch==1.11.0, datasets==2.10.1, tokenizers==0.12.1\", \"example_code\": \"\", \"performance\": {\"dataset\": \"mazkooleg/0-9up_google_speech_commands_augmented_raw\", \"accuracy\": 0.9973}, \"description\": \"This model is a fine-tuned version of microsoft/wavlm-base-plus on the None dataset. It achieves the following results on the evaluation set: Loss: 0.0093, Accuracy: 0.9973.\"}}", "category": "generic"}
{"question_id": 836, "text": " A German dentist needs help during sessions with his patients. Create an emotion detection system to analyze the emotions in his patients' responses.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"padmalcom/wav2vec2-large-emotion-detection-german\", \"api_call\": \"pipeline('audio-classification', model=Wav2Vec2ForCTC.from_pretrained('padmalcom/wav2vec2-large-emotion-detection-german'))\", \"api_arguments\": \"audio_file\", \"python_environment_requirements\": \"transformers, torch\", \"example_code\": \"from transformers import pipeline\\naudio_classifier = pipeline('audio-classification', model='padmalcom/wav2vec2-large-emotion-detection-german')\\nresult = audio_classifier(audio_file)\", \"performance\": {\"dataset\": \"emo-DB\", \"accuracy\": \"Not provided\"}, \"description\": \"This wav2vec2 based emotion detection model is trained on the emo-DB dataset. It can classify emotions in German audio files into seven classes: anger, boredom, disgust, fear, happiness, sadness, and neutral.\"}}", "category": "generic"}
{"question_id": 837, "text": " We are working on an application to transcribe large conference calls. We need an algorithm to detect when people are speaking in the recording.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Voice Activity Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Voice Activity Detection, Overlapped Speech Detection, Resegmentation\", \"api_name\": \"pyannote/segmentation\", \"api_call\": \"Output: VoiceActivityDetection(segmentation='pyannote/segmentation')\", \"api_arguments\": {\"onset\": 0.5, \"offset\": 0.5, \"min_duration_on\": 0.0, \"min_duration_off\": 0.0}, \"python_environment_requirements\": \"pyannote.audio 2.0\", \"example_code\": \"from pyannote.audio.pipelines import VoiceActivityDetection\\npipeline = VoiceActivityDetection(segmentation=pyannote/segmentation)\\nHYPER_PARAMETERS = {\\n  onset: 0.5, offset: 0.5,\\n  min_duration_on: 0.0,\\n  min_duration_off: 0.0\\n}\\npipeline.instantiate(HYPER_PARAMETERS)\\nvad = pipeline(audio.wav)\", \"performance\": {\"dataset\": {\"ami\": {\"accuracy\": {\"onset\": 0.684, \"offset\": 0.577, \"min_duration_on\": 0.181, \"min_duration_off\": 0.037}}, \"dihard\": {\"accuracy\": {\"onset\": 0.767, \"offset\": 0.377, \"min_duration_on\": 0.136, \"min_duration_off\": 0.067}}, \"voxconverse\": {\"accuracy\": {\"onset\": 0.767, \"offset\": 0.713, \"min_duration_on\": 0.182, \"min_duration_off\": 0.501}}}}, \"description\": \"Model from End-to-end speaker segmentation for overlap-aware resegmentation, by HervBredin and Antoine Laurent. It provides voice activity detection, overlapped speech detection, and resegmentation functionalities.\"}}", "category": "generic"}
{"question_id": 838, "text": " We are a food technology firm, and we want to explore possible options to classify the quality of wine.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Scikit-learn\", \"functionality\": \"Wine Quality classification\", \"api_name\": \"julien-c/wine-quality\", \"api_call\": \"'model = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))'\", \"api_arguments\": [\"X\"], \"python_environment_requirements\": [\"huggingface_hub\", \"joblib\", \"pandas\"], \"example_code\": \"from huggingface_hub import hf_hub_url, cached_download\\nimport joblib\\nimport pandas as pd\\nREPO_ID = julien-c/wine-quality\\nFILENAME = sklearn_model.joblib\\nmodel = joblib.load(cached_download(\\n hf_hub_url(REPO_ID, FILENAME)\\n))\\ndata_file = cached_download(\\n hf_hub_url(REPO_ID, winequality-red.csv)\\n)\\nwinedf = pd.read_csv(data_file, sep=;)\\nX = winedf.drop([quality], axis=1)\\nY = winedf[quality]\\nprint(X[:3])\\nlabels = model.predict(X[:3])\\nmodel.score(X, Y)\", \"performance\": {\"dataset\": \"julien-c/wine-quality\", \"accuracy\": 0.6616635397123202}, \"description\": \"A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.\"}}", "category": "generic"}
{"question_id": 839, "text": " We are currently developing a flower delivery app, and it should take user's input and classify which type of flower it is.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Classification\", \"framework\": \"Joblib\", \"functionality\": \"Transformers\", \"api_name\": \"abhishek/autotrain-iris-knn\", \"api_call\": \"model.predict(data)\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"scikit-learn/iris\", \"accuracy\": 0.9}, \"description\": \"A K-Nearest Neighbors (KNN) model trained on the Iris dataset for multi-class classification. The model is trained using AutoTrain and has an accuracy of 0.9.\"}}", "category": "generic"}
{"question_id": 840, "text": " The company is interested in predicting the carbon emissions of a new warehouse building.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Hugging Face\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"38507101578\", \"api_call\": \"Output: model = joblib.load('model.joblib')\", \"api_arguments\": [\"data\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"wangdy/autotrain-data-godaddy2\", \"accuracy\": {\"Loss\": 0.206, \"R2\": 0.997, \"MSE\": 0.042, \"MAE\": 0.081, \"RMSLE\": 0.052}}, \"description\": \"This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\"}}", "category": "generic"}
{"question_id": 841, "text": " We are an organization working on reducing carbon emissions. We have collected data on carbon emission factors and features. Estimate the carbon emissions using this given data.\\n###Input: Please provide a .csv file named \\\"data.csv\\\" containing the necessary features for carbon emission estimation.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"pcoloc/autotrain-only-rssi-1813762559\", \"api_call\": \"Output: model.predict(data)\", \"api_arguments\": {\"data\": \"data.csv\"}, \"python_environment_requirements\": {\"joblib\": \"latest\", \"pandas\": \"latest\"}, \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": {\"Loss\": 83.432, \"R2\": 0.312, \"MSE\": 6960.888, \"MAE\": 60.449, \"RMSLE\": 0.532}}, \"description\": \"A tabular regression model trained using AutoTrain for estimating carbon emissions from given features.\"}}", "category": "generic"}
{"question_id": 842, "text": " I would like to train a reinforcement learning model to play CartPole using pre-trained PPO.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"ppo-seals-CartPole-v0\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": \"python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/\", \"performance\": {\"dataset\": \"seals/CartPole-v0\", \"accuracy\": \"500.00 +/- 0.00\"}, \"description\": \"This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 843, "text": " During a showcase of your company, you are asked to demonstrate an interactive experience with an already trained model of a player playing Pong. How would you use the API to replay the game with the saved game data?\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"ppo-PongNoFrameskip-v4\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"RL Zoo\", \"SB3\", \"SB3 Contrib\"], \"example_code\": \"python -m rl_zoo3.load_from_hub --algo ppo --env PongNoFrameskip-v4 -orga sb3 -f logs/\", \"performance\": {\"dataset\": \"PongNoFrameskip-v4\", \"accuracy\": \"21.00 +/- 0.00\"}, \"description\": \"This is a trained model of a PPO agent playing PongNoFrameskip-v4 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 844, "text": " We are developing a kids app that teaches soccer strategies through reinforcement learning. We need a model to demonstrate the gameplay.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Unity ML-Agents\", \"functionality\": \"Train and play SoccerTwos\", \"api_name\": \"Raiden-1001/poca-Soccerv7.1\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"ml-agents\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"Not provided\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 845, "text": " Develop a tool that helps users find relevant code snippets online when given a prompt in natural language.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"microsoft/codebert-base\", \"api_call\": \"AutoModel.from_pretrained('microsoft/codebert-base')\", \"api_arguments\": \"n/a\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"n/a\", \"performance\": {\"dataset\": \"CodeSearchNet\", \"accuracy\": \"n/a\"}, \"description\": \"Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages. The model is trained on bi-modal data (documents & code) of CodeSearchNet. This model is initialized with Roberta-base and trained with MLM+RTD objective.\"}}", "category": "generic"}
{"question_id": 846, "text": " We have to create a Q&A system for our educational platform for the high school students. The students should be able to ask questions and get relevant answers from the textbook.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/dragon-plus-context-encoder\", \"api_call\": \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", \"api_arguments\": [\"pretrained\"], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Euge Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", \"performance\": {\"dataset\": \"MS MARCO\", \"accuracy\": 39.0}, \"description\": \"DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.\"}}", "category": "generic"}
{"question_id": 847, "text": " We are building an art product that merges Art and Literature. We need to convert painting images into meaningful text.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-textcaps\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", \"api_arguments\": \"image, text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 848, "text": " We are exploring marketing opportunities and our company wants to create a video advertisement. The ad illustrates an athlete running while promoting our brand new shoes.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video Generation\", \"api_name\": \"mo-di-bear-guitar\", \"api_call\": \"pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5)\", \"api_arguments\": {\"prompt\": \"string\", \"video_length\": \"int\", \"height\": \"int\", \"width\": \"int\", \"num_inference_steps\": \"int\", \"guidance_scale\": \"float\"}, \"python_environment_requirements\": [\"torch\", \"tuneavideo\"], \"example_code\": \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", \"performance\": {\"dataset\": \"Not mentioned\", \"accuracy\": \"Not mentioned\"}, \"description\": \"Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\"}}", "category": "generic"}
{"question_id": 849, "text": " Could you help me find the invoice number from an invoice image?\\n###Input: image_url=https://templates.invoicehome.com/invoice-template-us-neat-750px.png, question=What is the invoice number?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"impira/layoutlm-document-qa\", \"api_call\": \"layoutlm_document_qa = pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa', return_dict=True))\", \"api_arguments\": [\"image_url\", \"question\"], \"python_environment_requirements\": [\"PIL\", \"pytesseract\", \"PyTorch\", \"transformers\"], \"example_code\": \"nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)\", \"performance\": {\"dataset\": [\"SQuAD2.0\", \"DocVQA\"], \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.\"}}", "category": "generic"}
{"question_id": 850, "text": " Audrey is responsible for invoice management at her company. She needs a tool to assist with the extraction of invoice details such as the invoice number, total, and due date.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"impira/layoutlm-invoices\", \"api_call\": \"pipeline('question-answering', model='impira/layoutlm-invoices')\", \"api_arguments\": \"question, context\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"qa_pipeline(question='your question', context='your document context')\", \"performance\": {\"dataset\": \"proprietary dataset of invoices, SQuAD2.0, and DocVQA\", \"accuracy\": \"not provided\"}, \"description\": \"This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.\"}}", "category": "generic"}
{"question_id": 851, "text": " Look at the image for me and find out what's the title of this document.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"vision-encoder-decoder\", \"api_name\": \"jinhybr/OCR-DocVQA-Donut\", \"api_call\": \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut').model.vision_encoder\", \"api_arguments\": \"image_path, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\"}}", "category": "generic"}
{"question_id": 852, "text": " My company needs to extract information from invoices to analyze their business expenses. We need the algorithm to provide detailed information about each invoice.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa\", \"api_call\": \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3\", \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 8.5806}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 853, "text": " We would like to perform graph classification using the Graphormer model for our medical research project to analyze molecular structures.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Graph Machine Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Graph Classification\", \"api_name\": \"graphormer-base-pcqm4mv2\", \"api_call\": \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", \"api_arguments\": \"pretrained_model_name\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"See the Graph Classification with Transformers tutorial.\", \"performance\": {\"dataset\": \"PCQM4M-LSCv2\", \"accuracy\": \"Not provided\"}, \"description\": \"The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.\"}}", "category": "generic"}
{"question_id": 854, "text": " Our client is a startup working on a mobile navigation system. They need to calculate depth estimations from input images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu\", \"api_call\": \"'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)'\", \"api_arguments\": \"images, return_tensors\", \"python_environment_requirements\": [\"transformers\", \"torch\", \"numpy\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\", \"performance\": {\"dataset\": \"NYUv2\", \"accuracy\": \"Not provided\"}, \"description\": \"Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 855, "text": " I need to remember the perfect birthday cake for my party; could you help me compute the depth estimations from an image of a birthday cake?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221215-112116\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"DIODE\", \"accuracy\": \"\"}, \"description\": \"A depth estimation model fine-tuned on the DIODE dataset.\"}}", "category": "generic"}
{"question_id": 856, "text": " A company focuses on providing suitable advertisements for their customers based on their age. We should assess people's age.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Age Classification\", \"api_name\": \"nateraw/vit-age-classifier\", \"api_call\": \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nateraw/vit-age-classifier\"}, \"python_environment_requirements\": [\"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", \"performance\": {\"dataset\": \"fairface\", \"accuracy\": null}, \"description\": \"A vision transformer finetuned to classify the age of a given person's face.\"}}", "category": "generic"}
{"question_id": 857, "text": " Detect the main subject of the image in a photo album website.\\n###Input: Image URL: 'http://images.cocodataset.org/val2017/000000039769.jpg'\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/beit-base-patch16-224\", \"api_call\": \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"microsoft/beit-base-patch16-224\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"ImageNet\", \"accuracy\": \"Refer to tables 1 and 2 of the original paper\"}, \"description\": \"BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224.\"}}", "category": "generic"}
{"question_id": 858, "text": " A clothing store needs help to identify and label different parts of clothes. Help them with a program that will automatically segment the clothes in an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-base-coco-panoptic\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/mask2former-swin-base-coco-panoptic\"}, \"python_environment_requirements\": {\"packages\": [\"requests\", \"torch\", \"PIL\", \"transformers\"]}, \"example_code\": \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO panoptic segmentation\", \"accuracy\": null}, \"description\": \"Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\"}}", "category": "generic"}
{"question_id": 859, "text": " The CEO wants us to develop a single model that can perform multiple image segmentation tasks accurately.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"shi-labs/oneformer_ade20k_swin_large\", \"api_call\": \"OneFormerForUniversalSegmentation.from_pretrained(\\\\shi-labs/oneformer_ade20k_swin_large\\\\)\", \"api_arguments\": [\"images\", \"task_inputs\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"scene_parse_150\", \"accuracy\": null}, \"description\": \"OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\"}}", "category": "generic"}
{"question_id": 860, "text": " We are an infrastructure management company. We need to detect potholes in images from the road network.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8n-pothole-segmentation\", \"api_call\": \"'model.predict(image)'\", \"api_arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\", \"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": {\"ultralyticsplus\": \"0.0.23\", \"ultralytics\": \"8.0.21\"}, \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pothole-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.995, \"mAP@0.5(mask)\": 0.995}}, \"description\": \"A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\"}}", "category": "generic"}
{"question_id": 861, "text": " A film production company wants to estimate the depth of objects in an image taken from one of their video footages.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Depth Estimation\", \"api_name\": \"lllyasviel/sd-controlnet-depth\", \"api_call\": \"ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)\", \"api_arguments\": {\"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"PIL\", \"numpy\", \"torch\"], \"example_code\": {\"install_packages\": \"pip install diffusers transformers accelerate\", \"code\": [\"from transformers import pipeline\", \"from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\", \"from PIL import Image\", \"import numpy as np\", \"import torch\", \"from diffusers.utils import load_image\", \"depth_estimator = pipeline('depth-estimation')\", \"image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)\", \"image = depth_estimator(image)['depth']\", \"image = np.array(image)\", \"image = image[:, :, None]\", \"image = np.concatenate([image, image, image], axis=2)\", \"image = Image.fromarray(image)\", \"controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)\", \"pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\", \"pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\", \"pipe.enable_xformers_memory_efficient_attention()\", \"pipe.enable_model_cpu_offload()\", \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, \"performance\": {\"dataset\": \"3M depth-image, caption pairs\", \"accuracy\": \"500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 862, "text": " We are working on a project that involves converting a text description into its corresponding images. The model should also maintain the context of the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation\", \"api_name\": \"lllyasviel/control_v11p_sd15_seg\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg', torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_seg\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}", "category": "generic"}
{"question_id": 863, "text": " We are creating a digital art show and require random, high-quality computer-generated images to display.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Synthesis\", \"api_name\": \"google/ddpm-cifar10-32\", \"api_call\": \"Output: DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", \"api_arguments\": \"None\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.\"}}", "category": "generic"}
{"question_id": 864, "text": " An architect firm is looking forward to generating artificial images of church layouts. They mainly focus on the facade of the buildings.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ncsnpp-church-256\", \"api_call\": \"sde_ve = DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\", \"api_arguments\": \"model_id\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-church-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR-10\", \"accuracy\": {\"Inception_score\": 9.89, \"FID\": 2.2, \"likelihood\": 2.99}}, \"description\": \"Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.\"}}", "category": "generic"}
{"question_id": 865, "text": " I am writing a book and I need references which helps me to create illustrations of insects, especially butterflies.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ntrant7/sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\", \"api_arguments\": [], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies.\"}}", "category": "generic"}
{"question_id": 866, "text": " Create a solution for an app that categorizes videos into either violent or non-violent footage.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 0.7453}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 867, "text": " Create a model that predicts whether an image contains a dog or a cat given an image url.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-base-patch32\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\", \"api_arguments\": [\"text\", \"images\", \"return_tensors\", \"padding\"], \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-base-patch32)\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}", "category": "generic"}
{"question_id": 868, "text": " Create an image classifier that can identify diverse animals like cats, dogs, and birds.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\", \"api_arguments\": \"image_path, class_names\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"results = model(image_path, class_names='cat, dog, bird')\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"76.9\"}, \"description\": \"A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).\"}}", "category": "generic"}
{"question_id": 869, "text": " We are developing an app to classify what type of food is being consumed for a calorie-tracking feature. Implement a model to perform Zero-Shot Image Classification for this purpose.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w-laion2B-s13B-b82K\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"api_arguments\": {\"image_path\": \"path to the image file\", \"labels\": \"list of possible class names\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8 - 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}", "category": "generic"}
{"question_id": 870, "text": " In our online platform, we need to determine if customer reviews are positive or negative.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"lvwerra/distilbert-imdb\", \"api_call\": \"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\", \"pytorch\"], \"example_code\": \"classifier('I love this movie!')\", \"performance\": {\"dataset\": \"imdb\", \"accuracy\": 0.928}, \"description\": \"This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\"}}", "category": "generic"}
{"question_id": 871, "text": " Identify if a user inquiry is a question or a statement for a customer support chatbot.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"shahrukhx01/question-vs-statement-classifier\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\\nmodel = AutoModelForSequenceClassification.from_pretrained(shahrukhx01/question-vs-statement-classifier)\", \"performance\": {\"dataset\": \"Haystack\", \"accuracy\": \"Not provided\"}, \"description\": \"Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack\"}}", "category": "generic"}
{"question_id": 872, "text": " I am a restaurant owner, I manage a series of comments, and I have a hard time detecting if the comment is good or bad. I need a simple script to detect if my customer is satisfied or not.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"results-yelp\", \"api_call\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}, \"python_environment_requirements\": {\"Transformers\": \"4.18.0\", \"Pytorch\": \"1.10.0+cu111\", \"Datasets\": \"2.0.0\", \"Tokenizers\": \"0.12.1\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"Yelp\", \"accuracy\": 0.9302}, \"description\": \"This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\"}}", "category": "generic"}
{"question_id": 873, "text": " We need a name entity recognition program to extract information included in a text document, which are English, German, or Spanish. \\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"distilbert-base-multilingual-cased-ner-hrl\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')\", \"api_arguments\": {\"model\": \"Davlan/distilbert-base-multilingual-cased-ner-hrl\", \"tokenizer\": \"Davlan/distilbert-base-multilingual-cased-ner-hrl\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\nfrom transformers import pipeline\\ntokenizer = AutoTokenizer.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\\nmodel = AutoModelForTokenClassification.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\nexample = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\\nner_results = nlp(example)\\nprint(ner_results)\", \"performance\": {\"dataset\": [{\"name\": \"ANERcorp\", \"language\": \"Arabic\"}, {\"name\": \"conll 2003\", \"language\": \"German\"}, {\"name\": \"conll 2003\", \"language\": \"English\"}, {\"name\": \"conll 2002\", \"language\": \"Spanish\"}, {\"name\": \"Europeana Newspapers\", \"language\": \"French\"}, {\"name\": \"Italian I-CAB\", \"language\": \"Italian\"}, {\"name\": \"Latvian NER\", \"language\": \"Latvian\"}, {\"name\": \"conll 2002\", \"language\": \"Dutch\"}, {\"name\": \"Paramopama + Second Harem\", \"language\": \"Portuguese\"}, {\"name\": \"MSRA\", \"language\": \"Chinese\"}], \"accuracy\": \"Not specified\"}, \"description\": \"distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).\"}}", "category": "generic"}
{"question_id": 874, "text": " I need help to identify the entities in the given text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-ontonotes\", \"api_call\": \"Output: SequenceTagger.load(\\\\flair/ner-english-ontonotes\\\\)\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\ntagger = SequenceTagger.load(flair/ner-english-ontonotes)\\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\\ntagger.predict(sentence)\\nprint(sentence)\\nfor entity in sentence.get_spans('ner'):\\n    print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": \"89.27\"}, \"description\": \"This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 875, "text": " The team wants to implement a tool that can analyze the part-of-speech of a given text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Flair\", \"functionality\": \"Part-of-Speech Tagging\", \"api_name\": \"flair/upos-english\", \"api_call\": \"SequenceTagger.load('flair/upos-english')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": \"pip install flair\", \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\ntagger = SequenceTagger.load('flair/upos-english')\\nsentence = Sentence('I love Berlin.')\\ntagger.predict(sentence)\\nprint(sentence)\\nfor entity in sentence.get_spans('pos'):\\n    print(entity)\", \"performance\": {\"dataset\": \"ontonotes\", \"accuracy\": 98.6}, \"description\": \"This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 876, "text": " Answer a question related to a table/list of data.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lysandre/tiny-tapas-random-wtq\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('lysandre/tiny-tapas-random-wtq')\", \"api_arguments\": \"model = TapasForQuestionAnswering.from_pretrained('lysandre/tiny-tapas-random-wtq')\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": \"WTQ\", \"accuracy\": \"Not provided\"}, \"description\": \"A tiny TAPAS model trained on the WikiTableQuestions dataset for table question answering tasks.\"}}", "category": "generic"}
{"question_id": 877, "text": " We are running a sports trivia night at a fundraiser event, and we would like to answer questions based on Olympic Games data.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"neulab/omnitab-large-finetuned-wtq\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-finetuned-wtq').generate(**encoding)\", \"api_arguments\": {\"table\": \"pd.DataFrame\", \"query\": \"str\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForSeq2SeqLM\", \"pandas\": \"pd\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nimport pandas as pd\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-finetuned-wtq)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-finetuned-wtq)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": null}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab.\"}}", "category": "generic"}
{"question_id": 878, "text": " I am working on a project to create a chatbot for a college library. I'd like the chatbot to be able to answer questions related to specific data.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"microsoft/tapex-large-finetuned-wikisql\", \"api_call\": \"Output: BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\", \"api_arguments\": {\"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"query\"}, \"python_environment_requirements\": {\"transformers\": \"TapexTokenizer, BartForConditionalGeneration\", \"pandas\": \"pd\"}, \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"N/A\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiSQL dataset.\"}}", "category": "generic"}
{"question_id": 879, "text": " An education service provider wants to find a method to answer the questions based on the contents of a given table.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"table-question-answering-tapas\", \"api_call\": \"pipeline('table-question-answering', model='Meena/table-question-answering-tapas').model\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": [{\"name\": \"SQA (Sequential Question Answering by Microsoft)\", \"accuracy\": null}, {\"name\": \"WTQ (Wiki Table Questions by Stanford University)\", \"accuracy\": null}, {\"name\": \"WikiSQL (by Salesforce)\", \"accuracy\": null}]}, \"description\": \"TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\"}}", "category": "generic"}
{"question_id": 880, "text": " We have an inventory database and we need to answer some questions for our customers.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-mini-finetuned-sqa\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')\", \"api_arguments\": [\"model_name\", \"table\", \"queries\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.5148}, \"description\": \"TAPAS mini model fine-tuned on Sequential Question Answering (SQA)\"}}", "category": "generic"}
{"question_id": 881, "text": " An intern in my company is learning Python programming. They are asking for a function that can answer natural language questions given a specific context.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"bert-large-cased-whole-word-masking-finetuned-squad\", \"api_call\": \"AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\", \"api_arguments\": {\"model_name_or_path\": \"bert-large-cased-whole-word-masking\", \"dataset_name\": \"squad\", \"do_train\": true, \"do_eval\": true, \"learning_rate\": 3e-05, \"num_train_epochs\": 2, \"max_seq_length\": 384, \"doc_stride\": 128, \"output_dir\": \"./examples/models/wwm_cased_finetuned_squad/\", \"per_device_eval_batch_size\": 3, \"per_device_train_batch_size\": 3}, \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='bert-large-cased-whole-word-masking-finetuned-squad')\\nresult = qa_pipeline({'context': 'This is a context example.', 'question': 'What is this example for?'})\\nprint(result)\", \"performance\": {\"dataset\": [{\"name\": \"BookCorpus\", \"accuracy\": \"N/A\"}, {\"name\": \"English Wikipedia\", \"accuracy\": \"N/A\"}]}, \"description\": \"BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset.\"}}", "category": "generic"}
{"question_id": 882, "text": " Our company is expanding into France and we need a model that can classify French texts into categories such as sports, politics, and science.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"BaptisteDoyen/camembert-base-xnli\", \"api_call\": \"pipeline('zero-shot-classification', model='BaptisteDoyen/camembert-base-xnli')\", \"api_arguments\": {\"sequence\": \"str\", \"candidate_labels\": \"List[str]\", \"hypothesis_template\": \"str\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"sequence = L'uipe de France joue aujourd'hui au Parc des Princes\\ncandidate_labels = [sport,politique,science]\\nhypothesis_template = Ce texte parle de {}.\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\", \"performance\": {\"dataset\": \"xnli\", \"accuracy\": {\"validation\": 81.4, \"test\": 81.7}}, \"description\": \"Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French.\"}}", "category": "generic"}
{"question_id": 883, "text": " Our company wants to develop multilingual conversational agents and we want to examine the sentiments in the German news article. I need a code snippet for zero-shot classification.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Classification\", \"api_name\": \"Sahajtomar/German_Zeroshot\", \"api_call\": \"classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\", \"api_arguments\": {\"sequence\": \"string\", \"candidate_labels\": \"list of strings\", \"hypothesis_template\": \"string\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nclassifier = pipeline(zero-shot-classification, model=Sahajtomar/German_Zeroshot)\\nsequence = Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie\\ncandidate_labels = [Verbrechen,Tragie,Stehlen]\\nhypothesis_template = In deisem geht es um {}. ## Since monolingual model,its sensitive to hypothesis template. This can be experimented\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\", \"performance\": {\"dataset\": {\"XNLI DEV (german)\": {\"accuracy\": 85.5}, \"XNLI TEST (german)\": {\"accuracy\": 83.6}}}, \"description\": \"This model has GBERT Large as base model and fine-tuned it on xnli de dataset. The default hypothesis template is in English: This text is {}. While using this model, change it to In deisem geht es um {}. or something different. While inferencing through huggingface api may give poor results as it uses by default english template. Since model is monolingual and not multilingual, hypothesis template needs to be changed accordingly.\"}}", "category": "generic"}
{"question_id": 884, "text": " We need an AI to help our students to answer their questions in different subjects.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Translation, Summarization, Question Answering, Sentiment Analysis, Regression\", \"api_name\": \"t5-large\", \"api_call\": \"T5Model.from_pretrained('t5-large')\", \"api_arguments\": {\"input_ids\": \"tokenizer(..., return_tensors='pt').input_ids\", \"decoder_input_ids\": \"tokenizer(..., return_tensors='pt').input_ids\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import T5Tokenizer, T5Model\"}, \"example_code\": \"tokenizer = T5Tokenizer.from_pretrained('t5-large')\\nmodel = T5Model.from_pretrained('t5-large')\\ninput_ids = tokenizer('Studies have been shown that owning a dog is good for you', return_tensors='pt').input_ids\\ndecoder_input_ids = tokenizer('Studies show that', return_tensors='pt').input_ids\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"See research paper, Table 14\"}, \"description\": \"T5-Large is a Text-To-Text Transfer Transformer (T5) model with 770 million parameters. It is designed to handle a variety of NLP tasks, including translation, summarization, question answering, sentiment analysis, and regression. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on various supervised and unsupervised tasks.\"}}", "category": "generic"}
{"question_id": 885, "text": " My science department is about to release a new research paper. I need to summarize the findings before presenting it to the team.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"google/bigbird-pegasus-large-arxiv\", \"api_call\": \"BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\", \"api_arguments\": [\"attention_type\", \"block_size\", \"num_random_blocks\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-arxiv)\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-arxiv)\\ntext = Replace me by any text you'd like.\\ninputs = tokenizer(text, return_tensors='pt')\\nprediction = model.generate(**inputs)\\nprediction = tokenizer.batch_decode(prediction)\", \"performance\": {\"dataset\": \"scientific_papers\", \"accuracy\": {\"ROUGE-1\": 36.028, \"ROUGE-2\": 13.417, \"ROUGE-L\": 21.961, \"ROUGE-LSUM\": 29.648}}, \"description\": \"BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT's attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\"}}", "category": "generic"}
{"question_id": 886, "text": " I am a researcher working in the medical field, and there're some research articles that we would like to summarize to save time for our team.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Summarization\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/pegasus-pubmed\", \"api_call\": \"AutoModel.from_pretrained('google/pegasus-pubmed')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"Python 3.6+\", \"example_code\": \"\", \"performance\": {\"dataset\": [{\"name\": \"xsum\", \"accuracy\": \"47.60/24.83/39.64\"}, {\"name\": \"cnn_dailymail\", \"accuracy\": \"44.16/21.56/41.30\"}, {\"name\": \"newsroom\", \"accuracy\": \"45.98/34.20/42.18\"}, {\"name\": \"multi_news\", \"accuracy\": \"47.65/18.75/24.95\"}, {\"name\": \"gigaword\", \"accuracy\": \"39.65/20.47/36.76\"}, {\"name\": \"wikihow\", \"accuracy\": \"46.39/22.12/38.41\"}, {\"name\": \"reddit_tifu\", \"accuracy\": \"27.99/9.81/22.94\"}, {\"name\": \"big_patent\", \"accuracy\": \"52.29/33.08/41.66\"}, {\"name\": \"arxiv\", \"accuracy\": \"44.21/16.95/25.67\"}, {\"name\": \"pubmed\", \"accuracy\": \"45.97/20.15/28.25\"}, {\"name\": \"aeslc\", \"accuracy\": \"37.68/21.25/36.51\"}, {\"name\": \"billsum\", \"accuracy\": \"59.67/41.58/47.59\"}]}, \"description\": \"The PEGASUS model is designed for abstractive summarization. It is pretrained on a mixture of C4 and HugeNews datasets and stochastically samples important sentences. The model uses a gap sentence ratio between 15% and 45% and a sentencepiece tokenizer that encodes newline characters.\"}}", "category": "generic"}
{"question_id": 887, "text": " One of my friends is a robot fan, I want to send him a gift which is a robot. Please write me a message to ask him if he is interested.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"Zixtrauce/BaekBot\", \"api_call\": \"pipeline('conversational', model='Zixtrauce/BaekBot')\", \"api_arguments\": \"input_message\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"conversational_pipeline('input_message')\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"BaekBot is a conversational model based on the GPT-2 architecture for text generation. It can be used for generating human-like responses in a chat-like environment.\"}}", "category": "generic"}
{"question_id": 888, "text": " Assist me in engaging in a conversation with my brother about his favorite food for dinner and recommend a restaurant.\\n###Input: My brother's favorite food is pizza.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Conversational\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/blenderbot-90M\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\", \"api_arguments\": {\"input_message\": \"str\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\\ntokenizer = AutoTokenizer.from_pretrained('facebook/blenderbot-90M')\\nmodel = AutoModelForCausalLM.from_pretrained('facebook/blenderbot-90M')\\n# Chat with the model\\ninput_message = 'What is your favorite color?'\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors='pt')\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\nprint(response)\", \"performance\": {\"dataset\": \"blended_skill_talk\", \"accuracy\": \"Not provided\"}, \"description\": \"BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead.\"}}", "category": "generic"}
{"question_id": 889, "text": " Create a coherent story with a prompt \\\"Once upon a time, in a faraway land\\\".\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"facebook/opt-13b\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('facebook/opt-13b', torch_dtype=torch.float16).cuda().generate(input_ids)\", \"api_arguments\": [\"input_ids\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-13b, torch_dtype=torch.float16).cuda()\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-13b, use_fast=False)\\nprompt = Hello, I'm am conscious and\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\ngenerated_ids = model.generate(input_ids)\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\", \"performance\": {\"dataset\": \"GPT-3\", \"accuracy\": \"roughly match the performance and sizes of the GPT-3 class of models\"}, \"description\": \"OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The models are trained to match the performance and sizes of the GPT-3 class of models. The primary goal is to enable reproducible and responsible research at scale and to bring more voices to the table in studying the impact of large language models. OPT-13B is a 13-billion-parameter model trained predominantly with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\"}}", "category": "generic"}
{"question_id": 890, "text": " Imagine you are working as a newspaper editor, and you need to find a quick and brief summary of a long news article.\\n###Input: In a landmark decision by the United Nations General Assembly, global leaders have unanimously agreed to significantly reduce plastic waste production by the end of the decade in an effort to combat the devastating effects of plastic pollution on the environment. This historic announcement comes as a result of years-long negotiations and increasing public awareness of the environmental toll that plastic waste takes on the planet. By adopting a global framework for action, the UN aims to eliminate single-use plastics, improve waste management, and encourage sustainable alternatives. Countries are now expected to develop national action plans reflecting the goals of the agreement, and the UN will develop a monitoring system to evaluate progress in meeting these objectives.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Summarization\", \"api_name\": \"mrm8488/t5-base-finetuned-summarize-news\", \"api_call\": \"'summarize(text, max_length=150)'\", \"api_arguments\": [\"text\", \"max_length\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoModelWithLMHead, AutoTokenizer\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\ndef summarize(text, max_length=150):\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\nreturn preds[0]\", \"performance\": {\"dataset\": \"News Summary\", \"accuracy\": \"Not provided\"}, \"description\": \"Google's T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}}", "category": "generic"}
{"question_id": 891, "text": " A website owner wants to generate a short summary of their article in French by translating it from English.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Transformers\", \"functionality\": \"Text Generation\", \"api_name\": \"google/t5-v1_1-base\", \"api_call\": \"pipeline('text2text-generation', model='google/t5-v1_1-base') should be rewritten as T5ForConditionalGeneration.from_pretrained('google/t5-v1_1-base')\", \"api_arguments\": {\"model\": \"google/t5-v1_1-base\"}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"from transformers import pipeline\\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\\nt5('translate English to French: Hugging Face is a great company')\", \"performance\": {\"dataset\": \"c4\", \"accuracy\": \"Not provided\"}, \"description\": \"Google's T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks.\"}}", "category": "generic"}
{"question_id": 892, "text": " We are trying to build a chatbot for customer support. However, it's important that the language used by the chatbot is grammatically correct. So, in order to correct the grammar, I need a text-based generative model.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text2Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Grammar Correction\", \"api_name\": \"vennify/t5-base-grammar-correction\", \"api_call\": \"Output: HappyTextToText('T5', 'vennify/t5-base-grammar-correction')\", \"api_arguments\": {\"num_beams\": 5, \"min_length\": 1}, \"python_environment_requirements\": {\"package\": \"happytransformer\", \"installation\": \"pip install happytransformer\"}, \"example_code\": \"from happytransformer import HappyTextToText, TTSettings\\nhappy_tt = HappyTextToText(T5, vennify/t5-base-grammar-correction)\\nargs = TTSettings(num_beams=5, min_length=1)\\nresult = happy_tt.generate_text(grammar: This sentences has has bads grammar., args=args)\\nprint(result.text)\", \"performance\": {\"dataset\": \"jfleg\", \"accuracy\": \"Not provided\"}, \"description\": \"This model generates a revised version of inputted text with the goal of containing fewer grammatical errors. It was trained with Happy Transformer using a dataset called JFLEG.\"}}", "category": "generic"}
{"question_id": 893, "text": " Given an incomplete sentence during an online science exam, please find the appropriate word to complete the statement.\\n###Input: \\\"During photosynthesis, plants convert sunlight, carbon dioxide, and water into glucose and [MASK].\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Fill-Mask\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"dmis-lab/biobert-base-cased-v1.2\", \"api_call\": \"pipeline('fill-mask', model='dmis-lab/biobert-base-cased-v1.2')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"fill_mask('Hugging Face is a [MASK] company.')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.\"}}", "category": "generic"}
{"question_id": 894, "text": " Your task is to create a language model to cluster news articles into different categories based on similarity.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/all-distilroberta-v1\", \"api_call\": \"SentenceTransformer('sentence-transformers/all-distilroberta-v1')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": [{\"name\": \"s2orc\", \"accuracy\": \"Not provided\"}, {\"name\": \"MS Marco\", \"accuracy\": \"Not provided\"}, {\"name\": \"yahoo_answers_topics\", \"accuracy\": \"Not provided\"}]}, \"description\": \"This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 895, "text": " I want to find two similar sentence's score in the Chinese language more efficiently.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"shibing624/text2vec-base-chinese\", \"api_call\": \"Output: SentenceModel('shibing624/text2vec-base-chinese')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": [\"text2vec\", \"transformers\"], \"example_code\": \"from text2vec import SentenceModel\\nsentences = ['\\u5982\\u4f55\\u66f4\\u6362\\u82b1\\u5457\\u7ed1\\u5b9a\\u94f6\\u884c\\u5361', '\\u82b1\\u5457\\u66f4\\u6539\\u7ed1\\u5b9a\\u94f6\\u884c\\u5361']\\nmodel = SentenceModel('shibing624/text2vec-base-chinese')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": [{\"name\": \"ATEC\", \"accuracy\": \"31.93\"}, {\"name\": \"BQ\", \"accuracy\": \"42.67\"}, {\"name\": \"LCQMC\", \"accuracy\": \"70.16\"}, {\"name\": \"PAWSX\", \"accuracy\": \"17.21\"}, {\"name\": \"STS-B\", \"accuracy\": \"79.30\"}]}, \"description\": \"This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese. It maps sentences to a 768 dimensional dense vector space and can be used for tasks like sentence embeddings, text matching or semantic search.\"}}", "category": "generic"}
{"question_id": 896, "text": " I'm a content moderator, looking to detect similarity between two pieces of text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/gtr-t5-base\", \"api_call\": \"model.encode(sentences)\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": [\"sentence-transformers>=2.2.0\"], \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/gtr-t5-base')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"N/A\"}, \"description\": \"This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of semantic search.\"}}", "category": "generic"}
{"question_id": 897, "text": " Can you compare the similarity between different news articles to help me find related content to read?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"sentence-transformers/all-roberta-large-v1\", \"api_call\": \"SentenceTransformer('sentence-transformers/all-roberta-large-v1')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Automated evaluation\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 898, "text": " You have been asked to find the similarity between an instruction and a sentence for an educational text resource.\\n###Input: {\\\"instruction\\\": \\\"Explain the main idea of the research paper:\\\", \\\"sentence\\\": \\\"This study investigates the impact of climate change on agricultural productivity.\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentence Transformers\", \"api_name\": \"hkunlp/instructor-base\", \"api_call\": \"Output: INSTRUCTOR('hkunlp/instructor-base').encode([[instruction,sentence]])\", \"api_arguments\": {\"instruction\": \"string\", \"sentence\": \"string\"}, \"python_environment_requirements\": \"pip install InstructorEmbedding\", \"example_code\": \"from InstructorEmbedding import INSTRUCTOR\\nmodel = INSTRUCTOR('hkunlp/instructor-base')\\nsentence = 3D ActionSLAM: wearable person tracking in multi-floor environments\\ninstruction = Represent the Science title:\\nembeddings = model.encode([[instruction,sentence]])\\nprint(embeddings)\", \"performance\": {\"dataset\": \"MTEB AmazonCounterfactualClassification (en)\", \"accuracy\": 86.209}, \"description\": \"Instructor is an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor achieves state-of-the-art performance on 70 diverse embedding tasks.\"}}", "category": "generic"}
{"question_id": 899, "text": " We are planning to create an audiobook from a given text. Can you provide a Text-To-Speech model that could help?\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"mio/amadeus\", \"api_call\": \"No changes needed.\", \"api_arguments\": {\"model_name\": \"mio/amadeus\"}, \"python_environment_requirements\": {\"espnet\": \"d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\", \"transformers\": \"latest\"}, \"example_code\": \"cd espnet\\ngit checkout d5b5ec7b2e77bd3e10707141818b7e6c57ac6b3f\\npip install -e .\\ncd egs2/amadeus/tts1\\n./run.sh --skip_data_prep false --skip_train true --download_model mio/amadeus\", \"performance\": {\"dataset\": \"amadeus\", \"accuracy\": \"Not provided\"}, \"description\": \"This model was trained by mio using amadeus recipe in espnet.\"}}", "category": "generic"}
{"question_id": 900, "text": " Create a voice assistant that reads out this article summary: \\\"The tech giant announced its latest innovation in AI, which is set to revolutionize the world of technology.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"Fairseq\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"fastspeech2-en-male1\", \"api_call\": \"'TTSHubInterface.get_prediction(task, models[0], generator, sample)'\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"fairseq\", \"IPython\"], \"example_code\": \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\nimport IPython.display as ipd\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\n facebook/fastspeech2-en-200_speaker-cv4,\\n arg_overrides={vocoder: hifigan, fp16: False}\\n)\\nmodel = models[0]\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\ngenerator = task.build_generator(model, cfg)\\ntext = Hello, this is a test run.\\nsample = TTSHubInterface.get_model_input(task, text)\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\nipd.Audio(wav, rate=rate)\", \"performance\": {\"dataset\": \"common_voice\", \"accuracy\": null}, \"description\": \"FastSpeech 2 text-to-speech model from fairseq S^2. English, 200 male/female voices, trained on Common Voice v4.\"}}", "category": "generic"}
{"question_id": 901, "text": " Create an advertisement for a gardening company, and we need a voice-over with the following sentence in Chinese: \\\"\\u7ed9\\u4f60\\u7684\\u82b1\\u56ed\\u5e26\\u6765\\u9c9c\\u82b1\\u548c\\u7eff\\u610f\\u76ce\\u7136\\u7684\\u751f\\u547d\\u529b.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\", \"api_call\": \"Output: Text2Speech.from_pretrained('espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"torch\", \"espnet_model_zoo\"], \"example_code\": \"import soundfile\\nfrom espnet2.bin.tts_inference import Text2Speech\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\ntext = \\u6625\\u6c5f\\u6f6e\\u6c34\\u8fde\\u6d77\\u5e73\\uff0c\\u6d77\\u4e0a\\u660e\\u6708\\u5171\\u6f6e\\u751f\\nspeech = text2speech(text)[wav]\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\", \"performance\": {\"dataset\": \"csmsc\", \"accuracy\": \"Not specified\"}, \"description\": \"A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\"}}", "category": "generic"}
{"question_id": 902, "text": " We need a system that can convert text to speech in Taiwanese Hokkien accent for our language learning app.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"Fairseq\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\", \"api_call\": \"tts_model.get_prediction(tts_sample)\", \"api_arguments\": {\"unit\": \"Text input for the TTS model\"}, \"python_environment_requirements\": [\"fairseq\", \"huggingface_hub\", \"torchaudio\"], \"example_code\": \"import json\\nimport os\\nfrom pathlib import Path\\nimport IPython.display as ipd\\nfrom fairseq import hub_utils\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\nfrom huggingface_hub import snapshot_download\\nimport torchaudio\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\nlibrary_name = fairseq\\ncache_dir = (\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\n)\\ncache_dir = snapshot_download(\\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\\n)\\nx = hub_utils.from_pretrained(\\n cache_dir,\\n model.pt,\\n .,\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\n config_yaml=config.json,\\n fp16=False,\\n is_vocoder=True,\\n)\\nwith open(f{x['args']['data']}/config.json) as f:\\n vocoder_cfg = json.load(f)\\nassert (\\n len(x[args][model_path]) == 1\\n), Too many vocoder models in the input\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\ntts_sample = tts_model.get_model_input(unit)\\nwav, sr = tts_model.get_prediction(tts_sample)\\nipd.Audio(wav, rate=sr)\", \"performance\": {\"dataset\": \"TAT-TTS\", \"accuracy\": \"Not provided\"}, \"description\": \"Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.\"}}", "category": "generic"}
{"question_id": 903, "text": " I'm the CEO of an international e-commerce company. For voice assistants, there is a need to translate product names and descriptions from English to Japanese.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"ESPnet\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.\"}}", "category": "generic"}
{"question_id": 904, "text": " A customer wants to create an audio file from a German text sentence for an advertisement.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Text-to-Speech\", \"framework\": \"speechbrain\", \"functionality\": \"Text-to-Speech\", \"api_name\": \"padmalcom/tts-tacotron2-german\", \"api_call\": \"tacotron2.encode_text(text)\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"pip install speechbrain\"], \"example_code\": [\"import torchaudio\", \"from speechbrain.pretrained import Tacotron2\", \"from speechbrain.pretrained import HIFIGAN\", \"tacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\", \"hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir_vocoder)\", \"mel_output, mel_length, alignment = tacotron2.encode_text(Die Sonne schien den ganzen Tag.)\", \"waveforms = hifi_gan.decode_batch(mel_output)\", \"torchaudio.save('example_TTS.wav',waveforms.squeeze(1), 22050)\"], \"performance\": {\"dataset\": \"custom german dataset\", \"accuracy\": \"Not provided\"}, \"description\": \"Text-to-Speech (TTS) with Tacotron2 trained on a custom german dataset with 12 days voice using speechbrain. Trained for 39 epochs (english speechbrain models are trained for 750 epochs) so there is room for improvement and the model is most likely to be updated soon. The hifigan vocoder can fortunately be used language-independently.\"}}", "category": "generic"}
{"question_id": 905, "text": " We are building a platform where users can join a conference call. A feature we need is the ability to detect when multiple people talk over each other.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"pyannote.audio\", \"functionality\": \"overlapped-speech-detection\", \"api_name\": \"pyannote/overlapped-speech-detection\", \"api_call\": \"pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\", \"api_arguments\": [\"audio.wav\"], \"python_environment_requirements\": [\"pyannote.audio 2.1\"], \"example_code\": \"from pyannote.audio import Pipeline\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\noutput = pipeline(audio.wav)\\nfor speech in output.get_timeline().support():\\n  # two or more speakers are active between speech.start and speech.end\\n  ...\", \"performance\": {\"dataset\": \"ami\", \"accuracy\": null}, \"description\": \"Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\"}}", "category": "generic"}
{"question_id": 906, "text": " Our latest AI-based software aims to convert large audiobooks to text. Write a code to transcribe an audiobook.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Transformers\", \"functionality\": \"Transcription\", \"api_name\": \"facebook/wav2vec2-base-960h\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\", \"api_arguments\": [\"input_values\"], \"python_environment_requirements\": [\"transformers\", \"datasets\", \"torch\", \"jiwer\"], \"example_code\": \"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\nfrom datasets import load_dataset\\nimport torch\\nprocessor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\\nmodel = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\\nds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation')\\ninput_values = processor(ds[0]['audio']['array'], return_tensors='pt', padding='longest').input_values\\nlogits = model(input_values).logits\\npredicted_ids = torch.argmax(logits, dim=-1)\\ntranscription = processor.batch_decode(predicted_ids)\", \"performance\": {\"dataset\": \"LibriSpeech\", \"accuracy\": {\"clean\": 3.4, \"other\": 8.6}}, \"description\": \"Facebook's Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.\"}}", "category": "generic"}
{"question_id": 907, "text": " Our team is creating an application for automated Marathi language transcription of recorded audio files. Develop the appropriate code snippet for this task.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Automatic Speech Recognition\", \"api_name\": \"ravirajoshi/wav2vec2-large-xls-r-300m-marathi\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('ravirajoshi/wav2vec2-large-xls-r-300m-marathi')\", \"api_arguments\": \"model_name_or_path, input_values\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"None\", \"accuracy\": {\"Loss\": 0.5656, \"Wer\": 0.2156}}, \"description\": \"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset. It is designed for Automatic Speech Recognition in Marathi language.\"}}", "category": "generic"}
{"question_id": 908, "text": " I am building an application for foreign language learners. One of its features is to generate transcriptions of the words the users are practicing in Japanese language.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Recognition\", \"api_name\": \"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\", \"api_call\": \"SpeechRecognitionModel('jonatasgrosman/wav2vec2-large-xlsr-53-japanese')\", \"api_arguments\": [\"audio_paths\"], \"python_environment_requirements\": [\"huggingsound\", \"torch\", \"librosa\", \"datasets\", \"transformers\"], \"example_code\": \"from huggingsound import SpeechRecognitionModel\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\ntranscriptions = model.transcribe(audio_paths)\", \"performance\": {\"dataset\": \"common_voice\", \"accuracy\": {\"WER\": 81.8, \"CER\": 20.16}}, \"description\": \"Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.\"}}", "category": "generic"}
{"question_id": 909, "text": " Tell me the steps to transcribe an audio file in English and provide the text result using the whisper-medium model.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transcription and Translation\", \"api_name\": \"openai/whisper-medium\", \"api_call\": \"WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')\", \"api_arguments\": [\"sample\", \"sampling_rate\", \"language\", \"task\", \"skip_special_tokens\"], \"python_environment_requirements\": [\"transformers\", \"datasets\"], \"example_code\": \"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\nfrom datasets import load_dataset\\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\\nmodel.config.forced_decoder_ids = None\\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\nsample = ds[0][audio]\\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\npredicted_ids = model.generate(input_features)\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\", \"performance\": {\"dataset\": [{\"name\": \"LibriSpeech (clean)\", \"accuracy\": 2.9}, {\"name\": \"LibriSpeech (other)\", \"accuracy\": 5.9}, {\"name\": \"Common Voice 11.0\", \"accuracy\": 53.87}]}, \"description\": \"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.\"}}", "category": "generic"}
{"question_id": 910, "text": " We have a platform that offers tutoring services. We need to transcribe a tutor's speech to provide subtitles for deaf students.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Automatic Speech Recognition and Speech Translation\", \"api_name\": \"openai/whisper-large\", \"api_call\": \"WhisperForConditionalGeneration.from_pretrained('openai/whisper-large')\", \"api_arguments\": [\"audio\", \"sampling_rate\"], \"python_environment_requirements\": [\"transformers\", \"datasets\"], \"example_code\": \"from transformers import WhisperProcessor, WhisperForConditionalGeneration\\nfrom datasets import load_dataset\\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-large)\\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-large)\\nmodel.config.forced_decoder_ids = None\\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\nsample = ds[0][audio]\\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\npredicted_ids = model.generate(input_features)\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\", \"performance\": {\"dataset\": [{\"name\": \"LibriSpeech (clean)\", \"accuracy\": 3.0}, {\"name\": \"LibriSpeech (other)\", \"accuracy\": 5.4}, {\"name\": \"Common Voice 11.0\", \"accuracy\": 54.8}]}, \"description\": \"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.\"}}", "category": "generic"}
{"question_id": 911, "text": " Determine the different audio sources in a noisy environment and isolate those sources from the original audio.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Asteroid\", \"api_name\": \"mpariente/DPRNNTasNet-ks2_WHAM_sepclean\", \"api_call\": \"pipeline('audio-source-separation', model='mpariente/DPRNNTasNet-ks2_WHAM_sepclean')\", \"api_arguments\": \"audio_file\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"WHAM!\", \"si_sdr\": 19.316743490695334, \"si_sdr_imp\": 19.317895273889842, \"sdr\": 19.68085347190952, \"sdr_imp\": 19.5298092932871, \"sir\": 30.362213998701232, \"sir_imp\": 30.21116982007881, \"sar\": 20.15553251343315, \"sar_imp\": -129.02091762351188, \"stoi\": 0.97772664309074, \"stoi_imp\": 0.23968091518217424}, \"description\": \"This model was trained by Manuel Pariente using the wham/DPRNN recipe in Asteroid. It was trained on the sep_clean task of the WHAM! dataset.\"}}", "category": "generic"}
{"question_id": 912, "text": " A conference call with multiple speakers has just ended, and we need to separate the voices.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Asteroid\", \"api_name\": \"JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k\", \"api_call\": \"JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k\", \"api_arguments\": \"audio\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"Libri2Mix\", \"accuracy\": {\"si_sdr\": 10.617130949793383, \"si_sdr_imp\": 12.551811412989263, \"sdr\": 11.231867464482065, \"sdr_imp\": 13.059765009747343, \"sir\": 24.461138352988346, \"sir_imp\": 24.371856452307703, \"sar\": 11.5649982725426, \"sar_imp\": 4.662525705768228, \"stoi\": 0.8701085138712695, \"stoi_imp\": 0.2245418019822898}}, \"description\": \"This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_noisy task of the Libri2Mix dataset.\"}}", "category": "generic"}
{"question_id": 913, "text": " Help me build a speech-to-speech translation tool for translating English to Hokkien language, using an audio file.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio-to-Audio\", \"framework\": \"Fairseq\", \"functionality\": \"speech-to-speech-translation\", \"api_name\": \"xm_transformer_unity_en-hk\", \"api_call\": \"'load_model_ensemble_and_task_from_hf_hub(facebook/xm_transformer_unity_en-hk)'\", \"api_arguments\": {\"config_yaml\": \"config.yaml\", \"task\": \"speech_to_text\", \"cache_dir\": \"cache_dir\"}, \"python_environment_requirements\": [\"fairseq\", \"hub_utils\", \"torchaudio\", \"IPython.display\", \"huggingface_hub\"], \"example_code\": [\"import json\", \"import os\", \"from pathlib import Path\", \"import IPython.display as ipd\", \"from fairseq import hub_utils\", \"from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\", \"from fairseq.models.speech_to_text.hub_interface import S2THubInterface\", \"from fairseq.models.text_to_speech import CodeHiFiGANVocoder\", \"from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\", \"from huggingface_hub import snapshot_download\", \"import torchaudio\", \"cache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\", \"models, cfg, task = load_model_ensemble_and_task_from_hf_hub(\", \" facebook/xm_transformer_unity_en-hk,\", \" arg_overrides={config_yaml: config.yaml, task: speech_to_text},\", \" cache_dir=cache_dir,\", \")\", \"model = models[0].cpu()\", \"cfg[task].cpu = True\", \"generator = task.build_generator([model], cfg)\", \"audio, _ = torchaudio.load(/path/to/an/audio/file)\", \"sample = S2THubInterface.get_model_input(task, audio)\", \"unit = S2THubInterface.get_prediction(task, model, generator, sample)\", \"library_name = fairseq\", \"cache_dir = (\", \" cache_dir or (Path.home() / .cache / library_name).as_posix()\", \")\", \"cache_dir = snapshot_download(\", \" ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\", \")\", \"x = hub_utils.from_pretrained(\", \" cache_dir,\", \" model.pt,\", \" .,\", \" archive_map=CodeHiFiGANVocoder.hub_models(),\", \" config_yaml=config.json,\", \" fp16=False,\", \" is_vocoder=True,\", \")\", \"with open(f{x['args']['data']}/config.json) as f:\", \" vocoder_cfg = json.load(f)\", \"assert (\", \" len(x[args][model_path]) == 1\", \"), Too many vocoder models in the input\", \"vocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\", \"tts_model = VocoderHubInterface(vocoder_cfg, vocoder)\", \"tts_sample = tts_model.get_model_input(unit)\", \"wav, sr = tts_model.get_prediction(tts_sample)\", \"ipd.Audio(wav, rate=sr)\"], \"performance\": {\"dataset\": \"MuST-C\", \"accuracy\": null}, \"description\": \"Speech-to-speech translation model with two-pass decoder (UnitY) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.\"}}", "category": "generic"}
{"question_id": 914, "text": " Supervise a remote control system to react optimally to the emotions expressed by people.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Emotion Recognition\", \"api_name\": \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\", \"api_call\": \"Wav2Vec2ForCTC.from_pretrained('ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition')\", \"api_arguments\": \"wav2vec2, tokenizer\", \"python_environment_requirements\": \"transformers 4.8.2, pytorch 1.9.0+cu102, datasets 1.9.0, tokenizers 0.10.3\", \"example_code\": \"from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\", \"performance\": {\"dataset\": \"RAVDESS\", \"accuracy\": 0.8223}, \"description\": \"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised'].\"}}", "category": "generic"}
{"question_id": 915, "text": " An acting agency needs to analyze voice samples from aspirant actors to detect their predominant emotions in order to select the best-suited actors for various roles.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Audio Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Speech Emotion Recognition\", \"api_name\": \"harshit345/xlsr-wav2vec-speech-emotion-recognition\", \"api_call\": \"Wav2Vec2ForSpeechClassification.from_pretrained('harshit345/xlsr-wav2vec-speech-emotion-recognition')\", \"api_arguments\": {\"model_name_or_path\": \"harshit345/xlsr-wav2vec-speech-emotion-recognition\"}, \"python_environment_requirements\": [\"pip install git+https://github.com/huggingface/datasets.git\", \"pip install git+https://github.com/huggingface/transformers.git\", \"pip install torchaudio\", \"pip install librosa\"], \"example_code\": \"path = '/data/jtes_v1.1/wav/f01/ang/f01_ang_01.wav'\\noutputs = predict(path, sampling_rate)\", \"performance\": {\"dataset\": \"JTES v1.1\", \"accuracy\": {\"anger\": 0.82, \"disgust\": 0.85, \"fear\": 0.78, \"happiness\": 0.84, \"sadness\": 0.86, \"overall\": 0.806}}, \"description\": \"This model is trained on the JTES v1.1 dataset for speech emotion recognition. It uses the Wav2Vec2 architecture for audio classification and can recognize emotions like anger, disgust, fear, happiness, and sadness.\"}}", "category": "generic"}
{"question_id": 916, "text": " We are a real estate company, we have all historical data of our deals, and we would like to predict the price of a specific house using a machine learning model.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Single Column Regression\", \"api_name\": \"1771761511\", \"api_call\": \"model.predict(data)\", \"api_arguments\": {\"data\": \"pandas.DataFrame\"}, \"python_environment_requirements\": {\"joblib\": \"latest\", \"pandas\": \"latest\"}, \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"jwan2021/autotrain-data-us-housing-prices\", \"accuracy\": {\"Loss\": 134406.507, \"R2\": 0.861, \"MSE\": 18065109105.27, \"MAE\": 103271.843, \"RMSLE\": 0.139}}, \"description\": \"A model trained using AutoTrain for predicting US housing prices. The model is trained on the jwan2021/autotrain-data-us-housing-prices dataset and is a single column regression model with an ID of 1771761511.\"}}", "category": "generic"}
{"question_id": 917, "text": " A real estate company would like to display estimated home prices on their website based on a set of basic features. Help them integrate the prediction model.\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Single Column Regression\", \"api_name\": \"1771761510\", \"api_call\": \"Output: model.predict(data)\", \"api_arguments\": {\"data\": \"data.csv\"}, \"python_environment_requirements\": {\"joblib\": \"import joblib\", \"pandas\": \"import pandas as pd\", \"json\": \"import json\"}, \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv('data.csv')\\ndata = data[features]\\ndata.columns = ['feat_' + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"jwan2021/autotrain-data-us-housing-prices\", \"accuracy\": {\"Loss\": 102613.797, \"R2\": 0.919, \"MSE\": 10529591296.0, \"MAE\": 82375.211, \"RMSLE\": 0.1}}, \"description\": \"A tabular regression model trained using AutoTrain to predict US housing prices. The model is based on the dataset jwan2021/autotrain-data-us-housing-prices and has an R2 score of 0.919.\"}}", "category": "generic"}
{"question_id": 918, "text": " I want to predict the CO2 emissions for a list of cars given their features.\\n###Input: data.csv - a CSV file containing car features such as engine size, fuel type, and weight\\n \n Use this API documentation for reference:  {\"domain\": \"Tabular Tabular Regression\", \"framework\": \"Joblib\", \"functionality\": \"Carbon Emissions\", \"api_name\": \"40376105012\", \"api_call\": \"joblib.load('model.joblib')\", \"api_arguments\": [\"data.csv\"], \"python_environment_requirements\": [\"joblib\", \"pandas\"], \"example_code\": \"import json\\nimport joblib\\nimport pandas as pd\\nmodel = joblib.load('model.joblib')\\nconfig = json.load(open('config.json'))\\nfeatures = config['features']\\ndata = pd.read_csv(data.csv)\\ndata = data[features]\\ndata.columns = [feat_ + str(col) for col in data.columns]\\npredictions = model.predict(data)\", \"performance\": {\"dataset\": \"bibekbehera/autotrain-data-numeric_prediction\", \"accuracy\": {\"Loss\": 0.211, \"R2\": 0.339, \"MSE\": 0.045, \"MAE\": 0.104, \"RMSLE\": 0.145}}, \"description\": \"A model trained using AutoTrain for single column regression. The model predicts CO2 emissions in grams.\"}}", "category": "generic"}
{"question_id": 919, "text": " A startup is building new arcade games that incorporate artificial intelligence. We want to implement a PPO agent to play seals/CartPole-v0.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Stable-Baselines3\", \"functionality\": \"deep-reinforcement-learning\", \"api_name\": \"ppo-seals-CartPole-v0\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"algo\", \"env\", \"f\"], \"python_environment_requirements\": [\"rl_zoo3\", \"stable-baselines3\", \"stable-baselines3-contrib\"], \"example_code\": \"python -m rl_zoo3.load_from_hub --algo ppo --env seals/CartPole-v0 -orga HumanCompatibleAI -f logs/\", \"performance\": {\"dataset\": \"seals/CartPole-v0\", \"accuracy\": \"500.00 +/- 0.00\"}, \"description\": \"This is a trained model of a PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\"}}", "category": "generic"}
{"question_id": 920, "text": " A game developer desires to make a game user friendly using decision transformer in a hopping environment to design a responsive AI.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"decision-transformer-gym-hopper-medium\", \"api_call\": \"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-medium')\", \"api_arguments\": {\"mean\": [1.311279, -0.08469521, -0.5382719, -0.07201576, 0.04932366, 2.1066856, -0.15017354, 0.00878345, -0.2848186, -0.18540096, -0.28461286], \"std\": [0.17790751, 0.05444621, 0.21297139, 0.14530419, 0.6124444, 0.85174465, 1.4515252, 0.6751696, 1.536239, 1.6160746, 5.6072536]}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": \"See our Blog Post, Colab notebook or Example Script for usage.\", \"performance\": {\"dataset\": \"Gym Hopper environment\", \"accuracy\": \"Not provided\"}, \"description\": \"Decision Transformer model trained on medium trajectories sampled from the Gym Hopper environment.\"}}", "category": "generic"}
{"question_id": 921, "text": " Can you help our engineers to train and run a SoccerTwos simulation and use a trained model of a poca agent?\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"ML-Agents\", \"functionality\": \"Train and play SoccerTwos\", \"api_name\": \"ahmad-alismail/poca-SoccerTwos\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"your_configuration_file_path.yaml\", \"run_id\"], \"python_environment_requirements\": [\"ml-agents\"], \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"SoccerTwos\", \"accuracy\": \"Not specified\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 922, "text": " We are a technology solutions provider, and we need to help our client develop a reinforcement learning model for a Hopper robot.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"edbeeching/decision-transformer-gym-hopper-expert\", \"api_call\": \"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-hopper-expert')\", \"api_arguments\": {\"mean\": [1.3490015, -0.11208222, -0.5506444, -0.13188992, -0.00378754, 2.6071432, 0.02322114, -0.01626922, -0.06840388, -0.05183131, 0.04272673], \"std\": [0.15980862, 0.0446214, 0.14307782, 0.17629202, 0.5912333, 0.5899924, 1.5405099, 0.8152689, 2.0173461, 2.4107876, 5.8440027]}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"See our Blog Post, Colab notebook or Example Script for usage.\", \"performance\": {\"dataset\": \"Gym Hopper environment\", \"accuracy\": \"Not provided\"}, \"description\": \"Decision Transformer model trained on expert trajectories sampled from the Gym Hopper environment\"}}", "category": "generic"}
{"question_id": 923, "text": " Create a model for controlling a robotic cheetah capable of navigating through challenging terrain.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"edbeeching/decision-transformer-gym-halfcheetah-expert\", \"api_call\": \"AutoModel.from_pretrained('edbeeching/decision-transformer-gym-halfcheetah-expert')\", \"api_arguments\": {\"mean\": [-0.04489148, 0.03232588, 0.06034835, -0.17081226, -0.19480659, -0.05751596, 0.09701628, 0.03239211, 11.047426, -0.07997331, -0.32363534, 0.36297753, 0.42322603, 0.40836546, 1.1085187, -0.4874403, -0.0737481], \"std\": [0.04002118, 0.4107858, 0.54217845, 0.41522816, 0.23796624, 0.62036866, 0.30100912, 0.21737163, 2.2105937, 0.572586, 1.7255033, 11.844218, 12.06324, 7.0495934, 13.499867, 7.195647, 5.0264325]}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"See our Blog Post, Colab notebook or Example Script for usage.\", \"performance\": {\"dataset\": \"Gym HalfCheetah environment\", \"accuracy\": \"Not specified\"}, \"description\": \"Decision Transformer model trained on expert trajectories sampled from the Gym HalfCheetah environment\"}}", "category": "generic"}
{"question_id": 924, "text": " We are developing a soccer game AI using reinforcement learning. How do we run the training with the provided configuration file and run ID?\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning\", \"framework\": \"Unity ML-Agents\", \"functionality\": \"Reinforcement Learning\", \"api_name\": \"poca-Soccerv7.2\", \"api_call\": \"No changes needed. The `api_call` field does not include any model instantiation.\", \"api_arguments\": {\"configuration_file_path\": \"<your_configuration_file_path.yaml>\", \"run_id\": \"<run_id>\"}, \"python_environment_requirements\": {\"ML-Agents\": \"https://github.com/huggingface/ml-agents#get-started\"}, \"example_code\": \"mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\", \"performance\": {\"dataset\": \"ML-Agents-SoccerTwos\", \"accuracy\": \"Not provided\"}, \"description\": \"A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\"}}", "category": "generic"}
{"question_id": 925, "text": " I have a robotic arm in my warehouse, and I want it to perform efficient 6D grasping on objects.\\n \n Use this API documentation for reference:  {\"domain\": \"Reinforcement Learning Robotics\", \"framework\": \"Hugging Face\", \"functionality\": \"6D grasping\", \"api_name\": \"camusean/grasp_diffusion\", \"api_call\": \"AutoModel.from_pretrained('camusean/grasp_diffusion')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.\"}}", "category": "generic"}
