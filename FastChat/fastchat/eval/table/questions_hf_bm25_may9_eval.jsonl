{"question_id": 1, "text": " We are a medical research institute and want to extract features from biomedical texts to improve literature-based predictions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 2, "text": " I'm organizing an online conference with participants from different countries who speak different languages. I want to find a tool to extract information from their forum discussions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Masked Language Modeling\\', \\'api_name\\': \\'distilbert-base-multilingual-cased\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'distilbert-base-multilingual-cased\\')\", \\'api_arguments\\': [\\'pipeline\\', \\'fill-mask\\', \\'model\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nunmasker = pipeline(\\'fill-mask\\', model=\\'distilbert-base-multilingual-cased\\')\\\\nunmasker(Hello I\\'m a [MASK] model.)\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'XNLI\\', \\'accuracy\\': {\\'English\\': 78.2, \\'Spanish\\': 69.1, \\'Chinese\\': 64.0, \\'German\\': 66.3, \\'Arabic\\': 59.1, \\'Urdu\\': 54.7}}]}, \\'description\\': \\'This model is a distilled version of the BERT base multilingual model. It is trained on the concatenation of Wikipedia in 104 different languages. The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters. On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.\\'}', metadata={})]", "category": "generic"}
{"question_id": 3, "text": " Our company plans to build a video-sharing platform that automatically generates video descriptions. We need a model that extracts video features and understands human language.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': 'DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)', 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = Spiderman is surfing\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.'}\", metadata={})]", "category": "generic"}
{"question_id": 4, "text": " We are working on a project that can generate images from textual description for an online art gallery. We need to create diverse images from a given text prompt.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 5, "text": " I want to create a document digitization tool that converts printed documents into text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 6, "text": " We would like to present images of our product to create a caption for each image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 7, "text": " I need to build an interactive robot that can answer questions using an image input, process questions in natural language, and generate a response.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'vilt-finetuned-vqasi\\', \\'api_call\\': \"ViltModel.from_pretrained(\\'tufa15nik/vilt-finetuned-vqasi\\')\", \\'api_arguments\\': {\\'model\\': \\'tufa15nik/vilt-finetuned-vqasi\\', \\'tokenizer\\': \\'tufa15nik/vilt-finetuned-vqasi\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.11.3\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.\\'}', metadata={})]", "category": "generic"}
{"question_id": 8, "text": " I need assistance to describe an image of a stop sign in Australia.\\n###Input: The image URL is https://www.ilankelman.org/stopsigns/australia.jpg\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 9, "text": " We have an old dataset of charts from previous reports, and we want to generate tables from these charts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 10, "text": " I am a clothing designer and I need to analyze an image of a new fabric pattern with a model. Can you suggest how to do that to get the description of the fabric pattern?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 11, "text": " We want to create an application that allows users to upload photos of tourist attractions and provides them with information about the images. Implement a system that can answer questions about these images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 12, "text": " We are creating a virtual tour guide. The guide will answer the users' questions about the places they visit based on the images of those places.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'PegasusForConditionalGeneration.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 13, "text": " A bank provides us with account statements of their clients in PDF format. We need to extract information such as transaction date, transaction amount, and balance after each transaction.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 14, "text": " I need a personal assistant to help me analyze the financial reports and answer the question: What is the total revenue for the company?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 15, "text": " Develop an AI model for a local art gallery accepting scanned artwork, to help the staff easily find information about any piece of art by asking questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'saltacc/anime-ai-detect\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'saltacc/anime-ai-detect\\')\", \\'api_arguments\\': [\\'image\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'aibooru and imageboard sites\\', \\'accuracy\\': \\'96%\\'}, \\'description\\': \\'A BEiT classifier to see if anime art was made by an AI or a human.\\'}', metadata={})]", "category": "generic"}
{"question_id": 16, "text": " Working in a government agency, I need to extract specific information from a variety of unstructured forms and documents. Could you help me extract the answer to my question from the given context?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 17, "text": " We are building a virtual makeover app and we need to estimate the depth of various surfaces in an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 18, "text": " The digital marketing company needs to create a virtual reality environment for their product display. They asked us to estimate the depth of input images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 19, "text": " I am working on an indoor robot project that needs depth estimation for the robot's vision. Help sequence the proper code for depth estimation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 20, "text": " We are developing a smart mobility solution for the visually impaired. We want to estimate the depth of surrounding objects in real-time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 21, "text": " Let's find the depth estimation for an image captured in a warehouse.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 22, "text": " I am the owner of an online pet store. I need a solution that categorizes the uploaded photos of pets into cat or dog.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 23, "text": " To create a photo gallery app, we need a way to categorize images according to their content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 24, "text": " I am building an app that detects flower species from uploaded images. What model can I use?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 25, "text": " Recommend me an object detection library that can detect hard hats in construction areas to ensure safety compliance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-hard-hat-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\\\\n\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'hard-hat-detection\\', \\'accuracy\\': 0.811}, \\'description\\': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between \\'Hardhat\\' and \\'NO-Hardhat\\' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\"}', metadata={})]", "category": "generic"}
{"question_id": 26, "text": " We need to implement a zero-shot object detection system to detect cats and dogs in images for a pet adoption application.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-large-patch14\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-large-patch14\\')\", \\'api_arguments\\': {\\'model_name\\': \\'google/owlvit-large-patch14\\'}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': [\\'import requests\\', \\'from PIL import Image\\', \\'import torch\\', \\'from transformers import OwlViTProcessor, OwlViTForObjectDetection\\', \\'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)\\', \\'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)\\', \\'url = http://images.cocodataset.org/val2017/000000039769.jpg\\', \\'image = Image.open(requests.get(url, stream=True).raw)\\', \\'texts = [[a photo of a cat, a photo of a dog]\\', \\'inputs = processor(text=texts, images=image, return_tensors=pt)\\', \\'outputs = model(**inputs)\\', \\'target_sizes = torch.Tensor([image.size[::-1]])\\', \\'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'i = 0\\', \\'text = texts[i]\\', \\'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]\\', \\'score_threshold = 0.1\\', \\'for box, score, label in zip(boxes, scores, labels):\\', \\' box = [round(i, 2) for i in box.tolist()]\\', \\' if score >= score_threshold:\\', \\' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})\\'], \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 27, "text": " Our company needs to detect license plates of vehicles in our parking lot. Please provide object detection solution for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'License Plate Detection\\', \\'api_name\\': \\'keremberke/yolov5m-license-plate\\', \\'api_call\\': \"yolov5.load(\\'keremberke/yolov5m-license-plate\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic\\': False, \\'multi_label\\': False, \\'max_det\\': 1000, \\'img\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'size\\': 640, \\'augment\\': True}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': [\\'import yolov5\\', \"model = yolov5.load(\\'keremberke/yolov5m-license-plate\\')\", \\'model.conf = 0.25\\', \\'model.iou = 0.45\\', \\'model.agnostic = False\\', \\'model.multi_label = False\\', \\'model.max_det = 1000\\', \"img = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model(img, size=640)\\', \\'results = model(img, augment=True)\\', \\'predictions = results.pred[0]\\', \\'boxes = predictions[:, :4]\\', \\'scores = predictions[:, 4]\\', \\'categories = predictions[:, 5]\\', \\'results.show()\\', \"results.save(save_dir=\\'results/\\')\"], \\'performance\\': {\\'dataset\\': \\'keremberke/license-plate-object-detection\\', \\'accuracy\\': 0.988}, \\'description\\': \\'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\\'}', metadata={})]", "category": "generic"}
{"question_id": 28, "text": " We are working on a smart camera for recognizing people in crowded areas, please segment the given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 29, "text": " Our team at a blood test center is analyzing blood cell images. Find a way to detect blood cells in the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Blood Cell Detection\\', \\'api_name\\': \\'keremberke/yolov8n-blood-cell-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'blood-cell-object-detection\\', \\'accuracy\\': 0.893}, \\'description\\': \\'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 30, "text": " We are an agricultural company aiming to detect and classify segments of crops in aerial images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 31, "text": " We want to create an application to help enhance the security of buildings. It needs to detect and segment people who enter the building.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 32, "text": " Help us design a solution to segment objects inside images and classify them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'facebook/convnext-large-224\\', \\'api_call\\': \"ConvNextForImageClassification.from_pretrained(\\'facebook/convnext-large-224\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'facebook/convnext-large-224\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'Hugging Face Transformers\\', \\'torch\\': \\'PyTorch\\', \\'datasets\\': \\'Hugging Face Datasets\\'}, \\'example_code\\': {\\'import\\': [\\'from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\', \\'import torch\\', \\'from datasets import load_dataset\\'], \\'load_dataset\\': \"dataset = load_dataset(\\'huggingface/cats-image\\')\", \\'image\\': \"image = dataset[\\'test\\'][\\'image\\'][0]\", \\'feature_extractor\\': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained(\\'facebook/convnext-large-224\\')\", \\'model\\': \"model = ConvNextForImageClassification.from_pretrained(\\'facebook/convnext-large-224\\')\", \\'inputs\\': \"inputs = feature_extractor(image, return_tensors=\\'pt\\')\", \\'logits\\': \\'with torch.no_grad():\\\\n  logits = model(**inputs).logits\\', \\'predicted_label\\': \\'predicted_label = logits.argmax(-1).item()\\', \\'print\\': \\'print(model.config.id2label[predicted_label])\\'}, \\'performance\\': {\\'dataset\\': \\'imagenet-1k\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and \\'modernized\\' its design by taking the Swin Transformer as inspiration.\"}', metadata={})]", "category": "generic"}
{"question_id": 33, "text": " The company has recently launched an image classification software which recognizes all the objects in an image. How the software can be improved by a more sophisticated instance segmentation model to recognize individual objects and their boundaries within the image?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/mask2former-swin-small-coco-instance\\', \\'api_call\\': \"Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-small-coco-instance\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'facebook/mask2former-swin-small-coco-instance\\'}, \\'python_environment_requirements\\': [\\'requests\\', \\'torch\\', \\'PIL\\', \\'transformers\\'], \\'example_code\\': \"processor = AutoImageProcessor.from_pretrained(\\'facebook/mask2former-swin-small-coco-instance\\')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-small-coco-instance\\')\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result[\\'segmentation\\']\", \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\\'}', metadata={})]", "category": "generic"}
{"question_id": 34, "text": " We have a production line creating computer chips. We need to identify and segment any defects in the printed circuit boards(PCB) of these products.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 35, "text": " The art gallery is exploring innovative ways to display their collections. We are helping them to generate abstract artworks with the help of AI.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 36, "text": " Present a code for generating a colorful butterfly using A.I. technology.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 37, "text": " Develop an AI actor that can recognize scenes from a trailer and identify the specific movie genre.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 38, "text": " To improve the customer experience on our video streaming platform, we need to classify videos into proper categories based on content. Develop a video classification model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 39, "text": " Analyze a video and provide a classification based on the content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'facebook/timesformer-base-finetuned-k400\\', \\'api_call\\': \"TimesformerForVideoClassification.from_pretrained(\\'facebook/timesformer-base-finetuned-k400\\')\", \\'api_arguments\\': \\'video, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 40, "text": " As someone who loves photography, I want my app to be able to automatically identify what the subject is in my uploaded images.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 41, "text": " We are dealing with a project related to self-driving cars. In order to improve the navigation, we need to classify the location of an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 42, "text": " I'm trying to determine if a list of German sentences are positive, negative, or neutral in sentiment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 43, "text": " I need to choose the right answer from a set of options for a given question.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 44, "text": " I need to extract all company names from a list of news articles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Entity Extraction\\', \\'api_name\\': \\'903429548\\', \\'api_call\\': \"Output: AutoModelForTokenClassification.from_pretrained(\\'ismail-lucifer011/autotrain-company_all-903429548\\', use_auth_token=True)\", \\'api_arguments\\': {\\'inputs\\': \\'I love AutoTrain\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoModelForTokenClassification, AutoTokenizer\\'}, \\'example_code\\': \\'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)\\', \\'performance\\': {\\'dataset\\': \\'ismail-lucifer011/autotrain-data-company_all\\', \\'accuracy\\': 0.9979930566588805}, \\'description\\': \\'A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 45, "text": " Our company is working on a customer support tool that automatically extracts relevant information mentioned by users in their messages. Analyze customer messages to extract entities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 46, "text": " We are developing a system to identify the named entities in a given sentence.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 47, "text": " Identify the named entities in the given news article.\\n###Input: On August 21st, Apple Inc. announced their new product, the iPhone 13, will be released on September 14th. CEO Tim Cook led the event and highlighted the new features, including an improved camera and longer battery life. The starting price for the device is $699.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Cross-Encoder for Natural Language Inference\\', \\'api_name\\': \\'cross-encoder/nli-distilroberta-base\\', \\'api_call\\': \"CrossEncoder(\\'cross-encoder/nli-distilroberta-base\\')\", \\'api_arguments\\': \"(\\'A man is eating pizza\\', \\'A man eats something\\')\", \\'python_environment_requirements\\': [\\'sentence_transformers\\', \\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=\\'cross-encoder/nli-distilroberta-base\\')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\", \\'performance\\': {\\'dataset\\': \\'SNLI and MultiNLI\\', \\'accuracy\\': \\'See SBERT.net - Pretrained Cross-Encoder for evaluation results\\'}, \\'description\\': \\'This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 48, "text": " Design bedside nurse report for COVID-19 patients with easy comprehension. We need to provide an overview of a patient's condition to the nurse.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 49, "text": " My company needs to answer questions about sales data on a monthly basis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\')\", \\'api_arguments\\': {\\'model\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'tokenizer\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': \\'vqa(image_path, question)\\', \\'performance\\': {\\'dataset\\': \\'JosephusCheung/GuanacoVQADataset\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 50, "text": " We are building a knowledge base management system with AI assistance, and we want to ask it to explain concepts for us.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'distilgpt2\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'distilgpt2\\')\", \\'api_arguments\\': [\\'model\\'], \\'python_environment_requirements\\': [\\'from transformers import pipeline, set_seed\\'], \\'example_code\\': \\'set_seed(42)\\\\ngenerator(Hello, I\u2019m a language model, max_length=20, num_return_sequences=5)\\', \\'performance\\': {\\'dataset\\': \\'WikiText-103\\', \\'accuracy\\': \\'21.100\\'}, \\'description\\': \\'DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 51, "text": " Collect information about global warming and answer this question: \\\"What are the main causes of global warming?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 52, "text": " Our website contains news articles in French, and we need to categorize them into sports, politics, and science.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for\\\\nsentences = [\\'\u041f\u0440\u0438\u0432\u0435\u0442! \u041a\u0430\u043a \u0442\u0432\u043e\u0438 \u0434\u0435\u043b\u0430?\\',\\\\n             \\'\u0410 \u043f\u0440\u0430\u0432\u0434\u0430, \u0447\u0442\u043e 42 \u0442\u0432\u043e\u0435 \u043b\u044e\u0431\u0438\u043c\u043e\u0435 \u0447\u0438\u0441\u043b\u043e?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 53, "text": " We are a news outlet, and our editors need to categorize the incoming articles into politics, sports, and entertainment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 54, "text": " We would like to identify if the following hypothesis \\\"A woman is walking a dog\\\" represents a contradiction, entailment, or neutral relationship with the given premise \\\"A woman is strolling with her pet.\\\"\\n###Input: premise = \\\"A woman is strolling with her pet.\\\", hypothesis = \\\"A woman is walking a dog.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Captioning\\', \\'api_name\\': \\'nlpconnect/vit-gpt2-image-captioning\\', \\'api_call\\': \\'VisionEncoderDecoderModel.from_pretrained(\\\\\\\\nlpconnect/vit-gpt2-image-captioning\\\\\\\\)\\', \\'api_arguments\\': {\\'model\\': \\'nlpconnect/vit-gpt2-image-captioning\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'PIL\\'], \\'example_code\\': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\\\nimport torch\\\\nfrom PIL import Image\\\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\nmodel.to(device)\\\\nmax_length = 16\\\\nnum_beams = 4\\\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\\\ndef predict_step(image_paths):\\\\n images = []\\\\n for image_path in image_paths:\\\\n i_image = Image.open(image_path)\\\\n if i_image.mode != RGB:\\\\n i_image = i_image.convert(mode=RGB)\\\\nimages.append(i_image)\\\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\\\n pixel_values = pixel_values.to(device)\\\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\\\n preds = [pred.strip() for pred in preds]\\\\n return preds\\\\npredict_step([\\'doctor.e16ba4e4.jpg\\']) # [\\'a woman in a hospital bed with a woman in a hospital bed\\']\", \\'performance\\': {\\'dataset\\': \\'Not provided\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.\\'}', metadata={})]", "category": "generic"}
{"question_id": 55, "text": " My software needs to improve in conversational understanding. I want it to predict the logical relationship of two short texts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Natural Language Inference\\', \\'api_name\\': \\'cointegrated/rubert-base-cased-nli-threeway\\', \\'api_call\\': \\'AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\', \\'api_arguments\\': [\\'text1\\', \\'text2\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'sentencepiece\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nmodel_checkpoint = \\'cointegrated/rubert-base-cased-nli-threeway\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n model.cuda()\\\\ntext1 = \\'\u0421\u043e\u043a\u0440\u0430\u0442 - \u0447\u0435\u043b\u043e\u0432\u0435\u043a, \u0430 \u0432\u0441\u0435 \u043b\u044e\u0434\u0438 \u0441\u043c\u0435\u0440\u0442\u043d\u044b.\\'\\\\ntext2 = \\'\u0421\u043e\u043a\u0440\u0430\u0442 \u043d\u0438\u043a\u043e\u0433\u0434\u0430 \u043d\u0435 \u0443\u043c\u0440\u0451\u0442.\\'\\\\nwith torch.inference_mode():\\\\n out = model(**tokenizer(text1, text2, return_tensors=\\'pt\\').to(model.device))\\\\n proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\\\\nprint({v: proba[k] for k, v in model.config.id2label.items()})\", \\'performance\\': {\\'dataset\\': [\\'JOCI\\', \\'MNLI\\', \\'MPE\\', \\'SICK\\', \\'SNLI\\', \\'ANLI\\', \\'NLI-style FEVER\\', \\'IMPPRES\\'], \\'accuracy\\': {\\'ROC AUC\\': {\\'entailment\\': 0.91, \\'contradiction\\': 0.71, \\'neutral\\': 0.79}}}, \\'description\\': \\'This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 56, "text": " We are a multinational company, and our website received a review in Russian. We want to understand the content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 57, "text": " We are working on a research paper and one of the paragraphs needs translation from Italian to English. Can you help me to do it?\\n###Input: During my trip to Italy, I found out Italians are incredibly passionate about food. \\\"essenziale per la salute e il benessere, coscome il piacere che si ottiene da un pasto delizioso.\\\" The Italian cuisine is famous for its simplicity, with most dishes comprising only a few high-quality ingredients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 58, "text": " Create an email draft for a German colleague, but I need the mail to be available in Spanish since they requested it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 59, "text": " Prepare a software that extracts the main points of a German text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 60, "text": " I have a long Chinese article about sports news. I want to get a summary of the article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 61, "text": " Generate a story for kids about space traveling with the Big Red Button that could change everything.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 62, "text": " I want to start a new novel about my life. Can you please help me generate the first few lines?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 63, "text": " We need a powerful model to generate statistical analysis from a paragraph.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 64, "text": " Let the program suggest the next word in the sentence \\\"The AI model can complete any unfinished task by [MASK].\\\"\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'model.generate(input_ids, max_length=128)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}\", metadata={})]", "category": "generic"}
{"question_id": 65, "text": " Write a code that helps me autocompleting an input text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 66, "text": " We want to search for relevant content in customer feedback to improve our product. Identify the most similar feedback to a given query.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 67, "text": " Our company is creating a chatbot. Can you go through user messages and find the ones that are most similar to each other?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 68, "text": " I want to build an AI to answer if Sentences A and B are similar or not in nature.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 69, "text": " I am an employee in a big corporation, I need to find similar sentences in a document with many sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 70, "text": " Design a notification system that reads out upcoming events from a user's calendar using synthesized speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speaker Verification\\', \\'api_name\\': \\'speechbrain/spkrec-xvect-voxceleb\\', \\'api_call\\': \\'EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\\', \\'api_arguments\\': [\\'source\\', \\'savedir\\'], \\'python_environment_requirements\\': [\\'pip install speechbrain\\'], \\'example_code\\': \"import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nclassifier = EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\\\\nsignal, fs =torchaudio.load(\\'tests/samples/ASR/spk1_snt1.wav\\')\\\\nembeddings = classifier.encode_batch(signal)\", \\'performance\\': {\\'dataset\\': \\'Voxceleb1-test set (Cleaned)\\', \\'accuracy\\': \\'EER(%) 3.2\\'}, \\'description\\': \\'This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 71, "text": " My startup works within India, primarily in the Marathi language. Help me convert text to speech suited for a male voice in Marathi.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'EleutherAI/gpt-neo-2.7B\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'EleutherAI/gpt-neo-2.7B\\')\", \\'api_arguments\\': [\\'text\\', \\'do_sample\\', \\'min_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\n generator = pipeline(\\'text-generation\\', model=\\'EleutherAI/gpt-neo-2.7B\\')\\\\n generator(EleutherAI has, do_sample=True, min_length=50)\", \\'performance\\': {\\'dataset\\': \\'the_pile\\', \\'accuracy\\': {\\'Lambada_Acc\\': \\'62.22%\\', \\'Winogrande\\': \\'56.50%\\', \\'Hellaswag\\': \\'42.73%\\'}}, \\'description\\': \"GPT-Neo 2.7B is a transformer model designed using EleutherAI\\'s replication of the GPT-3 architecture. It was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. This model is best suited for generating texts from a prompt and can be used directly with a pipeline for text generation.\"}', metadata={})]", "category": "generic"}
{"question_id": 72, "text": " In our mobile application, we need a function to read the users messages in Telugu Male voice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Telugu_Male_TTS\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'SYSPIN/Telugu_Male_TTS\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.\\'}', metadata={})]", "category": "generic"}
{"question_id": 73, "text": " Create a system that translates the phrase \\\"Welcome to the team. We are excited to have you\\\" into speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 74, "text": " Recently, a hearing-impaired colleague requested an app to convert audio meetings to written text. Develop a solution that transcribes audio files.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 75, "text": " Develop a tool that can identify and differentiate speakers in a conversation for our transcription service.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 76, "text": " We are building an app for transcribing audio calls between multiple users. It's essential to divide overlapping speech into separate audio recordings for each user.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 77, "text": " We want to transcribe speech from a podcast into text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for\\\\nsentences = [\\'\u041f\u0440\u0438\u0432\u0435\u0442! \u041a\u0430\u043a \u0442\u0432\u043e\u0438 \u0434\u0435\u043b\u0430?\\',\\\\n             \\'\u0410 \u043f\u0440\u0430\u0432\u0434\u0430, \u0447\u0442\u043e 42 \u0442\u0432\u043e\u0435 \u043b\u044e\u0431\u0438\u043c\u043e\u0435 \u0447\u0438\u0441\u043b\u043e?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 78, "text": " Play this Chinese video and provide transcriptions.\\n###Input: /path/to/chinese_video.mp4\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'facebook/timesformer-base-finetuned-k400\\', \\'api_call\\': \"TimesformerForVideoClassification.from_pretrained(\\'facebook/timesformer-base-finetuned-k400\\')\", \\'api_arguments\\': \\'video, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 79, "text": " I have an audio file containing the sound of a machine and a person talking in the background. I want to separate the two sounds for better analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 80, "text": " Our customer is English-speaking and needs to communicate with Hokkien-speaking locals. We need to provide speech-to-speech translation from English to Hokkien.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'facebook/textless_sm_ro_en\\', \\'api_call\\': \"pipeline(\\'audio-to-audio\\', model=\\'facebook/textless_sm_ro_en\\')\", \\'api_arguments\\': \\'audio file or recording\\', \\'python_environment_requirements\\': \\'fairseq, huggingface_hub\\', \\'example_code\\': \\'https://huggingface.co/facebook/textless_sm_cs_en\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A speech-to-speech translation model for Romanian to English developed by Facebook AI\\'}', metadata={})]", "category": "generic"}
{"question_id": 81, "text": " We have an app that records sounds and we need to extract multiple voices from the environment. We'd like to try separating three speakers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 82, "text": " As a tourist to Romania, I just recorded a native speaker explaining directions, and I need a translation of this speech into English.\\n###Input: Romanian_speaker_audio_recording.wav\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 83, "text": " There's a noisy background sound in the recorded meeting we had today. Can you please enhance the speech signal?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'hubert-large-ll60k\\', \\'api_call\\': \"HubertModel.from_pretrained(\\'facebook/hubert-large-ll60k\\')\", \\'api_arguments\\': \\'pretrained model name\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"hubert = HubertModel.from_pretrained(\\'facebook/hubert-large-ll60k\\')\", \\'performance\\': {\\'dataset\\': \\'Libri-Light\\', \\'accuracy\\': \\'matches or improves upon the state-of-the-art wav2vec 2.0 performance\\'}, \\'description\\': \\'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.\\'}', metadata={})]", "category": "generic"}
{"question_id": 84, "text": " We're developing a voice assistant with emotion recognition. Help us classify emotions in spoken language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Spoken Language Identification\\', \\'api_name\\': \\'TalTechNLP/voxlingua107-epaca-tdnn\\', \\'api_call\\': \"\\'EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\'\", \\'api_arguments\\': [\\'signal\\'], \\'python_environment_requirements\\': [\\'speechbrain\\', \\'torchaudio\\'], \\'example_code\\': \\'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\', \\'performance\\': {\\'dataset\\': \\'VoxLingua107\\', \\'accuracy\\': \\'93%\\'}, \\'description\\': \\'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 85, "text": " I am developing an AI tool to spot specific keywords present in different audio files to categorize them. Provide me with a solution to detect keywords in the given audio files.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker Diarization\\', \\'api_name\\': \\'pyannote/speaker-diarization\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2.1\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': {\\'num_speakers\\': \\'int (optional)\\', \\'min_speakers\\': \\'int (optional)\\', \\'max_speakers\\': \\'int (optional)\\'}, \\'python_environment_requirements\\': \\'pyannote.audio 2.1.1\\', \\'example_code\\': [\\'from pyannote.audio import Pipeline\\', \\'pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2.1, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'diarization = pipeline(audio.wav)\\', \\'with open(audio.rttm, w) as rttm:\\', \\'  diarization.write_rttm(rttm)\\'], \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': {\\'DER%\\': \\'18.91\\', \\'FA%\\': \\'4.48\\', \\'Miss%\\': \\'9.51\\', \\'Conf%\\': \\'4.91\\'}}, \\'description\\': \\'This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 86, "text": " A customer support center needs a tool to analyze customers' satisfaction based on their voice messages. Recommend a solution for evaluating sentiment in Spanish voice messages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 87, "text": " Your company is building voice assistants for smartphones, and they must be able to recognize and execute spoken commands efficiently.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'microsoft/speecht5_tts\\', \\'api_call\\': \"model.generate_speech(inputs[\\'input_ids\\'], speaker_embeddings, vocoder=vocoder)\", \\'api_arguments\\': [\\'text\\', \\'return_tensors\\', \\'input_ids\\', \\'speaker_embeddings\\', \\'vocoder\\'], \\'python_environment_requirements\\': \\'!pip install git+https://github.com/huggingface/transformers sentencepiece datasets\\', \\'example_code\\': \"from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nimport soundfile as sf\\\\nprocessor = SpeechT5Processor.from_pretrained(\\'microsoft/speecht5_tts\\')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\\'microsoft/speecht5_tts\\')\\\\nvocoder = SpeechT5HifiGan.from_pretrained(\\'microsoft/speecht5_hifigan\\')\\\\ninputs = processor(text=\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nembeddings_dataset = load_dataset(\\'Matthijs/cmu-arctic-xvectors\\', split=\\'validation\\')\\\\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306][\\'xvector\\']).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs[\\'input_ids\\'], speaker_embeddings, vocoder=vocoder)\\\\nsf.write(\\'speech.wav\\', speech.numpy(), samplerate=16000)\", \\'performance\\': {\\'dataset\\': \\'LibriTTS\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. It is a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. It can be used for a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 88, "text": " A finance company needs a way to predict whether potential clients will have an income above $50,000 per year.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'abhishek/autotrain-adult-census-xgboost\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'scikit-learn/adult-census-income\\', \\'accuracy\\': 0.8750191923844618}, \\'description\\': \"A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual\\'s income is above or below $50,000 per year.\"}', metadata={})]", "category": "generic"}
{"question_id": 89, "text": " We are a data science agency, we are helping people to predict if a passenger of titanic will survive based on different features.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 90, "text": " We are a museum and we need to classify the survivers from Titanic according to their age, gender, and passenger class.\\n###Input: data = pd.read_csv('data.csv')\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'harithapliyal/autotrain-tatanic-survival-51030121311\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'harithapliyal/autotrain-data-tatanic-survival\\', \\'accuracy\\': 0.872}, \\'description\\': \\'A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.\\'}', metadata={})]", "category": "generic"}
{"question_id": 91, "text": " Estimate the carbon emissions of a vehicle using its underlying features.\\n###Input: {\\\"data\\\": {\\\"Engine Size(L)\\\": 1.5, \\\"Cylinders\\\": 4, \\\"Transmission\\\": \\\"Manual\\\", \\\"Fuel Type\\\": \\\"gas\\\", \\\"Fuel Consumption Comb (L/100 km)\\\": 6.0}}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'emotion\\', \\'api_name\\': \\'bhadresh-savani/distilbert-base-uncased-emotion\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'bhadresh-savani/distilbert-base-uncased-emotion\\', return_all_scores=True)\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"prediction = classifier(\\'I love using transformers. The best part is wide range of support and its easy to use\\')\", \\'performance\\': {\\'dataset\\': \\'Twitter-Sentiment-Analysis\\', \\'accuracy\\': 0.938}, \\'description\\': \"Distilbert is created with knowledge distillation during the pre-training phase which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding. It\\'s smaller, faster than Bert and any other Bert-based model. Distilbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer.\"}', metadata={})]", "category": "generic"}
{"question_id": 92, "text": " A real estate company needs a prediction tool to estimate housing prices for different regions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761512\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\', \\'json\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 122809.223, \\'R2\\': 0.884, \\'MSE\\': 15082105200.447, \\'MAE\\': 95586.887, \\'RMSLE\\': 0.13}}, \\'description\\': \\'A model trained using AutoTrain for predicting US housing prices with single column regression. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a CO2 Emissions of 50.5369 grams.\\'}', metadata={})]", "category": "generic"}
{"question_id": 93, "text": " I want to predict the carbon emissions of a specific car with given features.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 94, "text": " As an environmental organization, we want to estimate the carbon emissions from various factories given specific input data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1839063122\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 95, "text": " I am a data scientist working on a project to predict carbon emissions based on a set of input data. Provide instructions to load and use the model to make predictions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 96, "text": " A restaurant wants to predict tips given by customers based on factors like bill amount, day, time, and other attributes. We can use this to optimize staff allocation during peak hours.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 97, "text": " We need to predict fish's weight based on different measurements, as part of a fish farm management software.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 98, "text": " We are making an application that predicts stock closing prices using a regression model on financial data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 99, "text": " Create an AI model that can play CartPole-v1 game and store the results.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'valhalla/t5-base-e2e-qg\\', \\'api_call\\': \"pipeline(\\'e2e-qg\\', model=\\'valhalla/t5-base-e2e-qg\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'Hugging Face Transformers\\'], \\'example_code\\': \"from pipelines import pipeline\\\\n\\\\ntext = Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python\\'s design philosophy emphasizes code readability with its notable use of significant whitespace.\\\\n\\\\nnlp = pipeline(e2e-qg, model=valhalla/t5-base-e2e-qg)\\\\n\\\\nnlp(text)\", \\'performance\\': {\\'dataset\\': \\'squad\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a T5-base model trained for end-to-end question generation task. Simply input the text and the model will generate multiple questions. You can play with the model using the inference API, just put the text and see the results!\\'}', metadata={})]", "category": "generic"}
{"question_id": 100, "text": " A soccer match is being co-organized by a couple of sports organizations. The sports organizers want to prepare a learning-based simulation for the match.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Structured data learning with TabTransformer\\', \\'api_name\\': \\'keras-io/tab_transformer\\', \\'api_call\\': \\'TabTransformer.from_config()\\', \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'Hugging Face\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'United States Census Income Dataset\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model\\'s inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model.\"}', metadata={})]", "category": "generic"}
{"question_id": 101, "text": " The game development team needs a learning-based agent to create a character that can learn to hop in a game environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'satvikag/chatbot\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'output-small\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'microsoft/DialoGPT-small\\')\", \\'model\\': \"AutoModelWithLMHead.from_pretrained(\\'output-small\\')\"}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"for step in range(100):\\\\n  new_user_input_ids = tokenizer.encode(input(\\'&gt;&gt; User:\\') + tokenizer.eos_token, return_tensors=\\'pt\\')\\\\n  bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids\\\\n  chat_history_ids = model.generate(bot_input_ids, max_length=500, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3, do_sample=True, top_k=100, top_p=0.7, temperature = 0.8)\\\\n  print(\\'AI: {}\\'.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\", \\'performance\\': {\\'dataset\\': \\'Kaggle game script dataset\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'DialoGPT Trained on the Speech of a Game Character, Joshua from The World Ends With You.\\'}', metadata={})]", "category": "generic"}
{"question_id": 102, "text": " Create a game application that requires the interaction between two soccer-playing agents. The agents will learn how to play from each interaction.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-en-ROMANCE\\', \\'api_call\\': \"pipeline(\\'translation_en_to_ROMANCE\\', model=\\'Helsinki-NLP/opus-mt-en-ROMANCE\\')\", \\'api_arguments\\': \\'source languages, target languages\\', \\'python_environment_requirements\\': \\'transformers, torch\\', \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_en_to_ROMANCE\\', model=\\'Helsinki-NLP/opus-mt-en-ROMANCE\\')\\\\ntranslated_text = translation(\\'Hello, how are you?\\', tgt_lang=\\'es\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': 50.1, \\'chr-F\\': 0.693}}, \\'description\\': \\'A translation model trained on the OPUS dataset that supports translation between English and various Romance languages. It uses a transformer architecture and requires a sentence initial language token in the form of >>id<< (id = valid target language ID).\\'}', metadata={})]", "category": "generic"}
{"question_id": 103, "text": " A new robotic toy is being developed for kids. It recognizes objects and interacts with them accordingly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 104, "text": " A home robotics company requires object manipulation and indoor navigation capabilities. How can we achieve this using available APIs?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Reinforcement Learning Robotics', 'framework': 'Hugging Face Transformers', 'functionality': 'EmbodiedAI tasks, such as object manipulation and indoor navigation', 'api_name': 'facebook/vc1-large', 'api_call': 'Output: model_utils.load_model(model_utils.VC1_BASE_NAME)(transformed_img)', 'api_arguments': 'img', 'python_environment_requirements': 'from vc_models.models.vit import model_utils', 'example_code': 'model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\\\nimg = your_function_here ...\\\\ntransformed_img = model_transforms(img)\\\\nembedding = model(transformed_img)', 'performance': {'dataset': 'CortexBench', 'accuracy': '68.7 (Mean Success)'}, 'description': 'The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.'}\", metadata={})]", "category": "generic"}
{"question_id": 105, "text": " Create a new story to be read to a child by starting the story with \\\"Once upon a time in a land far away...\\\"\\n###Input: \\\"Once upon a time in a land far away...\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'bigscience/test-bloomd-6b3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'bigscience/test-bloomd-6b3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; generator = pipeline(\\'text-generation\\', model=\\'bigscience/test-bloomd-6b3\\'); generator(\\'Once upon a time\\')\", \\'performance\\': {\\'dataset\\': \\'Not specified\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A text generation model from Hugging Face, using the bigscience/test-bloomd-6b3 architecture. It can be used for generating text based on a given input.\\'}', metadata={})]", "category": "generic"}
{"question_id": 106, "text": " In the next product launch, we want to showcase the company mascot. We need a high-quality anime character with specific features.\\n###Input: 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'hakurei/waifu-diffusion\\', \\'api_call\\': \"pipe(prompt, guidance_scale=6)[\\'sample\\'][0]\", \\'api_arguments\\': {\\'prompt\\': \\'text\\', \\'guidance_scale\\': \\'number\\'}, \\'python_environment_requirements\\': {\\'torch\\': \\'torch\\', \\'autocast\\': \\'from torch\\', \\'StableDiffusionPipeline\\': \\'from diffusers\\'}, \\'example_code\\': \"import torch\\\\nfrom torch import autocast\\\\nfrom diffusers import StableDiffusionPipeline\\\\npipe = StableDiffusionPipeline.from_pretrained(\\\\n \\'hakurei/waifu-diffusion\\',\\\\n torch_dtype=torch.float32\\\\n).to(\\'cuda\\')\\\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\\\nwith autocast(cuda):\\\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\\\nimage.save(test.png)\", \\'performance\\': {\\'dataset\\': \\'high-quality anime images\\', \\'accuracy\\': \\'not available\\'}, \\'description\\': \\'waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 107, "text": " I need a script to create photorealistic images from a description I provide, so I can use the generated images for my art projects.\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 108, "text": " Develop a piece of code that generates a high-resolution image of a landscape using a given prompt and low-resolution latents in the provided AI model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 109, "text": " We have a scanned Japanese manga image and we want to extract the text from it.\\n###Input: image_path = \\\"scanned_manga_page.jpg\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 110, "text": " Please help us with describing the image, which we have submitted for our advertisement campaign.\\n###Input: \\\"<the image provided>\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 111, "text": " Our company is designing a virtual assistant that will answer customer questions based on images they submit. Implement a function that finds out how many people are in a picture submitted by the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 112, "text": " I would like to create an AI system that can provide a detailed textual description for a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 113, "text": " A children's publisher requested a system to generate stories based on images. For this purpose, we need to find a solution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-textvqa\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/git-large-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'For code examples, we refer to the documentation.\\', \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'See table 11 in the paper for more details.\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 114, "text": " I want to analyze and answer questions about a chart using image and text data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 115, "text": " I want to develop an application to read traffic signboards and provide output text in real-time. Can you tell me the model I can use and code to apply the model?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 116, "text": " We want to provide video summaries of news articles on our platform. We would like to turn our article's text into video summaries.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 117, "text": " As a children's author, I need my story to be converted to video clips. The text is in English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 118, "text": " Produce a short movie scene showing the protagonist breaking up with their partner while on a walk in the city under the rain.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 119, "text": " Please generate a code snippet for me to create an AI algorithm that answers questions based on the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Code Understanding and Generation\\', \\'api_name\\': \\'Salesforce/codet5-base\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'Salesforce/codet5-base\\')\", \\'api_arguments\\': [\\'text\\', \\'return_tensors\\', \\'input_ids\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import RobertaTokenizer, T5ForConditionalGeneration\\\\ntokenizer = RobertaTokenizer.from_pretrained(\\'Salesforce/codet5-base\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\'Salesforce/codet5-base\\')\\\\ntext = def greet(user): print(f\\'hello <extra_id_0>!\\')\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=8)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\", \\'performance\\': {\\'dataset\\': \\'code_search_net\\', \\'accuracy\\': \\'Refer to the paper for evaluation results on several downstream benchmarks\\'}, \\'description\\': \\'CodeT5 is a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. It supports both code understanding and generation tasks and allows for multi-task learning. The model can be used for tasks such as code summarization, code generation, code translation, code refinement, code defect detection, and code clone detection.\\'}', metadata={})]", "category": "generic"}
{"question_id": 120, "text": " We are an online study group specializing in history. Our members can upload an image of the text, and they ask questions related to the text. We need a Q&A model to answer these questions automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 121, "text": " Our company needs a solution to automatically extract specific information such as invoice numbers and dates from invoices in various formats.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 122, "text": " I have a document containing information about a company's products, and I want to find answers to my questions about these products.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 123, "text": " I need an electronic document examiner that can answer a question about my document content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 124, "text": " I need a molecule prediction model to predict the properties of different molecules in my research.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': 'Output: AutoModel.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.'}\", metadata={})]", "category": "generic"}
{"question_id": 125, "text": " As an autonomous car company, we want to estimate the depth of objects in front of the car's cameras.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Cross-Encoder for Natural Language Inference\\', \\'api_name\\': \\'cross-encoder/nli-deberta-v3-base\\', \\'api_call\\': \"CrossEncoder(\\'cross-encoder/nli-deberta-v3-base\\')\", \\'api_arguments\\': [\\'sentence_pairs\\'], \\'python_environment_requirements\\': [\\'sentence_transformers\\', \\'transformers\\'], \\'example_code\\': \"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\'cross-encoder/nli-deberta-v3-base\\')\\\\nscores = model.predict([(\\'A man is eating pizza\\', \\'A man eats something\\'), (\\'A black race car starts up in front of a crowd of people.\\', \\'A man is driving down a lonely road.\\')])\", \\'performance\\': {\\'dataset\\': {\\'SNLI-test\\': \\'92.38\\', \\'MNLI mismatched set\\': \\'90.04\\'}}, \\'description\\': \\'This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 126, "text": " I am building a robot that needs to navigate through a room. I need to estimate the depth of the room from an image.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 127, "text": " Our environmental agency wants to estimate the depth of plant cover in a forest from the given dataset of images to understand their growth patterns.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'model.generate(input_ids, max_length=128)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}\", metadata={})]", "category": "generic"}
{"question_id": 128, "text": " A local grocery store is developing an AI application to identify fruits and vegetables from their images. What kind of code could they use for image classification?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 129, "text": " We are developing an app that identifies objects in pictures. In order to classify images, I must find a suitable image classification model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 130, "text": " The security team at our company is working on improving camera surveillance in parking lots. We need a model to track objects and cars in the parking lots.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 131, "text": " Our construction safety management team needs to detect whether workers are using hard hats in a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-hard-hat-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\\\\n\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'hard-hat-detection\\', \\'accuracy\\': 0.811}, \\'description\\': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between \\'Hardhat\\' and \\'NO-Hardhat\\' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\"}', metadata={})]", "category": "generic"}
{"question_id": 132, "text": " As a company specialized in smart cities, we need to develop roadway maintenance tools. Recently we got a requirement for detecting potholes in road images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 133, "text": " Design a virtual assistant to help users create an effective Instagram Story image. The assistant should modify the input image by generating a stylized version as output.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 134, "text": " How can I insert a text prompt for creating an image which shows the text's meaning?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 135, "text": " I want to create a new image of a beautiful mountain scenery by providing input of a plain field picture.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-photoreal-2.0\\', \\'api_call\\': \"Output: StableDiffusionPipeline.from_pretrained(\\'dreamlike-art/dreamlike-photoreal-2.0\\', torch_dtype=torch.float16)(prompt).images[0]\", \\'api_arguments\\': {\\'prompt\\': \\'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\'}, \\'python_environment_requirements\\': {\\'torch\\': \\'torch.float16\\', \\'diffusers\\': \\'StableDiffusionPipeline\\'}, \\'example_code\\': \\'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'Stable Diffusion 1.5\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 136, "text": " I am the owner of a photography website and want to automatically generate pictures of cats in order to add content to my website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 137, "text": " We need to generate a high-quality image for the AI to analyze some human face expression so the algorithms should be improved.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image-to-Image\\', \\'api_name\\': \\'GreeneryScenery/SheepsControlV3\\', \\'api_call\\': \"pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \\'api_arguments\\': {\\'image\\': \\'Path to image file\\', \\'text_guidance\\': \\'Optional text guidance for the model\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': [\\'from transformers import pipeline\\', \"model = pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \"result = model({\\'image\\': \\'path/to/image.jpg\\', \\'text_guidance\\': \\'Optional text guidance\\'})\"], \\'performance\\': {\\'dataset\\': \\'GreeneryScenery/SheepsControlV3\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 138, "text": " We are an entertainment company specializing in creating realistic virtual characters. We want to generate synthetic images of human faces.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 139, "text": " Can you make a butterfly-themed birthday card for my niece?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 140, "text": " We need to come up with some creative and innovative facial designs for our new android.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 141, "text": " Can you make it work to generate an image of a cute butterfly for us?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 142, "text": " Animate a video based on the past images we have from the surveillance camera.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 143, "text": " I need help to classify the videos online for my content platform.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 144, "text": " Look at this visual poster design and find out if it is suitable for use in a bakery.\\n###Input: <url to the poster>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 145, "text": " We are building an app to provide information about objects around us. The user can take a picture of anything in front of them, and the app will tell them what it is.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 146, "text": " I am building a machine learning powered app that can classify food into three categories: fruit, vegetable, and junk food just by analyzing images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 147, "text": " Classify an image of food and determine if it's a burger or a pizza with a pre-trained model.\\n###Input: \\\"path/to/food_image.jpg\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 148, "text": " We have a box of photos for old family trips, but we can't identify the place. Can you help us estimate which country or region the pictures were taken in?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'lidiya/bart-large-xsum-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'lidiya/bart-large-xsum-samsum\\')\", \\'api_arguments\\': \\'conversation\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=lidiya/bart-large-xsum-samsum)\\\\nconversation = \\'\\'\\'Hannah: Hey, do you have Betty\\'s number?\\\\nAmanda: Lemme check\\\\nAmanda: Sorry, can\\'t find it.\\\\nAmanda: Ask Larry\\\\nAmanda: He called her last time we were at the park together\\\\nHannah: I don\\'t know him well\\\\nAmanda: Don\\'t be shy, he\\'s very nice\\\\nHannah: If you say so..\\\\nHannah: I\\'d rather you texted him\\\\nAmanda: Just text him \ud83d\ude42\\\\nHannah: Urgh.. Alright\\\\nHannah: Bye\\\\nAmanda: Bye bye <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\\', \\'accuracy\\': {\\'rouge1\\': 53.306, \\'rouge2\\': 28.355, \\'rougeL\\': 44.095}}, \\'description\\': \\'This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 149, "text": " A client of ours wants to classify images of animals when they upload them to their website to better categorize the content. We are looking for a solution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 150, "text": " We are a startup focusing on content creation for a Chinese audience. We want to recommend images related to poke categories automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 151, "text": " I run a review website where users submit their reviews of various products. I want to classify positive and negative reviews.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 152, "text": " We need to keep our platform clean from unsuitable content. Build a system that can identify and flag NSFW (not safe for work) text in user-generated content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 153, "text": " An AI application development company wants to develop an application that can classify emotions from a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'wav2vec2-xlsr-53-russian-emotion-recognition\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'facebook/wav2vec2-large-xlsr-53\\') .predict(\\'/path/to/russian_audio_speech.wav\\', sampling_rate=16000)\", \\'api_arguments\\': {\\'path\\': \\'/path/to/russian_audio_speech.wav\\', \\'sampling_rate\\': 16000}, \\'python_environment_requirements\\': [\\'torch\\', \\'torchaudio\\', \\'transformers\\', \\'librosa\\', \\'numpy\\'], \\'example_code\\': \"result = predict(\\'/path/to/russian_audio_speech.wav\\', 16000)\\\\nprint(result)\", \\'performance\\': {\\'dataset\\': \\'Russian Emotional Speech Dialogs\\', \\'accuracy\\': \\'72%\\'}, \\'description\\': \\'A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.\\'}', metadata={})]", "category": "generic"}
{"question_id": 154, "text": " Explain how to find out the date and location of a meeting mentioned in a sentence.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 155, "text": " I am the editor of a political blog, and we often use quotes from different politicians. I want to extract names of politicians from the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 156, "text": " Your manager needs a detailed report on a given text. Ensure that you include the part-of-speech tags for each word in the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 157, "text": " I live in Berlin and I want to develop a multilingual named entity recognition pipeline for processing German, English, and Russian texts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Babelscape/wikineural-multilingual-ner\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Babelscape/wikineural-multilingual-ner\\')\", \\'api_arguments\\': {\\'model\\': \\'AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\', \\'tokenizer\\': \\'AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \\'tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)\\', \\'performance\\': {\\'dataset\\': \\'Babelscape/wikineural-multilingual-ner\\', \\'accuracy\\': \\'span-based F1-score up to 6 points over previous state-of-the-art systems for data creation\\'}, \\'description\\': \\'A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 158, "text": " As an online tutor, I need to prepare a list of frequently asked questions about my subjects. I require a tool that can quickly answer questions related to a spreadsheet containing data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 159, "text": " I work for a tour agency and I need to answer customer questions using a database of historical Olympic Games. I need to know the host city of the 1904 Olympics.\\n###Input: {\\\"year\\\": [1896, 1900, 1904, 2004, 2008, 2012], \\\"city\\\": [\\\"athens\\\", \\\"paris\\\", \\\"st. louis\\\", \\\"athens\\\", \\\"beijing\\\", \\\"london\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'neulab/omnitab-large-finetuned-wtq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'neulab/omnitab-large-finetuned-wtq\\').generate(**encoding)\", \\'api_arguments\\': {\\'table\\': \\'pd.DataFrame\\', \\'query\\': \\'str\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\', \\'pandas\\': \\'pd\\'}, \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport pandas as pd\\\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-finetuned-wtq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-finetuned-wtq)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'wikitablequestions\\', \\'accuracy\\': None}, \\'description\\': \\'OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab.\\'}', metadata={})]", "category": "generic"}
{"question_id": 160, "text": " I want to prepare an AI model for an interactive storybook for kids. It should be able to answer questions related to the story.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'vilt-finetuned-vqasi\\', \\'api_call\\': \"ViltModel.from_pretrained(\\'tufa15nik/vilt-finetuned-vqasi\\')\", \\'api_arguments\\': {\\'model\\': \\'tufa15nik/vilt-finetuned-vqasi\\', \\'tokenizer\\': \\'tufa15nik/vilt-finetuned-vqasi\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.11.3\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.\\'}', metadata={})]", "category": "generic"}
{"question_id": 161, "text": " I want to classify text between three subjects. The subjects are art, science and history.\\n###Input: {\\\"text\\\": \\\"The invention of the telescope allowed astronomers to observe the solar system more effectively and led to the discovery of new planets and celestial objects.\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 162, "text": " Suppose my company wants to hire a candidate for a technical writer position. How could we determine whether an applicant's cover letter is aligned with the job expectations or not?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 163, "text": " To assist in efficient communication, we need a tool to help us translate comments on our multilingual social media posts to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Many-to-Many multilingual translation\\', \\'api_name\\': \\'facebook/m2m100_1.2B\\', \\'api_call\\': \"Output: M2M100ForConditionalGeneration.from_pretrained(\\'facebook/m2m100_1.2B\\')\", \\'api_arguments\\': {\\'encoded_input\\': \\'encoded text\\', \\'forced_bos_token_id\\': \\'target language id\\'}, \\'python_environment_requirements\\': \\'sentencepiece\\', \\'example_code\\': \\'from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\\\nhi_text = \u091c\u0940\u0935\u0928 \u090f\u0915 \u091a\u0949\u0915\u0932\u0947\u091f \u092c\u0949\u0915\u094d\u0938 \u0915\u0940 \u0924\u0930\u0939 \u0939\u0948\u0964\\\\nchinese_text = \u751f\u6d3b\u5c31\u50cf\u4e00\u76d2\u5de7\u514b\u529b\u3002\\\\nmodel = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_1.2B)\\\\ntokenizer.src_lang = hi\\\\nencoded_hi = tokenizer(hi_text, return_tensors=pt)\\\\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr))\\\\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\\', \\'performance\\': {\\'dataset\\': \\'M2M100\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.\\'}', metadata={})]", "category": "generic"}
{"question_id": 164, "text": " I need to translate an English paragraph to Arabic for my website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 165, "text": " I need a tool that can translate Dutch text to English language for an upcoming project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 166, "text": " Can you automatically summarize a long article in French language for me?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 167, "text": " I am a German management consulting firm. I need to summarize the content of business documents for my clients.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 168, "text": " Implement a communication method for our employee chatbot for quick and simple responses.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 169, "text": " Design a text summarizer for a Russian news website that will automatically condense the lengthy articles for the readers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 170, "text": " We are creating a chatbot that can participate in a live Dungeons and Dragons game. Bring Pygmalion-6B into the chat session to play a character.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 171, "text": " We have an online platform where psychologists chat with anonymous users. We want a virtual agent that will play the role of a psychologist for some of our users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 172, "text": " We are designing a conversational AI agent for our customers. We want the agent to respond naturally to users' questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 173, "text": " I need help generating a short piece of Python code that calculates the sum of square numbers for any input list.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 174, "text": " I own a small apparel store and I need help coming up with a catchy slogan for the clothing.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 175, "text": " Translate the following English text to Korean: \\\"The weather today is beautiful.\\\"\\n###Input: \\\"The weather today is beautiful.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/ner-english\\')\", \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load(\\'flair/ner-english\\')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence(\\'George Washington went to Washington\\')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint(\\'The following NER tags are found:\\')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'conll2003\\', \\'accuracy\\': \\'93.06\\'}, \\'description\\': \\'This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 176, "text": " Our team is working on a search engine, and we require a model that can generate a query from the given document text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\', \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 177, "text": " Our travel agency needs a tool that automatically translates a travel itinerary from English to French for our French-speaking clients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 178, "text": " We have a competition going on between our team members. Based on the text, we need to predict the next word.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 179, "text": " Transform the code snippet for generating a greeting message to a corresponding text summary.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Zixtrauce/JohnBot\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'Zixtrauce/JohnBot\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Zixtrauce/JohnBot.\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'JohnBot is a conversational model based on the gpt2 architecture and trained using the Hugging Face Transformers library. It can be used for generating text responses in a chat-based interface.\\'}', metadata={})]", "category": "generic"}
{"question_id": 180, "text": " I would like to measure whether two given sentences have similar meanings or not.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 181, "text": " As a part of our educational platform, we need to implement a text-to-speech functionality for visually impaired students to use.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 182, "text": " We are building a virtual assistant that speaks the text we provide as a response.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 183, "text": " I work for a broadcasting agency, and we need automated text-to-speech for news segments in a more natural way.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 184, "text": " Our company is developing an IVR system for the Telugu speaking population. We need a male voice to read out the menu options.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 185, "text": " As a Chinese language teacher, I am trying to create pronunciation exercises for my students. I need to convert some text into speech.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 186, "text": " I am running an online course with German content. Can you provide an audio version of the introductory lecture?\\n###Input: \\\"Willkommen zum Einfrungsvortrag! In dieser Online-Kursreihe werden wir verschiedene Themen untersuchen, um Ihnen ein umfassendes Verstdnis der behandelten Konzepte zu ermlichen. Bereiten Sie sich darauf vor, aufregende und interessante Informationen zu erhalten. Viel Spabeim Lernen!\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'svalabs/gbert-large-zeroshot-nli\\', \\'api_call\\': \"pipeline(\\'zero-shot-classification\\', model=\\'svalabs/gbert-large-zeroshot-nli\\')\", \\'api_arguments\\': [\\'sequence\\', \\'labels\\', \\'hypothesis_template\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nzershot_pipeline = pipeline(zero-shot-classification, model=svalabs/gbert-large-zeroshot-nli)\\\\nsequence = Ich habe ein Problem mit meinem Iphone das so schnell wie m\u00f6glich gel\u00f6st werden muss\\\\nlabels = [Computer, Handy, Tablet, dringend, nicht dringend]\\\\nhypothesis_template = In diesem Satz geht es um das Thema {}. \\', \\'performance\\': {\\'dataset\\': \\'XNLI TEST-Set\\', \\'accuracy\\': \\'85.6%\\'}, \\'description\\': \\'A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets.\\'}', metadata={})]", "category": "generic"}
{"question_id": 187, "text": " Develop a mechanism to monitor and report voice activity in meeting recordings, isolating ongoing conversational moments to improve transcription accuracy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 188, "text": " We are working on a customer service chatbot project. We need to transcribe client voice messages accurately.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 189, "text": " Our team wants to separate the vocals and instrumentals from an audio recording. How do we use a pretrained model to do this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'neuralmind/bert-base-portuguese-cased\\', \\'api_call\\': \"AutoModelForPreTraining.from_pretrained(\\'neuralmind/bert-base-portuguese-cased\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'neuralmind/bert-base-portuguese-cased\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\npipe = pipeline(\\'fill-mask\\', model=model, tokenizer=tokenizer)\\\\npipe(\\'Tinha uma [MASK] no meio do caminho.\\')\", \\'performance\\': {\\'dataset\\': \\'brWaC\\', \\'accuracy\\': \\'state-of-the-art\\'}, \\'description\\': \\'BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.\\'}', metadata={})]", "category": "generic"}
{"question_id": 190, "text": " Our client is a radio station and they have many programs in Hokkien language. They want to convert Hokkien programs into English for international audience.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 191, "text": " Our team is working on building an application for automating user emotion recognition from audio. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'superb/hubert-base-superb-er\\', \\'api_call\\': \"pipeline(\\'audio-classification\\', model=\\'superb/hubert-base-superb-er\\')\", \\'api_arguments\\': {\\'model\\': \\'superb/hubert-base-superb-er\\'}, \\'python_environment_requirements\\': {\\'libraries\\': [\\'transformers\\', \\'datasets\\', \\'librosa\\'], \\'versions\\': [\\'latest\\']}, \\'example_code\\': \\'from datasets import load_dataset\\\\nfrom transformers import pipeline\\\\ndataset = load_dataset(anton-l/superb_demo, er, split=session1)\\\\nclassifier = pipeline(audio-classification, model=superb/hubert-base-superb-er)\\\\nlabels = classifier(dataset[0][file], top_k=5)\\', \\'performance\\': {\\'dataset\\': \\'IEMOCAP\\', \\'accuracy\\': {\\'session1\\': 0.6492, \\'session2\\': 0.6359}}, \\'description\\': \"Hubert-Base for Emotion Recognition is a ported version of S3PRL\\'s Hubert for the SUPERB Emotion Recognition task. The base model is hubert-base-ls960, which is pretrained on 16kHz sampled speech audio. The model is used for predicting an emotion class for each utterance, and it is trained and evaluated on the IEMOCAP dataset.\"}', metadata={})]", "category": "generic"}
{"question_id": 192, "text": " A dating app needs to recognize the gender of the user who communicates with other users through voice messages. What model should we use?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Paraphrasing\\', \\'api_name\\': \\'prithivida/parrot_paraphraser_on_T5\\', \\'api_call\\': \"\\'Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\'\", \\'api_arguments\\': [\\'input_phrase\\', \\'diversity_ranker\\', \\'do_diverse\\', \\'max_return_phrases\\', \\'max_length\\', \\'adequacy_threshold\\', \\'fluency_threshold\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \\'from parrot import Parrot\\\\nimport torch\\\\nimport warnings\\\\nwarnings.filterwarnings(ignore)\\\\n\\\\nparrot = Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\\\nphrases = [Can you recommed some upscale restaurants in Newyork?,\\\\n What are the famous places we should not miss in Russia?\\\\n]\\\\nfor phrase in phrases:\\\\n print(-*100)\\\\n print(Input_phrase: , phrase)\\\\n print(-*100)\\\\n para_phrases = parrot.augment(input_phrase=phrase)\\\\n for para_phrase in para_phrases:\\\\n  print(para_phrase)\\', \\'performance\\': {\\'dataset\\': \\'Not mentioned\\', \\'accuracy\\': \\'Not mentioned\\'}, \\'description\\': \\'Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 193, "text": " We need a voice command ordering assistant ready to parse the numbers 0 to 9, and we need to classify which number is said by the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 194, "text": " Determine the sentiment of a given Spanish audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Classification\\', \\'api_name\\': \\'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\\', \\'api_call\\': \"Output: Wav2Vec2ForSequenceClassification.from_pretrained(\\'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\\')\", \\'api_arguments\\': {\\'model_name\\': \\'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.17.0\\', \\'pytorch\\': \\'1.10.0+cu111\\', \\'datasets\\': \\'2.0.0\\', \\'tokenizers\\': \\'0.11.6\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'MESD\\', \\'accuracy\\': 0.9308}, \\'description\\': \\'This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 195, "text": " I want to detect voice activity in Indian languages in my recordings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Voice Activity Detection\\', \\'api_name\\': \\'d4data/Indian-voice-cloning\\', \\'api_call\\': \"pipeline(\\'voice-activity-detection\\', model=\\'d4data/Indian-voice-cloning\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model for detecting voice activity in Indian languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 196, "text": " How do I create something to automatically transcribe meetings for me and help me detect who is talking during the meeting?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 197, "text": " We are trying to use this model to predict carbon emissions for our industrial client.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 198, "text": " Develop a code to predict the amount of tips a waiter can receive based on a certain dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 199, "text": " We are building an app to calculate housing prices in California neighborhoods. Can you help us with the prediction?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 200, "text": " I have a team of AI researchers and we are focusing on playing soccer using artificial agents. I want these agents to perform at a high level in our in-house soccer training environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 201, "text": " Determine the similarity of sentences from a provided document by obtaining sentence embeddings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\', \\'api_call\\': \\'model.encode(text)\\', \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'sentence-transformers library\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\\\\ntext = Replace me by any text you\\'d like.\\\\ntext_embbedding = model.encode(text)\", \\'performance\\': {\\'dataset\\': \\'1,097,953,922\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 202, "text": " I want to create a chatbot with the ability to provide sentence embeddings for a given sentence in Russian.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Generative Commonsense Reasoning\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-common_gen\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'mrm8488/t5-base-finetuned-common_gen\\').generate(input_ids=features[\\'input_ids\\'], attention_mask=features[\\'attention_mask\\'], max_length=max_length)\", \\'api_arguments\\': [\\'words\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\ndef gen_sentence(words, max_length=32):\\\\n input_text = words\\\\n features = tokenizer([input_text], return_tensors=\\'pt\\')\\\\noutput = model.generate(input_ids=features[\\'input_ids\\'], attention_mask=features[\\'attention_mask\\'], max_length=max_length)\\\\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\\\\nwords = tree plant ground hole dig\\\\ngen_sentence(words)\", \\'performance\\': {\\'dataset\\': \\'common_gen\\', \\'accuracy\\': {\\'ROUGE-2\\': 17.1, \\'ROUGE-L\\': 39.47}}, \\'description\\': \"Google\\'s T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.\"}', metadata={})]", "category": "generic"}
{"question_id": 203, "text": " I want to build a multilingual content recommendation system using LaBSE. Show me how to handle English, Italian, and Japanese sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 204, "text": " Our online forum has a feature to autocomplete parts of code snippets. We need a model to recognize tokens in the domain of coding languages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 205, "text": " We need to create a visual for a character concept from a given description.\\n###Input: A young android girl wearing a blue dress, holding an umbrella in one hand, and a small bird perched on her shoulder.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 206, "text": " Find a way to generate a high-resolution image of a deep blue car speeding on a highway in the rain.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'sshleifer/distilbart-cnn-12-6\\', \\'api_call\\': \"BartForConditionalGeneration.from_pretrained(\\'sshleifer/distilbart-cnn-12-6\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'huggingface/transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'Rouge 2\\': \\'22.12\\', \\'Rouge-L\\': \\'36.99\\'}}]}, \\'description\\': \"DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, \\'sshleifer/distilbart-cnn-12-6\\', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 207, "text": " Generate a caption for an image provided from the internet.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 208, "text": " I have an image containing text that is essential for a legal document. Can you extract the text for me?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 209, "text": " Use the model to generate captions for images uploaded to the application.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 210, "text": " I have an image that contains text, and I'd like to extract the text from the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'naver-clova-ix/donut-base\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'naver-clova-ix/donut-base\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'result = donut(image_path)\\', \\'performance\\': {\\'dataset\\': \\'arxiv:2111.15664\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\\'}', metadata={})]", "category": "generic"}
{"question_id": 211, "text": " In our next project, we are developing a system that answers questions about the images. Could you help us find the best model to use for this project?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 212, "text": " Our company prepares an e-commerce website, and we want to be able to automatically answer specific questions about product information and descriptions in our catalogue.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 213, "text": " Invoices for the business are piling up. Find out the due date for each invoice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 214, "text": " Analyze the given image to find depth information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Variations\\', \\'api_name\\': \\'lambdalabs/sd-image-variations-diffusers\\', \\'api_call\\': \"StableDiffusionImageVariationPipeline.from_pretrained(\\'lambdalabs/sd-image-variations-diffusers\\', revision=\\'v2.0\\')\", \\'api_arguments\\': {\\'revision\\': \\'v2.0\\'}, \\'python_environment_requirements\\': \\'Diffusers >=0.8.0\\', \\'example_code\\': \\'from diffusers import StableDiffusionImageVariationPipeline\\\\nfrom PIL import Image\\\\ndevice = cuda:0\\\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\\\n lambdalabs/sd-image-variations-diffusers,\\\\n revision=v2.0,\\\\n)\\\\nsd_pipe = sd_pipe.to(device)\\\\nim = Image.open(path/to/image.jpg)\\\\ntform = transforms.Compose([\\\\n transforms.ToTensor(),\\\\n transforms.Resize(\\\\n  (224, 224),\\\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\\\n  antialias=False,\\\\n ),\\\\n transforms.Normalize(\\\\n  [0.48145466, 0.4578275, 0.40821073],\\\\n  [0.26862954, 0.26130258, 0.27577711]),\\\\n])\\\\ninp = tform(im).to(device).unsqueeze(0)\\\\nout = sd_pipe(inp, guidance_scale=3)\\\\nout[images][0].save(result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'ChristophSchuhmann/improved_aesthetics_6plus\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\\'}', metadata={})]", "category": "generic"}
{"question_id": 215, "text": " I am a developer programming an autonomous robot. It needs to estimate depth from a single image to navigate successfully.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'shi-labs/oneformer_ade20k_swin_tiny\\', \\'api_call\\': \"\\'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\'\", \\'api_arguments\\': {\\'images\\': \\'image\\', \\'task_inputs\\': [\\'semantic\\', \\'instance\\', \\'panoptic\\'], \\'return_tensors\\': \\'pt\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\n\\\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\\\ninstance_outputs = model(**instance_inputs)\\\\n\\\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\\\n\\\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\\\npanoptic_outputs = model(**panoptic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\', \\'performance\\': {\\'dataset\\': \\'ADE20k\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 216, "text": " Our customer wants to use this model in their factory for an autonomous mobile robot application to estimate depth from images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/mask2former-swin-tiny-coco-instance\\', \\'api_call\\': \"Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-tiny-coco-instance\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'facebook/mask2former-swin-tiny-coco-instance\\'}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \"processor = AutoImageProcessor.from_pretrained(\\'facebook/mask2former-swin-tiny-coco-instance\\')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-tiny-coco-instance\\')\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result[\\'segmentation\\']\", \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 217, "text": " We need to create a system that can analyze images and predict their content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 218, "text": " Our customer is an animal shelter, and they frequently receive pictures of animals and food. We need to classify these images appropriately.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 219, "text": " A grocery store requested a solution to classify beans based on images taken through their inventory management system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 220, "text": " We need to detect the primary object in an image located at: \\\"http://images.cocodataset.org/val2017/000000039769.jpg\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'anomaly-detection\\', \\'api_name\\': \\'keras-io/timeseries-anomaly-detection\\', \\'api_call\\': \"TFAutoModelForSequenceClassification.from_pretrained(\\'keras-io/timeseries-anomaly-detection\\')\", \\'api_arguments\\': {\\'optimizer\\': {\\'name\\': \\'Adam\\', \\'learning_rate\\': 0.001, \\'decay\\': 0.0, \\'beta_1\\': 0.9, \\'beta_2\\': 0.999, \\'epsilon\\': 1e-07, \\'amsgrad\\': False}, \\'training_precision\\': \\'float32\\'}, \\'python_environment_requirements\\': [\\'tensorflow\\', \\'keras\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Numenta Anomaly Benchmark(NAB)\\', \\'accuracy\\': {\\'Train Loss\\': 0.006, \\'Validation Loss\\': 0.008}}, \\'description\\': \\'This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.\\'}', metadata={})]", "category": "generic"}
{"question_id": 221, "text": " We need a classifier to predict the category of items being processed at a food production line.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'impira/layoutlm-invoices\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'impira/layoutlm-invoices\\')\", \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"qa_pipeline(question=\\'your question\\', context=\\'your document context\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'not provided\\'}, \\'description\\': \\'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 222, "text": " Develop a bot with the capability to understand and segment objects and backgrounds within images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'facebook/opt-13b\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'facebook/opt-13b\\', torch_dtype=torch.float16).cuda().generate(input_ids)\", \\'api_arguments\\': [\\'input_ids\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-13b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-13b, use_fast=False)\\\\nprompt = Hello, I\\'m am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': \\'GPT-3\\', \\'accuracy\\': \\'roughly match the performance and sizes of the GPT-3 class of models\\'}, \\'description\\': \\'OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The models are trained to match the performance and sizes of the GPT-3 class of models. The primary goal is to enable reproducible and responsible research at scale and to bring more voices to the table in studying the impact of large language models. OPT-13B is a 13-billion-parameter model trained predominantly with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\\'}', metadata={})]", "category": "generic"}
{"question_id": 223, "text": " Our team at the architecture firm is trying to analyze the urban design of a city. We need a software that helps us in segmenting urban images into more manageable sections.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 224, "text": " Let's build a computer vision application that takes any image and then extracts the segmented objects from the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 225, "text": " Our client is a construction firm that need to extract the building plans to annotate their designs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'https://github.com/neonbjb/ocotillo\\', \\'performance\\': {\\'dataset\\': \\'librispeech validation set\\', \\'accuracy\\': \\'4.45%\\'}, \\'description\\': \\'This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 226, "text": " I have this idea to use GPT to generate a text while visualizing them at the same time. We already have ControlNet and DDPM separately. We now need to synthesize the image based on the text given by GPT.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 227, "text": " We need to produce realistic images for a fashion catalog.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 228, "text": " Imagine we are a company providing virtual church tours. We need to create a realistic image of a church that can be used for virtual tours.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-photoreal-2.0\\', \\'api_call\\': \"Output: StableDiffusionPipeline.from_pretrained(\\'dreamlike-art/dreamlike-photoreal-2.0\\', torch_dtype=torch.float16)(prompt).images[0]\", \\'api_arguments\\': {\\'prompt\\': \\'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\'}, \\'python_environment_requirements\\': {\\'torch\\': \\'torch.float16\\', \\'diffusers\\': \\'StableDiffusionPipeline\\'}, \\'example_code\\': \\'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'Stable Diffusion 1.5\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 229, "text": " We are making a visual content generator for a website requiring natural-looking images of buildings. Find a way to create images of churches.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 230, "text": " Generate a new Minecraft character's skin design.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Diffusers\\', \\'api_name\\': \\'WiNE-iNEFF/Minecraft-Skin-Diffusion-V2\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion-V2\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion-V2\\')\\\\nimage = pipeline().images[0].convert(\\'RGBA\\')\\\\nimage\", \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'An unconditional image generation model for generating Minecraft skin images using the diffusion model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 231, "text": " We are developing a nature educational app. We need to generate images of butterflies while preserving the artistic representation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 232, "text": " Detect the type of vehicles in the parking lot.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 233, "text": " I want to classify the location of an image by giving it a list of city names.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 234, "text": " We are building a content moderation bot that detects if an article or comment is computer-generated or produced by a human. We need to test it with the text \\\"Hello world! Is this content AI-generated?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Detect GPT-2 generated text\\', \\'api_name\\': \\'roberta-base-openai-detector\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'roberta-base-openai-detector\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))\\', \\'performance\\': {\\'dataset\\': \\'WebText\\', \\'accuracy\\': \\'95%\\'}, \\'description\\': \\'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 235, "text": " Our company wants to analyze the sentiment of restaurant reviews on Yelp to improve its recommendation system. We need to classify the reviews as positive or negative.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 236, "text": " Help me identify the named entities in a text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 237, "text": " Being a biology student, I am finding it difficult to get answers to biology-related questions. How can I get those answers?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 238, "text": " Create a tool to help users quickly find information in a long document about their favorite sport or hobbies.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 239, "text": " How can I use a model to understand when a soccer match was played in a paragraph based on the provided information?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 240, "text": " I need a moderation system for comments in our community app, it should be able to classify comments as harmful, friend, or advertisement.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Inferencing for stock-related comments\\', \\'api_name\\': \\'zhayunduo/roberta-base-stocktwits-finetuned\\', \\'api_call\\': \"Output: RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\", \\'api_arguments\\': {\\'model\\': \\'RobertaForSequenceClassification\\', \\'tokenizer\\': \\'RobertaTokenizer\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\\\nfrom transformers import pipeline\\\\nimport pandas as pd\\\\nimport emoji\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\\\nsentences = pd.Series([\\'just buy\\',\\'just sell it\\',\\'entity rocket to the sky!\\',\\'go down\\',\\'even though it is going up, I still think it will not keep this trend in the near future\\'])\\\\nsentences = list(sentences)\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'stocktwits\\', \\'accuracy\\': 0.9343}, \\'description\\': \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags \\'Bullish\\' or \\'Bearish\\'.\"}', metadata={})]", "category": "generic"}
{"question_id": 241, "text": " Analyze the given Russian legal document, and output the translated version in English.\\n###Input: \\u0420\\u043e\\u0441\\u0441\\u0438\\u044f \\u0433\\u0430\\u0440\\u0430\\u043d\\u0442\\u0438\\u0440\\u0443\\u0435\\u0442 \\u043f\\u0440\\u0430\\u0432\\u0430 \\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\\u0430 \\u0438 \\u0441\\u0442\\u0440\\u0435\\u043c\\u0438\\u0442\\u0441\\u044f \\u0441\\u043e\\u0437\\u0434\\u0430\\u0442\\u044c \\u0435\\u0434\\u0438\\u043d\\u044b\\u0439 \\u0434\\u043b\\u044f \\u0432\\u0441\\u0435\\u0445 \\u044e\\u0440\\u0438\\u0434\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0444\\u043e\\u043d\\u0434. \\u042d\\u0442\\u043e \\u0431\\u0430\\u0437\\u0438\\u0440\\u0443\\u0435\\u0442\\u0441\\u044f \\u043d\\u0430 \\u043f\\u0440\\u0438\\u043d\\u0446\\u0438\\u043f\\u0430\\u0445 \\u043c\\u0435\\u0436\\u0434\\u0443\\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u043e\\u0433\\u043e \\u0443\\u0440\\u043e\\u0432\\u043d\\u044f. \\u0417\\u0430\\u043a\\u043e\\u043d \\u0434\\u043e\\u043b\\u0436\\u0435\\u043d \\u0431\\u044b\\u0442\\u044c \\u0440\\u0430\\u0432\\u043d\\u044b\\u043c \\u0434\\u043b\\u044f \\u0432\\u0441\\u0435\\u0445 \\u0433\\u0440\\u0430\\u0436\\u0434\\u0430\\u043d.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'nlpaueb/legal-bert-small-uncased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nlpaueb/legal-bert-small-uncased\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModel\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\\\\nmodel = AutoModel.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\", \\'performance\\': {\\'dataset\\': \\'Legal Corpora\\', \\'accuracy\\': \\'Comparable to larger models\\'}, \\'description\\': \\'LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.\\'}', metadata={})]", "category": "generic"}
{"question_id": 242, "text": " Operating a media company studio, we gather hundreds of articles daily that need to be summarized.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 243, "text": " Our customer is a busy executive in the entertainment industry. They need to stay informed by regularly receiving news summaries.\\n###Input: \\\"Today, Warner Bros. announced the release date for the upcoming movie in the Harry Potter spinoff series, Fantastic Beasts and Where to Find Them 3. The long-awaited sequel is set to hit theaters on April 15, 2024. Eddie Redmayne, who stars as Newt Scamander, will reprise his role in the upcoming film. Fantastic Beasts 3 will be compared to the original series given its popularity among avid fans. The movie will follow Newt and his friends on a new adventure, with several returning cast members, including Katherine Waterston and Dan Fogler. Director David Yates, who has taken the helm for the previous entries in the series, will return for Fantastic Beasts 3 as well.\\\"\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'PegasusForConditionalGeneration.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 244, "text": " You are designing a chatbot for a taxi booking platform. Generate a response for a user asking about the taxi fare estimation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-3B\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-3B\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-3B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It is trained on the blended_skill_talk dataset and can engage in multi-turn conversations, providing engaging talking points, asking and answering questions, and displaying knowledge, empathy, and personality. The model is available through the Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 245, "text": " We need to write a short story about a lonely astronaut lost in deep space.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = stabilityai/stable-diffusion-2-base\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.'}\", metadata={})]", "category": "generic"}
{"question_id": 246, "text": " Can you help me create a model that generates a piece of text about climate change?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 247, "text": " Our company is working on a project that requires the development of a tool to synthesize code based on natural language instructions.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'model.generate(input_ids, max_length=128)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}\", metadata={})]", "category": "generic"}
{"question_id": 248, "text": " Write a script to continue the story 'Once upon a time in a small village, a young girl by the name of Alice set out to find the enchanted tree.'\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 249, "text": " Our language chatbot should interpret intents and respond to user commands. First, we want the model to translate the following text from French to English: \\\"Je taime.\\\"\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'punctuation prediction', 'api_name': 'oliverguhr/fullstop-punctuation-multilang-large', 'api_call': 'Output: PunctuationModel()', 'api_arguments': ['text'], 'python_environment_requirements': ['pip install deepmultilingualpunctuation'], 'example_code': 'from deepmultilingualpunctuation import PunctuationModel\\\\nmodel = PunctuationModel()\\\\ntext = My name is Clara and I live in Berkeley California Ist das eine Frage Frau M\u00fcller\\\\nresult = model.restore_punctuation(text)\\\\nprint(result)', 'performance': {'dataset': 'wmt/europarl', 'EN_accuracy': 0.775, 'DE_accuracy': 0.814, 'FR_accuracy': 0.782, 'IT_accuracy': 0.762}, 'description': 'This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :.'}\", metadata={})]", "category": "generic"}
{"question_id": 250, "text": " User posted reviews need to be filtered for grammar and correctness before displaying them on our website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 251, "text": " We are developing an AI-based tutor application, please suggest a solution that quickly completes a sentence containing a missing word for helping students improve vocabulary.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 252, "text": " I am an autonomous vehicle developer, and I often exchange messages with my international colleagues. Can you help me generate a fill-in-the-blank sentence about autonomous vehicles?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 253, "text": " Design an AI tool that supports the process of identifying medical terms in a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 254, "text": " I want to find the best word to fill the mask in this Japanese text: \\\"\\u4eca\\u65e5\\u306f[MASK]\\u3067\\u3059\\u3002\\\"\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 255, "text": " As a writer, check the dictionary for aandidate word to make the Dutch sentence meaningful.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\')\", \\'api_arguments\\': {\\'model_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.13.0\\', \\'torch\\': \\'1.10.0\\', \\'datasets\\': \\'1.14.0\\'}, \\'example_code\\': \\'processor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\ninput_values = processor(ds[0][audio][array], return_tensors=pt).input_values\\\\nwith torch.no_grad():\\\\n    logits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words.\\'}', metadata={})]", "category": "generic"}
{"question_id": 256, "text": " I want to help my users find similar news articles using the best sentence transformer to convert sentences to a 384-dimensional vector space.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 257, "text": " Determine if two sentences have similar meanings to create a summary of a long text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 258, "text": " Recommend me a solution to find a solution to semantically group articles into several topics on a multilingual news website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 259, "text": " Recommend me a sentence from a given list that is the most similar to a question I am going to provide you.\\n###Input: {\\\"query\\\": \\\"What is the capital city of France?\\\", \\\"docs\\\": [\\\"The capital of France is Paris.\\\", \\\"French cuisine is famous worldwide.\\\", \\\"The Eiffel Tower is an iconic landmark in France.\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 260, "text": " We are building an online language learning platform. Implement a feature to convert phrases to spoken audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 261, "text": " A software house needs an AI-powered narrator for narrating their audiobooks. We need a single-speaker model with a female voice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 262, "text": " Create a story-based audiobook in the Korean language from a given script.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Feature Extraction\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'kobart-base-v2\\', \\'api_call\\': \"BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\"}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'tokenizers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import PreTrainedTokenizerFast, BartModel\\\\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\\\\nmodel = BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'performance\\': {\\'dataset\\': \\'NSMC\\', \\'accuracy\\': 0.901}, \\'description\\': \\'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 263, "text": " The company is developing an app that reads books to its users. We need to convert text to speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 264, "text": " Our customer wants to transcribe recorded speeches given by famous Japanese influencers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 265, "text": " Extract the Chinese text from the provided audio file to create subtitles for a video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 266, "text": " I am working on a project that requires transcribing Esperanto speech, and I need a reliable automatic speech recognition model.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 267, "text": " Develop a solution to separate the audio of two speakers speaking simultaneously in a conversation.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 268, "text": " We need to enhance the audio quality in our newly developed language translation software.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 269, "text": " We are developing a call center software and our system needs to process audio files to increase the quality for each call. Can you suggest a model?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 270, "text": " We need to add an ambient noise management system to our efficient call center. Users consistently complain about excessive noise next to the operator on our sales team.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 271, "text": " One of my clients is from Spain and talks to me in Spanish. I need to classify the sentiment of the audio file that contains his spoken message.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 272, "text": " We are working on a project to predict potential customer churn based on customer data. Implement a model that can handle various feature types.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 273, "text": " Please help me prepare a recruitment test where candidates predict if a person makes over 50k a year based on several factors such as age, education, and work experience.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 274, "text": " A real estate developer needs a program to estimate the housing prices based on the data provided in the 'data.csv' file, using the pre-trained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'pygmalion-6b\\', \\'api_call\\': \"Output: model = AutoModel.from_pretrained(\\'uft-6b\\')\", \\'api_arguments\\': [\\'input_prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"[CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]\\\\n<START>\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI\\'s GPT-J-6B. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model was initialized from the uft-6b ConvoGPT model and fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.\"}', metadata={})]", "category": "generic"}
{"question_id": 275, "text": " We are working on reducing carbon emissions in our factory, and we need to have a prediction on how much we would save with some changes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 276, "text": " We are designing an application to predict the carbon emissions of various activities carried out in a city. We need to handle this data and provide the prediction.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 277, "text": " The company is building a real estate investment platform where the user will provide information about the real estate, and we need to predict the price of the house.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 278, "text": " I want an artificial intelligence to help me land a simulated lunar lander.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 279, "text": " We are trying to build an AI-powered sports management platform. We need to train our agents to play soccer and analyze soccer games.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 280, "text": " We're developing a game and we want to train an AI player to perform well in the Gym Hopper environment. Tell me a trained model that can solve this problem.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 281, "text": " I want to build a game, where players play football (soccer) as a two-player team. I want to train an agent for this game.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 282, "text": " In my robotics class, we are learning to play soccer twos. Would you introduce me to a pre-trained model in that game?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Inference API\\', \\'api_name\\': \\'Antheia/Hanna\\', \\'api_call\\': \"pipeline(\\'robotics\\', model=\\'Antheia/Hanna\\') should remain unchanged as it already only includes the model instantiation.\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'openai/webgpt_comparisons\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 283, "text": " We need to process medical research texts and extract important features to further analyze them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 284, "text": " I am writing a children's story where the girl learns how to fly, and I need an illustration for it.\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=BartForConditionalGeneration.from_pretrained(\\'philschmid/bart-large-cnn-samsum\\'))\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 285, "text": " Can you create a creative image of a snowy forest using the 'wavymulder/Analog-Diffusion' API?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 286, "text": " We are creating an app for translating Japanese manga to English. We need to extract Japanese text from manga images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 287, "text": " We need to generate captions for images using both unconditional and conditional captioning methods.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Captioning\\', \\'api_name\\': \\'nlpconnect/vit-gpt2-image-captioning\\', \\'api_call\\': \\'VisionEncoderDecoderModel.from_pretrained(\\\\\\\\nlpconnect/vit-gpt2-image-captioning\\\\\\\\)\\', \\'api_arguments\\': {\\'model\\': \\'nlpconnect/vit-gpt2-image-captioning\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'PIL\\'], \\'example_code\\': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\\\nimport torch\\\\nfrom PIL import Image\\\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\nmodel.to(device)\\\\nmax_length = 16\\\\nnum_beams = 4\\\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\\\ndef predict_step(image_paths):\\\\n images = []\\\\n for image_path in image_paths:\\\\n i_image = Image.open(image_path)\\\\n if i_image.mode != RGB:\\\\n i_image = i_image.convert(mode=RGB)\\\\nimages.append(i_image)\\\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\\\n pixel_values = pixel_values.to(device)\\\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\\\n preds = [pred.strip() for pred in preds]\\\\n return preds\\\\npredict_step([\\'doctor.e16ba4e4.jpg\\']) # [\\'a woman in a hospital bed with a woman in a hospital bed\\']\", \\'performance\\': {\\'dataset\\': \\'Not provided\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.\\'}', metadata={})]", "category": "generic"}
{"question_id": 288, "text": " Our gaming company has a story with great visuals. For marketing, we want some exciting captions for the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 289, "text": " A visual designer is building a professional website and wants to know if the website layout has important guidelines missing. Please check using AI technology.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 290, "text": " Our team is developing software to extract text from images like a signboard or street signs for an automotive client.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/git-base-textvqa\\', \\'api_call\\': \"git_base_textvqa = AutoModel.from_pretrained(\\'microsoft/git-base-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa_pipeline({\\'image\\': \\'path/to/image.jpg\\', \\'question\\': \\'What is in the image?\\'})\", \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'Refer to the paper\\'}, \\'description\\': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}', metadata={})]", "category": "generic"}
{"question_id": 291, "text": " Create a video of a dog playing on a beach using text-to-video generation.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': 'DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)', 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = Spiderman is surfing\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.'}\", metadata={})]", "category": "generic"}
{"question_id": 292, "text": " As a pet owner, I recently took a photo of my pets and want to know how many dogs are in the picture I just captured.\\n###Input: {\\\"raw_image\\\": \\\"https://example.com/my_pets_image.jpg\\\", \\\"question\\\": \\\"How many dogs are in the picture?\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'Salesforce/blip-vqa-capfilt-large\\', \\'api_call\\': \\'BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\', \\'api_arguments\\': {\\'raw_image\\': \\'RGB image\\', \\'question\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'BlipProcessor, BlipForQuestionAnswering\\'}, \\'example_code\\': \"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\\\nimg_url = \\'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\\'RGB\\')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \\'performance\\': {\\'dataset\\': \\'VQA\\', \\'accuracy\\': \\'+1.6% in VQA score\\'}, \\'description\\': \\'BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.\\'}', metadata={})]", "category": "generic"}
{"question_id": 293, "text": " For the new smart home assistant, create a way to receive questions about objects from an image and provide answers based on the image contents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 294, "text": " Suppose I am using an application to process scanned documents to answer questions. How would you recommend a model to help me?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-cased-distilled-squad\\', \\'api_call\\': \"DistilBertForQuestionAnswering.from_pretrained(\\'distilbert-base-cased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-cased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': {\\'Exact Match\\': 79.6, \\'F1\\': 86.996}}, \\'description\\': \"DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.\"}', metadata={})]", "category": "generic"}
{"question_id": 295, "text": " A student is reading a textbook and needs help answering questions based on the material. Instruct the model to help by answering the questions.\\n###Input: {\\n  \\\"question\\\": \\\"What is the Pythagorean theorem?\\\",\\n  \\\"context\\\": \\\"The Pythagorean theorem is a fundamental relation in Euclidean geometry between the three sides of a right triangle. It states that the square of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides. The theorem can be written as an equation relating the lengths of the sides a, b, and c, often called the Pythagorean equation: a^2 + b^2 = c^2.\\\"\\n}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 296, "text": " We need to estimate depth in our self-driving car system. The model should return the depth_map in meters for the given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 297, "text": " You are an AI-powered personal assistant, and your owner asks you: \\\"What are the breeds of these two cats in the pictures?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 298, "text": " Develop a model that can predict a person's age based on their facial image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Age Classification\\', \\'api_name\\': \\'nateraw/vit-age-classifier\\', \\'api_call\\': \"ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nateraw/vit-age-classifier\\'}, \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'transformers\\'], \\'example_code\\': \"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get(\\'https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true\\')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\ntransforms = ViTFeatureExtractor.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\n\\\\ninputs = transforms(im, return_tensors=\\'pt\\')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\", \\'performance\\': {\\'dataset\\': \\'fairface\\', \\'accuracy\\': None}, \\'description\\': \"A vision transformer finetuned to classify the age of a given person\\'s face.\"}', metadata={})]", "category": "generic"}
{"question_id": 299, "text": " I am building an AI-based photo management software that automatically categorizes images into predefined categories. Can you help me classify them based on their content?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 300, "text": " Take a picture of a dessert and create a system that can identify the type of dessert it is.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 301, "text": " We are a book scanning service that wants to extract tables from scanned pages. Can you suggest a model that can detect table areas in an image?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 302, "text": " I would like to create a system that can detect and identify objects in images. Please provide assistance in doing so.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-csgo-player-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-csgo-player-detection\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-csgo-player-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'csgo-object-detection\\', \\'accuracy\\': 0.892}, \\'description\\': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify \\'ct\\', \\'cthead\\', \\'t\\', and \\'thead\\' labels.\"}', metadata={})]", "category": "generic"}
{"question_id": 303, "text": " Our company needs a robot to help extract tables from artworks. Please help install this api to let the robot finish the work of extracting tables from artworks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 304, "text": " We have a website streaming eSports, we are building a feature to find the players in the frame and tag the players with the team that they belong to.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 305, "text": " We are developing an automated drone for monitoring forest areas. We need to detect wild animals in images taken by the drone.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 306, "text": " Our company is developing an AI-powered photo editor, and we need to integrate a feature that can segment objects in the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 307, "text": " I need a system that can analyze images of a street and identify different objects or structures in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 308, "text": " We are a company providing machine control for electronic PCB manufacturers. We want to inspect and detect defects in PCB boards automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8s-pcb-defect-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8s-pcb-defect-segmentation\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8s-pcb-defect-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pcb-defect-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.515, \\'mAP@0.5(mask)\\': 0.491}}, \\'description\\': \\'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\\'}', metadata={})]", "category": "generic"}
{"question_id": 309, "text": " I need to estimate the pose of a person in a picture with a chef cooking in the kitchen and save it as an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 310, "text": " Please assist me with extracting information about objects in a crowded street scene.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\', \\'api_call\\': \\'model.encode(text)\\', \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'sentence-transformers library\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\\\\ntext = Replace me by any text you\\'d like.\\\\ntext_embbedding = model.encode(text)\", \\'performance\\': {\\'dataset\\': \\'1,097,953,922\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 311, "text": " As a video game developer, our current project involves enhancing the resolution of low-quality images in our game. Help us with this task by using image super-resolution techniques to upscale the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 312, "text": " Design a program to enhance the resolution of an image for our home theater system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 313, "text": " Our customer has an image of a drawing, and they want the AI to imagine and produce a colorful version of the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'impira/layoutlm-invoices\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'impira/layoutlm-invoices\\')\", \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"qa_pipeline(question=\\'your question\\', context=\\'your document context\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'not provided\\'}, \\'description\\': \\'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 314, "text": " Build a digital artwork generator that produces pictures of butterflies that we can use for our chemistry lab as decoration.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 315, "text": " As a movie's visual effects designer, we ought to create a futuristic scene for our upcoming film project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 316, "text": " The company wants to build an AI art installation that generates realistic human faces. We need to use a generative model to create new images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'saltacc/anime-ai-detect\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'saltacc/anime-ai-detect\\')\", \\'api_arguments\\': [\\'image\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'aibooru and imageboard sites\\', \\'accuracy\\': \\'96%\\'}, \\'description\\': \\'A BEiT classifier to see if anime art was made by an AI or a human.\\'}', metadata={})]", "category": "generic"}
{"question_id": 317, "text": " Design an application that can recognize activities in videos and provide useful descriptions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 318, "text": " We're planning a butterfly-themed event and we need a collection of diverse butterfly images. Generate some images for us.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Paraphrasing\\', \\'api_name\\': \\'prithivida/parrot_paraphraser_on_T5\\', \\'api_call\\': \"\\'Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\'\", \\'api_arguments\\': [\\'input_phrase\\', \\'diversity_ranker\\', \\'do_diverse\\', \\'max_return_phrases\\', \\'max_length\\', \\'adequacy_threshold\\', \\'fluency_threshold\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \\'from parrot import Parrot\\\\nimport torch\\\\nimport warnings\\\\nwarnings.filterwarnings(ignore)\\\\n\\\\nparrot = Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\\\nphrases = [Can you recommed some upscale restaurants in Newyork?,\\\\n What are the famous places we should not miss in Russia?\\\\n]\\\\nfor phrase in phrases:\\\\n print(-*100)\\\\n print(Input_phrase: , phrase)\\\\n print(-*100)\\\\n para_phrases = parrot.augment(input_phrase=phrase)\\\\n for para_phrase in para_phrases:\\\\n  print(para_phrase)\\', \\'performance\\': {\\'dataset\\': \\'Not mentioned\\', \\'accuracy\\': \\'Not mentioned\\'}, \\'description\\': \\'Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 319, "text": " Our company is developing an advertising platform, and we need a tool to analyze video ads' content to ensure they comply with our guidelines.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 320, "text": " Automatically categorize videos in a folder to help organize them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 321, "text": " As a part of our new physical fitness startup, we need to classify workout videos into different exercise types.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 322, "text": " Our security team wants to classify actions in monitored videos. We are looking for a model pretrained on a large dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 323, "text": " I want to find out what species of animal is in a given photograph.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 324, "text": " Can you identify objects in the photos on my phone? I would like to categorize the photos into pets, vehicles, and landmarks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 325, "text": " As a social media monitoring company, we want to analyze images posted by users and classify their content. Identify what is in the given image.\\n###Input: \\\"path_to_image\\\": \\\"/path/to/specific/image.jpg\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 326, "text": " A friend of ours is starting a business that categorizes images of toys and wants to know what type of toy is in a specific image.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.'}\", metadata={})]", "category": "generic"}
{"question_id": 327, "text": " I would like to classify the sentiments of Twitter users about a stock.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 328, "text": " We are an online review aggregator and need to analyze the sentiment of user-generated reviews for better product recommendations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'bert-base-multilingual-uncased-sentiment\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'nlptown/bert-base-multilingual-uncased-sentiment\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"result = sentiment_pipeline(\\'I love this product!\\')\", \\'performance\\': {\\'dataset\\': [{\\'language\\': \\'English\\', \\'accuracy\\': {\\'exact\\': \\'67%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Dutch\\', \\'accuracy\\': {\\'exact\\': \\'57%\\', \\'off-by-1\\': \\'93%\\'}}, {\\'language\\': \\'German\\', \\'accuracy\\': {\\'exact\\': \\'61%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'French\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'Italian\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Spanish\\', \\'accuracy\\': {\\'exact\\': \\'58%\\', \\'off-by-1\\': \\'95%\\'}}]}, \\'description\\': \\'This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\\'}', metadata={})]", "category": "generic"}
{"question_id": 329, "text": " Please help me understand names and places in a sentence from an email that I received.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 330, "text": " A French company is searching for a model to identify organizations and individuals mentioned in news articles. We need to present them with the appropriate tool.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 331, "text": " In search of a way to identify proper nouns and words from a text in Chinese for my language analyzing company.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 332, "text": " As an urban planner, we need to answer questions about city characteristics based on a table containing urban data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\')\", \\'api_arguments\\': {\\'model\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'tokenizer\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': \\'vqa(image_path, question)\\', \\'performance\\': {\\'dataset\\': \\'JosephusCheung/GuanacoVQADataset\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 333, "text": " Identify a way to answer the quarterly sales from the available table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 334, "text": " We have a list of employees' names, departments, and salaries in a table format. We need to find the average salary for each department.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 335, "text": " The principal of a school is analyzing the performance of students in each department. He is interested in finding the department with the highest average marks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 336, "text": " A company executive has requested a report on Olympic host cities for various years. Generate an answer from the Olympiad records.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 337, "text": " We have data from our last meeting and want to answer a question about a specific value from that data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 338, "text": " We are working on a new product that is a language tool for researchers who need to quickly find answers related to any topic.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 339, "text": " I would like to know when the game was played. Given a context about a game, extract the date it was played.\\n###Input: {\\\"context\\\": \\\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\", \\\"question\\\": \\\"What day was the game played on?\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'csarron/bert-base-uncased-squad-v1\\', tokenizer=\\'csarron/bert-base-uncased-squad-v1\\')\", \\'api_arguments\\': {\\'model\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'tokenizer\\': \\'csarron/bert-base-uncased-squad-v1\\'}, \\'python_environment_requirements\\': \\'Python 3.7.5\\', \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\n question-answering,\\\\n model=csarron/bert-base-uncased-squad-v1,\\\\n tokenizer=csarron/bert-base-uncased-squad-v1\\\\n)\\\\npredictions = qa_pipeline({\\\\n \\'context\\': The game was played on February 7, 2016 at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California.,\\\\n \\'question\\': What day was the game played on?\\\\n})\\\\nprint(predictions)\", \\'performance\\': {\\'dataset\\': \\'SQuAD1.1\\', \\'accuracy\\': {\\'EM\\': 80.9, \\'F1\\': 88.2}}, \\'description\\': \\'BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 340, "text": " We have a blog post platform, and we want to categorize each blog post under one of the pre-defined categories: \\\"Technology\\\", \\\"Fashion\\\", \\\"Health\\\", or \\\"Travel\\\".\\n###Input: \\\"Google is planning to launch its augmented reality glasses next year. These high-tech glasses are set to revolutionize the way we interact with the world around us, merging the digital world with reality.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 341, "text": " We need to analyze a recent news message to know which category it belongs to: technology, sports or politics.\\n###Input: Apple just announced the newest iPhone X.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Cross-Encoder for Natural Language Inference\\', \\'api_name\\': \\'cross-encoder/nli-distilroberta-base\\', \\'api_call\\': \"CrossEncoder(\\'cross-encoder/nli-distilroberta-base\\')\", \\'api_arguments\\': \"(\\'A man is eating pizza\\', \\'A man eats something\\')\", \\'python_environment_requirements\\': [\\'sentence_transformers\\', \\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=\\'cross-encoder/nli-distilroberta-base\\')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\", \\'performance\\': {\\'dataset\\': \\'SNLI and MultiNLI\\', \\'accuracy\\': \\'See SBERT.net - Pretrained Cross-Encoder for evaluation results\\'}, \\'description\\': \\'This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 342, "text": " We received an email about our new app launch. Classify the sentiment of the email as 'positive', 'negative' or 'neutral'.\\n###Input: \\\"I'm so excited about the new app you just launched! The interface is clean and incredibly user-friendly.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 343, "text": " I need to translate Dutch food recipes for my American friends.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 344, "text": " I need to create a system to quickly translate emails between French and Spanish coworkers in my company.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 345, "text": " We are a news platform that is focusing on multi-lingual. The platform will analyze the conversation between multiple people, so the summarizers should be able to handle this efficiently.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 346, "text": " Our team is developing a chatbot for customer service. We need a chatbot that can engage in a conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-3B\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-3B\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-3B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It is trained on the blended_skill_talk dataset and can engage in multi-turn conversations, providing engaging talking points, asking and answering questions, and displaying knowledge, empathy, and personality. The model is available through the Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 347, "text": " Generate a story that starts with \\\"Once upon a time in a small village\\\".\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 348, "text": " We are building a translation service that translates user inputs from English to German. Can you suggest an API for this task?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 349, "text": " A user wants to translate a sentence from English to French. Give an example of how the user can do this using the given model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 350, "text": " An AI researcher wants to use a new model to automatically complete a sentence. He knows that the model should predict how the company can be perceived.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 351, "text": " Complete the following sentence in Dutch: \\\"Mijn favoriete eten is [MASK] omdat het erg lekker is.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/ner-english\\')\", \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load(\\'flair/ner-english\\')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence(\\'George Washington went to Washington\\')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint(\\'The following NER tags are found:\\')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'conll2003\\', \\'accuracy\\': \\'93.06\\'}, \\'description\\': \\'This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 352, "text": " A law firm wants to automate the completion of some legal texts. They need to fill in the blanks with relevant terms.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'nlpaueb/legal-bert-small-uncased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nlpaueb/legal-bert-small-uncased\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModel\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\\\\nmodel = AutoModel.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\", \\'performance\\': {\\'dataset\\': \\'Legal Corpora\\', \\'accuracy\\': \\'Comparable to larger models\\'}, \\'description\\': \\'LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.\\'}', metadata={})]", "category": "generic"}
{"question_id": 353, "text": " I have an important presentation in a foreign language but I need the translation to be done fast. Please help me translate my speech.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 354, "text": " We are building an audio guide application for tourist attractions in Japan. Provide various useful information about popular tourists spots in Japanese audio.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 355, "text": " Design a tool for the hearing impaired students that will aid in converting their teacher's lectures into a text format.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 356, "text": " Help me build a Chinese audio transcription tool for people who are deaf, so that they can convert audio to text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 357, "text": " Develop a tool to convert one language's speech to another language's speech as a part of a communication program.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 358, "text": " A podcast studio wants to correct their audio recordings with overlapping speakers. We need to help them separate the speaker's voices from the mixed audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 359, "text": " Need a solution to improve the quality of noisy speech samples with background noise and other disturbances.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Enhancement\\', \\'api_name\\': \\'speechbrain/sepformer-whamr-enhancement\\', \\'api_call\\': \"separator.from_hparams(source=speechbrain/sepformer-whamr-enhancement, savedir=\\'pretrained_models/sepformer-whamr-enhancement\\').separate_file(path=\\'speechbrain/sepformer-whamr-enhancement/example_whamr.wav\\')\", \\'api_arguments\\': {\\'path\\': \\'Path to the input audio file.\\'}, \\'python_environment_requirements\\': \\'pip install speechbrain\\', \\'example_code\\': \"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source=speechbrain/sepformer-whamr-enhancement, savedir=\\'pretrained_models/sepformer-whamr-enhancement\\')\\\\nest_sources = model.separate_file(path=\\'speechbrain/sepformer-whamr-enhancement/example_whamr.wav\\')\\\\ntorchaudio.save(enhanced_whamr.wav, est_sources[:, :, 0].detach().cpu(), 8000)\", \\'performance\\': {\\'dataset\\': \\'WHAMR!\\', \\'accuracy\\': \\'10.59 dB SI-SNR\\'}, \\'description\\': \\'This repository provides all the necessary tools to perform speech enhancement (denoising + dereverberation) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAMR! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.\\'}', metadata={})]", "category": "generic"}
{"question_id": 360, "text": " I have an automatic onboarding app for newcomers in my company. It takes details of each new employee and plays a welcome message in their native language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 361, "text": " We are developing a phone app to evaluate bird species by their vocalizations. We want to find out the species using the sound.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 362, "text": " The company is aiming to create a product for detecting emotions in voice recordings. We need to classify emotions from voice inputs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Emotion Recognition\\', \\'api_name\\': \\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\')\", \\'api_arguments\\': \\'wav2vec2, tokenizer\\', \\'python_environment_requirements\\': \\'transformers 4.8.2, pytorch 1.9.0+cu102, datasets 1.9.0, tokenizers 0.10.3\\', \\'example_code\\': \\'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\', \\'performance\\': {\\'dataset\\': \\'RAVDESS\\', \\'accuracy\\': 0.8223}, \\'description\\': \"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = [\\'angry\\', \\'calm\\', \\'disgust\\', \\'fearful\\', \\'happy\\', \\'neutral\\', \\'sad\\', \\'surprised\\'].\"}', metadata={})]", "category": "generic"}
{"question_id": 363, "text": " As a movie streaming platform, we aim to recommend movies to users by predicting their sentiment towards the movie descriptions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 364, "text": " We are working on designing an app that will create a regression model to estimate the worth of a home.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 365, "text": " Our organization is working towards reducing carbon emissions from vehicles. Develop a solution for estimating carbon emissions given vehicle information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pcoloc/autotrain-only-rssi-1813762559\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': {\\'data\\': \\'data.csv\\'}, \\'python_environment_requirements\\': {\\'joblib\\': \\'latest\\', \\'pandas\\': \\'latest\\'}, \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': {\\'Loss\\': 83.432, \\'R2\\': 0.312, \\'MSE\\': 6960.888, \\'MAE\\': 60.449, \\'RMSLE\\': 0.532}}, \\'description\\': \\'A tabular regression model trained using AutoTrain for estimating carbon emissions from given features.\\'}', metadata={})]", "category": "generic"}
{"question_id": 366, "text": " We are running an environmental campaign, and we want to predict the CO2 emissions in grams for our next project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 367, "text": " Predict the closing price of a stock using historical data.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'PegasusForConditionalGeneration.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 368, "text": " I am coaching a youth soccer team and want to build an AI model to analyze and optimize my players' performance. Can you provide a suggestion?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 369, "text": " An environment is created for controlling Ant-v3 robot. We are providing robotic arm control for a humanoid robot in a virtual environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 370, "text": " I want to train a soccer player AI to play against other players using the SoccerTwos environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 371, "text": " We are developing a soccer game using Unity. Integrate a trained soccer agent to perform well in SoccerTwos.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 372, "text": " Teach me how to make the machine play an arcade game similar to Breakout using reinforcement learning.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Neural machine translation\\', \\'api_name\\': \\'opus-mt-tc-big-en-pt\\', \\'api_call\\': \"Output: MarianMTModel.from_pretrained(model_name).generate(**tokenizer(src_text, return_tensors=\\'pt\\', padding=True))\", \\'api_arguments\\': {\\'src_text\\': \\'list of text strings with language tokens\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.16.2\\'}, \\'example_code\\': \\'from transformers import MarianMTModel, MarianTokenizer\\\\nsrc_text = [\\\\n &gt;&gt;por&lt;&lt; Tom tried to stab me.,\\\\n &gt;&gt;por&lt;&lt; He has been to Hawaii several times.\\\\n]\\\\nmodel_name = pytorch-models/opus-mt-tc-big-en-pt\\\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\\\nmodel = MarianMTModel.from_pretrained(model_name)\\\\ntranslated = model.generate(**tokenizer(src_text, return_tensors=pt, padding=True))\\\\nfor t in translated:\\\\n print( tokenizer.decode(t, skip_special_tokens=True) )\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'flores101-devtest\\', \\'accuracy\\': 50.4}, {\\'name\\': \\'tatoeba-test-v2021-08-07\\', \\'accuracy\\': 49.6}]}, \\'description\\': \\'Neural machine translation model for translating from English (en) to Portuguese (pt). This model is part of the OPUS-MT project, an effort to make neural machine translation models widely available and accessible for many languages in the world.\\'}', metadata={})]", "category": "generic"}
{"question_id": 373, "text": " We are a medical research company, and we need to extract features from a complex medical text in English.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 374, "text": " Develop a content moderation system for a platform where users upload videos, and it's crucial to identify inappropriate content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Masked Language Modeling\\', \\'api_name\\': \\'bert-base-cased\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'bert-base-cased\\')\", \\'api_arguments\\': [\\'model\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': [\\'from transformers import pipeline\\', \"unmasker = pipeline(\\'fill-mask\\', model=\\'bert-base-cased\\')\", \"unmasker(Hello I\\'m a [MASK] model.)\"], \\'performance\\': {\\'dataset\\': \\'GLUE\\', \\'accuracy\\': 79.6}, \\'description\\': \"BERT base model (cased) is a pre-trained transformer model on English language using a masked language modeling (MLM) objective. It was introduced in a paper and first released in a repository. This model is case-sensitive, which means it can differentiate between \\'english\\' and \\'English\\'. The model can be used for masked language modeling or next sentence prediction, but it\\'s mainly intended to be fine-tuned on a downstream task.\"}', metadata={})]", "category": "generic"}
{"question_id": 375, "text": " I represent a company creating a virtual spokesperson for an ad campaign, and I desire photo-realistic images of a mascot eating breakfast. Provide generated images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 376, "text": " A software company needs a new wallpaper for their office. They are interested in an image of people working together in a futuristic office environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 377, "text": " We are a startup creating graphic design software. We need a function to generate artwork in an analog style based on user text prompts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 378, "text": " Develop a system that can provide a description of a photo with an optional condition.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-base-patch16\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-base-patch16\\')\", \\'api_arguments\\': [\\'texts\\', \\'images\\'], \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'torch\\', \\'transformers\\'], \\'example_code\\': \\'processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\\\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = [[a photo of a cat, a photo of a dog]]\\\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\\'}', metadata={})]", "category": "generic"}
{"question_id": 379, "text": " I am an art curator looking to provide engaging descriptions for paintings in our museum. Generate a description for a painting based on an image.\\n###Input: {\\\"url\\\": \\\"https://i.imgur.com/N01U6aN.jpg\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 380, "text": " For an advertisement, create a video of a new car model based on the following description: \\\"The blue car has sleek design, runs on electric energy, and reaches speeds up to 200mph.\\\"\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Cross-Encoder for Natural Language Inference\\', \\'api_name\\': \\'cross-encoder/nli-deberta-v3-base\\', \\'api_call\\': \"CrossEncoder(\\'cross-encoder/nli-deberta-v3-base\\')\", \\'api_arguments\\': [\\'sentence_pairs\\'], \\'python_environment_requirements\\': [\\'sentence_transformers\\', \\'transformers\\'], \\'example_code\\': \"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\'cross-encoder/nli-deberta-v3-base\\')\\\\nscores = model.predict([(\\'A man is eating pizza\\', \\'A man eats something\\'), (\\'A black race car starts up in front of a crowd of people.\\', \\'A man is driving down a lonely road.\\')])\", \\'performance\\': {\\'dataset\\': {\\'SNLI-test\\': \\'92.38\\', \\'MNLI mismatched set\\': \\'90.04\\'}}, \\'description\\': \\'This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 381, "text": " Create an application that answers questions about an image's content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 382, "text": " I work with cats rescue organization. I need to analyze an image and tell how many cats are in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'dandelin/vilt-b32-finetuned-vqa\\', \\'api_call\\': \"ViltForQuestionAnswering.from_pretrained(\\'dandelin/vilt-b32-finetuned-vqa\\')\", \\'api_arguments\\': {\\'image\\': \\'Image.open(requests.get(url, stream=True).raw)\\', \\'text\\': \\'How many cats are there?\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'ViltProcessor, ViltForQuestionAnswering\\', \\'requests\\': \\'requests\\', \\'PIL\\': \\'Image\\'}, \\'example_code\\': \\'from transformers import ViltProcessor, ViltForQuestionAnswering\\\\nimport requests\\\\nfrom PIL import Image\\\\n\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntext = How many cats are there?\\\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\\\n\\\\nencoding = processor(image, text, return_tensors=pt)\\\\noutputs = model(**encoding)\\\\nlogits = outputs.logits\\\\nidx = logits.argmax(-1).item()\\\\nprint(Predicted answer:, model.config.id2label[idx])\\', \\'performance\\': {\\'dataset\\': \\'VQAv2\\', \\'accuracy\\': \\'to do\\'}, \\'description\\': \\'Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 383, "text": " We need to develop a chatbot that can answer questions about photos uploaded by users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\')\", \\'api_arguments\\': {\\'model\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'tokenizer\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': \\'vqa(image_path, question)\\', \\'performance\\': {\\'dataset\\': \\'JosephusCheung/GuanacoVQADataset\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 384, "text": " I received an image on my phone and I have a question about it, please answer my question and provide the code snippet.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 385, "text": " A user wants to get the answers to the questions on an image containing text. Help them to code this question-answering system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-textvqa\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/git-large-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'For code examples, we refer to the documentation.\\', \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'See table 11 in the paper for more details.\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 386, "text": " We need to perform data analysis on various documents and must answer questions based on different document layouts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'vilt-finetuned-vqasi\\', \\'api_call\\': \"ViltModel.from_pretrained(\\'tufa15nik/vilt-finetuned-vqasi\\')\", \\'api_arguments\\': {\\'model\\': \\'tufa15nik/vilt-finetuned-vqasi\\', \\'tokenizer\\': \\'tufa15nik/vilt-finetuned-vqasi\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.11.3\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.\\'}', metadata={})]", "category": "generic"}
{"question_id": 387, "text": " What needs to be done, if we are to use an AI model to answer questions about invoices?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\')\", \\'api_arguments\\': {\\'model\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'tokenizer\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': \\'vqa(image_path, question)\\', \\'performance\\': {\\'dataset\\': \\'JosephusCheung/GuanacoVQADataset\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 388, "text": " Create a depth estimation model for an autonomous vehicle navigation system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'tiny-random-DPTForDepthEstimation\\', \\'api_call\\': \"DPTForDepthEstimation.from_pretrained(\\'hf-tiny-model-private/tiny-random-DPTForDepthEstimation\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A tiny random DPT model for depth estimation using Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 389, "text": " I'm creating an app for hikers to estimate the depth of terrain. Can you build a model to calculate the depth in images?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 390, "text": " We are a robotics company and we want to integrate depth estimation into our autonomous navigation system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 391, "text": " As an application developer constructing virtual experiences, we need to determine the depth of objects in a given scene.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 392, "text": " We are the development team that creates mods for the GTA5 game. We need to train an AI model that can read and analyze the in-game processes using the MNIST dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 393, "text": " The company is testing an app for face filters that change the user's appearance. We need an Age Classifier to determine user's age.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 394, "text": " A food delivery app needs a tool to identify dishes from images so that it can recommend similar foods to users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 395, "text": " For the grocery store's checkout application, we need to classify various fruits and vegetables.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 396, "text": " You work for a satellite data company and your goal is to detect planes in satellite imagery.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-building-segmentation\\', \\'api_call\\': \"Output: YOLO(\\'keremberke/yolov8m-building-segmentation\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-building-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'satellite-building-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.623, \\'mAP@0.5(mask)\\': 0.613}}, \\'description\\': \\'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 397, "text": " I am a construction engineer, I got drone images of the construction site, and I want to predict the type of material by breaking that large image into a smaller region.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 398, "text": " We would like to detect and visualize potholes in an image taken by a drone.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-pothole-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-pothole-segmentation\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local image path\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-pothole-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pothole-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.858, \\'mAP@0.5(mask)\\': 0.895}}, \\'description\\': \\'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 399, "text": " As a parking lot owner we want to identify the potholes and avoid any issues within a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 400, "text": " Our client is a digital artist and wants to convert a text description into an image in his next project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/git-base-textvqa\\', \\'api_call\\': \"git_base_textvqa = AutoModel.from_pretrained(\\'microsoft/git-base-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa_pipeline({\\'image\\': \\'path/to/image.jpg\\', \\'question\\': \\'What is in the image?\\'})\", \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'Refer to the paper\\'}, \\'description\\': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}', metadata={})]", "category": "generic"}
{"question_id": 401, "text": " I own a website for photo editing, and I need to provide an image upscaling feature.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Upscaling\\', \\'api_name\\': \\'stabilityai/sd-x2-latent-upscaler\\', \\'api_call\\': \\'upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator)\\', \\'api_arguments\\': {\\'prompt\\': \\'text prompt\\', \\'image\\': \\'low resolution latents\\', \\'num_inference_steps\\': 20, \\'guidance_scale\\': 0, \\'generator\\': \\'torch generator\\'}, \\'python_environment_requirements\\': [\\'git+https://github.com/huggingface/diffusers.git\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': \\'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\\\nimport torch\\\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\\\npipeline.to(cuda)\\\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\\\nupscaler.to(cuda)\\\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\\\ngenerator = torch.manual_seed(33)\\\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\\\nupscaled_image.save(astronaut_1024.png)\\', \\'performance\\': {\\'dataset\\': \\'LAION-2B\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion\\'s latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\"}', metadata={})]", "category": "generic"}
{"question_id": 402, "text": " My client needs to create a tool that can transform an input imagery photo into a beautiful art painting.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'darkstorm2150/Protogen_x5.8_Official_Release', 'api_call': 'StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)', 'api_arguments': {'model_id': 'darkstorm2150/Protogen_v5.8_Official_Release', 'torch_dtype': 'torch.float16'}, 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nimport torch\\\\nprompt = (\\\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\\\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\\\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\\\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\\\n)\\\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\\\nimage.save(./result.jpg)', 'performance': {'dataset': 'unknown', 'accuracy': 'unknown'}, 'description': 'Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.'}\", metadata={})]", "category": "generic"}
{"question_id": 403, "text": " Our company needs to generate high-resolution images of human faces for an upcoming campaign. Please come up with a solution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'vintedois-diffusion-v0-1\\', \\'api_call\\': \"text2img = pipeline(\\'text-to-image\\', model=\\'22h/vintedois-diffusion-v0-1\\')\", \\'api_arguments\\': [\\'prompt\\', \\'CFG Scale\\', \\'Scheduler\\', \\'Steps\\', \\'Seed\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text2img(\\'photo of an old man in a jungle, looking at the camera\\', CFG Scale=7.5, Scheduler=\\'diffusers.EulerAncestralDiscreteScheduler\\', Steps=30, Seed=44)\", \\'performance\\': {\\'dataset\\': \\'large amount of high quality images\\', \\'accuracy\\': \\'not specified\\'}, \\'description\\': \\'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\\'}', metadata={})]", "category": "generic"}
{"question_id": 404, "text": " A pet store needs a unique image of an animal for their marketing campaign. We need to generate the image for them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-r-textcaps\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'microsoft/git-large-r-textcaps\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TextCaps\\', \\'accuracy\\': \\'\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 405, "text": " How can I build a model to classify videos of people doing activities?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 406, "text": " We have set up an online course platform where users can upload images of their paintings. Can you classify those paintings by their style (e.g., baroque, impressionism, cubism)?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 407, "text": " I am developing an online platform where users can upload an image and get instant feedback about the object in the image. We need to evaluate the CLIP model to classify the uploaded images into different categories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-base-patch16\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-base-patch16\\')\", \\'api_arguments\\': [\\'texts\\', \\'images\\'], \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'torch\\', \\'transformers\\'], \\'example_code\\': \\'processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\\\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = [[a photo of a cat, a photo of a dog]]\\\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\\'}', metadata={})]", "category": "generic"}
{"question_id": 408, "text": " The manager wants an AI-based system that can determine whether a given image is of a cat or a dog.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 409, "text": " As a biology student, I want to analyze insects in different pictures and categorize them automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 410, "text": " As a financial analyst, to increase efficiency and save time, we need to analyze a financial statement and identify the sentiment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 411, "text": " I want to write an AI-powered newsletter platform. The system needs to classify paraphrased text based on how they compare to the original text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 412, "text": " I am an AI software developing assistant, and I want to analyze a movie review's sentiment.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 413, "text": " We have a historical document dataset, and we need to find relevant answers quickly to answer questions about World War II.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 414, "text": " Recommend the best solution to examine the conversations taking place in a chat room and classify emotions displayed by different users.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': 'Output: AutoModel.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.'}\", metadata={})]", "category": "generic"}
{"question_id": 415, "text": " We are working on a project and need to find entities (such as person, organization, location) within a given sentence.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 416, "text": " I need to extract names, locations, and organizations from a broad range of texts in different languages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'dslim/bert-base-NER-uncased\\', \\'api_call\\': \"pipeline(\\'ner\\', model=\\'dslim/bert-base-NER-uncased\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.0.0\\'}, \\'example_code\\': \"nlp(\\'My name is John and I live in New York.\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 417, "text": " I am required to build a table for top competitor's sales numbers and I want to answer questions from it like, \\\"Which competitor had the highest sales?\\\"\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 418, "text": " Identify the best-selling product for last month from the provided sales data table and provide details about its revenue and total units sold.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-medium-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\", \\'api_arguments\\': \\'table, query\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query=\\'What is the total revenue?\\')\", \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6561}, \\'description\\': \\'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.\\'}', metadata={})]", "category": "generic"}
{"question_id": 419, "text": " A manager wants the assistat to extract the total revenue for COMPANY_A from a given quarterly performance table.\\n###Input: {'table': {'rows': [['Company', 'Q1 Revenue', 'Q2 Revenue', 'Q3 Revenue', 'Q4 Revenue'], ['COMPANY_A', '75000', '83000', '92000', '102000'], ['COMPANY_B', '81000', '88000', '99000', '110000']], 'columns': 5} , 'query': 'What is the total revenue for COMPANY_A?' }\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 420, "text": " Can you help me with building a program to extract answers to legal questions from contracts and documents?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'Rakib/roberta-base-on-cuad\\', \\'api_call\\': \"AutoModelForQuestionAnswering.from_pretrained(\\'Rakib/roberta-base-on-cuad\\')\", \\'api_arguments\\': {\\'tokenizer\\': \\'AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForQuestionAnswering\\', \\'tokenizer\\': \\'tokenizer = AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\\', \\'model\\': \\'model = AutoModelForQuestionAnswering.from_pretrained(Rakib/roberta-base-on-cuad)\\'}, \\'performance\\': {\\'dataset\\': \\'cuad\\', \\'accuracy\\': \\'46.6%\\'}, \\'description\\': \\'This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents.\\'}', metadata={})]", "category": "generic"}
{"question_id": 421, "text": " I'm writing an article on the influence of e-commerce on the retail industry. What are the main differences between brick-and-mortar stores and online shops?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'impira/layoutlm-document-qa\\', \\'api_call\\': \"layoutlm_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'impira/layoutlm-document-qa\\', return_dict=True))\", \\'api_arguments\\': [\\'image_url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': \\'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)\\', \\'performance\\': {\\'dataset\\': \\'SQuAD2.0 and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.\\'}', metadata={})]", "category": "generic"}
{"question_id": 422, "text": " Analyze a scientific statement about the planet Mars and find its logical relationship with the existing knowledge.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'PegasusForConditionalGeneration.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 423, "text": " Help me understand a review about a new album release by classifying it as either positive, negative or neutral.\\n###Input: \\\"I've been listening to the album non-stop since it was released. I absolutely love the new sound and creative approach the artist has taken. It feels fresh and innovative.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 424, "text": " I need to classify a news headline into the categories of politics, sports, or technology.\\n###Input: \\\"Apple just announced the newest iPhone X\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Zero-Shot Classification\\', \\'api_name\\': \\'cross-encoder/nli-roberta-base\\', \\'api_call\\': \"CrossEncoder(\\'cross-encoder/nli-roberta-base\\')\", \\'api_arguments\\': [\\'sentence1\\', \\'sentence2\\'], \\'python_environment_requirements\\': [\\'sentence_transformers\\', \\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=\\'cross-encoder/nli-roberta-base\\')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\", \\'performance\\': {\\'dataset\\': [\\'SNLI\\', \\'MultiNLI\\'], \\'accuracy\\': \\'See SBERT.net - Pretrained Cross-Encoder\\'}, \\'description\\': \\'Cross-Encoder for Natural Language Inference trained on the SNLI and MultiNLI datasets. Outputs three scores corresponding to the labels: contradiction, entailment, neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 425, "text": " I am working on a project to determine the intent of my customers queries so I can respond to them accordingly.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 426, "text": " What's the main pro and con of companies merging in the same vertical?\\n###Input: \\\"Merging companies in the same vertical often comes with a variety of pros and cons. One of the main benefits is operational efficiency due to the reduction of duplicated efforts and consolidation of resources. On the other hand, the most significant disadvantage is reduced market competition, which can lead to monopolistic behaviors and potentially negative consequences for consumers.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 427, "text": " I have a text document in English, and I want to generate a summary of it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 428, "text": " A company has a marketing team that wants to translate English brochures into German. We need to develop an NLP system to assist in translation.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Text2Text Generation', 'api_name': 'google/flan-t5-xxl', 'api_call': 'T5ForConditionalGeneration.from_pretrained(\\\\\\\\google/flan-t5-xxl\\\\\\\\)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-xxl)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-xxl)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))', 'performance': {'dataset': [{'name': 'MMLU', 'accuracy': '75.2%'}]}, 'description': 'FLAN-T5 XXL is a fine-tuned version of the T5 language model, achieving state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. It has been fine-tuned on more than 1000 additional tasks covering multiple languages, including English, German, and French. It can be used for research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning and question answering.'}\", metadata={})]", "category": "generic"}
{"question_id": 429, "text": " We are an online educational platform trying to reach international audiences; therefore, we need to translate our content to various languages. Help us to find the right tool to perform the translation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 430, "text": " The company wants to translate product descriptions from Italian to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english-ontonotes\\', \\'api_call\\': \\'Output: SequenceTagger.load(\\\\\\\\flair/ner-english-ontonotes\\\\\\\\)\\', \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(flair/ner-english-ontonotes)\\\\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'Ontonotes\\', \\'accuracy\\': \\'89.27\\'}, \\'description\\': \\'This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 431, "text": " We have a series of lengthy conversations that need to be summarized.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"Output: MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 432, "text": " I want to translate a French email to Spanish to better communicate with my colleagues in Spain.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 433, "text": " Construct a text summarization system to summarize news articles for a mobile application. The input text should be a complete news article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 434, "text": " We need to create a text generator that impersonates Elon Musk for our novelty conversational app.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 435, "text": " I want to create an engaging chatbot which can interact with users using personalized responses based on personality facts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'af1tang/personaGPT\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'af1tang/personaGPT\\')\", \\'api_arguments\\': {\\'do_sample\\': \\'True\\', \\'top_k\\': \\'10\\', \\'top_p\\': \\'.92\\', \\'max_length\\': \\'1000\\', \\'pad_token\\': \\'tokenizer.eos_token_id\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'GPT2Tokenizer, GPT2LMHeadModel\\', \\'torch\\': \\'torch\\'}, \\'example_code\\': {\\'load_model\\': \\'tokenizer = AutoTokenizer.from_pretrained(af1tang/personaGPT)\\\\nmodel = AutoModelForCausalLM.from_pretrained(af1tang/personaGPT)\\\\nif torch.cuda.is_available():\\\\n model = model.cuda()\\', \\'generate_response\\': \\'bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\\\\nmsg = generate_next(bot_input_ids)\\\\ndialog_hx.append(msg)\\\\nprint(Bot: {}.format(tokenizer.decode(msg, skip_special_tokens=True)))\\'}, \\'performance\\': {\\'dataset\\': \\'Persona-Chat\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'PersonaGPT is an open-domain conversational agent designed to do 2 tasks: decoding personalized responses based on input personality facts (the persona profile of the bot) and incorporating turn-level goals into its responses through action codes (e.g., talk about work, ask about favorite music). It builds on the DialoGPT-medium pretrained model based on the GPT-2 architecture.\\'}', metadata={})]", "category": "generic"}
{"question_id": 436, "text": " I have a Russian text and want to generate a continuation of it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 437, "text": " We need to create a short story for a children's book based on the theme of friendship.\\n###Input: \\\"<no input>\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 438, "text": " We are building an AI-based writing assistant. We have to create a web-based tool to generate blog titles and outlines.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 439, "text": " A history teacher wants to translate a text from Hindi to French. The text is about \\\"Life is like a box of chocolates.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 440, "text": " We are creating a language-learning app. Provide us with a way to fix grammar mistakes that our users make while learning a new language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 441, "text": " Create an algorithm that converts a Python code snippet into a corresponding text description of its functionality.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'valhalla/t5-base-e2e-qg\\', \\'api_call\\': \"pipeline(\\'e2e-qg\\', model=\\'valhalla/t5-base-e2e-qg\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'Hugging Face Transformers\\'], \\'example_code\\': \"from pipelines import pipeline\\\\n\\\\ntext = Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python\\'s design philosophy emphasizes code readability with its notable use of significant whitespace.\\\\n\\\\nnlp = pipeline(e2e-qg, model=valhalla/t5-base-e2e-qg)\\\\n\\\\nnlp(text)\", \\'performance\\': {\\'dataset\\': \\'squad\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a T5-base model trained for end-to-end question generation task. Simply input the text and the model will generate multiple questions. You can play with the model using the inference API, just put the text and see the results!\\'}', metadata={})]", "category": "generic"}
{"question_id": 442, "text": " Find the appropriate word to complete this sentence: \\\"The sun is the largest [MASK] in the solar system.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Masked Language Modeling\\', \\'api_name\\': \\'bert-base-multilingual-cased\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'bert-base-multilingual-cased\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nunmasker = pipeline(\\'fill-mask\\', model=\\'bert-base-multilingual-cased\\')\\\\nunmasker(Hello I\\'m a [MASK] model.)\", \\'performance\\': {\\'dataset\\': \\'wikipedia\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.\\'}', metadata={})]", "category": "generic"}
{"question_id": 443, "text": " To improve the language quality of my essay, I want to spot words or sentences that can be replaced with better alternatives.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 444, "text": " We would like to develop an AI chatbot in Portuguese language that is able to fill in the blanks in sentences, based on the context of the conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 445, "text": " We want to know the most relevant word to fill in the blank space in the given sentence.\\n###Input: Hugging Face is a [MASK] company.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 446, "text": " I'm a Japanese learner studying abroad. Please help me complete the following sentence: \\\"\\u4eca\\u65e5\\u306f[MASK]\\u3067\\u3059\\u3002\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/bigbird-pegasus-large-arxiv\\', \\'api_call\\': \"BigBirdPegasusForConditionalGeneration.from_pretrained(\\'google/bigbird-pegasus-large-arxiv\\')\", \\'api_arguments\\': [\\'attention_type\\', \\'block_size\\', \\'num_random_blocks\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\ntext = Replace me by any text you\\'d like.\\\\ninputs = tokenizer(text, return_tensors=\\'pt\\')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\", \\'performance\\': {\\'dataset\\': \\'scientific_papers\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.028, \\'ROUGE-2\\': 13.417, \\'ROUGE-L\\': 21.961, \\'ROUGE-LSUM\\': 29.648}}, \\'description\\': \"BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT\\'s attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\"}', metadata={})]", "category": "generic"}
{"question_id": 447, "text": " I work at a company dealing with customer complaints. I need to compare the similarity between a customer's complaint and a list of known issues in our database.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 448, "text": " Our company is developing an AI customer service system. We need to check if the customer's query is similar to the existing ones.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 449, "text": " Our company is developing a virtual assistant to help users with their daily tasks. We need a text-to-speech functionality for the assistant.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 450, "text": " I want to build a voice assistant that can read text out loud for me in Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 451, "text": " Suppose you want to create speech audio for the phrase \\\"We are now launching our newest project \\\" in Chinese audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Paraphrasing\\', \\'api_name\\': \\'prithivida/parrot_paraphraser_on_T5\\', \\'api_call\\': \"\\'Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\'\", \\'api_arguments\\': [\\'input_phrase\\', \\'diversity_ranker\\', \\'do_diverse\\', \\'max_return_phrases\\', \\'max_length\\', \\'adequacy_threshold\\', \\'fluency_threshold\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \\'from parrot import Parrot\\\\nimport torch\\\\nimport warnings\\\\nwarnings.filterwarnings(ignore)\\\\n\\\\nparrot = Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\\\nphrases = [Can you recommed some upscale restaurants in Newyork?,\\\\n What are the famous places we should not miss in Russia?\\\\n]\\\\nfor phrase in phrases:\\\\n print(-*100)\\\\n print(Input_phrase: , phrase)\\\\n print(-*100)\\\\n para_phrases = parrot.augment(input_phrase=phrase)\\\\n for para_phrase in para_phrases:\\\\n  print(para_phrase)\\', \\'performance\\': {\\'dataset\\': \\'Not mentioned\\', \\'accuracy\\': \\'Not mentioned\\'}, \\'description\\': \\'Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 452, "text": " Our company needs to transcribe a podcast that we recently recorded.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 453, "text": " I am building a smart conference recording system, and now I need to transcript the meeting minutes from the audio.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 454, "text": " Develop an app to convert a noisy audio recording to cleaner audio.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 455, "text": " A user is interested in enhancing the audio quality of their home recordings using a pre-trained model. Distortions such as room echoes exist, and they want to know which API will be useful in this situation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 456, "text": " We have noisy recordings, and when we review the recordings, the background noise disturbs us. Please give an enhancement method.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 457, "text": " We are working on a project to clean up and enhance audio recordings from various sources. How can we utilize pretrained models for this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 458, "text": " Our tourism business wants an automatic translator for customer support. The translator must be able to understand the speech in English and respond in French.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-it\\', \\'api_call\\': \"pipeline(\\'translation_en_to_it\\', model=\\'Helsinki-NLP/opus-mt-en-it\\')\", \\'api_arguments\\': {\\'source_language\\': \\'en\\', \\'target_language\\': \\'it\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; translator = pipeline(\\'translation_en_to_it\\', model=\\'Helsinki-NLP/opus-mt-en-it\\'); translator(\\'Hello, world!\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'newssyscomb2009.en.it\\': {\\'BLEU\\': 30.9, \\'chr-F\\': 0.606}, \\'newstest2009.en.it\\': {\\'BLEU\\': 31.9, \\'chr-F\\': 0.604}, \\'Tatoeba.en.it\\': {\\'BLEU\\': 48.2, \\'chr-F\\': 0.695}}}, \\'description\\': \\'A Transformer-based English to Italian translation model trained on the OPUS dataset. This model can be used for translation tasks using the Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 459, "text": "\\nDemonstrate an English to Hokkien speech-to-speech translation approach to test the audio translation quality of our audio processing system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'audio\\', \\'api_name\\': \\'textless_sm_cs_en\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(cached_download(\\'https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt\\'))\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\'], \\'example_code\\': \"from fairseq.models.wav2vec.wav2vec2_asr import Wav2Vec2Model\\\\nfrom huggingface_hub import cached_download\\\\n\\\\nmodel = Wav2Vec2Model.from_pretrained(cached_download(\\'https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt\\'))\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A speech-to-speech translation model for converting between languages without using text as an intermediate representation. This model is designed for the task of audio-to-audio translation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 460, "text": " We are conducting an analysis of Spanish voice samples in our call center. We want to classify the sentiment of the callers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 461, "text": " In our podcast platform, we want to detect voice activity so that we can separate speakers in the audio files.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Voice Activity Detection', 'framework': 'pyannote.audio', 'functionality': 'Automatic Speech Recognition', 'api_name': 'pyannote/voice-activity-detection', 'api_call': 'Pipeline.from_pretrained(pyannote/voice-activity-detection)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # active speech between speech.start and speech.end', 'performance': {'dataset': 'ami', 'accuracy': 'Not specified'}, 'description': 'A pretrained voice activity detection pipeline that detects active speech in audio files.'}\", metadata={})]", "category": "generic"}
{"question_id": 462, "text": " An environmental organization needs our help to estimate CO2 emissions using a dataset with characteristics of different plants and trees. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 463, "text": " I want to estimate the amount of CO2 emission generated for different cars. Please provide me the code to analyze the data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 464, "text": " Predict the carbon emissions of a dataset of vehicles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 465, "text": " Our vehicle production unit wants to predict CO2 emissions based on vehicle specifications, which regression model should we employ?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1702259728\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'Validation Metrics\\', \\'accuracy\\': 0.831}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 466, "text": " We are building an application to make the factory more eco-friendly. Estimate the carbon emissions of the factory using the given data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 467, "text": " Identifying the best strategies for a game, we need a pre-trained reinforcement learning model with great performance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Inference API\\', \\'api_name\\': \\'Antheia/Hanna\\', \\'api_call\\': \"pipeline(\\'robotics\\', model=\\'Antheia/Hanna\\') should remain unchanged as it already only includes the model instantiation.\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'openai/webgpt_comparisons\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 468, "text": " We want to automate the process of learning a robotic arm to balance an inverted pendulum.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 469, "text": " A gaming company requires a trained AI model to effectively control game characters in a soccer match. Provide the suitable Reinforcement Learning API.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 470, "text": " Create a research aid tool that summarizes the research papers in a selected field for users.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1-base', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': {'install_dependencies': 'pip install diffusers transformers accelerate scipy safetensors', 'code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)'}, 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.'}\", metadata={})]", "category": "generic"}
{"question_id": 471, "text": " Analyze an image for the clothing style of the person in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 472, "text": " I am working on medical terms normalization project. Please find me an appropriate model for the task in English.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 473, "text": " We are working on designing some science fiction themed digital illustrations from text for an upcoming graphic novel.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 474, "text": " As a writer, I would like to have an AI tool to create an illustration for my recent sci-fi story that involves a futuristic city with flying cars.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 475, "text": " Provide a smart tool that can generate a high-quality image of a lion in its natural habitat in the Serengeti, using natural light and high resolution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 476, "text": " A visually impaired user would like the AI to generate an image based on a written description. Implement a multimodal text-to-image pipeline to help the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/git-base-coco\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'microsoft/git-base-coco\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'See the model hub for fine-tuned versions on a task that interests you.\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Refer to the paper for evaluation results.\\'}, \\'description\\': \\'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\'}', metadata={})]", "category": "generic"}
{"question_id": 477, "text": " Design a tool that can create an artistic image based on a provided description.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.'}\", metadata={})]", "category": "generic"}
{"question_id": 478, "text": " I would like to create a tool that can convert Japanese manga text into English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 479, "text": " I am organizing a conference and need help with reading documents such as flyers, posters, and reports. The assistant should be capable of converting the text in the images to readable content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'facebook/opt-66b\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'facebook/opt-66b\\', torch_dtype=torch.float16).generate(input_ids)\", \\'api_arguments\\': [\\'input_ids\\', \\'do_sample\\', \\'num_return_sequences\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\\\nprompt = Hello, I am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\nset_seed(32)\\\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\\', \\'performance\\': {\\'dataset\\': \\'GPT-3\\', \\'accuracy\\': \\'roughly matched\\'}, \\'description\\': \\'OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 480, "text": " A tourist organization is using our software to give information about different places. We need to generate text descriptions based on the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 481, "text": " Our company works with social media and we require a tool to describe images in a textual manner.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 482, "text": " We have a video-based teaching platform. Come up with a way to create video content from written text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 483, "text": " Develop a solution for a video advertising agency to generate short videos based on text descriptions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 484, "text": " We have a large dataset of images and their descriptions, and we want to identify objects and actions within the images by asking questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 485, "text": " Build an application combining computer vision and natural language processing to answer questions based on a given document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 486, "text": " Please generate possible questions and answers based on a legal document about carbon emissions.\\n###Input: \\\"<noinput>\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'davanstrien/testwebhook\\', \\'api_call\\': \"pipeline(\\'question-answering\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'pile-of-law/pile-of-law\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model trained for answering questions related to legal documents and carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 487, "text": " Explain how I can estimate the depth of an given image while processing it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 488, "text": " We have a hotel management app that scans rooms with cameras. We need to figure out the depth of each room to organize 3D layouts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"Output: MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 489, "text": " We are working in the field of computer vision and need a model for depth estimation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'glpn-kitti-finetuned-diode-221214-123047\\', \\'api_call\\': \"pipeline(\\'depth-estimation\\', model=\\'sayakpaul/glpn-kitti-finetuned-diode-221214-123047\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers==4.24.0\\', \\'torch==1.12.1+cu116\\', \\'tokenizers==0.13.2\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'diode-subset\\', \\'accuracy\\': {\\'Loss\\': 0.3497, \\'Mae\\': 0.2847, \\'Rmse\\': 0.3977, \\'Abs Rel\\': 0.3477, \\'Log Mae\\': 0.1203, \\'Log Rmse\\': 0.1726, \\'Delta1\\': 0.5217, \\'Delta2\\': 0.8246, \\'Delta3\\': 0.9436}}, \\'description\\': \\'This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.\\'}', metadata={})]", "category": "generic"}
{"question_id": 490, "text": " Implement a depth estimation pipeline to analyze a given 3D image for potential health hazards.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'glpn-nyu-finetuned-diode-221116-110652\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sayakpaul/glpn-nyu-finetuned-diode-221116-110652\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'diode-subset\\', \\'accuracy\\': {\\'Loss\\': 0.4018, \\'Mae\\': 0.3272, \\'Rmse\\': 0.4546, \\'Abs Rel\\': 0.3934, \\'Log Mae\\': 0.138, \\'Log Rmse\\': 0.1907, \\'Delta1\\': 0.4598, \\'Delta2\\': 0.7659, \\'Delta3\\': 0.9082}}, \\'description\\': \\'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 491, "text": " As a clothing company, we need a system for identifying the types of clothing in images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 492, "text": " In our delivery business, we need assistance to categorize the items in the delivery pictures.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 493, "text": " In our project, we want to create an AI system that could help us recognize different types of beans according to the images provided by our drone.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 494, "text": " Write some code to help me process obtained tables through my mobile device and obtain the respective layout.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 495, "text": " A logistics company needs to build a system for detecting different types of packages in their warehouse. We need to train a system for object detection.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 496, "text": " We are developing a traffic monitoring system. We need to recognize traffic signs and vehicles in images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 497, "text": " A company specializing in creating virtual environments needs to segment an image to understand the different objects within it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'microsoft/swinv2-tiny-patch4-window8-256\\', \\'api_call\\': \"AutoModelForImageClassification.from_pretrained(\\'microsoft/swinv2-tiny-patch4-window8-256\\')\", \\'api_arguments\\': {\\'image\\': \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'imagenet-1k\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 498, "text": " I have an image of a park, and I want to know the type of vegetation that covers the largest area.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 499, "text": " Our team is working on a city planning project and needs a tool to segment the different components of some aerial images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Multilingual Question Answering\\', \\'api_name\\': \\'mrm8488/bert-multi-cased-finetuned-xquadv1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\', tokenizer=\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\')\", \\'api_arguments\\': {\\'context\\': \\'string\\', \\'question\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"qa_pipeline({\\\\n \\'context\\': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\\\n \\'question\\': Who has been working hard for hugginface/transformers lately?\\\\n})\", \\'performance\\': {\\'dataset\\': \\'XQuAD\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.\\'}', metadata={})]", "category": "generic"}
{"question_id": 500, "text": " A photography client has requested some variations of their image. You need to generate enhancements.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Variations\\', \\'api_name\\': \\'lambdalabs/sd-image-variations-diffusers\\', \\'api_call\\': \"StableDiffusionImageVariationPipeline.from_pretrained(\\'lambdalabs/sd-image-variations-diffusers\\', revision=\\'v2.0\\')\", \\'api_arguments\\': {\\'revision\\': \\'v2.0\\'}, \\'python_environment_requirements\\': \\'Diffusers >=0.8.0\\', \\'example_code\\': \\'from diffusers import StableDiffusionImageVariationPipeline\\\\nfrom PIL import Image\\\\ndevice = cuda:0\\\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\\\n lambdalabs/sd-image-variations-diffusers,\\\\n revision=v2.0,\\\\n)\\\\nsd_pipe = sd_pipe.to(device)\\\\nim = Image.open(path/to/image.jpg)\\\\ntform = transforms.Compose([\\\\n transforms.ToTensor(),\\\\n transforms.Resize(\\\\n  (224, 224),\\\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\\\n  antialias=False,\\\\n ),\\\\n transforms.Normalize(\\\\n  [0.48145466, 0.4578275, 0.40821073],\\\\n  [0.26862954, 0.26130258, 0.27577711]),\\\\n])\\\\ninp = tform(im).to(device).unsqueeze(0)\\\\nout = sd_pipe(inp, guidance_scale=3)\\\\nout[images][0].save(result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'ChristophSchuhmann/improved_aesthetics_6plus\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\\'}', metadata={})]", "category": "generic"}
{"question_id": 501, "text": " I'm an artist, wanting to sketch something based on a word or phrase I provide. Generate an image based on my input.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 502, "text": " I am an artist, I need a tool to generate a picture of sheep in a new artistic style.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 503, "text": " Our design team is working on a bathroom wall picture and they need the picture without the previously used watermark.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transcription and Translation\\', \\'api_name\\': \\'openai/whisper-tiny\\', \\'api_call\\': \"WhisperForConditionalGeneration.from_pretrained(\\'openai/whisper-tiny\\')\", \\'api_arguments\\': {\\'forced_decoder_ids\\': \"WhisperProcessor.get_decoder_prompt_ids(language=\\'english\\', task=\\'transcribe\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'datasets\\'], \\'example_code\\': [\\'from transformers import WhisperProcessor, WhisperForConditionalGeneration\\', \\'from datasets import load_dataset\\', \"processor = WhisperProcessor.from_pretrained(\\'openai/whisper-tiny\\')\", \"model = WhisperForConditionalGeneration.from_pretrained(\\'openai/whisper-tiny\\')\", \\'model.config.forced_decoder_ids = None\\', \"ds = load_dataset(\\'hf-internal-testing/librispeech_asr_dummy\\', \\'clean\\', split=\\'validation\\')\", \"sample = ds[0][\\'audio\\']\", \"input_features = processor(sample[\\'array\\'], sampling_rate=sample[\\'sampling_rate\\'], return_tensors=\\'pt\\').input_features\", \\'predicted_ids = model.generate(input_features)\\', \\'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech (clean)\\', \\'accuracy\\': 7.54}, \\'description\\': \\'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model that can be used for transcription and translation tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 504, "text": " A game environment company has requested images of randomly generated churches. Please provide these images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-base-finetuned-wikisql-supervised\\', \\'api_call\\': \"TapasForQuestionAnswering.from_pretrained(\\'google/tapas-base-finetuned-wikisql-supervised\\')\", \\'api_arguments\\': [\\'question\\', \\'table\\'], \\'python_environment_requirements\\': [\\'PyTorch\\', \\'TensorFlow\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'wikisql\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It was pretrained with two objectives: Masked language modeling (MLM) and Intermediate pre-training. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQA and WikiSQL.\\'}', metadata={})]", "category": "generic"}
{"question_id": 505, "text": " Create a computer-generated artwork of a cosmic landscape using unconditional image generation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'Apocalypse-19/shoe-generator\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'Apocalypse-19/shoe-generator\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'Apocalypse-19/shoe-generator\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'custom dataset\\', \\'accuracy\\': \\'128x128 resolution\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of shoes trained on a custom dataset at 128x128 resolution.\\'}', metadata={})]", "category": "generic"}
{"question_id": 506, "text": " Our team is working on a project to create an app that generates random images of cute animals as avatars for new users without providing any conditions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 507, "text": " We need to process videos for our new AI video content generation platform. Please provide the necessary tools for video classification.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 508, "text": " We are building a fitness app that can recognize human actions by processing short video clips.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 509, "text": " The advertising team needs a tool to automatically recognize different types of video content for ad placements. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 510, "text": " Our team received a video and we didn't understand its content. We would like to classify the video by checking if it belongs to any known categories or has any hidden meanings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/xclip-base-patch32\\', \\'api_call\\': \"XClipModel.from_pretrained(\\'microsoft/xclip-base-patch32\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'For code examples, we refer to the documentation.\\', \\'performance\\': {\\'dataset\\': \\'Kinetics 400\\', \\'accuracy\\': {\\'top-1\\': 80.4, \\'top-5\\': 95.0}}, \\'description\\': \\'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\\'}', metadata={})]", "category": "generic"}
{"question_id": 511, "text": " Please classify an accident in a given video clip.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Age Classification\\', \\'api_name\\': \\'nateraw/vit-age-classifier\\', \\'api_call\\': \"ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nateraw/vit-age-classifier\\'}, \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'transformers\\'], \\'example_code\\': \"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get(\\'https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true\\')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\ntransforms = ViTFeatureExtractor.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\n\\\\ninputs = transforms(im, return_tensors=\\'pt\\')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\", \\'performance\\': {\\'dataset\\': \\'fairface\\', \\'accuracy\\': None}, \\'description\\': \"A vision transformer finetuned to classify the age of a given person\\'s face.\"}', metadata={})]", "category": "generic"}
{"question_id": 512, "text": " As a pharmaceutical company, we get thousands of drug images every day. We need a model to classify those images into different drug categories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 513, "text": " I would like to know which category my dog belongs to by analyzing its image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 514, "text": " We have a picture and want to sort it into the potentially relevant categories: cat, dog, or fish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"Output: MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 515, "text": " Analyze an image for a maintenance company to identify possible issues with equipment in the facility.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 516, "text": " I am developing an application in Korean for a local fashion store where customers upload their pictures of fashion items and the app identifies them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 517, "text": " As a stock trader, I receive many stock-related comments every day. I need a model to help me understand the sentiment for these comments.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Inferencing for stock-related comments\\', \\'api_name\\': \\'zhayunduo/roberta-base-stocktwits-finetuned\\', \\'api_call\\': \"Output: RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\", \\'api_arguments\\': {\\'model\\': \\'RobertaForSequenceClassification\\', \\'tokenizer\\': \\'RobertaTokenizer\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\\\nfrom transformers import pipeline\\\\nimport pandas as pd\\\\nimport emoji\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\\\nsentences = pd.Series([\\'just buy\\',\\'just sell it\\',\\'entity rocket to the sky!\\',\\'go down\\',\\'even though it is going up, I still think it will not keep this trend in the near future\\'])\\\\nsentences = list(sentences)\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'stocktwits\\', \\'accuracy\\': 0.9343}, \\'description\\': \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags \\'Bullish\\' or \\'Bearish\\'.\"}', metadata={})]", "category": "generic"}
{"question_id": 518, "text": " Recommend to me the most relevant search result from a list of passages for my research question about the population of Berlin.\\n###Input: {\\\"question\\\": \\\"What is the population of Berlin?\\\", \\\"possible_passages\\\": [\\\"Berlin is the capital and the largest city of Germany by both area and population. Its population is approximately 3.7 million.\\\", \\\"Berlin enjoys a very rich cultural scene, featuring numerous museums, art galleries, and historical monuments.\\\", \\\"New York City is one of the most populous cities in the United States, with over 8 million residents.\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 519, "text": " A company analyses online reviews of its products, and they need an auto-generated sentiment score ranging from Positive, Negative to Neutral to understand their customers' feedback.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 520, "text": " Our company is a pharmaceutical firm. We want to analyze the keywords in a research articles about cancer drugs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 521, "text": " Our company is interested in answering queries related to a table, and we need to select a suitable Table Question Answering model to use.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-medium-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\", \\'api_arguments\\': \\'table, query\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query=\\'What is the total revenue?\\')\", \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6561}, \\'description\\': \\'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.\\'}', metadata={})]", "category": "generic"}
{"question_id": 522, "text": " We have a CSV file available with sales data for last year. We want to query sales information for multiple products.\\n###Input: [{\\\"Product\\\":\\\"Monitor\\\", \\\"Region\\\":\\\"Europe\\\", \\\"Units Sold\\\":\\\"550\\\", \\\"Revenue\\\":\\\"180000\\\"}, {\\\"Product\\\":\\\"Laptop\\\", \\\"Region\\\":\\\"Asia\\\", \\\"Units Sold\\\":\\\"750\\\", \\\"Revenue\\\":\\\"240000\\\"}, {\\\"Product\\\":\\\"Keyboard\\\", \\\"Region\\\":\\\"North America\\\", \\\"Units Sold\\\":\\\"1200\\\", \\\"Revenue\\\":\\\"60000\\\"}]\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"Output: MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 523, "text": " I am designing an application to boost productivity. I want to include a feature that can answer questions based on text given by the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 524, "text": " We'd like to find answers in a Korean language document by a querist who needs information about policies and programs of the government.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Feature Extraction\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'kobart-base-v2\\', \\'api_call\\': \"BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\"}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'tokenizers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import PreTrainedTokenizerFast, BartModel\\\\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\\\\nmodel = BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'performance\\': {\\'dataset\\': \\'NSMC\\', \\'accuracy\\': 0.901}, \\'description\\': \\'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 525, "text": " The customer requires a question answer pipeline that works on an example in which context and questions are given.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 526, "text": " I am looking for a personal assistant for helping me making a quote in investing in China stock market. First, we need to find how many companies are listed in Shanghai Stock Exchange.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=BartForConditionalGeneration.from_pretrained(\\'philschmid/bart-large-cnn-samsum\\'))\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 527, "text": " Develop a recommendation engine using a model to understand if a given sentence is similar to another sentence provided (semantic similarity).\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\', \\'api_call\\': \\'model.encode(text)\\', \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'sentence-transformers library\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\\\\ntext = Replace me by any text you\\'d like.\\\\ntext_embbedding = model.encode(text)\", \\'performance\\': {\\'dataset\\': \\'1,097,953,922\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 528, "text": " We need to evaluate the similarity of the following pair of sentences: \\\"A child is sitting on a swing in the park\\\" and \\\"A person is standing in a park putting their hands on their hips\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 529, "text": " We are designing a translator software for tourists. They will be able to translate their English sentences into French.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 530, "text": " The team is researching a new diet called \\\"intermittent fasting\\\". They need a summary of the latest article about intermittent fasting they just found.\\n###Input: \\\"Intermittent fasting is a popular dietary trend that involves cycling between periods of fasting and eating. The most common methods include the 16:8 method, which involves fasting for 16 hours a day and eating during an eight-hour window, and the 5:2 method, which involves eating normally for five days a week and restricting calorie intake to 500-600 calories for the remaining two days. Advocates of intermittent fasting claim that it can lead to weight loss, improved metabolism, and increased energy levels. Some research supports these claims, with studies showing that intermittent fasting can help individuals lose weight, improve insulin sensitivity, and lower inflammation levels. However, not all experts agree on the benefits of intermittent fasting, as some argue that it can be difficult to adhere to for extended periods of time and may lead to disordered eating patterns or nutritional deficiencies. It is essential for individuals considering intermittent fasting to consult with a healthcare professional before making significant changes to their diet.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 531, "text": " Provide a method to translate a given Spanish text to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 532, "text": " Please convert a message from Italian to English.\\n###Input: \\\"Ciao! Come stai? Ho sentito che il tuo viaggio stato fantastico.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 533, "text": " Our mobile app includes a chat feature, and we want to translate user messages from German to Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 534, "text": " We have a Swedish company and for our international expansion, we are in need of a solution that can help us to send English translated emails to our clients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 535, "text": " Can you help us come up with a brief summary of an input passage for our busy editor?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 536, "text": " We are building a chatting product that translates the user's feed written in French into Spanish to reach users in Latin America.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 537, "text": " Our client is looking for an AI chatbot. Can you create one for us?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 538, "text": " The company needs a way to improve customer support. We want a conversational agent to talk to the users and provide them with helpful information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/blenderbot-90M\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\", \\'api_arguments\\': {\\'input_message\\': \\'str\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\n\\\\n# Chat with the model\\\\ninput_message = \\'What is your favorite color?\\'\\\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors=\\'pt\\')\\\\n\\\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n\\\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead.\\'}', metadata={})]", "category": "generic"}
{"question_id": 539, "text": " We want to build a virtual travel advisor assistant that would participate in a conversation with its client.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 540, "text": " Our customer needs a brief overview of how artificial intelligence has evolved over the years.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'bigscience/bloom-560m\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'bigscience/bloom-560m\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nmodel_name = \\'bigscience/bloom-560m\\'\\\\napi = pipeline(\\'text-generation\\', model=model_name)\\\\n\\\\ntext = \\'The history of artificial intelligence began in the \\'\\\\noutput = api(text)\\\\nprint(output[0][\\'generated_text\\'])\", \\'performance\\': {\\'dataset\\': \\'Validation\\', \\'accuracy\\': {\\'Training Loss\\': 2.0, \\'Validation Loss\\': 2.2, \\'Perplexity\\': 8.9}}, \\'description\\': \\'BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads.\\'}', metadata={})]", "category": "generic"}
{"question_id": 541, "text": " We are developing a content-creation tool which can be used by users to generate academic articles on various topics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 542, "text": " Consdier the type of job you have. Generate a fictional story about a language model that comes to life.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 543, "text": " Our company aims to generate conversational text based on provided prompts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 544, "text": " As a language learning platform, we want to offer a feature where users can ask for translations of sentences from their native language into the language they are trying to learn.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 545, "text": " I want a ready-to-use NLP model to complete this sentence: \\\"Today, I went to the [MASK] to buy some groceries.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 546, "text": " I am working on a chatbot that is supposed to have knowledge about geography. One question it should be able to answer is, what is the capital city of France?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 547, "text": " Analyze the following user comments and compare their similarity to find out which comments have the closest meaning.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Inferencing for stock-related comments\\', \\'api_name\\': \\'zhayunduo/roberta-base-stocktwits-finetuned\\', \\'api_call\\': \"Output: RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\", \\'api_arguments\\': {\\'model\\': \\'RobertaForSequenceClassification\\', \\'tokenizer\\': \\'RobertaTokenizer\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\\\nfrom transformers import pipeline\\\\nimport pandas as pd\\\\nimport emoji\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\\\nsentences = pd.Series([\\'just buy\\',\\'just sell it\\',\\'entity rocket to the sky!\\',\\'go down\\',\\'even though it is going up, I still think it will not keep this trend in the near future\\'])\\\\nsentences = list(sentences)\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'stocktwits\\', \\'accuracy\\': 0.9343}, \\'description\\': \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags \\'Bullish\\' or \\'Bearish\\'.\"}', metadata={})]", "category": "generic"}
{"question_id": 548, "text": " I want to assess the similarity between various sentences to find related content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 549, "text": " Can you help me find the most relevant document among these three Chinese sentences about my business proposal?\\n###Input: {\\\"source_sentence\\\": \\\"\\u6211\\u6b63\\u5728\\u4e3a\\u4e00\\u5bb6\\u65b0\\u7684\\u79d1\\u6280\\u516c\\u53f8\\u5236\\u5b9a\\u4e1a\\u52a1\\u8ba1\\u5212\\u3002\\\",  \\\"sentences_to_compare\\\": [\\\"\\u5173\\u4e8e\\u65b0\\u516c\\u53f8\\u7684\\u4e1a\\u52a1\\u8ba1\\u5212\\u3002\\\", \\\"\\u79d1\\u6280\\u884c\\u4e1a\\u7684\\u53d1\\u5c55\\u8d8b\\u52bf\\u3002\\\", \\\"\\u7b80\\u4ecb\\u53ca\\u4e86\\u89e3\\u516c\\u53f8\\u3002\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 550, "text": " We are a library, and we want to find the similarity between two sentences about books.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 551, "text": " An AI recommended me an Indian male voice for reading Marathi text loudly. I will use Text-to-Speech conversion for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 552, "text": " Convert a text written in the Telugu language to an audio file, spoken by a male voice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Spoken Language Identification\\', \\'api_name\\': \\'TalTechNLP/voxlingua107-epaca-tdnn\\', \\'api_call\\': \"\\'EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\'\", \\'api_arguments\\': [\\'signal\\'], \\'python_environment_requirements\\': [\\'speechbrain\\', \\'torchaudio\\'], \\'example_code\\': \\'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\', \\'performance\\': {\\'dataset\\': \\'VoxLingua107\\', \\'accuracy\\': \\'93%\\'}, \\'description\\': \\'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 553, "text": " Our team builds a mobile application that reads out news articles. We want to include audio output from text read by a text-to-speech engine.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 554, "text": " Write an email for a marketing campaign and create an audio file to check its effectiveness.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'hustvl/yolos-tiny\\', \\'api_call\\': \"YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-tiny\\')\", \\'api_arguments\\': {\\'images\\': \\'image\\', \\'return_tensors\\': \\'pt\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained(\\'hustvl/yolos-tiny\\')\\\\nmodel = YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-tiny\\')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\", \\'performance\\': {\\'dataset\\': \\'COCO 2017 validation\\', \\'accuracy\\': \\'28.7 AP\\'}, \\'description\\': \\'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 555, "text": " Our company is developing an app for learning Dutch. We would like to implement a feature to auto-convert the user's speech into Dutch text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Recognition\\', \\'api_name\\': \\'jonatasgrosman/wav2vec2-large-xlsr-53-dutch\\', \\'api_call\\': \"SpeechRecognitionModel(\\'jonatasgrosman/wav2vec2-large-xlsr-53-dutch\\')\", \\'api_arguments\\': [\\'audio_paths\\'], \\'python_environment_requirements\\': [\\'huggingsound\\', \\'torch\\', \\'librosa\\', \\'datasets\\', \\'transformers\\'], \\'example_code\\': \"from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(\\'jonatasgrosman/wav2vec2-large-xlsr-53-dutch\\')\\\\naudio_paths = [\\'/path/to/file.mp3\\', \\'/path/to/another_file.wav\\']\\\\ntranscriptions = model.transcribe(audio_paths)\", \\'performance\\': {\\'dataset\\': \\'Common Voice nl\\', \\'accuracy\\': {\\'Test WER\\': 15.72, \\'Test CER\\': 5.35, \\'Test WER (+LM)\\': 12.84, \\'Test CER (+LM)\\': 4.64}}, \\'description\\': \\'Fine-tuned XLSR-53 large model for speech recognition in Dutch. Fine-tuned on Dutch using the train and validation splits of Common Voice 6.1 and CSS10.\\'}', metadata={})]", "category": "generic"}
{"question_id": 556, "text": " Our company is developing an AI assistant to perform live transcription of meetings. We need to convert recorded speech into text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 557, "text": " A web browser extension automatically transcribes meetings in real-time to help people with hearing disabilities or those who join late to understand what is being discussed.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 558, "text": " As an international company, our phone conference calls happen in multiple languages. We want to create a tool to convert English speech input to French speech output for our meetings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 559, "text": " Create a tourist-oriented mobile app that translates spoken statements from a Romanian-speaking tour guide to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 560, "text": " We would like to improve the customer support quality of our call center by understanding the emotions of our clients in realtime.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 561, "text": " Develop a software for classifying keywords spoken by users. It should predict the top five keywords.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'superb/wav2vec2-base-superb-ks\\', \\'api_call\\': \"pipeline(\\'audio-classification\\', model=\\'superb/wav2vec2-base-superb-ks\\')\", \\'api_arguments\\': {\\'model\\': \\'superb/wav2vec2-base-superb-ks\\'}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\', \\'torchaudio\\', \\'datasets\\'], \\'example_code\\': \\'from datasets import load_dataset\\\\nfrom transformers import pipeline\\\\ndataset = load_dataset(anton-l/superb_demo, ks, split=test)\\\\nclassifier = pipeline(audio-classification, model=superb/wav2vec2-base-superb-ks)\\\\nlabels = classifier(dataset[0][file], top_k=5)\\', \\'performance\\': {\\'dataset\\': \\'Speech Commands dataset v1.0\\', \\'accuracy\\': {\\'s3prl\\': 0.9623, \\'transformers\\': 0.9643}}, \\'description\\': \\'Wav2Vec2-Base for Keyword Spotting (KS) task in the SUPERB benchmark. The base model is pretrained on 16kHz sampled speech audio. The KS task detects preregistered keywords by classifying utterances into a predefined set of words. The model is trained on the Speech Commands dataset v1.0.\\'}', metadata={})]", "category": "generic"}
{"question_id": 562, "text": " Automate the process of categorizing noises on a blog with video content, analyzing and describing the type of sound in the video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'hubert-large-ll60k\\', \\'api_call\\': \"HubertModel.from_pretrained(\\'facebook/hubert-large-ll60k\\')\", \\'api_arguments\\': \\'pretrained model name\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"hubert = HubertModel.from_pretrained(\\'facebook/hubert-large-ll60k\\')\", \\'performance\\': {\\'dataset\\': \\'Libri-Light\\', \\'accuracy\\': \\'matches or improves upon the state-of-the-art wav2vec 2.0 performance\\'}, \\'description\\': \\'Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.\\'}', metadata={})]", "category": "generic"}
{"question_id": 563, "text": " We are an e-Learning platform providing language lessons. Detect the speaker from a given speech segment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 564, "text": " An audio application is developed which requires speaker verification. Can you include the appropriate API call and example code?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker Diarization\\', \\'api_name\\': \\'pyannote/speaker-diarization\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2.1\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': {\\'num_speakers\\': \\'int (optional)\\', \\'min_speakers\\': \\'int (optional)\\', \\'max_speakers\\': \\'int (optional)\\'}, \\'python_environment_requirements\\': \\'pyannote.audio 2.1.1\\', \\'example_code\\': [\\'from pyannote.audio import Pipeline\\', \\'pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2.1, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'diarization = pipeline(audio.wav)\\', \\'with open(audio.rttm, w) as rttm:\\', \\'  diarization.write_rttm(rttm)\\'], \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': {\\'DER%\\': \\'18.91\\', \\'FA%\\': \\'4.48\\', \\'Miss%\\': \\'9.51\\', \\'Conf%\\': \\'4.91\\'}}, \\'description\\': \\'This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 565, "text": " I am a software engineer, and I want to build an application that can control smart home devices using voice commands. The model must identify which command has been given by the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 566, "text": " I am building a web application to predict a person's income based on adult census data, and I need to know how to use the XGBoost model to make the predictions. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 567, "text": " We need an AI model that can predict the carbon emission levels of different products based on certain factors.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': 'Not specified.', 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = prompthero/openjourney\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.'}\", metadata={})]", "category": "generic"}
{"question_id": 568, "text": " As a speaker factory, we want to know the correlation beteween components weight and carbon emmisions during production.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 569, "text": " Generate the amount of carbon emissions for given building data in order to predict the efficiency at which it produces.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1839063122\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 570, "text": " I oversee management of a company with warehouse robots. I want our robots to move in a more natural gait. Can you provide a suitable model for this purpose?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=BartForConditionalGeneration.from_pretrained(\\'philschmid/bart-large-cnn-samsum\\'))\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 571, "text": " Our software team wants to implement an AI-driven multi-agent soccer game. Identify an existing model to use.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 572, "text": " We'd like to analyze a customer review in Korean and help identify the most relevant parts for potential improvements.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Feature Extraction\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'kobart-base-v2\\', \\'api_call\\': \"BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\"}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'tokenizers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import PreTrainedTokenizerFast, BartModel\\\\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\\\\nmodel = BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'performance\\': {\\'dataset\\': \\'NSMC\\', \\'accuracy\\': 0.901}, \\'description\\': \\'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 573, "text": " Extract linguistic features from an audio sample.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 574, "text": " Analyze the trend of people's fashion style based on the designer's description and generate related images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 575, "text": " An online manga translation platform needs to recognize text in Japanese manga and want to convert it into plain text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 576, "text": " Recognize and extract text from an image that contains a printed sample.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 577, "text": " A magazine editor is interested in extracting text from a printed image of a handwritten manuscript to include in the publication.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 578, "text": " I want to create a security system based on text identification in my company. It will be used for reading texts from images captured by the security system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 579, "text": " A marketing team needs to create high-quality content for product promotion. They want to turn text descriptions into short video clips to engage their audience on social media.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 580, "text": " As part of a social-aware project, we are analyzing images and answering questions based on those images using pretrained models.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 581, "text": " We want to build a program that would answer questions based on visuals.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 582, "text": " Create an AI tool that answers simple questions related to an image provided as input.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'davanstrien/testwebhook\\', \\'api_call\\': \"pipeline(\\'question-answering\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'pile-of-law/pile-of-law\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model trained for answering questions related to legal documents and carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 583, "text": " Someone is sending me pictures of famous places, but they want me to guess the place by asking questions. Can you help me respond to the vital question?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\', \\'api_call\\': \\'model.encode(text)\\', \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'sentence-transformers library\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\\\\ntext = Replace me by any text you\\'d like.\\\\ntext_embbedding = model.encode(text)\", \\'performance\\': {\\'dataset\\': \\'1,097,953,922\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 584, "text": " An accounting firm would like to extract specific information, such as invoice numbers, from scanned images of documents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 585, "text": " Can you explain how I can extract answers from a document image with a specific question?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 586, "text": " I have some scanned documents, and I want a tool that extracts information from these images to answer questions related to the content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 587, "text": " We would like to extract important information from an invoice for bookkeeping and accounting purposes.\\n###Input: {\\\"url\\\": \\\"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\\", \\\"question\\\": \\\"What is the total amount due?\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 588, "text": " A robot company wants to create a model to predict the distance of objects from the camera. They want to know which API would be best suitable for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-textcaps\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'microsoft/git-large-textcaps\\')\", \\'api_arguments\\': \\'image, text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'TextCaps\\', \\'accuracy\\': \\'Refer to the paper\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 589, "text": " We need to classify images of animals as a part of our game development process. How can we use an ML model to achieve this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 590, "text": " For an article about popular street foods, we need to find out if the given image is a hotdog.\\n###Input: path/to/image\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 591, "text": " Our company sells custom-made aprons. We want to use an AI tool to identify images of aprons and analyze the customer feedback.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 592, "text": " We need to automatically segment items in images to improve our warehouse management system. Help us to detect objects in the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 593, "text": " I want a chat product to classify whether an image is healthy by checking the presence of red blood cells, white blood cells and platelets in the bloodstream.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Blood Cell Detection\\', \\'api_name\\': \\'keremberke/yolov8n-blood-cell-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'blood-cell-object-detection\\', \\'accuracy\\': 0.893}, \\'description\\': \\'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 594, "text": " My client manages a real estate company, they want to segment the rooms in their apartment pictures to showcase them to their potential customers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot_small-90M\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot_small-90M\\')\", \\'api_arguments\\': [\\'message\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot_small-90M.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 595, "text": " Our city management wants to analyze urban landscape images to identify various elements such as buildings, roads, and trees.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Realistic_Vision_V1.4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=SG161222/Realistic_Vision_V1.4)\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'negative_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nmodel = pipeline(\\'text-to-image\\', model=\\'SG161222/Realistic_Vision_V1.4\\')\\\\n\\\\nprompt = \\'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\\'\\\\nnegative_prompt = \\'(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\\'\\\\n\\\\nresult = model(prompt, negative_prompt=negative_prompt)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 596, "text": " We need to detect potholes in a provided image to help the city government take necessary actions in fixing damaged roads.\\n###Input: {\\\"image\\\": \\\"https://example.com/pothole_image.jpg\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-pothole-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-pothole-segmentation\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local image path\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-pothole-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pothole-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.858, \\'mAP@0.5(mask)\\': 0.895}}, \\'description\\': \\'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 597, "text": " You are working with a fashion magazine that needs a tool to generate a variation of a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 598, "text": " A travel agency wants to use computer vision for generating virtual tours of tourist attractions based on the provided text description.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 599, "text": " Generate an image for marketing purposes depicting a concert of a famous pop star based on the line art as input.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 600, "text": " We are aiming to help a designer to come up with a poster visual for their new home decor product line inspired by a royal chamber with a fancy bed.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 601, "text": " We are building a website to showcase various churches. Write a Python script that generates a high-quality image of a church using Denoising Diffusion Probabilistic Models (DDPM).\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-bedroom-256', 'api_call': 'DDPMPipeline.from_pretrained(model_id)', 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-ema-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics, capable of producing high-quality image synthesis results. The model can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. It obtains an Inception score of 9.46 and a state-of-the-art FID score of 3.17 on the unconditional CIFAR10 dataset.'}\", metadata={})]", "category": "generic"}
{"question_id": 602, "text": " How can I generate an image using the provided API for a creative project?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'distilgpt2\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'distilgpt2\\')\", \\'api_arguments\\': [\\'model\\'], \\'python_environment_requirements\\': [\\'from transformers import pipeline, set_seed\\'], \\'example_code\\': \\'set_seed(42)\\\\ngenerator(Hello, I\u2019m a language model, max_length=20, num_return_sequences=5)\\', \\'performance\\': {\\'dataset\\': \\'WikiText-103\\', \\'accuracy\\': \\'21.100\\'}, \\'description\\': \\'DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. With 82 million parameters, it was developed using knowledge distillation and designed to be a faster, lighter version of GPT-2. It can be used for text generation, writing assistance, creative writing, entertainment, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 603, "text": " We want to prepare some Minecraft skin images for a gaming event. Use an API to generate some unique skins.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 604, "text": " Generate an image of a galaxy using a pre-trained diffusion model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 605, "text": " Develop a method for us to categorize videos according to their contents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 606, "text": " An application that can identify human actions in short video clips is needed. For this task, select a model suitable for video classification.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'facebook/timesformer-base-finetuned-k400\\', \\'api_call\\': \"TimesformerForVideoClassification.from_pretrained(\\'facebook/timesformer-base-finetuned-k400\\')\", \\'api_arguments\\': \\'video, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 607, "text": " Our content provides physical fitness videos to customers. Identifying and classifying the type of exercise in each video would help provide personalized recommendations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 608, "text": " We need to classify some video clips for the exercise videos' application. Use a suitable video model to do this.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'facebook/timesformer-base-finetuned-k400\\', \\'api_call\\': \"TimesformerForVideoClassification.from_pretrained(\\'facebook/timesformer-base-finetuned-k400\\')\", \\'api_arguments\\': \\'video, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 609, "text": " I'm creating a security system for my company. I want to analyze video feeds and decide the main action occurring in the video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 610, "text": " A member of a museum curatorial team needs to categorize new art pieces recently acquired for their online database. They want to classify them automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 611, "text": " Imagine you are building an app that identifies dog breed from a photo. We need to know the breed when user uploads a new photo.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 612, "text": " We have a finance startup working on making investing easy. We want to analyze sentiment of financial news.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 613, "text": " I would like to create a tool for investors to analyze stock-related comments and classify them as \\\"Bullish\\\" or \\\"Bearish\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Inferencing for stock-related comments\\', \\'api_name\\': \\'zhayunduo/roberta-base-stocktwits-finetuned\\', \\'api_call\\': \"Output: RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\", \\'api_arguments\\': {\\'model\\': \\'RobertaForSequenceClassification\\', \\'tokenizer\\': \\'RobertaTokenizer\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\\\nfrom transformers import pipeline\\\\nimport pandas as pd\\\\nimport emoji\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\\\nsentences = pd.Series([\\'just buy\\',\\'just sell it\\',\\'entity rocket to the sky!\\',\\'go down\\',\\'even though it is going up, I still think it will not keep this trend in the near future\\'])\\\\nsentences = list(sentences)\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'stocktwits\\', \\'accuracy\\': 0.9343}, \\'description\\': \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags \\'Bullish\\' or \\'Bearish\\'.\"}', metadata={})]", "category": "generic"}
{"question_id": 614, "text": " In my department store, analyze the customers' reviews in the German language for better feedback.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'svalabs/gbert-large-zeroshot-nli\\', \\'api_call\\': \"pipeline(\\'zero-shot-classification\\', model=\\'svalabs/gbert-large-zeroshot-nli\\')\", \\'api_arguments\\': [\\'sequence\\', \\'labels\\', \\'hypothesis_template\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nzershot_pipeline = pipeline(zero-shot-classification, model=svalabs/gbert-large-zeroshot-nli)\\\\nsequence = Ich habe ein Problem mit meinem Iphone das so schnell wie m\u00f6glich gel\u00f6st werden muss\\\\nlabels = [Computer, Handy, Tablet, dringend, nicht dringend]\\\\nhypothesis_template = In diesem Satz geht es um das Thema {}. \\', \\'performance\\': {\\'dataset\\': \\'XNLI TEST-Set\\', \\'accuracy\\': \\'85.6%\\'}, \\'description\\': \\'A German zeroshot classification model based on the German BERT large model from deepset.ai and finetuned for natural language inference using machine-translated nli sentence pairs from mnli, anli, and snli datasets.\\'}', metadata={})]", "category": "generic"}
{"question_id": 615, "text": " Our software system has communication interactions with users. We need to identify whether the received message is gibberish or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 616, "text": " We have online medical records and we want to remove sensitive information like names, dates, addresses, etc. Is there any API available to simplify this process?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"Output: MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 617, "text": " I have a list of information on the Olympic Games and I need to find the cities and years they were held in. Can you generate a table for me?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=BartForConditionalGeneration.from_pretrained(\\'philschmid/bart-large-cnn-samsum\\'))\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 618, "text": " The company we are working with is trying to get information about the medical industry. They want to get answers to their questions based on information from tables.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 619, "text": " A medical research team needs an AI model to extract answers from medical articles for their research project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'facebook/nllb-200-distilled-600M\\', \\'api_call\\': \"pipeline(\\'translation_xx_to_yy\\', model=\\'facebook/nllb-200-distilled-600M\\')\", \\'api_arguments\\': [\\'model\\', \\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline; translator = pipeline(\\'translation_xx_to_yy\\', model=\\'facebook/nllb-200-distilled-600M\\'); translator(\\'Hello World\\')\", \\'performance\\': {\\'dataset\\': \\'Flores-200\\', \\'accuracy\\': \\'BLEU, spBLEU, chrF++\\'}, \\'description\\': \\'NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 620, "text": " I need information about Mount Everest in Chinese. Please tell me how high it is.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\', \\'api_call\\': \\'model.encode(text)\\', \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'sentence-transformers library\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\\\\ntext = Replace me by any text you\\'d like.\\\\ntext_embbedding = model.encode(text)\", \\'performance\\': {\\'dataset\\': \\'1,097,953,922\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 621, "text": " I'm a retinal surgeon, and I want to detect which diseases have been cured or quite stable based on a given clinical note.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 622, "text": " Assess the compatibility of two Russian sentences you are given.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 623, "text": " I need to display this phrase \\\"Bonjour, comment  va?\\\" in English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 624, "text": " We need to translate a recipe from our Italian cookbook to English for international customers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 625, "text": " Can you help me to create an implementation to translate English text to French text?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 626, "text": " A tourist company requested that for each destination, they need to include a detailed brief written in different languages. They want you to create a system that will handle this.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\', \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 627, "text": " As a content-manager in the field of computer AI, I want to generate summaries for blog posts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 628, "text": " My company receives many lengthy reports, and we need a tool to automatically generate summaries.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 629, "text": " I'm working on a research project about patents in the tech industry. I need help summarizing lengthy patent documents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Multilingual Question Answering\\', \\'api_name\\': \\'mrm8488/bert-multi-cased-finetuned-xquadv1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\', tokenizer=\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\')\", \\'api_arguments\\': {\\'context\\': \\'string\\', \\'question\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"qa_pipeline({\\\\n \\'context\\': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\\\n \\'question\\': Who has been working hard for hugginface/transformers lately?\\\\n})\", \\'performance\\': {\\'dataset\\': \\'XQuAD\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.\\'}', metadata={})]", "category": "generic"}
{"question_id": 630, "text": " Our client, a news website, is looking for an automated system to provide summaries of long articles in Russian. Generate a summary for a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'DialogLED-base-16384\\', \\'api_call\\': \"LEDForConditionalGeneration.from_pretrained(\\'MingZhong/DialogLED-base-16384\\')\", \\'api_arguments\\': \\'input_text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'arxiv\\', \\'accuracy\\': \\'2109.02492\\'}, \\'description\\': \\'DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.\\'}', metadata={})]", "category": "generic"}
{"question_id": 631, "text": " I want to create a conversation with a fictional character I am designing. Can you help me generate text in the character's voice?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 632, "text": " We are looking to implement a chatbot that can engage in open-domain conversations with our customers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 633, "text": " Generate a short summary of the highlighted benefits of a low-carb diet and the possible drawbacks.\\n###Input: A low-carb diet can bring numerous benefits, including weight loss, improved blood sugar control, enhanced mental focus, lower blood pressure, and reduced hunger. However, there are potential drawbacks, such as nutrient deficiencies, increased risk of heart disease due to higher saturated fat consumption, and an initial period of low energy and mood swings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 634, "text": " I have limited computational resources. Design an AI that can complete short stories from given prompts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 635, "text": " Greg desires a translation of the following sentence \\\"What is your favorite color?\\\" from English to German.\\n###Input: What is your favorite color?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/blenderbot-90M\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\", \\'api_arguments\\': {\\'input_message\\': \\'str\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\n\\\\n# Chat with the model\\\\ninput_message = \\'What is your favorite color?\\'\\\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors=\\'pt\\')\\\\n\\\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n\\\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead.\\'}', metadata={})]", "category": "generic"}
{"question_id": 636, "text": " Give me a brief summary of a long conversation between five people discussing the future of technology and privacy issues.\\n###Input: '<long_conversation>'\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/bigbird-pegasus-large-arxiv\\', \\'api_call\\': \"BigBirdPegasusForConditionalGeneration.from_pretrained(\\'google/bigbird-pegasus-large-arxiv\\')\", \\'api_arguments\\': [\\'attention_type\\', \\'block_size\\', \\'num_random_blocks\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\ntext = Replace me by any text you\\'d like.\\\\ninputs = tokenizer(text, return_tensors=\\'pt\\')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\", \\'performance\\': {\\'dataset\\': \\'scientific_papers\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.028, \\'ROUGE-2\\': 13.417, \\'ROUGE-L\\': 21.961, \\'ROUGE-LSUM\\': 29.648}}, \\'description\\': \"BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT\\'s attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\"}', metadata={})]", "category": "generic"}
{"question_id": 637, "text": " Create a simple text-based calculator that uses a language model for arithmetic operations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Spoken Language Identification\\', \\'api_name\\': \\'TalTechNLP/voxlingua107-epaca-tdnn\\', \\'api_call\\': \"\\'EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\'\", \\'api_arguments\\': [\\'signal\\'], \\'python_environment_requirements\\': [\\'speechbrain\\', \\'torchaudio\\'], \\'example_code\\': \\'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\', \\'performance\\': {\\'dataset\\': \\'VoxLingua107\\', \\'accuracy\\': \\'93%\\'}, \\'description\\': \\'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 638, "text": " Create a program that will predict what word is needed to fill in the blanks in the following incomplete sentence. \\\"She decided to ____ a movie tonight because she was tired.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 639, "text": " I have a sentence about my cat. The cat is a missing word in Spanish. Help me complete the sentence.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 640, "text": " We need a model to evaluate the similarity between two sentences for a text-analysis application.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 641, "text": " Can you generate Japanese audio files of the given text?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 642, "text": " Setup a voice assistant to play Korean novel audiobooks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Feature Extraction\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'kobart-base-v2\\', \\'api_call\\': \"BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\"}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'tokenizers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import PreTrainedTokenizerFast, BartModel\\\\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\\\\nmodel = BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'performance\\': {\\'dataset\\': \\'NSMC\\', \\'accuracy\\': 0.901}, \\'description\\': \\'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 643, "text": " The language learning app developers have requested that we implement TTS functionality for their Chinese language study materials.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-zh\\', \\'api_call\\': \"pipeline(\\'translation_en_to_zh\\', model=\\'Helsinki-NLP/opus-mt-en-zh\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_en_to_zh\\', model=\\'Helsinki-NLP/opus-mt-en-zh\\')\\\\ntranslated_text = translation(\\'Hello, world!\\')\", \\'performance\\': {\\'dataset\\': \\'Tatoeba-test.eng.zho\\', \\'accuracy\\': {\\'BLEU\\': 31.4, \\'chr-F\\': 0.268}}, \\'description\\': \"A translation model for English to Chinese using the Hugging Face Transformers library. It is based on the Marian NMT model and trained on the OPUS dataset. The model requires a sentence initial language token in the form of \\'>>id<<\\' (id = valid target language ID).\"}', metadata={})]", "category": "generic"}
{"question_id": 644, "text": " I need a solution to transcribe voice memos from my phone into written text.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 645, "text": " Develop an automatic speech recognition system for a customer service hotline.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transcription\\', \\'api_name\\': \\'openai/whisper-tiny.en\\', \\'api_call\\': \"WhisperForConditionalGeneration.from_pretrained(\\'openai/whisper-tiny.en\\')\", \\'api_arguments\\': {\\'model_name\\': \\'openai/whisper-tiny.en\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'datasets\\', \\'torch\\'], \\'example_code\\': [\\'from transformers import WhisperProcessor, WhisperForConditionalGeneration\\', \\'from datasets import load_dataset\\', \\'processor = WhisperProcessor.from_pretrained(openai/whisper-tiny.en)\\', \\'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-tiny.en)\\', \\'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\', \\'sample = ds[0][audio]\\', \\'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\', \\'predicted_ids = model.generate(input_features)\\', \\'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech (clean)\\', \\'accuracy\\': 8.437}, \\'description\\': \\'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 646, "text": " I would like to clean up some noisy audio recordings so they can be played at a conference.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 647, "text": " Our company is developing an app that separates different speakers in a podcast recording. We need to find a transformer model that can help us achieve this.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 648, "text": " Convert the Hokkien speech into English speech so that everyone can understand it.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Voice Activity Detection', 'framework': 'pyannote.audio', 'functionality': 'Automatic Speech Recognition', 'api_name': 'pyannote/voice-activity-detection', 'api_call': 'Pipeline.from_pretrained(pyannote/voice-activity-detection)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # active speech between speech.start and speech.end', 'performance': {'dataset': 'ami', 'accuracy': 'Not specified'}, 'description': 'A pretrained voice activity detection pipeline that detects active speech in audio files.'}\", metadata={})]", "category": "generic"}
{"question_id": 649, "text": " Develop an audio-to-audio translation script that converts a short audio file from one language to another without using text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'audio\\', \\'api_name\\': \\'textless_sm_cs_en\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(cached_download(\\'https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt\\'))\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\'], \\'example_code\\': \"from fairseq.models.wav2vec.wav2vec2_asr import Wav2Vec2Model\\\\nfrom huggingface_hub import cached_download\\\\n\\\\nmodel = Wav2Vec2Model.from_pretrained(cached_download(\\'https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt\\'))\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A speech-to-speech translation model for converting between languages without using text as an intermediate representation. This model is designed for the task of audio-to-audio translation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 650, "text": " The company's call center needs a solution to enhance the quality of noisy recordings for better customer interactions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Emotion Recognition\\', \\'api_name\\': \\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\')\", \\'api_arguments\\': \\'wav2vec2, tokenizer\\', \\'python_environment_requirements\\': \\'transformers 4.8.2, pytorch 1.9.0+cu102, datasets 1.9.0, tokenizers 0.10.3\\', \\'example_code\\': \\'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\', \\'performance\\': {\\'dataset\\': \\'RAVDESS\\', \\'accuracy\\': 0.8223}, \\'description\\': \"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = [\\'angry\\', \\'calm\\', \\'disgust\\', \\'fearful\\', \\'happy\\', \\'neutral\\', \\'sad\\', \\'surprised\\'].\"}', metadata={})]", "category": "generic"}
{"question_id": 651, "text": " I want to create an app for emotion recognition, which can detect emotions in people's voices based on a given audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'padmalcom/wav2vec2-large-emotion-detection-german\\', \\'api_call\\': \"pipeline(\\'audio-classification\\', model=Wav2Vec2ForCTC.from_pretrained(\\'padmalcom/wav2vec2-large-emotion-detection-german\\'))\", \\'api_arguments\\': \\'audio_file\\', \\'python_environment_requirements\\': \\'transformers, torch\\', \\'example_code\\': \"from transformers import pipeline\\\\naudio_classifier = pipeline(\\'audio-classification\\', model=\\'padmalcom/wav2vec2-large-emotion-detection-german\\')\\\\nresult = audio_classifier(audio_file)\", \\'performance\\': {\\'dataset\\': \\'emo-DB\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This wav2vec2 based emotion detection model is trained on the emo-DB dataset. It can classify emotions in German audio files into seven classes: anger, boredom, disgust, fear, happiness, sadness, and neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 652, "text": " Our company provides a global voice assistant. We need to identify the language used in incoming calls to provide the right service to callers in their language. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 653, "text": " Analyze a recording of a Russian customer service representative to determine their emotion during the conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 654, "text": " Create a system that can detect voice activity in audio files, estimate the speech-to-noise ratio, and measure the C50 room acoustics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 655, "text": " I have a set of plant observations with various features, and I need to categorize them into different species. What classifier can you suggest and what do I need to implement it?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 656, "text": " Generate predictions for carbon emissions from a dataset provided as a .csv file to make our factory more eco-friendly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 657, "text": " I want to predict future carbon emissions within a location. Develop an automation tool that provides continuous data analysis and estimates future changes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 658, "text": " I am an environmental expert, and my job is to predict carbon emissions given a dataset. The dataset has columns for different factors affecting emissions.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 659, "text": " Can you analyze the dataset containing information about electric cars and predict the carbon emissions for specific vehicles?\\n###Input: {'data': 'data.csv'}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-1.3b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', \\'PygmalionAI/pygmalion-1.3b\\')\", \\'api_arguments\\': \\'input_prompt\\', \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"[CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI\\'s pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 660, "text": " You are designing a gaming AI and you want to use the existing sb3/dqn-Acrobot-v1 model as your starting point. Train your gaming AI using this model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/blenderbot-90M\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\", \\'api_arguments\\': {\\'input_message\\': \\'str\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\n\\\\n# Chat with the model\\\\ninput_message = \\'What is your favorite color?\\'\\\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors=\\'pt\\')\\\\n\\\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n\\\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead.\\'}', metadata={})]", "category": "generic"}
{"question_id": 661, "text": " A gaming company is eager to develop a new reinforcement learning game based on ant locomotion. We need to use the pre-trained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Inference API\\', \\'api_name\\': \\'Antheia/Hanna\\', \\'api_call\\': \"pipeline(\\'robotics\\', model=\\'Antheia/Hanna\\') should remain unchanged as it already only includes the model instantiation.\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'openai/webgpt_comparisons\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 662, "text": " I have a Soccer simulation with two teams, and I would like to train agents to play effectively in the simulation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 663, "text": " The factory needs an intelligent robot to perform complex tasks with real-time interaction. The robot needs to have a visual model that can manipulate objects and navigate in an indoor environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation, Summarization, Question Answering, Text Classification\\', \\'api_name\\': \\'t5-base\\', \\'api_call\\': \"T5Model.from_pretrained(\\'t5-base\\')\", \\'api_arguments\\': [\\'input_ids\\', \\'decoder_input_ids\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import T5Tokenizer, T5Model\\\\ntokenizer = T5Tokenizer.from_pretrained(\\'t5-base\\')\\\\nmodel = T5Model.from_pretrained(\\'t5-base\\')\\\\ninput_ids = tokenizer(\\'Studies have been shown that owning a dog is good for you\\', return_tensors=\\'pt\\').input_ids\\\\ndecoder_input_ids = tokenizer(\\'Studies show that\\', return_tensors=\\'pt\\').input_ids\\\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\\\nlast_hidden_states = outputs.last_hidden_state\", \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'See research paper, Table 14\\'}, \\'description\\': \\'T5-Base is a Text-To-Text Transfer Transformer (T5) model with 220 million parameters. It is designed to perform various NLP tasks, including machine translation, document summarization, question answering, and text classification. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be used with the Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 664, "text": " Please generate a representation of an image from the URL: 'http://images.cocodataset.org/val2017/000000039769.jpg'\\n###Input: http://images.cocodataset.org/val2017/000000039769.jpg\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 665, "text": " Classify the content of the video clip.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 666, "text": " Our client wants to create an image from a text description, and also needs to inpaint a specific part of the image using a mask.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 667, "text": " Develop an AI service that generates pictures of people in the context of various professions, such as a doctor, a teacher, or a chef.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 668, "text": " I need the GPT model to generate a higher-resolution image of a small simple yellow bird in a forest.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 669, "text": " I would like to recognize the handwritten text in an image in the future with a given URL.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 670, "text": " At a party, people want to know what is in the image displayed in the living room. Could you help them answer some questions related to the image?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 671, "text": " Our clothes store client wants to build a webpage that suggests the right combination or matching colors for various types of fashion items.\\n###Input: {\\\"image_path\\\": \\\"path/to/fashion_item_image.jpg\\\", \\\"question\\\": \\\"What colors should be combined with the color in this fashion item?\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'TehVenom/PPO_Pygway-V8p4_Dev-6b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'TehVenom/PPO_Pygway-V8p4_Dev-6b\\') should be rewritten by looking at the example code and determining the right model instantiation. However, there is not enough information in the example code to determine the correct model instantiation. Therefore, no changes should be made to the value of the `api_call` field.\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'TODO card. Mix of (GPT-J-6B-Janeway + PPO_HH_GPT-J) + Pygmalion-6b-DEV (V8 / Part 4). At a ratio of GPT-J-6B-Janeway - 20%, PPO_HH_GPT-J - 20%, Pygmalion-6b DEV (V8 / Part 4) - 60%.\\'}', metadata={})]", "category": "generic"}
{"question_id": 672, "text": " Can you guide me on how to extract specific information from a document using a model that can answer questions based on the document layout?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-cased-distilled-squad\\', \\'api_call\\': \"DistilBertForQuestionAnswering.from_pretrained(\\'distilbert-base-cased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-cased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': {\\'Exact Match\\': 79.6, \\'F1\\': 86.996}}, \\'description\\': \"DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.\"}', metadata={})]", "category": "generic"}
{"question_id": 673, "text": " We are an educational institution experiencing so many forms getting filled out. For our research purpose we want to pick a particular answer \\\"parent's name\\\" given a picture of form filled out.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 674, "text": " I am an accountant in a company, and I need to extract information from invoices. Can you please assist me in designing a solution for it?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=BartForConditionalGeneration.from_pretrained(\\'philschmid/bart-large-cnn-samsum\\'))\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 675, "text": " Design a system that takes document images and user questions to provide answers from the document.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Document Question Answer', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_call': 'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023', 'api_arguments': {}, 'python_environment_requirements': {'transformers': '>=4.11.0'}, 'example_code': {}, 'performance': {'dataset': {}, 'accuracy': {}}, 'description': 'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.'}\", metadata={})]", "category": "generic"}
{"question_id": 676, "text": " I have a document containing a college transcript. I'd like to know the student's overall GPA displayed on the transcript.\\n###Input: \\\"your_question\\\": \\\"What is the overall GPA on the transcript?\\\", \\\"your_context\\\": \\\"Multimodal Document Question Answer\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 677, "text": " Our client wants to categorize their product images into different categories. Prepare a solution to classify those images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 678, "text": " I need to find a way to automatically detect objects in images, ideally using a model suited towards smaller images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 679, "text": " Our e-sports team needs to detect objects like enemies, teammates, and dropped spike in the Valorant game.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-valorant-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-valorant-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-valorant-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'valorant-object-detection\\', \\'accuracy\\': 0.965}, \\'description\\': \\'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\\'}', metadata={})]", "category": "generic"}
{"question_id": 680, "text": " Our company is responsible for the safety of workers in a warehouse. We need to detect forklifts and persons in the surveillance images to prevent accidents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 681, "text": " We want to extract the outline and label each element in an image of a factory.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 682, "text": " A client needs to generate handwritten texts using controlnets with diffusion. Implement the controlnet model with canny edges and generate a colorized image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-r-textcaps\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'microsoft/git-large-r-textcaps\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TextCaps\\', \\'accuracy\\': \\'\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 683, "text": " For my online art gallery, I need to create an image of a \\\"mystical forest with colorful animals\\\" based on textual description.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 684, "text": " As a photographer, I would like to generate an image of \\\"a tropical beach at sunset\\\" using text-to-image generation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 685, "text": " A group of gamers needs to generate unique Minecraft skins for their Minecraft characters.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Diffusers\\', \\'api_name\\': \\'Minecraft-Skin-Diffusion\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\\\\nimage = pipeline().images[0].convert(\\'RGBA\\')\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods.\\'}', metadata={})]", "category": "generic"}
{"question_id": 686, "text": " I have video frames and I am going to build a tool to classify their activities.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 687, "text": " Create a smart security system that can detect any suspicious person inside an indoor building captured in a video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english-ontonotes\\', \\'api_call\\': \\'Output: SequenceTagger.load(\\\\\\\\flair/ner-english-ontonotes\\\\\\\\)\\', \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(flair/ner-english-ontonotes)\\\\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'Ontonotes\\', \\'accuracy\\': \\'89.27\\'}, \\'description\\': \\'This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 688, "text": " An art gallery requires a tool for automatically classifying pictures based on their content. Use an appropriate API to predict the content of an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 689, "text": " I have an image and want to know whether it is a picture of a car, a bicycle, or a person. How can I do this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 690, "text": " Implement a program that can identify if an image is showing animals, food, or vehicles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 691, "text": " Can you create a function to identify the presence of animals in photos, like cats, dogs, and birds? The model must also classify the animal type.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 692, "text": " Develop a simple text box for users to test the language detection model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-large-patch14\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-large-patch14\\')\", \\'api_arguments\\': {\\'model_name\\': \\'google/owlvit-large-patch14\\'}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': [\\'import requests\\', \\'from PIL import Image\\', \\'import torch\\', \\'from transformers import OwlViTProcessor, OwlViTForObjectDetection\\', \\'processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)\\', \\'model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)\\', \\'url = http://images.cocodataset.org/val2017/000000039769.jpg\\', \\'image = Image.open(requests.get(url, stream=True).raw)\\', \\'texts = [[a photo of a cat, a photo of a dog]\\', \\'inputs = processor(text=texts, images=image, return_tensors=pt)\\', \\'outputs = model(**inputs)\\', \\'target_sizes = torch.Tensor([image.size[::-1]])\\', \\'results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'i = 0\\', \\'text = texts[i]\\', \\'boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]\\', \\'score_threshold = 0.1\\', \\'for box, score, label in zip(boxes, scores, labels):\\', \\' box = [round(i, 2) for i in box.tolist()]\\', \\' if score >= score_threshold:\\', \\' print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})\\'], \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 693, "text": " As a support agent for a streaming service, we want to know how the users feel about their content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 694, "text": " A manager of an e-commerce store needs to automatically classify customer reviews based on their emotional content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'lvwerra/distilbert-imdb\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'lvwerra/distilbert-imdb\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\', \\'pytorch\\'], \\'example_code\\': \"classifier(\\'I love this movie!\\')\", \\'performance\\': {\\'dataset\\': \\'imdb\\', \\'accuracy\\': 0.928}, \\'description\\': \\'This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\\'}', metadata={})]", "category": "generic"}
{"question_id": 695, "text": " I have recently authored an online article, and I need to make sure it's safe for a work environment. Can you check if it contains explicit content?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 696, "text": " The company wants to build a product that answers user queries based on provided tables. How to approach this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/dragon-plus-context-encoder\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\", \\'api_arguments\\': [\\'pretrained\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\\\\nquery = \\'Where was Marie Curie born?\\'\\\\ncontexts = [\\\\n  \\'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\',\\\\n  \\'Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\u00e8ne Curie, a doctor of French Catholic origin from Alsace.\\'\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\'pt\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\'pt\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscore1 = query_emb @ ctx_emb[0]\\\\nscore2 = query_emb @ ctx_emb[1]\", \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': 39.0}, \\'description\\': \\'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.\\'}', metadata={})]", "category": "generic"}
{"question_id": 697, "text": " A news agency wants to extract names of people, organizations, and locations mentioned in a given article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 698, "text": " Our users need to answer questions based on the given data in a table format. We need a model to assist them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 699, "text": " We are a sports analytics company. I need to know in which year Beijing hosted the Olympic Games from a given table of Olympic years and cities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/tapex-base-finetuned-wikisql\\', \\'api_call\\': \"BartForConditionalGeneration.from_pretrained(\\'microsoft/tapex-base-finetuned-wikisql\\')\", \\'api_arguments\\': {\\'tokenizer\\': \\'TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wikisql)\\', \\'table\\': \\'pd.DataFrame.from_dict(data)\\', \\'query\\': \\'str\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'pandas\\'], \\'example_code\\': \\'from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wikisql)\\\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wikisql)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'wikisql\\'}, \\'description\\': \\'TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.\\'}', metadata={})]", "category": "generic"}
{"question_id": 700, "text": " We are designing a website that offers a question and answer service for users to provide information based on tables of data. Help us generate answers from given tables.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 701, "text": " I have tables in my app and I want to create a functionality when users can ask questions and the system can pick the correct value from the table.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 702, "text": " We need to create a table-based assistant to respond to queries related to a specific dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'hustvl/yolos-tiny\\', \\'api_call\\': \"YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-tiny\\')\", \\'api_arguments\\': {\\'images\\': \\'image\\', \\'return_tensors\\': \\'pt\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained(\\'hustvl/yolos-tiny\\')\\\\nmodel = YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-tiny\\')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\", \\'performance\\': {\\'dataset\\': \\'COCO 2017 validation\\', \\'accuracy\\': \\'28.7 AP\\'}, \\'description\\': \\'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 703, "text": " We need an AI-based voice assistant for answering questions from a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 704, "text": " A student wants help identifying the main subject of a given text passage. Assist the student with the information they need.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 705, "text": " Review this article on cancer treatment, identify the types of therapies mentioned, and answer any questions I might have.\\n###Input: Cancer therapy has come a long way and has given rise to multiple treatment options. Some common cancer treatments include surgery, chemotherapy, and radiation-based treatments. Immunotherapy, an area gaining more attention, uses the body's immune system to fight cancer cells. Targeted therapies are another option, which selectively block the growth and spread of cancer cells. Finally, hormonal therapy helps regulate hormone levels to slow the growth of hormone-sensitive cancers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 706, "text": " Determine whether the following statement is true, false, or unrelated: \\\"The earth is flat.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-german\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/ner-german\\')\", \\'api_arguments\\': [\\'Sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load(\\'flair/ner-german\\')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence(\\'George Washington ging nach Washington\\')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint(\\'The following NER tags are found:\\')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'conll2003\\', \\'accuracy\\': \\'87.94\\'}, \\'description\\': \\'This is the standard 4-class NER model for German that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 707, "text": " Please create an AI that reads deployment meeting schedule requests and classifies them into high, medium, or low priority.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'julien-c/hotdog-not-hotdog\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'julien-c/hotdog-not-hotdog\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': 0.825}, \\'description\\': \\'A model that classifies images as hotdog or not hotdog.\\'}', metadata={})]", "category": "generic"}
{"question_id": 708, "text": " I'm working on a text completion software that helps users finish their sentences. The model needs to generate the rest of given incomplete sentence.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'sultan/BioM-ELECTRA-Large-SQuAD2\\', \\'api_call\\': \\'sultan/BioM-ELECTRA-Large-SQuAD2\\', \\'api_arguments\\': None, \\'python_environment_requirements\\': [\\'transformers\\', \\'sentencepiece\\'], \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\'question-answering\\', model=\\'sultan/BioM-ELECTRA-Large-SQuAD2\\')\\\\nresult = qa_pipeline({\\'context\\': \\'your_context\\', \\'question\\': \\'your_question\\'})\", \\'performance\\': {\\'dataset\\': \\'SQuAD2.0 Dev\\', \\'accuracy\\': {\\'exact\\': 84.33420365535248, \\'f1\\': 87.49354241889522}}, \\'description\\': \\'BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 709, "text": " Can you please help me communicate to my workers in Germany? My message to them is \\\"Gute Arbeit, weiter so!\\\"\\n###Input: Gute Arbeit, weiter so!\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 710, "text": " One of our customers submitted a request in French, and we need to translate it into English.\\n###Input: Bonjour, j'aimerais connare les dimensions de ce produit, s'il vous pla.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 711, "text": " Your manager requested you to automate sending replies to Brazilian wholesalers' emails reporting the availability of goods.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 712, "text": " Translate the following Italian sentence into English: \\\"Mi chiamo Marco e lavoro come ingegnere.\\\"\\n###Input:  Mi chiamo Marco e lavoro come ingegnere.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 713, "text": " A team working on news articles requests an AI tool to summarize articles automatically so they can save time digesting.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 714, "text": " As a news agency, we need a summary of this long article: \\\"In many cities around the world, urban parks provide an opportunity for residents to escape the hustle and bustle of daily life, allowing them to relax and recharge. Whether walking the dog or playing with the kids, city dwellers can forget about the stresses of work and enjoy the simple pleasures nature provides. London's eight Royal Parks, for example, are a true urban oasis, covering over 5,000 acres of parkland and attracting millions of visitors each year. From Hyde Park to Kensington Gardens, these green spaces offer a range of activities and experiences, including boating on the Serpentine, picnicking in the sun or visiting numerous cultural attractions such as art galleries and historical landmarks.\\\"\\n###Input: \\\"In many cities around the world, urban parks provide an opportunity for residents to escape the hustle and bustle of daily life, allowing them to relax and recharge. Whether walking the dog or playing with the kids, city dwellers can forget about the stresses of work and enjoy the simple pleasures nature provides. London's eight Royal Parks, for example, are a true urban oasis, covering over 5,000 acres of parkland and attracting millions of visitors each year. From Hyde Park to Kensington Gardens, these green spaces offer a range of activities and experiences, including boating on the Serpentine, picnicking in the sun or visiting numerous cultural attractions such as art galleries and historical landmarks.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 715, "text": " Translate a sentence from French to Spanish.\\n###Input: Bonjour, comment  va?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for\\\\nsentences = [\\'\u041f\u0440\u0438\u0432\u0435\u0442! \u041a\u0430\u043a \u0442\u0432\u043e\u0438 \u0434\u0435\u043b\u0430?\\',\\\\n             \\'\u0410 \u043f\u0440\u0430\u0432\u0434\u0430, \u0447\u0442\u043e 42 \u0442\u0432\u043e\u0435 \u043b\u044e\u0431\u0438\u043c\u043e\u0435 \u0447\u0438\u0441\u043b\u043e?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 716, "text": " Give me the summary of the following news article: IBM has announced its plan to acquire Bluestacks, a leading mobile virtualization software, to integrate with its Bigfix endpoint management platform. The goal of this acquisition is to strengthen IBM's mobile device management capabilities and expand the services offered to commercial and government clients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'emotion\\', \\'api_name\\': \\'bhadresh-savani/distilbert-base-uncased-emotion\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'bhadresh-savani/distilbert-base-uncased-emotion\\', return_all_scores=True)\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"prediction = classifier(\\'I love using transformers. The best part is wide range of support and its easy to use\\')\", \\'performance\\': {\\'dataset\\': \\'Twitter-Sentiment-Analysis\\', \\'accuracy\\': 0.938}, \\'description\\': \"Distilbert is created with knowledge distillation during the pre-training phase which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding. It\\'s smaller, faster than Bert and any other Bert-based model. Distilbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer.\"}', metadata={})]", "category": "generic"}
{"question_id": 717, "text": " I have a long academic article due tomorrow, and it's too long. Help me summarize it into a more digestible format.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 718, "text": " I need to find inspiration for a speech I will give at my company's annual meeting. Create a conversation that starts with \\\"Our company is future driven.\\\"\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 719, "text": " I want a model that can help me generate text to finish my sentences on topics related to artificial intelligence, particularly generative AI.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 720, "text": " Write a Python function that takes in a text and generates 10 lines of different programming languages code snippets.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'valhalla/t5-base-e2e-qg\\', \\'api_call\\': \"pipeline(\\'e2e-qg\\', model=\\'valhalla/t5-base-e2e-qg\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'Hugging Face Transformers\\'], \\'example_code\\': \"from pipelines import pipeline\\\\n\\\\ntext = Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python\\'s design philosophy emphasizes code readability with its notable use of significant whitespace.\\\\n\\\\nnlp = pipeline(e2e-qg, model=valhalla/t5-base-e2e-qg)\\\\n\\\\nnlp(text)\", \\'performance\\': {\\'dataset\\': \\'squad\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a T5-base model trained for end-to-end question generation task. Simply input the text and the model will generate multiple questions. You can play with the model using the inference API, just put the text and see the results!\\'}', metadata={})]", "category": "generic"}
{"question_id": 721, "text": " Generate a paragraph describing the benefits of using renewable energy sources.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Generative Commonsense Reasoning\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-common_gen\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'mrm8488/t5-base-finetuned-common_gen\\').generate(input_ids=features[\\'input_ids\\'], attention_mask=features[\\'attention_mask\\'], max_length=max_length)\", \\'api_arguments\\': [\\'words\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\ndef gen_sentence(words, max_length=32):\\\\n input_text = words\\\\n features = tokenizer([input_text], return_tensors=\\'pt\\')\\\\noutput = model.generate(input_ids=features[\\'input_ids\\'], attention_mask=features[\\'attention_mask\\'], max_length=max_length)\\\\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\\\\nwords = tree plant ground hole dig\\\\ngen_sentence(words)\", \\'performance\\': {\\'dataset\\': \\'common_gen\\', \\'accuracy\\': {\\'ROUGE-2\\': 17.1, \\'ROUGE-L\\': 39.47}}, \\'description\\': \"Google\\'s T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.\"}', metadata={})]", "category": "generic"}
{"question_id": 722, "text": " The company plans to build a text-to-text translation system using artificial intelligence, focusing on Korean language conversion. How do we accomplish that?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Feature Extraction\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'kobart-base-v2\\', \\'api_call\\': \"BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\"}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'tokenizers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import PreTrainedTokenizerFast, BartModel\\\\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\\'gogamza/kobart-base-v2\\')\\\\nmodel = BartModel.from_pretrained(\\'gogamza/kobart-base-v2\\')\", \\'performance\\': {\\'dataset\\': \\'NSMC\\', \\'accuracy\\': 0.901}, \\'description\\': \\'KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 723, "text": " Generate a list of questions from a document. The document describes the history of the company Apple Inc.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 724, "text": " I am a high school student and want to train a language model that can complete my sentences. For instance, when I say \\\"I am a\\\", the model should be able to predict the next word.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 725, "text": " I work in a pharmaceutical company and have a text with a missing word related to our field. Please help me find the most suitable word for the context.\\n###Input: \\\"report showed an increase in [MASK] levels after exposure to the new drug.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 726, "text": " I need a software tool to help me complete and format code snippets in various programming languages like Python, Java, and Ruby.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Masked Language Modeling Prediction\\', \\'api_name\\': \\'CodeBERTa-small-v1\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'huggingface/CodeBERTa-small-v1\\')\", \\'api_arguments\\': [\\'task\\', \\'model\\', \\'tokenizer\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'fill_mask(PHP_CODE)\\', \\'performance\\': {\\'dataset\\': \\'code_search_net\\', \\'accuracy\\': None}, \\'description\\': \\'CodeBERTa is a RoBERTa-like model trained on the CodeSearchNet dataset from GitHub. It supports languages like Go, Java, JavaScript, PHP, Python, and Ruby. The tokenizer is a Byte-level BPE tokenizer trained on the corpus using Hugging Face tokenizers. The small model is a 6-layer, 84M parameters, RoBERTa-like Transformer model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 727, "text": " We need a powerful AI to predict the missing words in this statement: \\\"Bob went to the ___ to buy some groceries.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\', \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 728, "text": " I want to find the most similar sentences among a list of sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 729, "text": " We need to find the similarity between multiple sentences for use in our article summarization tool.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 730, "text": " Our organization is creating a search filter to find relevant tweets about a specific topic on Twitter.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 731, "text": " Help us implement a semantic search model to find relevant information in our internal document repository.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/mask2former-swin-large-cityscapes-semantic\\', \\'api_call\\': \"Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-large-cityscapes-semantic\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'facebook/mask2former-swin-large-cityscapes-semantic\\'}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"processor = AutoImageProcessor.from_pretrained(\\'facebook/mask2former-swin-large-cityscapes-semantic\\')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-large-cityscapes-semantic\\')\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \\'performance\\': {\\'dataset\\': \\'Cityscapes\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\\'}', metadata={})]", "category": "generic"}
{"question_id": 732, "text": " You are running an e-commerce, you need to cluster similar customer questions together to enhance the product accuracy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 733, "text": " Our company is creating a virtual assistant, and we want to integrate text-to-speech functionality into our product for better user experience.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 734, "text": " I need an AI that would convert written words into spoken words for audiobooks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\')\", \\'api_arguments\\': {\\'model_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.13.0\\', \\'torch\\': \\'1.10.0\\', \\'datasets\\': \\'1.14.0\\'}, \\'example_code\\': \\'processor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\ninput_values = processor(ds[0][audio][array], return_tensors=pt).input_values\\\\nwith torch.no_grad():\\\\n    logits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words.\\'}', metadata={})]", "category": "generic"}
{"question_id": 735, "text": " We are adding a feature for converting speech to text in meetings to create transcripts automatically. How can we implement this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 736, "text": " We aim to develop a tool to remove background noise from voice recordings. Let's access the available enhancement model to preprocess the recordings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/dragon-plus-context-encoder\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\", \\'api_arguments\\': [\\'pretrained\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\\\\nquery = \\'Where was Marie Curie born?\\'\\\\ncontexts = [\\\\n  \\'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\',\\\\n  \\'Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\u00e8ne Curie, a doctor of French Catholic origin from Alsace.\\'\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\'pt\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\'pt\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscore1 = query_emb @ ctx_emb[0]\\\\nscore2 = query_emb @ ctx_emb[1]\", \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': 39.0}, \\'description\\': \\'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.\\'}', metadata={})]", "category": "generic"}
{"question_id": 737, "text": " I would like to create an application that allows me to translate someone's speech in Hokkien into English audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Language model\\', \\'api_name\\': \\'google/flan-t5-small\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'google/flan-t5-small\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import T5Tokenizer, T5ForConditionalGeneration\\\\ntokenizer = T5Tokenizer.from_pretrained(google/flan-t5-small)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(google/flan-t5-small)\\\\ninput_text = translate English to German: How old are you?\\\\ninput_ids = tokenizer(input_text, return_tensors=pt).input_ids\\\\noutputs = model.generate(input_ids)\\\\nprint(tokenizer.decode(outputs[0]))\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'MMLU\\', \\'accuracy\\': \\'75.2%\\'}]}, \\'description\\': \\'FLAN-T5 small is a fine-tuned version of T5 language model on more than 1000 additional tasks covering multiple languages. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. The model is designed for research on language models, including zero-shot and few-shot NLP tasks, reasoning, question answering, fairness, and safety research. It has not been tested in real-world applications and should not be used directly in any application without prior assessment of safety and fairness concerns specific to the application.\\'}', metadata={})]", "category": "generic"}
{"question_id": 738, "text": " We are a live streaming company with a lot of environmental noises. Find a way to classify the noises present.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 739, "text": " We need a way to classify audio tracks from smartphones or smart audio devices to recognize if the audio is coming from a particular speaker or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'superb/hubert-large-superb-sid\\', \\'api_call\\': \"pipeline(\\'audio-classification\\', model=\\'superb/hubert-large-superb-sid\\')\", \\'api_arguments\\': \\'file, top_k\\', \\'python_environment_requirements\\': \\'datasets, transformers, librosa\\', \\'example_code\\': \\'from datasets import load_dataset\\\\nfrom transformers import pipeline\\\\ndataset = load_dataset(anton-l/superb_demo, si, split=test)\\\\nclassifier = pipeline(audio-classification, model=superb/hubert-large-superb-sid)\\\\nlabels = classifier(dataset[0][file], top_k=5)\\', \\'performance\\': {\\'dataset\\': \\'VoxCeleb1\\', \\'accuracy\\': 0.9035}, \\'description\\': \\'Hubert-Large for Speaker Identification. This model is pretrained on 16kHz sampled speech audio and should be used with speech input also sampled at 16Khz. It is used for the SUPERB Speaker Identification task and can classify each utterance for its speaker identity as a multi-class classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 740, "text": " Analyze audio recordings from company meetings and identify the segments where multiple people are talking at the same time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Entity Extraction\\', \\'api_name\\': \\'903429548\\', \\'api_call\\': \"Output: AutoModelForTokenClassification.from_pretrained(\\'ismail-lucifer011/autotrain-company_all-903429548\\', use_auth_token=True)\", \\'api_arguments\\': {\\'inputs\\': \\'I love AutoTrain\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoModelForTokenClassification, AutoTokenizer\\'}, \\'example_code\\': \\'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)\\', \\'performance\\': {\\'dataset\\': \\'ismail-lucifer011/autotrain-data-company_all\\', \\'accuracy\\': 0.9979930566588805}, \\'description\\': \\'A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 741, "text": " We are developing an application to help wine enthusiasts determine the quality of wines. Use the provided API to classify the input wine samples based on their properties.\\n###Input: {\\\"samples\\\": [{\\\"fixed_acidity\\\": 7.4, \\\"volatile_acidity\\\": 0.7, \\\"citric_acid\\\": 0.0, \\\"residual_sugar\\\": 1.9, \\\"chlorides\\\": 0.076, \\\"free_sulfur_dioxide\\\": 11.0, \\\"total_sulfur_dioxide\\\": 34.0, \\\"density\\\": 0.9978, \\\"pH\\\": 3.51, \\\"sulphates\\\": 0.56, \\\"alcohol\\\": 9.4}, {\\\"fixed_acidity\\\": 7.8, \\\"volatile_acidity\\\": 0.88, \\\"citric_acid\\\": 0.0, \\\"residual_sugar\\\": 2.6, \\\"chlorides\\\": 0.098, \\\"free_sulfur_dioxide\\\": 25.0, \\\"total_sulfur_dioxide\\\": 67.0, \\\"density\\\": 0.9968, \\\"pH\\\": 3.2, \\\"sulphates\\\": 0.68, \\\"alcohol\\\": 9.8}]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 742, "text": " Estimate the carbon emissions of the factory from the provided data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 743, "text": " We need to estimate the carbon emissions of a fleet of vehicles based on the dataset containing various features.\\n###Input: data = pd.read_csv('data.csv')\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/dragon-plus-context-encoder\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\", \\'api_arguments\\': [\\'pretrained\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\\\\nquery = \\'Where was Marie Curie born?\\'\\\\ncontexts = [\\\\n  \\'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\',\\\\n  \\'Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\u00e8ne Curie, a doctor of French Catholic origin from Alsace.\\'\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\'pt\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\'pt\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscore1 = query_emb @ ctx_emb[0]\\\\nscore2 = query_emb @ ctx_emb[1]\", \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': 39.0}, \\'description\\': \\'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.\\'}', metadata={})]", "category": "generic"}
{"question_id": 744, "text": " I am building a game and I would like to train an AI character with reinforcement learning in the CartPole environment using PPO.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 745, "text": " I'm building a game based on Lunar Lander. I want to use your model for testing the game performance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'csarron/bert-base-uncased-squad-v1\\', tokenizer=\\'csarron/bert-base-uncased-squad-v1\\')\", \\'api_arguments\\': {\\'model\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'tokenizer\\': \\'csarron/bert-base-uncased-squad-v1\\'}, \\'python_environment_requirements\\': \\'Python 3.7.5\\', \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\n question-answering,\\\\n model=csarron/bert-base-uncased-squad-v1,\\\\n tokenizer=csarron/bert-base-uncased-squad-v1\\\\n)\\\\npredictions = qa_pipeline({\\\\n \\'context\\': The game was played on February 7, 2016 at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California.,\\\\n \\'question\\': What day was the game played on?\\\\n})\\\\nprint(predictions)\", \\'performance\\': {\\'dataset\\': \\'SQuAD1.1\\', \\'accuracy\\': {\\'EM\\': 80.9, \\'F1\\': 88.2}}, \\'description\\': \\'BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 746, "text": " We need a program that will be able to learn how to control a robotic arm for pick and place operations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 747, "text": " An AI company is developing a soccer game. They want to train AI for playing Soccer Twos using your model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/blenderbot-90M\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\", \\'api_arguments\\': {\\'input_message\\': \\'str\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\n\\\\n# Chat with the model\\\\ninput_message = \\'What is your favorite color?\\'\\\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors=\\'pt\\')\\\\n\\\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n\\\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead.\\'}', metadata={})]", "category": "generic"}
{"question_id": 748, "text": " I'm looking for a useful agent for Ant-v3 environment to use the agent, train it, evaluate it, and add it to my hub.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 749, "text": " Our company is developing a soccer training application. We need an agent that can learn and play soccer effectively.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 750, "text": " Please find a method to identify the similarity of English phrases using a feature extraction model.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-video-synthesis', 'api_name': 'damo-vilab/text-to-video-ms-1.7b', 'api_call': 'DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)', 'api_arguments': ['prompt', 'num_inference_steps', 'num_frames'], 'python_environment_requirements': ['pip install git+https://github.com/huggingface/diffusers transformers accelerate'], 'example_code': 'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = Spiderman is surfing\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)', 'performance': {'dataset': 'Webvid', 'accuracy': 'Not specified'}, 'description': 'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.'}\", metadata={})]", "category": "generic"}
{"question_id": 751, "text": " Can you please generate an image of a mysterious knight with a shining armor in a gothic artstyle?\\n###Input: \\\"mysterious knight with shining armor in gothic artstyle\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 752, "text": " We need to extract the text from the image URL for an OCR application.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 753, "text": " Imagine you are designing an app that would allow the user to take a photo and ask questions about it. They are going to take a picture of their living room and have questions about the objects in the room.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 754, "text": " Design an interface to provide the user with the best image captions and interactive responses to questions considering a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 755, "text": " We want to build a system that takes input as a rendered image and generates nice looking textual descriptions for the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 756, "text": " Let's create a Python function that will take an image URL as an input and returns the recognized text in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'valhalla/t5-base-e2e-qg\\', \\'api_call\\': \"pipeline(\\'e2e-qg\\', model=\\'valhalla/t5-base-e2e-qg\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'Hugging Face Transformers\\'], \\'example_code\\': \"from pipelines import pipeline\\\\n\\\\ntext = Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python\\'s design philosophy emphasizes code readability with its notable use of significant whitespace.\\\\n\\\\nnlp = pipeline(e2e-qg, model=valhalla/t5-base-e2e-qg)\\\\n\\\\nnlp(text)\", \\'performance\\': {\\'dataset\\': \\'squad\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a T5-base model trained for end-to-end question generation task. Simply input the text and the model will generate multiple questions. You can play with the model using the inference API, just put the text and see the results!\\'}', metadata={})]", "category": "generic"}
{"question_id": 757, "text": " Design a tool for hearing-impaired people where they can visualize spoken words.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 758, "text": " I have a small company making movies. I want to generate different movie scenes from text input describing the scenes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 759, "text": " I want to design a tool that will take an image and a question as inputs and provide an answer to the question based on the contents of the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 760, "text": " Develop software that answers questions based on an input image for an educational app.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 761, "text": " Using a Visual Question Answering model, provide me with details on where I can find the price of a product on a webpage.\\n###Input: <<<image>>>: image_path, <<<question>>>: \\\"Where can I find the price of a product on this webpage?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 762, "text": " I want to develop an AI-powered program that can scan images of documents with questions and automatically answer them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 763, "text": " The HR department receives a large number of documents every day. They need to find answers to specific questions from these documents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 764, "text": " We are working on a drug discovery application, and we need to classify molecules based on their molecular structure.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 765, "text": " While creating virtual reality games, I need to estimate the depth of objects in 3D space.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 766, "text": " Add a feature to our drone that allows it to estimate the depth of different elements in its field of view.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 767, "text": " I need an AI system that can predict the depth map of a single color image. Can you suggest any models?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 768, "text": " I want to analyze the depth of objects in an image using AI.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 769, "text": " We are building a robot to follow a person while maintaining a specific distance. We need to estimate the depth of each object in the scene.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 770, "text": " I am building an app that can classify the type of animal in a given image. Can you help me configure the code for this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=BartForConditionalGeneration.from_pretrained(\\'philschmid/bart-large-cnn-samsum\\'))\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 771, "text": " Detect the severity of Diabetic Retinopathy in the eye images of patients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 772, "text": " We want to create a machine learning model that can identify objects in photographs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 773, "text": " I need to create a tool that identifies objects in an image to help blind people see the world more effectively.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 774, "text": " I'm developing a mobile app that classifies whether an image contains a cat or not. I need to use an API that can provide excellent accuracy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 775, "text": " There is a security breach in the company's server room. We must detect any unauthorized personnel in the CCTV footage.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 776, "text": " Our company works with processing various types of documents. We need a solution to detect tables in documents, including bordered and borderless ones.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Table Extraction\\', \\'api_name\\': \\'keremberke/yolov8s-table-extraction\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8s-table-extraction\\').predict(image)\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000, \\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8s-table-extraction\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'table-extraction\\', \\'accuracy\\': 0.984}, \\'description\\': \\'A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\\'}', metadata={})]", "category": "generic"}
{"question_id": 777, "text": " Monitor the work of your warehouse employees by detecting forklifts and persons on your security camera images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-forklift-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-forklift-detection\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-forklift-detection\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'forklift-object-detection\\', \\'accuracy\\': 0.846}, \\'description\\': \\'A YOLOv8 model for detecting forklifts and persons in images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 778, "text": " There is a need to segment specific objects in a landscape image for better visual processing.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/git-base-textvqa\\', \\'api_call\\': \"git_base_textvqa = AutoModel.from_pretrained(\\'microsoft/git-base-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa_pipeline({\\'image\\': \\'path/to/image.jpg\\', \\'question\\': \\'What is in the image?\\'})\", \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'Refer to the paper\\'}, \\'description\\': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}', metadata={})]", "category": "generic"}
{"question_id": 779, "text": " We are developing an autonomous car and we need to segment the road and surrounding environment in real-time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 780, "text": " Generate a high-quality image of a human face using a Denoising Diffusion Probabilistic Model.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Denoising Diffusion Probabilistic Models (DDPM)', 'api_name': 'google/ddpm-ema-bedroom-256', 'api_call': 'DDPMPipeline.from_pretrained(model_id)', 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-ema-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics, capable of producing high-quality image synthesis results. The model can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. It obtains an Inception score of 9.46 and a state-of-the-art FID score of 3.17 on the unconditional CIFAR10 dataset.'}\", metadata={})]", "category": "generic"}
{"question_id": 781, "text": " A e-commerce platform wants to generate images of butterflies to boost sales of butterfly related products. Please provide suitable recommendations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 782, "text": " I want to classify an online video about traveling without any label and find out if it's about nature, food, or city life.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 783, "text": " Create a program that can classify a given video file into various categories like sports, music, etc.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 784, "text": " Create a model to classify short videos based on their content for our video-sharing application.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/wavlm-large\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'microsoft/wavlm-large\\')\", \\'api_arguments\\': \\'speech input\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\\', \\'performance\\': {\\'dataset\\': \\'SUPERB benchmark\\', \\'accuracy\\': \\'state-of-the-art performance\\'}, \\'description\\': \\'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 785, "text": " I want to build an app that can recognize images and classify them into categories I choose. The content of the image is like a tool for work, a handheld device, or an instrument for entertainment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 786, "text": " I am an art collector and want to create a recommendation system for categorizing and recognizing diverse art styles. I want to distinguish between different styles, such as impressionism, cubism, and abstract art.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 787, "text": " I have a photo archive that I need to label with different collectibles, such as stamps, coins, and antiques. Automate the process of identifying what's in each picture using a zero-shot image classification model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'NLI-based Zero Shot Text Classification\\', \\'api_name\\': \\'facebook/bart-large-mnli\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'facebook/bart-large-mnli\\')\", \\'api_arguments\\': {\\'sequence_to_classify\\': \\'one day I will see the world\\', \\'candidate_labels\\': \"[\\'travel\\', \\'cooking\\', \\'dancing\\']\"}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\\'}, \\'example_code\\': {\\'with_pipeline\\': \"from transformers import pipeline\\\\nclassifier = pipeline(\\'zero-shot-classification\\', model=\\'facebook/bart-large-mnli\\')\\\\nsequence_to_classify = \\'one day I will see the world\\'\\\\ncandidate_labels = [\\'travel\\', \\'cooking\\', \\'dancing\\']\\\\nclassifier(sequence_to_classify, candidate_labels)\", \\'with_manual_pytorch\\': \"from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nnli_model = AutoModelForSequenceClassification.from_pretrained(\\'facebook/bart-large-mnli\\')\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/bart-large-mnli\\')\\\\npremise = sequence\\\\nhypothesis = f\\'This example is {label}.\\'\\\\nx = tokenizer.encode(premise, hypothesis, return_tensors=\\'pt\\', truncation_strategy=\\'only_first\\')\\\\nlogits = nli_model(x.to(device))[0]\\\\nentail_contradiction_logits = logits[:,[0,2]]\\\\nprobs = entail_contradiction_logits.softmax(dim=1)\\\\nprob_label_is_true = probs[:,1]\"}, \\'performance\\': {\\'dataset\\': \\'multi_nli\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset. The model can be used for zero-shot text classification by posing the sequence to be classified as the NLI premise and constructing a hypothesis from each candidate label. The probabilities for entailment and contradiction are then converted to label probabilities.\\'}', metadata={})]", "category": "generic"}
{"question_id": 788, "text": " Design a system to identify the category of the given image from a collection of categories including animals, vehicles, and nature.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'model.generate(input_ids, max_length=128)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}\", metadata={})]", "category": "generic"}
{"question_id": 789, "text": " I am building a food recommendation service from images. For that, I would like you to determine what the dish is supposed to be based on the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 790, "text": " We need to analyze multiple financial news headlines to conclude the sentiment of each headline.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 791, "text": " Could you please analyze the sentiment of the provided customer review text?\\n###Input: \\\"El producto llegen perfectas condiciones, pero tardmucho en llegar.\\\" \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 792, "text": " I am building an app to read tweets and help users understand the general sentiment of those tweets. We want to know if tweets are positive, negative, or neutral.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 793, "text": " Categorize this input comment as toxic or non-toxic.\\n###Input: You're an absolute idiot and I hate everything about you.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 794, "text": " The company received emails from customers. We need to classify them into questions or statements.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 795, "text": " As a company focusing on cybersecurity, we would like to integrate a model into our email filter to detect gibberish text for better scanning.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/xclip-base-patch32\\', \\'api_call\\': \"XClipModel.from_pretrained(\\'microsoft/xclip-base-patch32\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'For code examples, we refer to the documentation.\\', \\'performance\\': {\\'dataset\\': \\'Kinetics 400\\', \\'accuracy\\': {\\'top-1\\': 80.4, \\'top-5\\': 95.0}}, \\'description\\': \\'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\\'}', metadata={})]", "category": "generic"}
{"question_id": 796, "text": " A content moderation company requires an efficient way to detect inappropriate text in the chat rooms. They need help.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 797, "text": " Clients ask us to identify persons, businesses and regions in a text to provide data-driven analytics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 798, "text": " As part of our customer support platform, we need to develop a service that can detect companies mentioned in users' questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 799, "text": " Our company is planning to analyze customer feedback from users in China. We want a system to perform part-of-speech tagging on the Chinese text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Flair\\', \\'functionality\\': \\'Part-of-Speech Tagging\\', \\'api_name\\': \\'flair/upos-english\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/upos-english\\')\", \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': \\'pip install flair\\', \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(\\'flair/upos-english\\')\\\\nsentence = Sentence(\\'I love Berlin.\\')\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans(\\'pos\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'ontonotes\\', \\'accuracy\\': 98.6}, \\'description\\': \\'This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 800, "text": " We are developing a grammar-checking application. We need to identify the part-of-speech tags of input text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 801, "text": " Identify a solution for classifying user's question and their corresponding data in a given table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 802, "text": " Our research team needs assistance in answering questions from a given table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 803, "text": " Our task is to quickly analyze various datasets for an upcoming business presentation. We need answers for several questions related to the dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transcription and Translation\\', \\'api_name\\': \\'openai/whisper-small\\', \\'api_call\\': \"WhisperForConditionalGeneration.from_pretrained(\\'openai/whisper-small\\')\", \\'api_arguments\\': {\\'language\\': \\'english\\', \\'task\\': \\'transcribe\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'datasets\\': \\'latest\\'}, \\'example_code\\': [\\'from transformers import WhisperProcessor, WhisperForConditionalGeneration\\', \\'from datasets import load_dataset\\', \\'processor = WhisperProcessor.from_pretrained(openai/whisper-small)\\', \\'model = WhisperForConditionalGeneration.from_pretrained(openai/whisper-small)\\', \\'model.config.forced_decoder_ids = None\\', \\'ds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\', \\'sample = ds[0][audio]\\', \\'input_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\', \\'predicted_ids = model.generate(input_features)\\', \\'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\', \\'print(transcription)\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech (clean) test set\\', \\'accuracy\\': \\'3.432 WER\\'}, \\'description\\': \\'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and supports transcription and translation in various languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 804, "text": " We are a translation agency focusing on various languages. We have a document containing Korean content, and we need to answer questions based on the context.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 805, "text": " Create a legal-question answering system to extract information from contracts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speaker Verification\\', \\'api_name\\': \\'speechbrain/spkrec-xvect-voxceleb\\', \\'api_call\\': \\'EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\\', \\'api_arguments\\': [\\'source\\', \\'savedir\\'], \\'python_environment_requirements\\': [\\'pip install speechbrain\\'], \\'example_code\\': \"import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nclassifier = EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\\\\nsignal, fs =torchaudio.load(\\'tests/samples/ASR/spk1_snt1.wav\\')\\\\nembeddings = classifier.encode_batch(signal)\", \\'performance\\': {\\'dataset\\': \\'Voxceleb1-test set (Cleaned)\\', \\'accuracy\\': \\'EER(%) 3.2\\'}, \\'description\\': \\'This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 806, "text": " I am an AI scientist checking COVID-19 impacts on different industries. I have some documents that I want to analyze whether the text mentions the impact of the virus on a specific industry.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 807, "text": " I am a Spanish editor, I need to figure out the best category for a news article for subsequent publishing and promotion.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 808, "text": " Our team is working on a new app to encourage people to recycle more. We would like to use AI to classify user submissions in categories like : 'plastic', 'paper', 'glass', 'metal', 'electronics'.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 809, "text": " Our customer is an international law firm, we need to translate contracts from English to French. Make sure the translations are accurate and context-based.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 810, "text": " I have a customer inquiry asking about a refund policy. I need the answer in my native language, French. Can you help me get the details from the company policy?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 811, "text": " Our Team needs a group translator to convert all project documents from listed Romance languages (French, Spanish, Portuguese, Italian, Romanian) to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-ROMANCE-en\\', \\'api_call\\': \"MarianMTModel.from_pretrained(\\'Helsinki-NLP/opus-mt-ROMANCE-en\\') and MarianTokenizer.from_pretrained(\\'Helsinki-NLP/opus-mt-ROMANCE-en\\')\", \\'api_arguments\\': [\\'source languages\\', \\'target languages\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': 62.2, \\'chr-F\\': 0.75}}, \\'description\\': \\'A model for translating Romance languages to English, trained on the OPUS dataset. It supports multiple source languages such as French, Spanish, Portuguese, Italian, and Romanian, among others. The model is based on the transformer architecture and uses normalization and SentencePiece for pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 812, "text": " As a news organization, we require a system to summarize long articles for our readers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 813, "text": " You are helping a journalist to generate a concise news summary to be posted on social media.\\n###Input: <<<ARTICLE>>>: \\\"Tech giant Apple Inc. announced today that the all-new MacBook Pro will feature the company's latest in-house ARM-based M1 chip, dramatically improving performance and battery life. The new MacBook Pro is expected to be up to 3.5 times faster in processing power, and up to six times faster in graphics, compared to its predecessors powered by Intel processors. Apple's shift from Intel processors to its own custom-designed chips marks a significant change in the company's approach to hardware manufacturing. The move is set to make production more efficient, cost-effective, and environmentally friendly. However, the transition is not without its challenges, as software developers will need to adapt their applications to run smoothly on the new hardware. Apple plans to complete the two-year transition period by the end of 2022, with more products featuring the M1 chip expected to be unveiled in the coming months.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 814, "text": " We are a news agency looking to automatically summarize lengthy news articles while maintaining the original meaning.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 815, "text": " Suppose we are building a friendly chatbot to keep clients engaged while waiting for our service, and we want to generate suggestions for their next vacations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 816, "text": " The company wants an AI bot that replies to its employees to answer their questions without human intervention.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'vilt-finetuned-vqasi\\', \\'api_call\\': \"ViltModel.from_pretrained(\\'tufa15nik/vilt-finetuned-vqasi\\')\", \\'api_arguments\\': {\\'model\\': \\'tufa15nik/vilt-finetuned-vqasi\\', \\'tokenizer\\': \\'tufa15nik/vilt-finetuned-vqasi\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.11.3\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.\\'}', metadata={})]", "category": "generic"}
{"question_id": 817, "text": " Help me build a solution that recognizes historical figures, and retrieves interesting details on their achievements. Write a python code as an example.\\n###Input: instruction=\\\"Who was Albert Einstein?\\\", knowledge=\\\"Albert Einstein was a theoretical physicist who developed the theory of relativity. His work is also known for its influence on the philosophy of science. He received the Nobel Prize in Physics in 1921 for his services to theoretical physics, and especially for his discovery of the photoelectric effect, a pivotal step in the development of quantum theory.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 818, "text": " Let's make some interesting stories for young kids.Get the beginning lines of 5 stories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'google/vit-base-patch16-384\\', \\'api_call\\': \"ViTForImageClassification.from_pretrained(\\'google/vit-base-patch16-384\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'google/vit-base-patch16-384\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\\'google/vit-base-patch16-384\\')\\\\nmodel = ViTForImageClassification.from_pretrained(\\'google/vit-base-patch16-384\\')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \\'performance\\': {\\'dataset\\': \\'ImageNet\\', \\'accuracy\\': \\'Refer to tables 2 and 5 of the original paper\\'}, \\'description\\': \\'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\\'}', metadata={})]", "category": "generic"}
{"question_id": 819, "text": " The author can't continue their story. Help them with writing the next few lines.\\n###Input: Hello, I'm am conscious and\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'facebook/opt-13b\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'facebook/opt-13b\\', torch_dtype=torch.float16).cuda().generate(input_ids)\", \\'api_arguments\\': [\\'input_ids\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\nimport torch\\\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-13b, torch_dtype=torch.float16).cuda()\\\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-13b, use_fast=False)\\\\nprompt = Hello, I\\'m am conscious and\\\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\\\ngenerated_ids = model.generate(input_ids)\\\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': \\'GPT-3\\', \\'accuracy\\': \\'roughly match the performance and sizes of the GPT-3 class of models\\'}, \\'description\\': \\'OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The models are trained to match the performance and sizes of the GPT-3 class of models. The primary goal is to enable reproducible and responsible research at scale and to bring more voices to the table in studying the impact of large language models. OPT-13B is a 13-billion-parameter model trained predominantly with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\\'}', metadata={})]", "category": "generic"}
{"question_id": 820, "text": " We are developing an AI model to communicate with humans, give me an example based on the prompt \\\"What is your purpose?\\\"\\n###Input: What is your purpose?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 821, "text": " My son has a difficult time translating English to German, and he needs help with his homework. Develop a translator which can assist.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 822, "text": " We are supporting French language learning. The students should fill the blank in the sentence: \\\"Le camembert est ____\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'camembert-base\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'camembert-base\\', tokenizer=\\'camembert-base\\')\", \\'api_arguments\\': [\\'model\\', \\'tokenizer\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline; camembert_fill_mask = pipeline(\\'fill-mask\\', model=\\'camembert-base\\', tokenizer=\\'camembert-base\\'); results = camembert_fill_mask(\\'Le camembert est <mask> :)\\')\", \\'performance\\': {\\'dataset\\': \\'oscar\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 823, "text": " Predict the missing word in the following text: \\\"I am an AI who can perform various NLP [MASK].\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'cerebras/Cerebras-GPT-111M\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'cerebras/Cerebras-GPT-111M\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'cerebras/Cerebras-GPT-111M\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForCausalLM\\'}, \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(cerebras/Cerebras-GPT-111M)\\\\nmodel = AutoModelForCausalLM.from_pretrained(cerebras/Cerebras-GPT-111M)\\\\ntext = Generative AI is \\', \\'performance\\': {\\'dataset\\': \\'The Pile\\', \\'accuracy\\': {\\'PILE_test_xent\\': 2.566, \\'Hella-Swag\\': 0.268, \\'PIQA\\': 0.594, \\'Wino-Grande\\': 0.488, \\'Lambada\\': 0.194, \\'ARC-e\\': 0.38, \\'ARC-c\\': 0.166, \\'OpenBookQA\\': 0.118, \\'Downstream_Average\\': 0.315}}, \\'description\\': \\'Cerebras-GPT-111M is a transformer-based language model with 111M parameters, trained on the Pile dataset using the GPT-3 style architecture. It is intended for use in research and as a foundation model for NLP applications, ethics, and alignment research. The model can be fine-tuned for various tasks and is licensed under Apache 2.0.\\'}', metadata={})]", "category": "generic"}
{"question_id": 824, "text": " Calculate the similarity between two movie reviews to find out if two users have similar opinions on the film.\\n###Input: \\nReview 1: \\\"I thoroughly enjoyed this movie - the acting was great, the storyline was interesting, and the cinematography was stunning.\\\"\\nReview 2: \\\"The film was a delightful experience, with outstanding performances, an engaging plot, and exceptional visual effects.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'lvwerra/distilbert-imdb\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'lvwerra/distilbert-imdb\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\', \\'pytorch\\'], \\'example_code\\': \"classifier(\\'I love this movie!\\')\", \\'performance\\': {\\'dataset\\': \\'imdb\\', \\'accuracy\\': 0.928}, \\'description\\': \\'This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\\'}', metadata={})]", "category": "generic"}
{"question_id": 825, "text": " We are building a chatbot and we need to be able to compare which response is the most suitable match for the input message from the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 826, "text": " I am building transcript summaries of meeting audio recordings. I need to extract only the most important sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 827, "text": " I have several paragraphs of text from different sources. I want to analyze their similarity in content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 828, "text": " Create a prototype artificial voice for the novel character that we described in the product description. It will be used in an audiobook.\\n###Input: \\\"<noinput>\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-1.3b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', \\'PygmalionAI/pygmalion-1.3b\\')\", \\'api_arguments\\': \\'input_prompt\\', \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"[CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI\\'s pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 829, "text": " A client is launching their website in Arabic and wants a voice-over for their promotional video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot_small-90M\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot_small-90M\\')\", \\'api_arguments\\': [\\'message\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot_small-90M.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 830, "text": " In a podcast with 3 speakers, we want to separate the speakers by segmenting the audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 831, "text": " Design a system to transcribe a podcast episode in a format that can be used for closed captions, including accurate punctuation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 832, "text": " A company wants to develop a voice assistant to be integrated into their web-based services. The main objective of this voice assistant is to transcribe user's voice commands and generate appropriate text inputs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 833, "text": " Analyze customer calls and identify the spoken words in various languages from an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/wavlm-large\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'microsoft/wavlm-large\\')\", \\'api_arguments\\': \\'speech input\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\\', \\'performance\\': {\\'dataset\\': \\'SUPERB benchmark\\', \\'accuracy\\': \\'state-of-the-art performance\\'}, \\'description\\': \\'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 834, "text": " My team is building a customer support chatbot that can detect customer's emotions from their voice messages. We need to evaluate emotions from an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'michellejieli/emotion_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/emotion_text_classifier\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I love this!)\\', \\'performance\\': {\\'dataset\\': [\\'Crowdflower (2016)\\', \\'Emotion Dataset, Elvis et al. (2018)\\', \\'GoEmotions, Demszky et al. (2020)\\', \\'ISEAR, Vikash (2018)\\', \\'MELD, Poria et al. (2019)\\', \\'SemEval-2018, EI-reg, Mohammad et al. (2018)\\', \\'Emotion Lines (Friends)\\'], \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\\'}', metadata={})]", "category": "generic"}
{"question_id": 835, "text": " Develop a voice interface for our robotic vacuum. The robot should recognize different commands from pre-recorded audio files.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Voice Activity Detection', 'framework': 'pyannote.audio', 'functionality': 'Automatic Speech Recognition', 'api_name': 'pyannote/voice-activity-detection', 'api_call': 'Pipeline.from_pretrained(pyannote/voice-activity-detection)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # active speech between speech.start and speech.end', 'performance': {'dataset': 'ami', 'accuracy': 'Not specified'}, 'description': 'A pretrained voice activity detection pipeline that detects active speech in audio files.'}\", metadata={})]", "category": "generic"}
{"question_id": 836, "text": " A German dentist needs help during sessions with his patients. Create an emotion detection system to analyze the emotions in his patients' responses.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'padmalcom/wav2vec2-large-emotion-detection-german\\', \\'api_call\\': \"pipeline(\\'audio-classification\\', model=Wav2Vec2ForCTC.from_pretrained(\\'padmalcom/wav2vec2-large-emotion-detection-german\\'))\", \\'api_arguments\\': \\'audio_file\\', \\'python_environment_requirements\\': \\'transformers, torch\\', \\'example_code\\': \"from transformers import pipeline\\\\naudio_classifier = pipeline(\\'audio-classification\\', model=\\'padmalcom/wav2vec2-large-emotion-detection-german\\')\\\\nresult = audio_classifier(audio_file)\", \\'performance\\': {\\'dataset\\': \\'emo-DB\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This wav2vec2 based emotion detection model is trained on the emo-DB dataset. It can classify emotions in German audio files into seven classes: anger, boredom, disgust, fear, happiness, sadness, and neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 837, "text": " We are working on an application to transcribe large conference calls. We need an algorithm to detect when people are speaking in the recording.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 838, "text": " We are a food technology firm, and we want to explore possible options to classify the quality of wine.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 839, "text": " We are currently developing a flower delivery app, and it should take user's input and classify which type of flower it is.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 840, "text": " The company is interested in predicting the carbon emissions of a new warehouse building.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 841, "text": " We are an organization working on reducing carbon emissions. We have collected data on carbon emission factors and features. Estimate the carbon emissions using this given data.\\n###Input: Please provide a .csv file named \\\"data.csv\\\" containing the necessary features for carbon emission estimation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 842, "text": " I would like to train a reinforcement learning model to play CartPole using pre-trained PPO.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Inference API\\', \\'api_name\\': \\'Antheia/Hanna\\', \\'api_call\\': \"pipeline(\\'robotics\\', model=\\'Antheia/Hanna\\') should remain unchanged as it already only includes the model instantiation.\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'openai/webgpt_comparisons\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 843, "text": " During a showcase of your company, you are asked to demonstrate an interactive experience with an already trained model of a player playing Pong. How would you use the API to replay the game with the saved game data?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 844, "text": " We are developing a kids app that teaches soccer strategies through reinforcement learning. We need a model to demonstrate the gameplay.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 845, "text": " Develop a tool that helps users find relevant code snippets online when given a prompt in natural language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 846, "text": " We have to create a Q&A system for our educational platform for the high school students. The students should be able to ask questions and get relevant answers from the textbook.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 847, "text": " We are building an art product that merges Art and Literature. We need to convert painting images into meaningful text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 848, "text": " We are exploring marketing opportunities and our company wants to create a video advertisement. The ad illustrates an athlete running while promoting our brand new shoes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 849, "text": " Could you help me find the invoice number from an invoice image?\\n###Input: image_url=https://templates.invoicehome.com/invoice-template-us-neat-750px.png, question=What is the invoice number?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'impira/layoutlm-document-qa\\', \\'api_call\\': \"layoutlm_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'impira/layoutlm-document-qa\\', return_dict=True))\", \\'api_arguments\\': [\\'image_url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': \\'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)\\', \\'performance\\': {\\'dataset\\': \\'SQuAD2.0 and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.\\'}', metadata={})]", "category": "generic"}
{"question_id": 850, "text": " Audrey is responsible for invoice management at her company. She needs a tool to assist with the extraction of invoice details such as the invoice number, total, and due date.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'impira/layoutlm-document-qa\\', \\'api_call\\': \"layoutlm_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'impira/layoutlm-document-qa\\', return_dict=True))\", \\'api_arguments\\': [\\'image_url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': \\'nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)\\', \\'performance\\': {\\'dataset\\': \\'SQuAD2.0 and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.\\'}', metadata={})]", "category": "generic"}
{"question_id": 851, "text": " Look at the image for me and find out what's the title of this document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 852, "text": " My company needs to extract information from invoices to analyze their business expenses. We need the algorithm to provide detailed information about each invoice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 853, "text": " We would like to perform graph classification using the Graphormer model for our medical research project to analyze molecular structures.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Graph Machine Learning\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Graph Classification\\', \\'api_name\\': \\'graphormer-base-pcqm4mv2\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'clefourrier/graphormer-base-pcqm4mv2\\')\", \\'api_arguments\\': \\'pretrained_model_name\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'See the Graph Classification with Transformers tutorial.\\', \\'performance\\': {\\'dataset\\': \\'PCQM4M-LSCv2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.\\'}', metadata={})]", "category": "generic"}
{"question_id": 854, "text": " Our client is a startup working on a mobile navigation system. They need to calculate depth estimations from input images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\', \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 855, "text": " I need to remember the perfect birthday cake for my party; could you help me compute the depth estimations from an image of a birthday cake?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 856, "text": " A company focuses on providing suitable advertisements for their customers based on their age. We should assess people's age.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 857, "text": " Detect the main subject of the image in a photo album website.\\n###Input: Image URL: 'http://images.cocodataset.org/val2017/000000039769.jpg'\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'openai/clip-vit-large-patch14\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'openai/clip-vit-large-patch14\\')\", \\'api_arguments\\': {\\'text\\': [\\'a photo of a cat\\', \\'a photo of a dog\\'], \\'images\\': \\'image\\', \\'return_tensors\\': \\'pt\\', \\'padding\\': \\'True\\'}, \\'python_environment_requirements\\': {\\'packages\\': [\\'PIL\\', \\'requests\\', \\'transformers\\']}, \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-large-patch14)\\\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-large-patch14)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [\\'Food101\\', \\'CIFAR10\\', \\'CIFAR100\\', \\'Birdsnap\\', \\'SUN397\\', \\'Stanford Cars\\', \\'FGVC Aircraft\\', \\'VOC2007\\', \\'DTD\\', \\'Oxford-IIIT Pet dataset\\', \\'Caltech101\\', \\'Flowers102\\', \\'MNIST\\', \\'SVHN\\', \\'IIIT5K\\', \\'Hateful Memes\\', \\'SST-2\\', \\'UCF101\\', \\'Kinetics700\\', \\'Country211\\', \\'CLEVR Counting\\', \\'KITTI Distance\\', \\'STL-10\\', \\'RareAct\\', \\'Flickr30\\', \\'MSCOCO\\', \\'ImageNet\\', \\'ImageNet-A\\', \\'ImageNet-R\\', \\'ImageNet Sketch\\', \\'ObjectNet (ImageNet Overlap)\\', \\'Youtube-BB\\', \\'ImageNet-Vid\\'], \\'accuracy\\': \\'varies depending on the dataset\\'}, \\'description\\': \\'The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\\'}', metadata={})]", "category": "generic"}
{"question_id": 858, "text": " A clothing store needs help to identify and label different parts of clothes. Help them with a program that will automatically segment the clothes in an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 859, "text": " The CEO wants us to develop a single model that can perform multiple image segmentation tasks accurately.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Computer Vision Image Segmentation', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'shi-labs/oneformer_ade20k_swin_large', 'api_call': 'OneFormerForUniversalSegmentation.from_pretrained(\\\\\\\\shi-labs/oneformer_ade20k_swin_large\\\\\\\\)', 'api_arguments': ['images', 'task_inputs', 'return_tensors'], 'python_environment_requirements': ['transformers', 'PIL', 'requests'], 'example_code': 'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]', 'performance': {'dataset': 'scene_parse_150', 'accuracy': None}, 'description': 'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.'}\", metadata={})]", "category": "generic"}
{"question_id": 860, "text": " We are an infrastructure management company. We need to detect potholes in images from the road network.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 861, "text": " A film production company wants to estimate the depth of objects in an image taken from one of their video footages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 862, "text": " We are working on a project that involves converting a text description into its corresponding images. The model should also maintain the context of the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 863, "text": " We are creating a digital art show and require random, high-quality computer-generated images to display.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 864, "text": " An architect firm is looking forward to generating artificial images of church layouts. They mainly focus on the facade of the buildings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-photoreal-2.0\\', \\'api_call\\': \"Output: StableDiffusionPipeline.from_pretrained(\\'dreamlike-art/dreamlike-photoreal-2.0\\', torch_dtype=torch.float16)(prompt).images[0]\", \\'api_arguments\\': {\\'prompt\\': \\'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\'}, \\'python_environment_requirements\\': {\\'torch\\': \\'torch.float16\\', \\'diffusers\\': \\'StableDiffusionPipeline\\'}, \\'example_code\\': \\'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'Stable Diffusion 1.5\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 865, "text": " I am writing a book and I need references which helps me to create illustrations of insects, especially butterflies.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 866, "text": " Create a solution for an app that categorizes videos into either violent or non-violent footage.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 867, "text": " Create a model that predicts whether an image contains a dog or a cat given an image url.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-base-patch16\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-base-patch16\\')\", \\'api_arguments\\': [\\'texts\\', \\'images\\'], \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'torch\\', \\'transformers\\'], \\'example_code\\': \\'processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\\\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = [[a photo of a cat, a photo of a dog]]\\\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\\'}', metadata={})]", "category": "generic"}
{"question_id": 868, "text": " Create an image classifier that can identify diverse animals like cats, dogs, and birds.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/git-base-coco\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'microsoft/git-base-coco\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'See the model hub for fine-tuned versions on a task that interests you.\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Refer to the paper for evaluation results.\\'}, \\'description\\': \\'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\'}', metadata={})]", "category": "generic"}
{"question_id": 869, "text": " We are developing an app to classify what type of food is being consumed for a calorie-tracking feature. Implement a model to perform Zero-Shot Image Classification for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 870, "text": " In our online platform, we need to determine if customer reviews are positive or negative.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 871, "text": " Identify if a user inquiry is a question or a statement for a customer support chatbot.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 872, "text": " I am a restaurant owner, I manage a series of comments, and I have a hard time detecting if the comment is good or bad. I need a simple script to detect if my customer is satisfied or not.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 873, "text": " We need a name entity recognition program to extract information included in a text document, which are English, German, or Spanish. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 874, "text": " I need help to identify the entities in the given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 875, "text": " The team wants to implement a tool that can analyze the part-of-speech of a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 876, "text": " Answer a question related to a table/list of data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-large-finetuned-wikisql-supervised\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-large-finetuned-wikisql-supervised\\')\", \\'api_arguments\\': {\\'model\\': \\'google/tapas-large-finetuned-wikisql-supervised\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'tensorflow\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nqa_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-large-finetuned-wikisql-supervised\\')\\\\n\\\\nresult = qa_pipeline(question=\\'What is the capital of France?\\', table=table)\\\\nprint(result)\", \\'performance\\': {\\'dataset\\': \\'wikisql\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.\\'}', metadata={})]", "category": "generic"}
{"question_id": 877, "text": " We are running a sports trivia night at a fundraiser event, and we would like to answer questions based on Olympic Games data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 878, "text": " I am working on a project to create a chatbot for a college library. I'd like the chatbot to be able to answer questions related to specific data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-400M-distill\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-400M-distill\\')\", \\'api_arguments\\': \"[\\'message\\']\", \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-400M-distill.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.\\'}', metadata={})]", "category": "generic"}
{"question_id": 879, "text": " An education service provider wants to find a method to answer the questions based on the contents of a given table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 880, "text": " We have an inventory database and we need to answer some questions for our customers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 881, "text": " An intern in my company is learning Python programming. They are asking for a function that can answer natural language questions given a specific context.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-cased-distilled-squad\\', \\'api_call\\': \"DistilBertForQuestionAnswering.from_pretrained(\\'distilbert-base-cased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-cased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': {\\'Exact Match\\': 79.6, \\'F1\\': 86.996}}, \\'description\\': \"DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.\"}', metadata={})]", "category": "generic"}
{"question_id": 882, "text": " Our company is expanding into France and we need a model that can classify French texts into categories such as sports, politics, and science.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 883, "text": " Our company wants to develop multilingual conversational agents and we want to examine the sentiments in the German news article. I need a code snippet for zero-shot classification.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 884, "text": " We need an AI to help our students to answer their questions in different subjects.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 885, "text": " My science department is about to release a new research paper. I need to summarize the findings before presenting it to the team.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 886, "text": " I am a researcher working in the medical field, and there're some research articles that we would like to summarize to save time for our team.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 887, "text": " One of my friends is a robot fan, I want to send him a gift which is a robot. Please write me a message to ask him if he is interested.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 888, "text": " Assist me in engaging in a conversation with my brother about his favorite food for dinner and recommend a restaurant.\\n###Input: My brother's favorite food is pizza.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/blenderbot-90M\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\", \\'api_arguments\\': {\\'input_message\\': \\'str\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import AutoModelForCausalLM, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\'facebook/blenderbot-90M\\')\\\\n\\\\n# Chat with the model\\\\ninput_message = \\'What is your favorite color?\\'\\\\ntokenized_input = tokenizer.encode(input_message + tokenizer.eos_token, return_tensors=\\'pt\\')\\\\n\\\\noutput = model.generate(tokenized_input, max_length=1000, pad_token_id=tokenizer.eos_token_id)\\\\n\\\\nresponse = tokenizer.decode(output[:, tokenized_input.shape[-1]:][0], skip_special_tokens=True)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BlenderBot-90M is a conversational AI model developed by Facebook AI. It is trained on the Blended Skill Talk dataset and aims to provide engaging and human-like responses in a multi-turn dialogue setting. The model is deprecated, and it is recommended to use the identical model https://huggingface.co/facebook/blenderbot_small-90M instead.\\'}', metadata={})]", "category": "generic"}
{"question_id": 889, "text": " Create a coherent story with a prompt \\\"Once upon a time, in a faraway land\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Generative Commonsense Reasoning\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-common_gen\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'mrm8488/t5-base-finetuned-common_gen\\').generate(input_ids=features[\\'input_ids\\'], attention_mask=features[\\'attention_mask\\'], max_length=max_length)\", \\'api_arguments\\': [\\'words\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\ndef gen_sentence(words, max_length=32):\\\\n input_text = words\\\\n features = tokenizer([input_text], return_tensors=\\'pt\\')\\\\noutput = model.generate(input_ids=features[\\'input_ids\\'], attention_mask=features[\\'attention_mask\\'], max_length=max_length)\\\\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\\\\nwords = tree plant ground hole dig\\\\ngen_sentence(words)\", \\'performance\\': {\\'dataset\\': \\'common_gen\\', \\'accuracy\\': {\\'ROUGE-2\\': 17.1, \\'ROUGE-L\\': 39.47}}, \\'description\\': \"Google\\'s T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.\"}', metadata={})]", "category": "generic"}
{"question_id": 890, "text": " Imagine you are working as a newspaper editor, and you need to find a quick and brief summary of a long news article.\\n###Input: In a landmark decision by the United Nations General Assembly, global leaders have unanimously agreed to significantly reduce plastic waste production by the end of the decade in an effort to combat the devastating effects of plastic pollution on the environment. This historic announcement comes as a result of years-long negotiations and increasing public awareness of the environmental toll that plastic waste takes on the planet. By adopting a global framework for action, the UN aims to eliminate single-use plastics, improve waste management, and encourage sustainable alternatives. Countries are now expected to develop national action plans reflecting the goals of the agreement, and the UN will develop a monitoring system to evaluate progress in meeting these objectives.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 891, "text": " A website owner wants to generate a short summary of their article in French by translating it from English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-r-textcaps\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'microsoft/git-large-r-textcaps\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TextCaps\\', \\'accuracy\\': \\'\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 892, "text": " We are trying to build a chatbot for customer support. However, it's important that the language used by the chatbot is grammatically correct. So, in order to correct the grammar, I need a text-based generative model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 893, "text": " Given an incomplete sentence during an online science exam, please find the appropriate word to complete the statement.\\n###Input: \\\"During photosynthesis, plants convert sunlight, carbon dioxide, and water into glucose and [MASK].\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'naver-clova-ix/donut-base\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'naver-clova-ix/donut-base\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'result = donut(image_path)\\', \\'performance\\': {\\'dataset\\': \\'arxiv:2111.15664\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\\'}', metadata={})]", "category": "generic"}
{"question_id": 894, "text": " Your task is to create a language model to cluster news articles into different categories based on similarity.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k\\', \\'api_call\\': \"clip.load(\\'timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k\\')\", \\'api_arguments\\': \\'image, class_names\\', \\'python_environment_requirements\\': \\'huggingface_hub, openai, transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This model is a zero-shot image classification model based on OpenCLIP. It can be used for classifying images into various categories without any additional training.\\'}', metadata={})]", "category": "generic"}
{"question_id": 895, "text": " I want to find two similar sentence's score in the Chinese language more efficiently.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 896, "text": " I'm a content moderator, looking to detect similarity between two pieces of text.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 897, "text": " Can you compare the similarity between different news articles to help me find related content to read?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 898, "text": " You have been asked to find the similarity between an instruction and a sentence for an educational text resource.\\n###Input: {\\\"instruction\\\": \\\"Explain the main idea of the research paper:\\\", \\\"sentence\\\": \\\"This study investigates the impact of climate change on agricultural productivity.\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 899, "text": " We are planning to create an audiobook from a given text. Can you provide a Text-To-Speech model that could help?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 900, "text": " Create a voice assistant that reads out this article summary: \\\"The tech giant announced its latest innovation in AI, which is set to revolutionize the world of technology.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 901, "text": " Create an advertisement for a gardening company, and we need a voice-over with the following sentence in Chinese: \\\"\\u7ed9\\u4f60\\u7684\\u82b1\\u56ed\\u5e26\\u6765\\u9c9c\\u82b1\\u548c\\u7eff\\u610f\\u76ce\\u7136\\u7684\\u751f\\u547d\\u529b.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 902, "text": " We need a system that can convert text to speech in Taiwanese Hokkien accent for our language learning app.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 903, "text": " I'm the CEO of an international e-commerce company. For voice assistants, there is a need to translate product names and descriptions from English to Japanese.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"Output: BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 904, "text": " A customer wants to create an audio file from a German text sentence for an advertisement.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'Einmalumdiewelt/T5-Base_GNAD\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'Einmalumdiewelt/T5-Base_GNAD\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': {\\'Loss\\': 2.1025, \\'Rouge1\\': 27.5357, \\'Rouge2\\': 8.5623, \\'Rougel\\': 19.1508, \\'Rougelsum\\': 23.9029, \\'Gen Len\\': 52.7253}}, \\'description\\': \\'This model is a fine-tuned version of Einmalumdiewelt/T5-Base_GNAD on an unknown dataset. It is intended for German text summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 905, "text": " We are building a platform where users can join a conference call. A feature we need is the ability to detect when multiple people talk over each other.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 906, "text": " Our latest AI-based software aims to convert large audiobooks to text. Write a code to transcribe an audiobook.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 907, "text": " Our team is creating an application for automated Marathi language transcription of recorded audio files. Develop the appropriate code snippet for this task.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'model.generate(input_ids, max_length=128)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}\", metadata={})]", "category": "generic"}
{"question_id": 908, "text": " I am building an application for foreign language learners. One of its features is to generate transcriptions of the words the users are practicing in Japanese language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 909, "text": " Tell me the steps to transcribe an audio file in English and provide the text result using the whisper-medium model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"Output: AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True).to(self.device)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 910, "text": " We have a platform that offers tutoring services. We need to transcribe a tutor's speech to provide subtitles for deaf students.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"Output: MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 911, "text": " Determine the different audio sources in a noisy environment and isolate those sources from the original audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Classification\\', \\'api_name\\': \\'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\\', \\'api_call\\': \"DeBERTaModel.from_pretrained(\\'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\\')\", \\'api_arguments\\': {\\'model\\': \\'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\\', \\'sequence_to_classify\\': \\'Angela Merkel is a politician in Germany and leader of the CDU\\', \\'candidate_labels\\': [\\'politics\\', \\'economy\\', \\'entertainment\\', \\'environment\\'], \\'multi_label\\': \\'False\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.13\\'}, \\'example_code\\': \\'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli)\\\\nsequence_to_classify = Angela Merkel is a politician in Germany and leader of the CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)\\', \\'performance\\': {\\'dataset\\': {\\'mnli-m\\': 0.903, \\'mnli-mm\\': 0.903, \\'fever-nli\\': 0.777, \\'anli-all\\': 0.579, \\'anli-r3\\': 0.495}, \\'accuracy\\': {\\'mnli-m\\': 0.903, \\'mnli-mm\\': 0.903, \\'fever-nli\\': 0.777, \\'anli-all\\': 0.579, \\'anli-r3\\': 0.495}}, \\'description\\': \\'This model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the ANLI benchmark. The base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper.\\'}', metadata={})]", "category": "generic"}
{"question_id": 912, "text": " A conference call with multiple speakers has just ended, and we need to separate the voices.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 913, "text": " Help me build a speech-to-speech translation tool for translating English to Hokkien language, using an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'facebook/textless_sm_ro_en\\', \\'api_call\\': \"pipeline(\\'audio-to-audio\\', model=\\'facebook/textless_sm_ro_en\\')\", \\'api_arguments\\': \\'audio file or recording\\', \\'python_environment_requirements\\': \\'fairseq, huggingface_hub\\', \\'example_code\\': \\'https://huggingface.co/facebook/textless_sm_cs_en\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A speech-to-speech translation model for Romanian to English developed by Facebook AI\\'}', metadata={})]", "category": "generic"}
{"question_id": 914, "text": " Supervise a remote control system to react optimally to the emotions expressed by people.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'michellejieli/emotion_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/emotion_text_classifier\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I love this!)\\', \\'performance\\': {\\'dataset\\': [\\'Crowdflower (2016)\\', \\'Emotion Dataset, Elvis et al. (2018)\\', \\'GoEmotions, Demszky et al. (2020)\\', \\'ISEAR, Vikash (2018)\\', \\'MELD, Poria et al. (2019)\\', \\'SemEval-2018, EI-reg, Mohammad et al. (2018)\\', \\'Emotion Lines (Friends)\\'], \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\\'}', metadata={})]", "category": "generic"}
{"question_id": 915, "text": " An acting agency needs to analyze voice samples from aspirant actors to detect their predominant emotions in order to select the best-suited actors for various roles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Emotion Recognition\\', \\'api_name\\': \\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\\')\", \\'api_arguments\\': \\'wav2vec2, tokenizer\\', \\'python_environment_requirements\\': \\'transformers 4.8.2, pytorch 1.9.0+cu102, datasets 1.9.0, tokenizers 0.10.3\\', \\'example_code\\': \\'from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\\', \\'performance\\': {\\'dataset\\': \\'RAVDESS\\', \\'accuracy\\': 0.8223}, \\'description\\': \"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task. The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are: emotions = [\\'angry\\', \\'calm\\', \\'disgust\\', \\'fearful\\', \\'happy\\', \\'neutral\\', \\'sad\\', \\'surprised\\'].\"}', metadata={})]", "category": "generic"}
{"question_id": 916, "text": " We are a real estate company, we have all historical data of our deals, and we would like to predict the price of a specific house using a machine learning model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 917, "text": " A real estate company would like to display estimated home prices on their website based on a set of basic features. Help them integrate the prediction model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 918, "text": " I want to predict the CO2 emissions for a list of cars given their features.\\n###Input: data.csv - a CSV file containing car features such as engine size, fuel type, and weight\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 919, "text": " A startup is building new arcade games that incorporate artificial intelligence. We want to implement a PPO agent to play seals/CartPole-v0.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'https://github.com/neonbjb/ocotillo\\', \\'performance\\': {\\'dataset\\': \\'librispeech validation set\\', \\'accuracy\\': \\'4.45%\\'}, \\'description\\': \\'This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 920, "text": " A game developer desires to make a game user friendly using decision transformer in a hopping environment to design a responsive AI.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'csarron/bert-base-uncased-squad-v1\\', tokenizer=\\'csarron/bert-base-uncased-squad-v1\\')\", \\'api_arguments\\': {\\'model\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'tokenizer\\': \\'csarron/bert-base-uncased-squad-v1\\'}, \\'python_environment_requirements\\': \\'Python 3.7.5\\', \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\n question-answering,\\\\n model=csarron/bert-base-uncased-squad-v1,\\\\n tokenizer=csarron/bert-base-uncased-squad-v1\\\\n)\\\\npredictions = qa_pipeline({\\\\n \\'context\\': The game was played on February 7, 2016 at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California.,\\\\n \\'question\\': What day was the game played on?\\\\n})\\\\nprint(predictions)\", \\'performance\\': {\\'dataset\\': \\'SQuAD1.1\\', \\'accuracy\\': {\\'EM\\': 80.9, \\'F1\\': 88.2}}, \\'description\\': \\'BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 921, "text": " Can you help our engineers to train and run a SoccerTwos simulation and use a trained model of a poca agent?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 922, "text": " We are a technology solutions provider, and we need to help our client develop a reinforcement learning model for a Hopper robot.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 923, "text": " Create a model for controlling a robotic cheetah capable of navigating through challenging terrain.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2-1', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2-1\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.'}\", metadata={})]", "category": "generic"}
{"question_id": 924, "text": " We are developing a soccer game AI using reinforcement learning. How do we run the training with the provided configuration file and run ID?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 925, "text": " I have a robotic arm in my warehouse, and I want it to perform efficient 6D grasping on objects.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
