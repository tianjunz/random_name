{"question_id": 1, "text": " I want to get deep features from a given text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"YituTech/conv-bert-base\", \"api_call\": \"AutoModel.from_pretrained('YituTech/conv-bert-base')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 2, "text": " Building a healthcare chatbot requires extracting features from a user's medical question to provide more accurate responses.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"dmis-lab/biobert-v1.1\", \"api_call\": \"pipeline('feature-extraction', model='dmis-lab/biobert-v1.1')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.\"}}", "category": "generic"}
{"question_id": 3, "text": " We are a medical research institute and want to extract features from biomedical texts to improve literature-based predictions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", \"api_call\": \"AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\", \"api_arguments\": \"input_ids, attention_mask\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]\", \"performance\": {\"dataset\": \"UMLS\", \"accuracy\": \"N/A\"}, \"description\": \"SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output.\"}}", "category": "generic"}
{"question_id": 4, "text": " We are developing a chatbot that should understand users\\u2019 sentences semantically, so it can find similar sentences in its database.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"princeton-nlp/unsup-simcse-roberta-base\", \"api_call\": \"AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\", \"api_arguments\": null, \"python_environment_requirements\": [\"transformers\"], \"example_code\": null, \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture.\"}}", "category": "generic"}
{"question_id": 5, "text": " I'm a teacher, I need to generate a summary of an article to help my students studying. Can you help?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"facebook/bart-large\", \"api_call\": \"BartModel.from_pretrained('facebook/bart-large')\", \"api_arguments\": {\"pretrained_model_name\": \"facebook/bart-large\"}, \"python_environment_requirements\": {\"library\": \"transformers\", \"version\": \"latest\"}, \"example_code\": \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\\nmodel = BartModel.from_pretrained('facebook/bart-large')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"arxiv\", \"accuracy\": \"Not provided\"}, \"description\": \"BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).\"}}", "category": "generic"}
{"question_id": 6, "text": " As an online newspaper, we need to generate summaries of articles to help our readers skim through content quickly.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"facebook/bart-base\", \"api_call\": \"BartModel.from_pretrained('facebook/bart-base')\", \"api_arguments\": [\"inputs\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import BartTokenizer, BartModel\\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\\nmodel = BartModel.from_pretrained('facebook/bart-base')\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"arxiv\", \"accuracy\": \"Not provided\"}, \"description\": \"BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).\"}}", "category": "generic"}
{"question_id": 7, "text": " We're a startup focusing on video analytics for retail stores. We want to classify customer activities using a pre-trained model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"facebook/dino-vitb16\", \"api_call\": \"ViTModel.from_pretrained('facebook/dino-vitb16')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/dino-vitb16\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"PIL\": \"latest\", \"requests\": \"latest\"}, \"example_code\": \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.\"}}", "category": "generic"}
{"question_id": 8, "text": " We are a startup building a recommendation system for books. We want to sort books by their categories using their cover images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"facebook/dino-vits8\", \"api_call\": \"ViTModel.from_pretrained('facebook/dino-vits8')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import ViTFeatureExtractor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": null}, \"description\": \"Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\\u00e9 J\\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.\"}}", "category": "generic"}
{"question_id": 9, "text": " I am writing a NLP-based tool to extract meaning from religious writings. How can we extract the features from the text?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"kobart-base-v2\", \"api_call\": \"BartModel.from_pretrained('gogamza/kobart-base-v2')\", \"api_arguments\": {\"tokenizer\": \"PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"tokenizers\": \"latest\"}, \"example_code\": \"from transformers import PreTrainedTokenizerFast, BartModel\\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')\", \"performance\": {\"dataset\": \"NSMC\", \"accuracy\": 0.901}, \"description\": \"KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more.\"}}", "category": "generic"}
{"question_id": 10, "text": " Our social media monitoring system needs to understand Indonesian viral posts and their main key topics. Implement a code to access the context-based representation.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Contextual Representation\", \"api_name\": \"indobenchmark/indobert-base-p1\", \"api_call\": \"AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\", \"api_arguments\": [\"BertTokenizer\", \"AutoModel\", \"tokenizer.encode\", \"torch.LongTensor\", \"model(x)[0].sum()\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import BertTokenizer, AutoModel\\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\\nprint(x, model(x)[0].sum())\", \"performance\": {\"dataset\": \"Indo4B\", \"accuracy\": \"23.43 GB of text\"}, \"description\": \"IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective.\"}}", "category": "generic"}
{"question_id": 11, "text": " We are a company developing an application that assists in code review. We want to extract features from the given code snippet to improve our code review process.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"microsoft/codebert-base\", \"api_call\": \"AutoModel.from_pretrained('microsoft/codebert-base')\", \"api_arguments\": \"n/a\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"n/a\", \"performance\": {\"dataset\": \"CodeSearchNet\", \"accuracy\": \"n/a\"}, \"description\": \"Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages. The model is trained on bi-modal data (documents & code) of CodeSearchNet. This model is initialized with Roberta-base and trained with MLM+RTD objective.\"}}", "category": "generic"}
{"question_id": 12, "text": " The healthcare organization needs to automate extraction of medical terms from patients' records in English.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"GanjinZero/UMLSBert_ENG\", \"api_call\": \"AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER\"}}", "category": "generic"}
{"question_id": 13, "text": " We are an audio recognition company that wants to use the Hubert-Large model to build an application.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"hubert-large-ll60k\", \"api_call\": \"HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", \"api_arguments\": \"pretrained model name\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')\", \"performance\": {\"dataset\": \"Libri-Light\", \"accuracy\": \"matches or improves upon the state-of-the-art wav2vec 2.0 performance\"}, \"description\": \"Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss.\"}}", "category": "generic"}
{"question_id": 14, "text": " I need an Assistant to help me compare the similarity of two sentences.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"sup-simcse-roberta-large\", \"api_call\": \"AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')\", \"api_arguments\": [\"AutoTokenizer\", \"AutoModel\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\", \"performance\": {\"dataset\": \"STS tasks\", \"accuracy\": \"Spearman's correlation (See associated paper Appendix B)\"}, \"description\": \"A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks.\"}}", "category": "generic"}
{"question_id": 15, "text": " I'm writing a story but got stuck in the middle after a few lines. Please help me to come up with a creative continuation.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lewtun/tiny-random-mt5\", \"api_call\": \"pipeline('text-generation', model='lewtun/tiny-random-mt5')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"nlp('Once upon a time...')\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random mt5 model for text generation\"}}", "category": "generic"}
{"question_id": 16, "text": " Our company is working on an AI project, where we need to extract semantic features from Russian text documents for understanding their value.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"DeepPavlov/rubert-base-cased\", \"api_call\": \"AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"Russian part of Wikipedia and news data\", \"accuracy\": \"\"}, \"description\": \"RuBERT (Russian, cased, 12\\u2011layer, 768\\u2011hidden, 12\\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\\u2011base as an initialization for RuBERT[1].\"}}", "category": "generic"}
{"question_id": 17, "text": " We are building an application for transcription services. Extract spoken content from a given audio file of a meeting.\\n \n Use this API documentation for reference:  {\"domain\": \"Audio Automatic Speech Recognition\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"microsoft/wavlm-large\", \"api_call\": \"Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')\", \"api_arguments\": \"speech input\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\", \"performance\": {\"dataset\": \"SUPERB benchmark\", \"accuracy\": \"state-of-the-art performance\"}, \"description\": \"WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\"}}", "category": "generic"}
{"question_id": 18, "text": " I have an image URL that I would like to analyze and extract features with a Computer Vision Transformer model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"google/vit-base-patch16-224-in21k\", \"api_call\": \"ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"google/vit-base-patch16-224-in21k\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import ViTImageProcessor, ViTModel\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlast_hidden_states = outputs.last_hidden_state\", \"performance\": {\"dataset\": \"ImageNet-21k\", \"accuracy\": \"Refer to tables 2 and 5 of the original paper\"}, \"description\": \"The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\"}}", "category": "generic"}
{"question_id": 19, "text": " In order to improve our code search engine results, we need to extract relevant features from various programming languages.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Engineering\", \"api_name\": \"microsoft/unixcoder-base\", \"api_call\": \"AutoModel.from_pretrained('microsoft/unixcoder-base')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModel\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks.\"}}", "category": "generic"}
{"question_id": 20, "text": " Our organization focuses on creating music applications for streaming platforms. We need to extract spectrogram features from audio files to enrich our recommendations.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"audio-spectrogram-transformer\", \"api_call\": \"AutoFeatureExtractor.from_pretrained('Ericwang/tiny-random-ast')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"One custom ast model for testing of HF repos\"}}", "category": "generic"}
{"question_id": 21, "text": " I am an AI enthusiast, and I want to answer questions about dogs using my pre-trained AI model DPR. How can I embed my question to make it compatible with my model?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"facebook/dpr-question_encoder-single-nq-base\", \"api_call\": \"DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\", \"api_arguments\": [\"input_ids\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\\nembeddings = model(input_ids).pooler_output\", \"performance\": {\"dataset\": [{\"name\": \"NQ\", \"accuracy\": {\"top_20\": 78.4, \"top_100\": 85.4}}, {\"name\": \"TriviaQA\", \"accuracy\": {\"top_20\": 79.4, \"top_100\": 85.0}}, {\"name\": \"WQ\", \"accuracy\": {\"top_20\": 73.2, \"top_100\": 81.4}}, {\"name\": \"TREC\", \"accuracy\": {\"top_20\": 79.8, \"top_100\": 89.1}}, {\"name\": \"SQuAD\", \"accuracy\": {\"top_20\": 63.2, \"top_100\": 77.2}}]}, \"description\": \"Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019).\"}}", "category": "generic"}
{"question_id": 22, "text": " I'm organizing an online conference with participants from different countries who speak different languages. I want to find a tool to extract information from their forum discussions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"rasa/LaBSE\", \"api_call\": \"AutoModel.from_pretrained('rasa/LaBSE')\", \"api_arguments\": \"input_text\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages.\"}}", "category": "generic"}
{"question_id": 23, "text": " I am building a recommendation system that provides quotes based on user's queries. I need to find the most similar quote to a given query.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"sentence-transformers/distilbert-base-nli-mean-tokens\", \"api_call\": \"SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": \"pip install -U sentence-transformers\", \"example_code\": \"from sentence_transformers import SentenceTransformer\\nsentences = [This is an example sentence, Each sentence is converted]\\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\", \"performance\": {\"dataset\": \"https://seb.sbert.net\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\"}}", "category": "generic"}
{"question_id": 24, "text": " Analyze two research papers and find their document representations or embeddings.\\n###Input: {'papers': [{'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context.'}, {'title': 'Attention is All You Need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new transformer which handles variability by introducing a self-attention mechanism for the entire network without using an encoder-decoder architecture.'}]}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document-level embeddings of research papers\", \"api_name\": \"malteos/scincl\", \"api_call\": \"AutoModel.from_pretrained('malteos/scincl')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('malteos/scincl')\", \"model\": \"AutoModel.from_pretrained('malteos/scincl')\"}, \"python_environment_requirements\": {\"transformers\": \"4.13.0\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\\nmodel = AutoModel.from_pretrained('malteos/scincl')\\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\\nresult = model(**inputs)\\nembeddings = result.last_hidden_state[:, 0, :]\", \"performance\": {\"dataset\": \"SciDocs\", \"accuracy\": {\"mag-f1\": 81.2, \"mesh-f1\": 89.0, \"co-view-map\": 85.3, \"co-view-ndcg\": 92.2, \"co-read-map\": 87.7, \"co-read-ndcg\": 94.0, \"cite-map\": 93.6, \"cite-ndcg\": 97.4, \"cocite-map\": 91.7, \"cocite-ndcg\": 96.5, \"recomm-ndcg\": 54.3, \"recomm-P@1\": 19.6}}, \"description\": \"SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph.\"}}", "category": "generic"}
{"question_id": 25, "text": " A marketing agency need a short story generated about their new AI product for kids.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"sberbank-ai/sbert_large_mt_nlu_ru\", \"api_call\": \"AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')\", \"api_arguments\": [\"sentences\", \"padding\", \"truncation\", \"max_length\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModel\\nimport torch\\n\\n# Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n    return sum_embeddings / sum_mask\\n\\n# Sentences we want sentence embeddings for\\nsentences = ['\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442! \\u041a\\u0430\\u043a \\u0442\\u0432\\u043e\\u0438 \\u0434\\u0435\\u043b\\u0430?',\\n             '\\u0410 \\u043f\\u0440\\u0430\\u0432\\u0434\\u0430, \\u0447\\u0442\\u043e 42 \\u0442\\u0432\\u043e\\u0435 \\u043b\\u044e\\u0431\\u0438\\u043c\\u043e\\u0435 \\u0447\\u0438\\u0441\\u043b\\u043e?']\\n# Load AutoModel from huggingface model repository\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n# Perform pooling. In this case, mean pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\", \"performance\": {\"dataset\": \"Russian SuperGLUE\", \"accuracy\": \"Not provided\"}, \"description\": \"BERT large model multitask (cased) for Sentence Embeddings in Russian language.\"}}", "category": "generic"}
{"question_id": 26, "text": " Our company needs to compare the similarity of English, Italian, and Japanese sentences in a language-independent manner.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Sentence Similarity\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"setu4993/LaBSE\", \"api_call\": \"BertModel.from_pretrained('setu4993/LaBSE')\", \"api_arguments\": [\"english_sentences\", \"italian_sentences\", \"japanese_sentences\"], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import BertModel, BertTokenizerFast\\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\\nmodel = model.eval()\\nenglish_sentences = [\\n 'dog',\\n 'Puppies are nice.',\\n 'I enjoy taking long walks along the beach with my dog.',\\n]\\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\\nwith torch.no_grad():\\n english_outputs = model(**english_inputs)\\nenglish_embeddings = english_outputs.pooler_output\", \"performance\": {\"dataset\": \"CommonCrawl and Wikipedia\", \"accuracy\": \"Not Specified\"}, \"description\": \"Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.\"}}", "category": "generic"}
{"question_id": 27, "text": " How can I implement an automatic code snippet extraction system to find relevant code snippets from a text document?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"lanwuwei/BERTOverflow_stackoverflow_github\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"lanwuwei/BERTOverflow_stackoverflow_github\"}, \"python_environment_requirements\": {\"transformers\": \"*\", \"torch\": \"*\"}, \"example_code\": \"from transformers import *\\nimport torch\\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\", \"performance\": {\"dataset\": \"StackOverflow's 10 year archive\", \"accuracy\": \"Not provided\"}, \"description\": \"BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow.\"}}", "category": "generic"}
{"question_id": 28, "text": " Our company plans to build a video-sharing platform that automatically generates video descriptions. We need a model that extracts video features and understands human language.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Feature Extraction\", \"api_name\": \"microsoft/xclip-base-patch16-zero-shot\", \"api_call\": \"XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": [{\"name\": \"HMDB-51\", \"accuracy\": 44.6}, {\"name\": \"UCF-101\", \"accuracy\": 72.0}, {\"name\": \"Kinetics-600\", \"accuracy\": 65.2}]}, \"description\": \"X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\"}}", "category": "generic"}
{"question_id": 29, "text": " Our team is making a new line of book covers. Can you generate an abstract image based on the story \\\"The Old Man and the Sea\\\"?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"runwayml/stable-diffusion-v1-5\", \"api_call\": \"'image = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)(prompt).images[0]'\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": {\"diffusers\": \"from diffusers import StableDiffusionPipeline\", \"torch\": \"import torch\"}, \"example_code\": {\"model_id\": \"model_id = runwayml/stable-diffusion-v1-5\", \"pipe\": \"pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\", \"pipe_to_cuda\": \"pipe = pipe.to(cuda)\", \"prompt\": \"prompt = a photo of an astronaut riding a horse on mars\", \"image\": \"image = pipe(prompt).images[0]\", \"save_image\": \"image.save(astronaut_rides_horse.png)\"}, \"performance\": {\"dataset\": \"COCO2017\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\"}}", "category": "generic"}
{"question_id": 30, "text": " You would like to create a search engine that recommends articles for users by identifying contextually relevant information from a given query.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Feature Extraction\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/dragon-plus-context-encoder\", \"api_call\": \"AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\", \"api_arguments\": [\"pretrained\"], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import AutoTokenizer, AutoModel\\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\\nquery = 'Where was Marie Curie born?'\\ncontexts = [\\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\\u00e8ne Curie, a doctor of French Catholic origin from Alsace.'\\n]\\nquery_input = tokenizer(query, return_tensors='pt')\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\nscore1 = query_emb @ ctx_emb[0]\\nscore2 = query_emb @ ctx_emb[1]\", \"performance\": {\"dataset\": \"MS MARCO\", \"accuracy\": 39.0}, \"description\": \"DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.\"}}", "category": "generic"}
{"question_id": 31, "text": " I want a program that generates images based on a textual description, such as \\\"a picture of a sunset on the beach.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"CompVis/stable-diffusion-v1-4\", \"api_call\": \"No changes needed.\", \"api_arguments\": [\"prompt\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"scipy\"], \"example_code\": \"import torch\\nfrom diffusers import StableDiffusionPipeline\\nmodel_id = CompVis/stable-diffusion-v1-4\\ndevice = cuda\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(device)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\"}}", "category": "generic"}
{"question_id": 32, "text": " Design a surrealism-inspired AI-generated artwork based on the text prompt \\\"floating elephants with butterfly wings under a rainbow\\\".\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"prompthero/openjourney\", \"api_call\": \"Not specified.\", \"api_arguments\": {\"prompt\": \"string\"}, \"python_environment_requirements\": [\"diffusers\", \"torch\"], \"example_code\": \"from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = prompthero/openjourney\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\nimage = pipe(prompt).images[0]\\nimage.save(./retro_cars.png)\", \"performance\": {\"dataset\": \"Midjourney images\", \"accuracy\": \"Not specified\"}, \"description\": \"Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\"}}", "category": "generic"}
{"question_id": 33, "text": " We are building a new room decoration software where the user types the room setup, and then it generates a realistic image of what it will look like.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Generation\", \"api_name\": \"runwayml/stable-diffusion-inpainting\", \"api_call\": \"pipe(prompt=prompt, image=image, mask_image=mask_image)\", \"api_arguments\": {\"prompt\": \"Text prompt\", \"image\": \"PIL image\", \"mask_image\": \"PIL image (mask)\"}, \"python_environment_requirements\": {\"diffusers\": \"from diffusers import StableDiffusionInpaintPipeline\"}, \"example_code\": {\"import_code\": \"from diffusers import StableDiffusionInpaintPipeline\", \"instantiate_code\": \"pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)\", \"generate_image_code\": \"image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\", \"save_image_code\": \"image.save(./yellow_cat_on_park_bench.png)\"}, \"performance\": {\"dataset\": {\"name\": \"LAION-2B (en)\", \"accuracy\": \"Not optimized for FID scores\"}}, \"description\": \"Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\"}}", "category": "generic"}
{"question_id": 34, "text": " We are working on a project that can generate images from textual description for an online art gallery. We need to create diverse images from a given text prompt.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"stabilityai/stable-diffusion-2-1-base\", \"api_call\": \"pipe(prompt).images[0]\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": {\"install_dependencies\": \"pip install diffusers transformers accelerate scipy safetensors\", \"code\": \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\"}, \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\"}}", "category": "generic"}
{"question_id": 35, "text": " Create an AI-generated image of a character using a textual description.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"hakurei/waifu-diffusion\", \"api_call\": \"pipe(prompt, guidance_scale=6)['sample'][0]\", \"api_arguments\": {\"prompt\": \"text\", \"guidance_scale\": \"number\"}, \"python_environment_requirements\": {\"torch\": \"torch\", \"autocast\": \"from torch\", \"StableDiffusionPipeline\": \"from diffusers\"}, \"example_code\": \"import torch\\nfrom torch import autocast\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\n 'hakurei/waifu-diffusion',\\n torch_dtype=torch.float32\\n).to('cuda')\\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\nwith autocast(cuda):\\n image = pipe(prompt, guidance_scale=6)[sample][0] \\nimage.save(test.png)\", \"performance\": {\"dataset\": \"high-quality anime images\", \"accuracy\": \"not available\"}, \"description\": \"waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\"}}", "category": "generic"}
{"question_id": 36, "text": " We are hosting an art event soon. We need a painting made from a text description: \\\"A serene sunset over a beautiful mountain landscape.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"stabilityai/sd-vae-ft-mse\", \"api_call\": \"Output: StableDiffusionPipeline.from_pretrained(model, vae=vae)\", \"api_arguments\": {\"model\": \"CompVis/stable-diffusion-v1-4\", \"vae\": \"AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)\", \"performance\": {\"dataset\": [{\"name\": \"COCO 2017 (256x256, val, 5000 images)\", \"accuracy\": {\"rFID\": \"4.70\", \"PSNR\": \"24.5 +/- 3.7\", \"SSIM\": \"0.71 +/- 0.13\", \"PSIM\": \"0.92 +/- 0.27\"}}, {\"name\": \"LAION-Aesthetics 5+ (256x256, subset, 10000 images)\", \"accuracy\": {\"rFID\": \"1.88\", \"PSNR\": \"27.3 +/- 4.7\", \"SSIM\": \"0.83 +/- 0.11\", \"PSIM\": \"0.65 +/- 0.34\"}}]}, \"description\": \"This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.\"}}", "category": "generic"}
{"question_id": 37, "text": " Help me generate a picture of a futuristic city at night with flying cars.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"stabilityai/stable-diffusion-2-1\", \"api_call\": \"pipe(prompt).images[0]\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nmodel_id = stabilityai/stable-diffusion-2-1\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", \"performance\": {\"dataset\": \"COCO2017\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\"}}", "category": "generic"}
{"question_id": 38, "text": " We have an AI-based project where we require text-to-image generation for various scenarios like forming a yellow cat's face in high resolution, sitting on a park bench.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image generation and modification based on text prompts\", \"api_name\": \"stabilityai/stable-diffusion-2-inpainting\", \"api_call\": \"pipe(prompt=prompt, image=image, mask_image=mask_image)\", \"api_arguments\": [\"prompt\", \"image\", \"mask_image\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionInpaintPipeline\\npipe = StableDiffusionInpaintPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-inpainting,\\n torch_dtype=torch.float16,\\n)\\npipe.to(cuda)\\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\\nimage.save(./yellow_cat_on_park_bench.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\"}}", "category": "generic"}
{"question_id": 39, "text": " I'm a photographer focusing on architecture, and I need inspiration for a church photo in a rural setting. Help me come up with a unique angle or perspective.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"dreamlike-art/dreamlike-photoreal-2.0\", \"api_call\": \"Output: StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0', torch_dtype=torch.float16)(prompt).images[0]\", \"api_arguments\": {\"prompt\": \"photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\"}, \"python_environment_requirements\": {\"torch\": \"torch.float16\", \"diffusers\": \"StableDiffusionPipeline\"}, \"example_code\": \"from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\nimage = pipe(prompt).images[0]\\nimage.save(./result.jpg)\", \"performance\": {\"dataset\": \"Stable Diffusion 1.5\", \"accuracy\": \"Not specified\"}, \"description\": \"Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\"}}", "category": "generic"}
{"question_id": 40, "text": " I want to create images of my short story characters so that the audience can have visuals of the story.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Generation\", \"api_name\": \"stabilityai/stable-diffusion-2\", \"api_call\": \"pipe(prompt).images[0]\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nmodel_id = stabilityai/stable-diffusion-2\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\"}}", "category": "generic"}
{"question_id": 41, "text": " Our marketing team needs engaging images for social media posts. We want to generate a visually creative image of an anime character wearing a school uniform.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"andite/anything-v4.0\", \"api_call\": \"Output: StableDiffusionPipeline.from_pretrained('andite/anything-v4.0', torch_dtype=torch.float16).to('cuda')(prompt).images[0]\", \"api_arguments\": {\"model_id\": \"andite/anything-v4.0\", \"torch_dtype\": \"torch.float16\", \"device\": \"cuda\", \"prompt\": \"hatsune_miku\"}, \"python_environment_requirements\": {\"diffusers\": \"StableDiffusionPipeline\", \"torch\": \"torch\"}, \"example_code\": {\"from diffusers import StableDiffusionPipeline\": \"\", \"import torch\": \"\", \"model_id = andite/anything-v4.0\": \"\", \"pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\": \"\", \"pipe = pipe.to(cuda)\": \"\", \"prompt = hatsune_miku\": \"\", \"image = pipe(prompt).images[0]\": \"\", \"image.save(./hatsune_miku.png)\": \"\"}, \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model.\"}}", "category": "generic"}
{"question_id": 42, "text": " Can you create an AI generated image based on the abstract concept of \\\"time is running out\\\"?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"prompthero/openjourney-v4\", \"api_call\": \"pipeline('text-to-image', model='prompthero/openjourney-v4')\", \"api_arguments\": {\"text\": \"string\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"generate_image('your text here')\", \"performance\": {\"dataset\": \"Midjourney v4 images\", \"accuracy\": \"Not provided\"}, \"description\": \"Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\"}}", "category": "generic"}
{"question_id": 43, "text": " I am building an online platform for imagining fictional characters. How do I generate characters based on text descriptions?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"stabilityai/sd-vae-ft-ema\", \"api_call\": \"StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, vae=AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema))\", \"api_arguments\": {\"model\": \"CompVis/stable-diffusion-v1-4\", \"vae\": \"AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\"}, \"python_environment_requirements\": {\"diffusers\": \"diffusers library\"}, \"example_code\": \"from diffusers.models import AutoencoderKL\\nfrom diffusers import StableDiffusionPipeline\\nmodel = CompVis/stable-diffusion-v1-4\\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)\", \"performance\": {\"dataset\": {\"COCO 2017 (256x256, val, 5000 images)\": {\"accuracy\": {\"rFID\": 4.42, \"PSNR\": \"23.8 +/- 3.9\", \"SSIM\": \"0.69 +/- 0.13\", \"PSIM\": \"0.96 +/- 0.27\"}}, \"LAION-Aesthetics 5+ (256x256, subset, 10000 images)\": {\"accuracy\": {\"rFID\": 1.77, \"PSNR\": \"26.7 +/- 4.8\", \"SSIM\": \"0.82 +/- 0.12\", \"PSIM\": \"0.67 +/- 0.34\"}}}}, \"description\": \"This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.\"}}", "category": "generic"}
{"question_id": 44, "text": " We are a game development company trying to generate landscape variations based on textual descriptions provided by our designers.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Generate and modify images based on text prompts\", \"api_name\": \"stabilityai/stable-diffusion-2-depth\", \"api_call\": \"StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth', torch_dtype=torch.float16).to('cuda')\", \"api_arguments\": {\"prompt\": \"Text prompt to generate image\", \"image\": \"Initial image (optional)\", \"negative_prompt\": \"Negative text prompt to avoid certain features\", \"strength\": \"Strength of the prompt effect on the generated image\"}, \"python_environment_requirements\": [\"pip install -U git+https://github.com/huggingface/transformers.git\", \"pip install diffusers transformers accelerate scipy safetensors\"], \"example_code\": \"import torch\\nimport requests\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionDepth2ImgPipeline\\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\\n stabilityai/stable-diffusion-2-depth,\\n torch_dtype=torch.float16,\\n).to(cuda)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\ninit_image = Image.open(requests.get(url, stream=True).raw)\\nprompt = two tigers\\nn_propmt = bad, deformed, ugly, bad anotomy\\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\"}}", "category": "generic"}
{"question_id": 45, "text": " Your company is expanding its product line to include anime merchandise. We need to create images for those new products.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"EimisAnimeDiffusion_1.0v\", \"api_call\": \"hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v')\", \"api_arguments\": \"['prompt']\", \"python_environment_requirements\": \"huggingface_hub\", \"example_code\": \"from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\"}}", "category": "generic"}
{"question_id": 46, "text": " Provide an example of generating an image based on the input text, such as \\\"a giant robot protecting a city\\\".\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image generation\", \"api_name\": \"stabilityai/stable-diffusion-2-base\", \"api_call\": \"pipe(prompt).images[0]\", \"api_arguments\": {\"prompt\": \"a photo of an astronaut riding a horse on mars\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-2-base\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = a photo of an astronaut riding a horse on mars\\nimage = pipe(prompt).images[0]\\nimage.save(astronaut_rides_horse.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\"}}", "category": "generic"}
{"question_id": 47, "text": " Design an assistant to generate an image of a dreamy landscape with a fantasy waterfall and a colorful rainbow.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"nitrosocke/nitro-diffusion\", \"api_call\": \"pipe(prompt).images[0]\", \"api_arguments\": [\"prompt\"], \"python_environment_requirements\": [\"torch\", \"diffusers\"], \"example_code\": \"from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = nitrosocke/nitro-diffusion\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = archer arcane style magical princess with golden hair\\nimage = pipe(prompt).images[0]\\nimage.save(./magical_princess.png)\", \"performance\": {\"dataset\": \"Stable Diffusion\", \"accuracy\": \"N/A\"}, \"description\": \"Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\"}}", "category": "generic"}
{"question_id": 48, "text": " I want to describe a landscape and generate an image based on the description, like a \\\"tranquil forest with a small river flowing.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"Linaqruf/anything-v3.0\", \"api_call\": \"pipeline('text-to-image', model='Linaqruf/anything-v3.0') should be rewritten as Text2ImagePipeline(model='Linaqruf/anything-v3.0')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A text-to-image model that generates images from text descriptions.\"}}", "category": "generic"}
{"question_id": 49, "text": " We want to design a website that generates images with analog style based on user-provided text prompts.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"wavymulder/Analog-Diffusion\", \"api_call\": \"pipeline('text-to-image', model='wavymulder/Analog-Diffusion')\", \"api_arguments\": [\"prompt\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"text_to_image('analog style landscape')\", \"performance\": {\"dataset\": \"analog photographs\", \"accuracy\": \"Not specified\"}, \"description\": \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}}", "category": "generic"}
{"question_id": 50, "text": " A bookseller needs to create a unique and dreamlike illustration for their upcoming bestseller, a mysterious adventure novel about a young wizard.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"dreamlike-art/dreamlike-anime-1.0\", \"api_call\": \"pipe(prompt, negative_prompt=negative_prompt)\", \"api_arguments\": [\"prompt\", \"negative_prompt\"], \"python_environment_requirements\": [\"diffusers\", \"torch\"], \"example_code\": \"from diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(cuda)\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\nnegative_prompt = 'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry'\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\nimage.save(./result.jpg)\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}}", "category": "generic"}
{"question_id": 51, "text": " I'm a scriptwriter for a movie and I want to create an image of a fantastical landscape based on my description.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"Lykon/DreamShaper\", \"api_call\": \"pipeline('text-to-image', model=Lykon/DreamShaper)\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers, torch\", \"example_code\": \"https://huggingface.co/spaces/Lykon/DreamShaper-webui\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\"}}", "category": "generic"}
{"question_id": 52, "text": " The company wants to generate a marketing image based on a description. I need some guidelines on how to proceed.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"darkstorm2150/Protogen_v2.2_Official_Release\", \"api_call\": \"StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\", \"api_arguments\": {\"model_id\": \"darkstorm2150/Protogen_v2.2_Official_Release\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": {\"diffusers\": \"StableDiffusionPipeline, DPMSolverMultistepScheduler\", \"torch\": \"torch\"}, \"example_code\": \"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)\", \"performance\": {\"dataset\": \"Various datasets\", \"accuracy\": \"Not specified\"}, \"description\": \"Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\"}}", "category": "generic"}
{"question_id": 53, "text": " We want to create an advertisement that follows the description: \\\"A cheerful girl with blue jeans and a red shirt holding a hot cup of coffee while standing in a park.\\\". \\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"gsdf/Counterfeit-V2.5\", \"api_call\": \"pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\", \"performance\": {\"dataset\": \"EasyNegative\", \"accuracy\": \"Not provided\"}, \"description\": \"Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\"}}", "category": "generic"}
{"question_id": 54, "text": " I am a designer looking to upscale a low-resolution image of a cat near the window by utilizing text prompts.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image generation and modification based on text prompts\", \"api_name\": \"stabilityai/stable-diffusion-x4-upscaler\", \"api_call\": \"StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler', torch_dtype=torch.float16)\", \"api_arguments\": {\"model_id\": \"stabilityai/stable-diffusion-x4-upscaler\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\", \"xformers (optional, for memory efficient attention)\"], \"example_code\": \"pip install diffusers transformers accelerate scipy safetensors\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom diffusers import StableDiffusionUpscalePipeline\\nimport torch\\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipeline = pipeline.to(cuda)\\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\\nresponse = requests.get(url)\\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\\nlow_res_img = low_res_img.resize((128, 128))\\nprompt = a white cat\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\\nupscaled_image.save(upsampled_cat.png)\", \"performance\": {\"dataset\": \"COCO2017 validation set\", \"accuracy\": \"Not optimized for FID scores\"}, \"description\": \"Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\"}}", "category": "generic"}
{"question_id": 55, "text": " An online art gallery needs to create unique images based on descriptions. They require surreal visualizations of a dreamlike forest with mystical creatures and glowing trees.\\n###Input: \\\"In a dreamlike forest, mystical creatures roam among glowing trees, a scene filled with vibrant colors and mystique ambiance.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image\", \"api_name\": \"darkstorm2150/Protogen_x5.8_Official_Release\", \"api_call\": \"StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\", \"api_arguments\": {\"model_id\": \"darkstorm2150/Protogen_v5.8_Official_Release\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"torch\", \"diffusers\"], \"example_code\": \"from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\nimport torch\\nprompt = (\\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \\nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \\nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \\ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\\n)\\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe = pipe.to(cuda)\\nimage = pipe(prompt, num_inference_steps=25).images[0]\\nimage.save(./result.jpg)\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": \"unknown\"}, \"description\": \"Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model.\"}}", "category": "generic"}
{"question_id": 56, "text": " I am an entrepreneur who wants to create an application that generates captions for images that users upload.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Captioning\", \"api_name\": \"nlpconnect/vit-gpt2-image-captioning\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained(\\\\nlpconnect/vit-gpt2-image-captioning\\\\)\", \"api_arguments\": {\"model\": \"nlpconnect/vit-gpt2-image-captioning\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\"], \"example_code\": \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\nimport torch\\nfrom PIL import Image\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\nmodel.to(device)\\nmax_length = 16\\nnum_beams = 4\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\ndef predict_step(image_paths):\\n images = []\\n for image_path in image_paths:\\n i_image = Image.open(image_path)\\n if i_image.mode != RGB:\\n i_image = i_image.convert(mode=RGB)\\nimages.append(i_image)\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\n pixel_values = pixel_values.to(device)\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n preds = [pred.strip() for pred in preds]\\n return preds\\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']\", \"performance\": {\"dataset\": \"Not provided\", \"accuracy\": \"Not provided\"}, \"description\": \"An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.\"}}", "category": "generic"}
{"question_id": 57, "text": " We have a photo of an astronaut in low resolution. Make it high-resolution and ultra-realistic.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Upscaling\", \"api_name\": \"stabilityai/sd-x2-latent-upscaler\", \"api_call\": \"upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator)\", \"api_arguments\": {\"prompt\": \"text prompt\", \"image\": \"low resolution latents\", \"num_inference_steps\": 20, \"guidance_scale\": 0, \"generator\": \"torch generator\"}, \"python_environment_requirements\": [\"git+https://github.com/huggingface/diffusers.git\", \"transformers\", \"accelerate\", \"scipy\", \"safetensors\"], \"example_code\": \"from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\nimport torch\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\npipeline.to(cuda)\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\nupscaler.to(cuda)\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\ngenerator = torch.manual_seed(33)\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\nupscaled_image.save(astronaut_1024.png)\", \"performance\": {\"dataset\": \"LAION-2B\", \"accuracy\": \"Not specified\"}, \"description\": \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\"}}", "category": "generic"}
{"question_id": 58, "text": " Automatically convert the Japanese scanned text from a manga image to textual content for translation purposes.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"kha-white/manga-ocr-base\", \"api_call\": \"pipeline('ocr', model='kha-white/manga-ocr-base').model\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"manga109s\", \"accuracy\": \"\"}, \"description\": \"Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\"}}", "category": "generic"}
{"question_id": 59, "text": " As a designer creating advertising material for products, I need to generate a caption for an image of a new product for social media.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Captioning\", \"api_name\": \"blip-image-captioning-base\", \"api_call\": \"BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\", \"api_arguments\": [\"raw_image\", \"text\", \"return_tensors\"], \"python_environment_requirements\": [\"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\ntext = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": {\"CIDEr\": \"+2.8%\"}}, \"description\": \"BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).\"}}", "category": "generic"}
{"question_id": 60, "text": " The company wants to develop a smartphone application that recommends places of interest based on the photos the users have taken.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Transformers\", \"functionality\": \"Image Captioning\", \"api_name\": \"blip-image-captioning-large\", \"api_call\": \"BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\", \"api_arguments\": {\"raw_image\": \"Image\", \"text\": \"Optional Text\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForConditionalGeneration\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": {\"import_requests\": \"import requests\", \"import_PIL\": \"from PIL import Image\", \"import_transformers\": \"from transformers import BlipProcessor, BlipForConditionalGeneration\", \"load_processor\": \"processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)\", \"load_model\": \"model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\", \"load_image\": \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", \"conditional_captioning\": \"text = a photography of\\ninputs = processor(raw_image, text, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"unconditional_captioning\": \"inputs = processor(raw_image, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\"}, \"performance\": {\"dataset\": \"COCO\", \"accuracy\": {\"image-text retrieval\": \"+2.7% recall@1\", \"image captioning\": \"+2.8% CIDEr\", \"VQA\": \"+1.6% VQA score\"}}, \"description\": \"BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\"}}", "category": "generic"}
{"question_id": 61, "text": " I want to create a document digitization tool that converts printed documents into text.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-base-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 62, "text": " A startup company is working on an app to support blind people in their daily life. They need a system to answer questions about the contents of the images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"blip2-opt-2.7b\", \"api_call\": \"Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", \"api_arguments\": {\"img_url\": \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\", \"question\": \"how many dogs are in the picture?\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": {\"import_requests\": \"import requests\", \"import_PIL\": \"from PIL import Image\", \"import_transformers\": \"from transformers import BlipProcessor, Blip2ForConditionalGeneration\", \"load_processor\": \"processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')\", \"load_model\": \"model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')\", \"load_image\": \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", \"process_inputs\": \"inputs = processor(raw_image, question, return_tensors='pt')\", \"generate_output\": \"out = model.generate(**inputs)\", \"decode_output\": \"print(processor.decode(out[0], skip_special_tokens=True))\"}, \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not specified\"}, \"description\": \"BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 63, "text": " Our customers need a tool to convert handwritten documents to digital text for archival purposes.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-small-handwritten\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"IAM\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 64, "text": " We want to describe an image of Mars Rover landing for our Space Museum poster.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"naver-clova-ix/donut-base\", \"api_call\": \"AutoModel.from_pretrained('naver-clova-ix/donut-base')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"result = donut(image_path)\", \"performance\": {\"dataset\": \"arxiv:2111.15664\", \"accuracy\": \"Not provided\"}, \"description\": \"Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\"}}", "category": "generic"}
{"question_id": 65, "text": " Creating a program to describe objects in images for the visually impaired.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-coco\", \"api_call\": \"pipeline('text-generation', model='microsoft/git-base-coco')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"See the model hub for fine-tuned versions on a task that interests you.\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Refer to the paper for evaluation results.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 66, "text": " I am the manager of a mail center. I am looking for a text description of the images that are sent out while following user-provided instructions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"promptcap-coco-vqa\", \"api_call\": \"'model.caption(prompt, image)'\", \"api_arguments\": {\"prompt\": \"string\", \"image\": \"string\"}, \"python_environment_requirements\": \"pip install promptcap\", \"example_code\": [\"import torch\", \"from promptcap import PromptCap\", \"model = PromptCap(vqascore/promptcap-coco-vqa)\", \"if torch.cuda.is_available():\", \"  model.cuda()\", \"prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\", \"image = glove_boy.jpeg\", \"print(model.caption(prompt, image))\"], \"performance\": {\"dataset\": {\"coco\": {\"accuracy\": \"150 CIDEr\"}, \"OK-VQA\": {\"accuracy\": \"60.4%\"}, \"A-OKVQA\": {\"accuracy\": \"59.6%\"}}}, \"description\": \"PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\"}}", "category": "generic"}
{"question_id": 67, "text": " We would like to present images of our product to create a caption for each image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"AICVTG_What_if_a_machine_could_create_captions_automatically\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de') .generate(pixel_values, **gen_kwargs)\", \"api_arguments\": {\"image_paths\": \"List of image file paths\", \"max_length\": 20, \"num_beams\": 8}, \"python_environment_requirements\": {\"transformers\": \"from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\", \"torch\": \"import torch\", \"Image\": \"from PIL import Image\"}, \"example_code\": \"predict_step(['Image URL.jpg'])\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"This is an image captioning model training by Zayn\"}}", "category": "generic"}
{"question_id": 68, "text": " Develop a model to describe the images along with specific questions related to the content of these images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"blip2-flan-t5-xl\", \"api_call\": \"Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)\", \"api_arguments\": [\"raw_image\", \"question\"], \"python_environment_requirements\": [\"transformers\", \"requests\", \"PIL\"], \"example_code\": [\"import requests\", \"from PIL import Image\", \"from transformers import BlipProcessor, Blip2ForConditionalGeneration\", \"processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)\", \"model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)\", \"img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\", \"raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\", \"question = how many dogs are in the picture?\", \"inputs = processor(raw_image, question, return_tensors=pt)\", \"out = model.generate(**inputs)\", \"print(processor.decode(out[0], skip_special_tokens=True))\"], \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not provided\"}, \"description\": \"BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 69, "text": " I need to build an interactive robot that can answer questions using an image input, process questions in natural language, and generate a response.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"blip2-flan-t5-xxl\", \"api_call\": \"Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\", \"api_arguments\": {\"raw_image\": \"Image\", \"question\": \"Text\"}, \"python_environment_requirements\": [\"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not provided\"}, \"description\": \"BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 70, "text": " Design a solution that extracts text from an image URL.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-large-handwritten\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"microsoft/trocr-large-handwritten\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"IAM\", \"accuracy\": \"Not specified\"}, \"description\": \"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 71, "text": " We are a group of photo enthusiasts, and we need to know the description of the photo that we just took.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"blip2-opt-6.7b\", \"api_call\": \"pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')\", \"api_arguments\": \"image, optional text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"Refer to the documentation\", \"performance\": {\"dataset\": \"LAION\", \"accuracy\": \"Not specified\"}, \"description\": \"BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.\"}}", "category": "generic"}
{"question_id": 72, "text": " My team is developing an application to extract text from historical handwritten documents. I need to recognize the text in a given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-base-handwritten\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"microsoft/trocr-base-handwritten\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"IAM\", \"accuracy\": \"Not specified\"}, \"description\": \"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 73, "text": " We have a collection of images and would like to have a corresponding description for each image. Provide an example of how to convert one of the images into a text description.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"donut-base-finetuned-cord-v2\", \"api_call\": \"naver-clova-ix/donut-base-finetuned-cord-v2\", \"api_arguments\": {\"image\": \"path_to_image\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')\", \"performance\": {\"dataset\": \"CORD\", \"accuracy\": \"Not provided\"}, \"description\": \"Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.\"}}", "category": "generic"}
{"question_id": 74, "text": " We are a travel agency that wants to generate descriptions for images captured by tourists to use them in marketing purposes.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-coco\", \"api_call\": \"Output: GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')\", \"api_arguments\": \"image, text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"See table 11 in the paper for more details.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 75, "text": " The company is developing a creative application that generates descriptions for images. We need to process images to extract text descriptions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/pix2struct-chartqa-base\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\", \"api_arguments\": [\"t5x_checkpoint_path\", \"pytorch_dump_path\", \"use-large\"], \"python_environment_requirements\": \"transformers\", \"example_code\": \"python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE\", \"performance\": {\"dataset\": \"ChartQA\", \"accuracy\": \"Not provided\"}, \"description\": \"Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.\"}}", "category": "generic"}
{"question_id": 76, "text": " Our company wants to create automatic captions for social media images. We need assistance in generating meaningful captions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/pix2struct-base\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\", \"api_arguments\": {\"t5x_checkpoint_path\": \"PATH_TO_T5X_CHECKPOINTS\", \"pytorch_dump_path\": \"PATH_TO_SAVE\"}, \"python_environment_requirements\": {\"transformers\": \"4.15.0\", \"torch\": \"1.10.1\"}, \"example_code\": \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\\nmodel.push_to_hub(USERNAME/MODEL_NAME)\\nprocessor.push_to_hub(USERNAME/MODEL_NAME)\", \"performance\": {\"dataset\": [{\"name\": \"Documents\", \"accuracy\": \"N/A\"}, {\"name\": \"Illustrations\", \"accuracy\": \"N/A\"}, {\"name\": \"User Interfaces\", \"accuracy\": \"N/A\"}, {\"name\": \"Natural Images\", \"accuracy\": \"N/A\"}]}, \"description\": \"Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.\"}}", "category": "generic"}
{"question_id": 77, "text": " I need assistance to describe an image of a stop sign in Australia.\\n###Input: The image URL is https://www.ilankelman.org/stopsigns/australia.jpg\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/pix2struct-textcaps-base\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\", \"api_arguments\": {\"images\": \"image\", \"text\": \"text\", \"return_tensors\": \"pt\", \"max_patches\": 512}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": [\"import requests\", \"from PIL import Image\", \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\", \"url = https://www.ilankelman.org/stopsigns/australia.jpg\", \"image = Image.open(requests.get(url, stream=True).raw)\", \"model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)\", \"processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)\", \"inputs = processor(images=image, return_tensors=pt)\", \"predictions = model.generate(**inputs)\", \"print(processor.decode(predictions[0], skip_special_tokens=True))\"], \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"state-of-the-art\"}, \"description\": \"Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.\"}}", "category": "generic"}
{"question_id": 78, "text": " Please build an AI model to analyze the images we provide and caption relevantly.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Captioning\", \"api_name\": \"microsoft/git-base\", \"api_call\": \"pipeline('image-to-text', model='microsoft/git-base')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"git_base(image)\", \"performance\": {\"dataset\": [\"COCO\", \"Conceptual Captions (CC3M)\", \"SBU\", \"Visual Genome (VG)\", \"Conceptual Captions (CC12M)\", \"ALT200M\"], \"accuracy\": \"Refer to the paper for evaluation results\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 79, "text": " The organization has found an old document with important information. We need to extract the text from it to analyze the content.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-large-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\", \"api_arguments\": {\"TrOCRProcessor\": \"from_pretrained('microsoft/trocr-large-printed')\", \"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"pip install transformers\", \"PIL\": \"pip install pillow\", \"requests\": \"pip install requests\"}, \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\"}}", "category": "generic"}
{"question_id": 80, "text": " We have an old dataset of charts from previous reports, and we want to generate tables from these charts.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"google/deplot\", \"api_call\": \"Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\", \"api_arguments\": {\"images\": \"image\", \"text\": \"question\", \"return_tensors\": \"pt\", \"max_new_tokens\": 512}, \"python_environment_requirements\": {\"transformers\": \"Pix2StructForConditionalGeneration, Pix2StructProcessor\", \"requests\": \"requests\", \"PIL\": \"Image\"}, \"example_code\": \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\nimport requests\\nfrom PIL import Image\\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\npredictions = model.generate(**inputs, max_new_tokens=512)\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"ChartQA\", \"accuracy\": \"24.0% improvement over finetuned SOTA\"}, \"description\": \"DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.\"}}", "category": "generic"}
{"question_id": 81, "text": " Generate a descriptive text about an image that will be used for an online article.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-textcaps\", \"api_call\": \"AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')\", \"api_arguments\": \"image, text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 82, "text": " As a YouTube content creator, I need a description generated for my movie review video thumbnail to entice viewers.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-r-textcaps\", \"api_call\": \"pipeline('text-generation', model='microsoft/git-large-r-textcaps')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"TextCaps\", \"accuracy\": \"\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 83, "text": " Can you help me digitize the handwritten text from a digital scanned image?\\n###Input: https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-small-stage1\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\", \"api_arguments\": {\"url\": \"https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\", \"torch\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nimport torch\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\\npixel_values = processor(image, return_tensors='pt').pixel_values\\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\", \"performance\": {\"dataset\": \"IAM\", \"accuracy\": \"Not provided\"}, \"description\": \"TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.\"}}", "category": "generic"}
{"question_id": 84, "text": " The marketing team is creating a series of commercials and requires a visual representation of multiple scenarios based on their scripts.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video Synthesis\", \"api_name\": \"modelscope-damo-text-to-video-synthesis\", \"api_call\": \"pipeline('text-to-video-synthesis', model_dir.as_posix())\", \"api_arguments\": {\"text\": \"A short text description in English\"}, \"python_environment_requirements\": [\"modelscope==1.4.2\", \"open_clip_torch\", \"pytorch-lightning\"], \"example_code\": \"from huggingface_hub import snapshot_download\\nfrom modelscope.pipelines import pipeline\\nfrom modelscope.outputs import OutputKeys\\nimport pathlib\\nmodel_dir = pathlib.Path('weights')\\nsnapshot_download('damo-vilab/modelscope-damo-text-to-video-synthesis',\\n repo_type='model', local_dir=model_dir)\\npipe = pipeline('text-to-video-synthesis', model_dir.as_posix())\\ntest_text = {\\n 'text': 'A panda eating bamboo on a rock.',\\n}\\noutput_video_path = pipe(test_text,)[OutputKeys.OUTPUT_VIDEO]\\nprint('output_video_path:', output_video_path)\", \"performance\": {\"dataset\": \"Webvid, ImageNet, LAION5B\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\"}}", "category": "generic"}
{"question_id": 85, "text": " For our latest project, we would like to extract text from the license plates of parked vehicles.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/trocr-small-printed\", \"api_call\": \"VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\", \"api_arguments\": {\"image\": \"Image.open(requests.get(url, stream=True).raw).convert('RGB')\", \"processor\": \"TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\\nfrom PIL import Image\\nimport requests\\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\\npixel_values = processor(images=image, return_tensors='pt').pixel_values\\ngenerated_ids = model.generate(pixel_values)\\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\", \"performance\": {\"dataset\": \"SROIE\", \"accuracy\": \"Not specified\"}, \"description\": \"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.\"}}", "category": "generic"}
{"question_id": 86, "text": " Our client needs a method to extract text from items like license plates to process parking tickets more efficiently.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Image-to-Text\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"mgp-str\", \"api_call\": \"MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\", \"api_arguments\": {\"model_name\": \"alibaba-damo/mgp-str-base\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\"]}, \"example_code\": \"from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\\nimport requests\\nfrom PIL import Image\\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\\npixel_values = processor(images=image, return_tensors=pt).pixel_values\\noutputs = model(pixel_values)\\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']\", \"performance\": {\"dataset\": \"MJSynth and SynthText\", \"accuracy\": null}, \"description\": \"MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images.\"}}", "category": "generic"}
{"question_id": 87, "text": " We plan to produce a series of commercials for our online streaming platform. We want to generate a video of Spider-Man surfing.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-video synthesis\", \"api_name\": \"damo-vilab/text-to-video-ms-1.7b\", \"api_call\": \"DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\", \"api_arguments\": {\"torch_dtype\": \"torch.float16\", \"variant\": \"fp16\"}, \"python_environment_requirements\": \"pip install diffusers transformers accelerate\", \"example_code\": \"import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)\", \"performance\": {\"dataset\": \"Webvid, ImageNet, LAION5B\", \"accuracy\": \"N/A\"}, \"description\": \"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\"}}", "category": "generic"}
{"question_id": 88, "text": " I'm a writer and I use AI to quickly generate video clips explaining scenes in my script. Can you help me figure out the best model I can use and how to use it?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"chavinlo/TempoFunk\", \"api_call\": \"text_to_video = pipeline('text-to-video', model='chavinlo/TempoFunk')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\"}}", "category": "generic"}
{"question_id": 89, "text": " A video advertisement company requires a tool to generate short video clips based on the provided descriptions for their social media campaigns.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"ImRma/Brucelee\", \"api_call\": \"pipeline('text-to-video', model='ImRma/Brucelee')\", \"api_arguments\": [\"your_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Hugging Face model for converting Persian and English text into video.\"}}", "category": "generic"}
{"question_id": 90, "text": " We have a client who wants to create a video description of their product based on a text prompt.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"camenduru/text2-video-zero\", \"api_call\": \"pipeline('text-to-video', model='camenduru/text2-video-zero')\", \"api_arguments\": [\"input_text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\"}}", "category": "generic"}
{"question_id": 91, "text": " Create a short video to interact with kids, with the text \\\"A cute puppy is playing with a blue ball.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-video-synthesis\", \"api_name\": \"damo-vilab/text-to-video-ms-1.7b\", \"api_call\": \"DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\", \"api_arguments\": [\"prompt\", \"num_inference_steps\", \"num_frames\"], \"python_environment_requirements\": [\"pip install git+https://github.com/huggingface/diffusers transformers accelerate\"], \"example_code\": \"pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)\", \"performance\": {\"dataset\": \"Webvid\", \"accuracy\": \"Not specified\"}, \"description\": \"A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.\"}}", "category": "generic"}
{"question_id": 92, "text": " Help me to create a video of Superman flying over a city.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video Synthesis\", \"api_name\": \"damo-vilab/text-to-video-ms-1.7b-legacy\", \"api_call\": \"DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\", \"api_arguments\": [\"prompt\", \"num_inference_steps\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"import torch\\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\\nfrom diffusers.utils import export_to_video\\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\nprompt = Spiderman is surfing\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\nvideo_path = export_to_video(video_frames)\", \"performance\": {\"dataset\": [\"LAION5B\", \"ImageNet\", \"Webvid\"], \"accuracy\": \"Not provided\"}, \"description\": \"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\"}}", "category": "generic"}
{"question_id": 93, "text": " Our team is working on a project where we need to create educational videos from textual content. We are looking for a model to generate videos from text.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video\", \"api_name\": \"duncan93/video\", \"api_call\": \"N/A (There is no model instantiation provided in the `api_call` field)\", \"api_arguments\": \"\", \"python_environment_requirements\": \"Asteroid\", \"example_code\": \"\", \"performance\": {\"dataset\": \"OpenAssistant/oasst1\", \"accuracy\": \"\"}, \"description\": \"A text-to-video model trained on OpenAssistant/oasst1 dataset.\"}}", "category": "generic"}
{"question_id": 94, "text": " We are a video content generation company, and we need to create a short film about a magical princess playing the guitar. But, it must contain 50 inference steps.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video Generation\", \"api_name\": \"mo-di-bear-guitar\", \"api_call\": \"pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5)\", \"api_arguments\": {\"prompt\": \"string\", \"video_length\": \"int\", \"height\": \"int\", \"width\": \"int\", \"num_inference_steps\": \"int\", \"guidance_scale\": \"float\"}, \"python_environment_requirements\": [\"torch\", \"tuneavideo\"], \"example_code\": \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/mo-di-diffusion\\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = a magical princess is playing guitar, modern disney style\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", \"performance\": {\"dataset\": \"Not mentioned\", \"accuracy\": \"Not mentioned\"}, \"description\": \"Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\"}}", "category": "generic"}
{"question_id": 95, "text": " As an interactive entertainment corporation, we want to create a video based on the prompt \\\"a superhero flying through the city\\\".\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Text-to-Video\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Video Generation\", \"api_name\": \"redshift-man-skiing\", \"api_call\": \"pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5)\", \"api_arguments\": {\"prompt\": \"string\", \"video_length\": \"int\", \"height\": \"int\", \"width\": \"int\", \"num_inference_steps\": \"int\", \"guidance_scale\": \"float\"}, \"python_environment_requirements\": [\"torch\", \"tuneavideo\"], \"example_code\": \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\nfrom tuneavideo.util import save_videos_grid\\nimport torch\\npretrained_model_path = nitrosocke/redshift-diffusion\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\npipe.enable_xformers_memory_efficient_attention()\\nprompt = (redshift style) spider man is skiing\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\nsave_videos_grid(video, f./{prompt}.gif)\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\"}}", "category": "generic"}
{"question_id": 96, "text": " I am an architect and I need to analyze the images from a construction project. Can you recommend an efficient way to build a system that can tell me what's in the images and answer questions?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-textvqa\", \"api_call\": \"git_base_textvqa = AutoModel.from_pretrained('microsoft/git-base-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}}", "category": "generic"}
{"question_id": 97, "text": " I am an AI researcher working on home robotics. I need to get an understanding of the specific objects in a room by answering questions based on a visual input.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-base-vqav2\", \"api_call\": \"pipeline('visual-question-answering', model='microsoft/git-base-vqav2')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"vqa(image='path/to/image.jpg', question='What is in the image?')\", \"performance\": {\"dataset\": \"VQAv2\", \"accuracy\": \"Refer to the paper for evaluation results\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 98, "text": " I am a clothing designer and I need to analyze an image of a new fabric pattern with a model. Can you suggest how to do that to get the description of the fabric pattern?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"ivelin/donut-refexp-combined-v1\", \"api_call\": \"pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"vqa(image='path/to/image.jpg', question='What is the color of the object?')\", \"performance\": {\"dataset\": \"ivelin/donut-refexp-combined-v1\", \"accuracy\": \"N/A\"}, \"description\": \"A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\"}}", "category": "generic"}
{"question_id": 99, "text": " We want to create an application that allows users to upload photos of tourist attractions and provides them with information about the images. Implement a system that can answer questions about these images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/git-large-vqav2\", \"api_call\": \"AutoModel.from_pretrained('microsoft/git-large-vqav2')\", \"api_arguments\": {\"model\": \"microsoft/git-large-vqav2\", \"task\": \"visual-question-answering\", \"device\": 0}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})\", \"performance\": {\"dataset\": \"VQAv2\", \"accuracy\": \"Refer to the paper\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}}", "category": "generic"}
{"question_id": 100, "text": " Predict the answer to the question \\\"How many cats are there?\\\" using a transformers model that is trained on a Multimodal Visual Question Answering dataset.\\n###Input: {\\\"image_url\\\": \\\"http://images.cocodataset.org/val2017/000000039769.jpg\\\", \\\"question\\\": \\\"How many cats are there?\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"dandelin/vilt-b32-finetuned-vqa\", \"api_call\": \"ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')\", \"api_arguments\": {\"image\": \"Image.open(requests.get(url, stream=True).raw)\", \"text\": \"How many cats are there?\"}, \"python_environment_requirements\": {\"transformers\": \"ViltProcessor, ViltForQuestionAnswering\", \"requests\": \"requests\", \"PIL\": \"Image\"}, \"example_code\": \"from transformers import ViltProcessor, ViltForQuestionAnswering\\nimport requests\\nfrom PIL import Image\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntext = How many cats are there?\\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\\nencoding = processor(image, text, return_tensors=pt)\\noutputs = model(**encoding)\\nlogits = outputs.logits\\nidx = logits.argmax(-1).item()\\nprint(Predicted answer:, model.config.id2label[idx])\", \"performance\": {\"dataset\": \"VQAv2\", \"accuracy\": \"to do\"}, \"description\": \"Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 101, "text": " We have a field of horses and we want to know how many of them are in the stable.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"blip-vqa-base\", \"api_call\": \"Output: BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-base').generate(**inputs)\", \"api_arguments\": {\"raw_image\": \"Image\", \"question\": \"String\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForQuestionAnswering\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-base)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-base)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"VQA\", \"accuracy\": \"+1.6% in VQA score\"}, \"description\": \"BLIP is a Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is trained on visual question answering with a base architecture (using ViT base backbone).\"}}", "category": "generic"}
{"question_id": 102, "text": " We are creating a virtual tour guide. The guide will answer the users' questions about the places they visit based on the images of those places.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"Salesforce/blip-vqa-capfilt-large\", \"api_call\": \"BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\", \"api_arguments\": {\"raw_image\": \"RGB image\", \"question\": \"string\"}, \"python_environment_requirements\": {\"transformers\": \"BlipProcessor, BlipForQuestionAnswering\"}, \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \"performance\": {\"dataset\": \"VQA\", \"accuracy\": \"+1.6% in VQA score\"}, \"description\": \"BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA.\"}}", "category": "generic"}
{"question_id": 103, "text": " I am building an AI that will give me the answers from an image I have taken from a book which contains answers. Help me build the AI that will do this.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"git-large-textvqa\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')\", \"api_arguments\": \"image, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"TextVQA\", \"accuracy\": \"See table 11 in the paper for more details.\"}, \"description\": \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}}", "category": "generic"}
{"question_id": 104, "text": " We have a facial recognition software involved in a Q&A system that needs to answer questions about images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"hf-tiny-model-private/tiny-random-ViltForQuestionAnswering\", \"api_call\": \"ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')\", \"api_arguments\": {\"image\": \"path/to/image/file\", \"question\": \"your_question\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random model for Visual Question Answering using the VILT framework.\"}}", "category": "generic"}
{"question_id": 105, "text": " We need a solution for answering customers' questions based on the images they send to us.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"azwierzc/vilt-b32-finetuned-vqa-pl\", \"api_call\": \"vqa_pipeline = pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')\", \"api_arguments\": {\"image\": \"path_to_image\", \"question\": \"question_text\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Visual Question Answering model fine-tuned on the Polish language.\"}}", "category": "generic"}
{"question_id": 106, "text": " I have a group of images and their captions in a dataframe for a class project. I want to perform question answering on these images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"sheldonxxxx/OFA_model_weights\", \"api_call\": \"AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.\"}}", "category": "generic"}
{"question_id": 107, "text": " Our company is developing an app that provides answers to questions about images. Identify the most suitable model and the code to implement this functionality.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"vilt-finetuned-vqasi\", \"api_call\": \"ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')\", \"api_arguments\": {\"model\": \"tufa15nik/vilt-finetuned-vqasi\", \"tokenizer\": \"tufa15nik/vilt-finetuned-vqasi\"}, \"python_environment_requirements\": {\"transformers\": \">=4.11.3\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.\"}}", "category": "generic"}
{"question_id": 108, "text": " Set up a model to answer questions about images in multiple languages, such as English, Chinese, Japanese, and German. Users will send images along with questions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"JosephusCheung/GuanacoVQA\", \"api_call\": \"pipeline('visual-question-answering', model='GuanacoVQA').\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers, torch\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"JosephusCheung/GuanacoVQADataset\", \"accuracy\": \"N/A\"}, \"description\": \"A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4.\"}}", "category": "generic"}
{"question_id": 109, "text": " Help our customer James to find an answer to a question about an image content he is curious about.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"temp_vilt_vqa\", \"api_call\": \"pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')\", \"api_arguments\": {\"model\": \"Bingsu/temp_vilt_vqa\", \"tokenizer\": \"Bingsu/temp_vilt_vqa\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A visual question answering model for answering questions related to images using the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 110, "text": " A mobile app needs to answer questions about what's happening in specific images to help users with visual impairments.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face\", \"functionality\": \"Visual Question Answering\", \"api_name\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"api_call\": \"pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')\", \"api_arguments\": {\"model\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\", \"tokenizer\": \"JosephusCheung/GuanacoVQAOnConsumerHardware\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": \"vqa(image_path, question)\", \"performance\": {\"dataset\": \"JosephusCheung/GuanacoVQADataset\", \"accuracy\": \"unknown\"}, \"description\": \"A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\"}}", "category": "generic"}
{"question_id": 111, "text": " In the accounting department, we are processing invoices and need to find the invoice number from a given image of an invoice.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"impira/layoutlm-document-qa\", \"api_call\": \"layoutlm_document_qa = pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa', return_dict=True))\", \"api_arguments\": [\"image_url\", \"question\"], \"python_environment_requirements\": [\"PIL\", \"pytesseract\", \"PyTorch\", \"transformers\"], \"example_code\": \"nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)\", \"performance\": {\"dataset\": [\"SQuAD2.0\", \"DocVQA\"], \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.\"}}", "category": "generic"}
{"question_id": 112, "text": " A bank provides us with account statements of their clients in PDF format. We need to extract information such as transaction date, transaction amount, and balance after each transaction.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased-finetuned-docvqa\", \"api_call\": \"layoutlmv2-base-uncased-finetuned-docvqa\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.12.2\", \"torch==1.8.0+cu101\", \"datasets==1.14.0\", \"tokenizers==0.10.3\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 1.194}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 113, "text": " A friend has sent me a pdf invoice and I need to extract the overall amount from it.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"impira/layoutlm-invoices\", \"api_call\": \"pipeline('question-answering', model='impira/layoutlm-invoices')\", \"api_arguments\": \"question, context\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"qa_pipeline(question='your question', context='your document context')\", \"performance\": {\"dataset\": \"proprietary dataset of invoices, SQuAD2.0, and DocVQA\", \"accuracy\": \"not provided\"}, \"description\": \"This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.\"}}", "category": "generic"}
{"question_id": 114, "text": " Create a system that can extract specific information from documents to answer users' questions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"xhyi/layoutlmv3_docvqa_t11c5000\", \"api_call\": \"pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')\", \"api_arguments\": \"question, context\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"\"}, \"description\": \"LayoutLMv3 model trained for document question answering task.\"}}", "category": "generic"}
{"question_id": 115, "text": " Develop a Q&A system for an online textbook platform. It should be able to answer questions from the text and images within the document.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"dperales/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')\", \"api_arguments\": {\"model\": \"dperales/layoutlmv2-base-uncased_finetuned_docvqa\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for Document Question Answering based on the LayoutLMv2 architecture, fine-tuned on the DocVQA dataset.\"}}", "category": "generic"}
{"question_id": 116, "text": " I need a personal assistant to help me analyze the financial reports and answer the question: What is the total revenue for the company?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"vision-encoder-decoder\", \"api_name\": \"naver-clova-ix/donut-base-finetuned-docvqa\", \"api_call\": \"donut-base-finetuned-docvqa\", \"api_arguments\": {\"image\": \"path_to_image\", \"question\": \"your_question\"}, \"python_environment_requirements\": \"Transformers\", \"example_code\": \"from transformers import pipeline\\n# Initialize the pipeline\\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\\n# Load an image and ask a question\\nimage_path = 'path_to_image'\\nquestion = 'your_question'\\n# Get the answer\\nanswer = doc_qa({'image': image_path, 'question': question})\\nprint(answer)\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\"}}", "category": "generic"}
{"question_id": 117, "text": " Our company is managing a large digital archive. We require a tool to answer questions about table content in official documents.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"CZ_DVQA_layoutxlm-base\", \"api_call\": \"LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Document Question Answering model based on LayoutXLM.\"}}", "category": "generic"}
{"question_id": 118, "text": " As a student, I have an article in PDF format. I want to extract the answer to a specific question from that article.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"layoutlm-vqa\", \"api_call\": \"pipeline('question-answering', model='pardeepSF/layoutlm-vqa')\", \"api_arguments\": {\"question\": \"string\", \"context\": \"string\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for document question answering using the LayoutLM architecture.\"}}", "category": "generic"}
{"question_id": 119, "text": " I need a model to extract detailed information from a textual invoice. Please tell me which API can extract specific details from an invoice.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"layoutlm-invoices\", \"api_call\": \"faisalraza/layoutlm-invoices\", \"api_arguments\": \"question, context\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"nlp(question='What is the total amount?', context='your_invoice_text')\", \"performance\": {\"dataset\": \"proprietary dataset of invoices, SQuAD2.0, and DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\"}}", "category": "generic"}
{"question_id": 120, "text": " We are a publishing company. We need to find an answer in a book with multiple pages.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"layoutlmv3-base-mpdocvqa\", \"api_call\": \"'LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)'\", \"api_arguments\": [\"image\", \"question\", \"context\", \"boxes\"], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\\nimage = Image.open(example.jpg).convert(RGB)\\nquestion = Is this a question?\\ncontext = [Example]\\nboxes = [0, 0, 1000, 1000]\\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\\noutputs = model(**document_encoding)\\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\\nend_idx = torch.argmax(outputs.end_logits, axis=1)\\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()\", \"performance\": {\"dataset\": \"rubentito/mp-docvqa\", \"accuracy\": {\"ANLS\": 0.4538, \"APPA\": 51.9426}}, \"description\": \"This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA.\"}}", "category": "generic"}
{"question_id": 121, "text": " Help me extract answers from a scanned document for a question related to the subject or topic of the document.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"vision-encoder-decoder\", \"api_name\": \"jinhybr/OCR-DocVQA-Donut\", \"api_call\": \"pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut').model.vision_encoder\", \"api_arguments\": \"image_path, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"doc_vqa(image_path='path/to/image.jpg', question='What is the title?')\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\"}}", "category": "generic"}
{"question_id": 122, "text": " Develop an AI model for a local art gallery accepting scanned artwork, to help the staff easily find information about any piece of art by asking questions.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"tiny-random-LayoutLMv3ForQuestionAnswering\", \"api_call\": \"LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')\", \"api_arguments\": {\"image\": \"path/to/image/file\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"tensorflow\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API.\"}}", "category": "generic"}
{"question_id": 123, "text": " Our company is working on automating document analysis. We need to identify relevant information within a given document.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"DataIntelligenceTeam/eurocorpV4\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('DataIntelligenceTeam/eurocorpV4')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers>=4.26.0.dev0, torch>=1.12.1+cu113, datasets>=2.2.2, tokenizers>=0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"sroie\", \"accuracy\": 0.982}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv3-large on the sroie dataset. It achieves the following results on the evaluation set: Loss: 0.1239, Precision: 0.9548, Recall: 0.9602, F1: 0.9575, Accuracy: 0.9819\"}}", "category": "generic"}
{"question_id": 124, "text": " We want to develop our online documentation service platform to be more user-friendly. Users can ask questions and get answers about our website's user manual.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa_v2\", \"api_call\": \"layoutlmv2-base-uncased_finetuned_docvqa_v2\", \"api_arguments\": {\"question\": \"string\", \"context\": \"string\"}, \"python_environment_requirements\": {\"transformers\": \"4.26.0\", \"pytorch\": \"1.13.1+cu116\", \"datasets\": \"2.9.0\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"None\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset for Document Question Answering tasks.\"}}", "category": "generic"}
{"question_id": 125, "text": " Guidance is required for making code that identify's an object in a document to answer a specific question or to confirm if the object is present in it.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"frizwankhan/entity-linking-model-final\", \"api_call\": \"pipeline('question-answering', model='frizwankhan/entity-linking-model-final')\", \"api_arguments\": {\"image\": \"path/to/image\", \"question\": \"your question\"}, \"python_environment_requirements\": {\"huggingface\": \"4.12.0\", \"torch\": \"1.9.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Document Question Answering model based on layoutlmv2\"}}", "category": "generic"}
{"question_id": 126, "text": " I am a bank manager and I want to extract specific data from my company documents. What can I use to achieve this?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"seungwon12/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"DocVQA\", \"accuracy\": \"\"}, \"description\": \"A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}", "category": "generic"}
{"question_id": 127, "text": " We have a repository of scanned documents. We need an assistant that can answer questions based on them. Find a matching API.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023\", \"api_call\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023\", \"api_arguments\": {\"question\": \"string\", \"context\": \"string\"}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A LayoutLMv2 model for document question answering.\"}}", "category": "generic"}
{"question_id": 128, "text": " Working in a government agency, I need to extract specific information from a variety of unstructured forms and documents. Could you help me extract the answer to my question from the given context?\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa\", \"api_arguments\": \"{'question': 'your_question', 'context': 'your_context'}\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased.\"}}", "category": "generic"}
{"question_id": 129, "text": " Explain how this model can be implemented for answering questions from a mutlimodal document.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased-finetuned-infovqa\", \"api_call\": \"layoutlmv2-base-uncased-finetuned-infovqa\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \"4.12.2\", \"pytorch\": \"1.8.0+cu101\", \"datasets\": \"1.14.0\", \"tokenizers\": \"0.10.3\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 2.087}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 130, "text": " The finance team at your company received a scanned invoice and they need to know the total amount. Implement a solution to extract that information.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_arguments\": {\"model\": \"hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa\"}, \"python_environment_requirements\": {\"transformers\": \"4.27.4\", \"pytorch\": \"2.0.0+cu117\", \"datasets\": \"2.11.0\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\\nquestion = 'What is the total amount?'\\ndocument = 'path/to/your/image/file.png'\\nresult = qa_pipeline(question=question, document=document)\\nprint(result)\", \"performance\": {\"dataset\": \"None\", \"accuracy\": {\"Loss\": 4.843}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.\"}}", "category": "generic"}
{"question_id": 131, "text": " I need to identify the cost of a product from a document image where the same question is repeated, but only one is answered.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-large-uncased-finetuned-infovqa\", \"api_call\": \"layoutlmv2-large-uncased-finetuned-infovqa\", \"api_arguments\": \"question, context\", \"python_environment_requirements\": \"transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3\", \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 2.2207}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 132, "text": " We want to extract information from images containing text and layout information in scanned documents.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\", \"api_call\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \">=4.11.0\"}, \"example_code\": {}, \"performance\": {\"dataset\": {}, \"accuracy\": {}}, \"description\": \"A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.\"}}", "category": "generic"}
{"question_id": 133, "text": " A user submits an image of a document with a table, figures, and text. Develop a feature to answer questions based on the content of the document.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023\", \"api_call\": \"LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')\", \"api_arguments\": {\"question\": \"string\", \"context\": \"string\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A LayoutLM model for document question answering.\"}}", "category": "generic"}
{"question_id": 134, "text": " How can I extract answers from a document? I need to pass the image of the document and provide the question.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_call\": \"layoutlmv2-base-uncased_finetuned_docvqa\", \"api_arguments\": \"question, image\", \"python_environment_requirements\": \"transformers, torch, datasets, tokenizers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"None\", \"accuracy\": {\"Loss\": 4.3167}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset.\"}}", "category": "generic"}
{"question_id": 135, "text": " I have a startup and my friend gave a document on his project detail. I want to find the budget of his project from the document.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"tiny-random-LayoutLMForQuestionAnswering\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture.\"}}", "category": "generic"}
{"question_id": 136, "text": " A user wants to build an environment checker tool which can answer questions related to pollution and surroundings. The user has some legal documents about carbon emissions and wants answers from them.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face\", \"functionality\": \"Document Question Answering\", \"api_name\": \"davanstrien/testwebhook\", \"api_call\": \"pipeline('question-answering')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"pile-of-law/pile-of-law\", \"accuracy\": \"\"}, \"description\": \"A model trained for answering questions related to legal documents and carbon emissions.\"}}", "category": "generic"}
{"question_id": 137, "text": " We are making a budget analysis tool for a financial institution. Extract information from scanned receipts and invoices.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face\", \"functionality\": \"Question Answering\", \"api_name\": \"impira/layoutlm-document-qa\", \"api_call\": \"layoutlm_qa = pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa', return_dict=True))\", \"api_arguments\": [\"image_url\", \"question\"], \"python_environment_requirements\": [\"PIL\", \"pytesseract\", \"PyTorch\", \"transformers\"], \"example_code\": \"nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)\", \"performance\": {\"dataset\": \"SQuAD2.0 and DocVQA\", \"accuracy\": \"Not provided\"}, \"description\": \"A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents.\"}}", "category": "generic"}
{"question_id": 138, "text": " I'm a book publisher and got a question from a reader about some content. Help me to find an answer in the book provided.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa\", \"api_call\": \"pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')\", \"api_arguments\": \"question, context\", \"python_environment_requirements\": [\"transformers==4.15.0\", \"torch==1.8.0+cu101\", \"datasets==1.17.0\", \"tokenizers==0.10.3\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 4.3332}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 139, "text": " Create a system that classifies molecules based on their properties using graph representations.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Graph Machine Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Graph Classification\", \"api_name\": \"graphormer-base-pcqm4mv2\", \"api_call\": \"AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')\", \"api_arguments\": \"pretrained_model_name\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"See the Graph Classification with Transformers tutorial.\", \"performance\": {\"dataset\": \"PCQM4M-LSCv2\", \"accuracy\": \"Not provided\"}, \"description\": \"The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling.\"}}", "category": "generic"}
{"question_id": 140, "text": " Our team needs an AI model to help answer questions about documents containing visual information.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa\", \"api_call\": \"pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3\", \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": {\"Loss\": 8.5806}}, \"description\": \"This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 141, "text": " Identify potential targets for drug development by predicting the properties of molecules using a pre-trained multimodal graph Transformer model.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Graph Machine Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"graphormer-base-pcqm4mv1\", \"api_call\": \"Output: AutoModel.from_pretrained(model_name)\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"See the Graph Classification with Transformers tutorial\", \"performance\": {\"dataset\": \"PCQM4M-LSC\", \"accuracy\": \"1st place on the KDD CUP 2021 (quantum prediction track)\"}, \"description\": \"The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.\"}}", "category": "generic"}
{"question_id": 142, "text": " Our company needs to extract information from invoices for customer billing purposes. Extract the information required from the input image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Document Question Answer\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Document Question Answering\", \"api_name\": \"CQI_Visual_Question_Awnser_PT_v0\", \"api_call\": \"layoutlm_document_qa = pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))\", \"api_arguments\": [\"url\", \"question\"], \"python_environment_requirements\": [\"PIL\", \"pytesseract\", \"PyTorch\", \"transformers\"], \"example_code\": [\"nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')\", \"nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')\", \"nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')\"], \"performance\": {\"dataset\": [{\"accuracy\": 0.9943977}, {\"accuracy\": 0.9912159}, {\"accuracy\": 0.59147286}]}, \"description\": \"A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\"}}", "category": "generic"}
{"question_id": 143, "text": " In our autonomous vehicle project, we need to estimate the depth of the environment using computer vision techniques.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"tiny-random-DPTForDepthEstimation\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random DPT model for depth estimation using Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 144, "text": " We are working on applications of image-based depth estimation techniques. we need to know how to estimate the depth.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"dpt-large-redesign\", \"api_call\": \"torch.hub.load('nielsr/dpt-large-redesign')\", \"api_arguments\": [], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A depth estimation model based on the DPT architecture.\"}}", "category": "generic"}
{"question_id": 145, "text": " Our team is working on a self-driving car project. We need to integrate a depth estimation AI to our system.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\", \"api_call\": \"AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')\", \"api_arguments\": [], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 146, "text": " Create a smart city project where depth estimation is essential. It needs to analyze images to understand potential risks.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-kitti\", \"api_call\": \"'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)'\", \"api_arguments\": \"images, return_tensors\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\", \"performance\": {\"dataset\": \"KITTI\", \"accuracy\": \"Not provided\"}, \"description\": \"Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 147, "text": " We are building a virtual makeover app and we need to estimate the depth of various surfaces in an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-230131-041708\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4425, \"Mae\": 0.427, \"Rmse\": 0.6196, \"Abs_Rel\": 0.4543, \"Log_Mae\": 0.1732, \"Log_Rmse\": 0.2288, \"Delta1\": 0.3787, \"Delta2\": 0.6298, \"Delta3\": 0.8083}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.\"}}", "category": "generic"}
{"question_id": 148, "text": " The digital marketing company needs to create a virtual reality environment for their product display. They asked us to estimate the depth of input images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Monocular Depth Estimation\", \"api_name\": \"Intel/dpt-large\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('Intel/dpt-large')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"Intel/dpt-large\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import DPTImageProcessor, DPTForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\", \"performance\": {\"dataset\": \"MIX 6\", \"accuracy\": \"10.82\"}, \"description\": \"Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\"}}", "category": "generic"}
{"question_id": 149, "text": " Implement an AI-based robotic vacuum cleaner that navigates its surroundings while maneuvering around obstacles. The vacuum cleaner will use computer vision to estimate depth for movement planning.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu\", \"api_call\": \"'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)'\", \"api_arguments\": \"images, return_tensors\", \"python_environment_requirements\": [\"transformers\", \"torch\", \"numpy\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\nimport torch\\nimport numpy as np\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\", \"performance\": {\"dataset\": \"NYUv2\", \"accuracy\": \"Not provided\"}, \"description\": \"Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 150, "text": " Construct a 3D model of the residential area. We need the depth information from the surrounding images for that purpose.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4359, \"Rmse\": 0.4276}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 151, "text": " We are designing a parking assistant system that needs depth estimation to help drivers park their cars properly.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221215-093747\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"DIODE\", \"accuracy\": \"\"}, \"description\": \"A depth estimation model fine-tuned on the DIODE dataset.\"}}", "category": "generic"}
{"question_id": 152, "text": " I am working on an indoor robot project that needs depth estimation for the robot's vision. Help sequence the proper code for depth estimation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"Intel/dpt-hybrid-midas\", \"api_call\": \"DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"Intel/dpt-hybrid-midas\", \"low_cpu_mem_usage\": \"True\"}, \"python_environment_requirements\": [\"torch\", \"transformers\", \"PIL\", \"numpy\", \"requests\"], \"example_code\": \"from PIL import Image\\nimport numpy as np\\nimport requests\\nimport torch\\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n predicted_depth = outputs.predicted_depth\\nprediction = torch.nn.functional.interpolate(\\n predicted_depth.unsqueeze(1),\\n size=image.size[::-1],\\n mode=bicubic,\\n align_corners=False,\\n)\\noutput = prediction.squeeze().cpu().numpy()\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\ndepth = Image.fromarray(formatted)\\ndepth.show()\", \"performance\": {\"dataset\": \"MIX 6\", \"accuracy\": \"11.06\"}, \"description\": \"Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\"}}", "category": "generic"}
{"question_id": 153, "text": " We are filming a movie with expansive sceneries. Provide us a depth estimation model that helps us choose the best backgrounds.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221215-092352\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')\", \"api_arguments\": {}, \"python_environment_requirements\": {\"huggingface_transformers\": \"4.13.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"DIODE\", \"accuracy\": \"\"}, \"description\": \"A depth estimation model fine-tuned on the DIODE dataset.\"}}", "category": "generic"}
{"question_id": 154, "text": " Determine the depth of objects in an image to aid in an autonomous vehicle's perception system.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221215-095508\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')\", \"api_arguments\": null, \"python_environment_requirements\": [\"transformers\"], \"example_code\": null, \"performance\": {\"dataset\": \"DIODE\", \"accuracy\": null}, \"description\": \"A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture.\"}}", "category": "generic"}
{"question_id": 155, "text": " We are working on an autonomous car project and need to estimate the depth of objects in the environment.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221215-112116\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"DIODE\", \"accuracy\": \"\"}, \"description\": \"A depth estimation model fine-tuned on the DIODE dataset.\"}}", "category": "generic"}
{"question_id": 156, "text": " The real estate company wants to develop a tool that estimates the depth of a room from an image of the room.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221122-030603\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-030603')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.24.0\", \"torch==1.12.1\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3597, \"Mae\": 0.3054, \"Rmse\": 0.4481, \"Abs Rel\": 0.3462, \"Log Mae\": 0.1256, \"Log Rmse\": 0.1798, \"Delta1\": 0.5278, \"Delta2\": 0.8055, \"Delta3\": 0.9191}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 157, "text": " Help me to create an application to measure the depth of objects in images captured by a camera.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-kitti-finetuned-diode\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.5845, \"Rmse\": 0.6175}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 158, "text": " We are developing a smart mobility solution for the visually impaired. We want to estimate the depth of surrounding objects in real-time.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode-221116-054332\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')\", \"api_arguments\": {\"model_name\": \"sayakpaul/glpn-nyu-finetuned-diode-221116-054332\"}, \"python_environment_requirements\": {\"transformers\": \"4.24.0\", \"pytorch\": \"1.13.0+cu117\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.6028, \"Rmse\": \"nan\"}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 159, "text": " Our gaming sector is working on implementing AI in the popular game GTA 5. We need an AI model trained on MNIST dataset and save it as an onnx file with limited data cap.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Graph Machine Learning\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Train AI model for GTA5 game and save the model\", \"api_name\": \"GTA5_PROCESS_LEARNING_AI\", \"api_call\": \"No changes needed.\", \"api_arguments\": {\"model\": \"NanoCircuit\", \"data_loader\": \"train_loader\", \"criterion\": \"nn.CrossEntropyLoss\", \"optimizer\": \"optim.SGD\", \"device\": \"torch.device\", \"data_cap_gb\": 10}, \"python_environment_requirements\": [\"contextlib\", \"os\", \"matplotlib\", \"numpy\", \"torch\", \"torch.nn\", \"torch.optim\", \"requests\", \"torchvision\", \"psutil\", \"time\", \"subprocess\", \"onnxruntime\", \"numexpr\", \"transformers\"], \"example_code\": {\"import_libraries\": [\"import contextlib\", \"import os\", \"from matplotlib import pyplot as plt\", \"import numpy as np\", \"import torch\", \"import torch.nn as nn\", \"import torch.optim as optim\", \"import requests\", \"from torchvision import datasets, transforms\", \"import psutil\", \"import time\", \"import subprocess\", \"import onnxruntime as ort\", \"import matplotlib.pyplot as plt\", \"import numpy as np\", \"import numexpr as ne\", \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\"], \"define_neural_network\": [\"class NanoCircuit(nn.Module):\", \" def init(self):\", \" super(NanoCircuit, self).init()\", \" self.fc1 = nn.Linear(784, 128)\", \" self.fc2 = nn.Linear(128, 10)\", \"def forward(self, x):\", \" x = x.view(-1, 784)\", \" x = torch.relu(self.fc1(x))\", \" x = self.fc2(x)\", \" return x\"], \"train_with_data_cap\": [\"def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):\", \" data_processed = 0\", \" data_cap_bytes = data_cap_gb * (1024 ** 3)\", \" epoch = 0\", \"while data_processed < data_cap_bytes:\", \" running_loss = 0.0\", \" for i, data in enumerate(data_loader, 0):\", \" inputs, labels = data\", \" inputs, labels = inputs.to(device), labels.to(device)\", \" data_processed += inputs.nelement() * inputs.element_size()\", \" if data_processed >= data_cap_bytes:\", \" break\", \" optimizer.zero_grad()\", \" outputs = model(inputs.view(-1, 28 * 28))\", \" loss = criterion(outputs, labels)\", \" loss.backward()\", \" optimizer.step()\", \" running_loss += loss.item()\", \"epoch += 1\", \"print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})\", \"print(fData processed: {data_processed / (1024 ** 3):.2f} GB)\", \"return model\"]}, \"performance\": {\"dataset\": \"MNIST\", \"accuracy\": \"Not specified\"}, \"description\": \"This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available.\"}}", "category": "generic"}
{"question_id": 160, "text": " I have an image of a room and I would like to estimate the depth of the objects within the room.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode-221116-062619\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')\", \"api_arguments\": \"None\", \"python_environment_requirements\": \"Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2\", \"example_code\": \"None\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.548, \"Rmse\": \"nan\"}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 161, "text": " Create a system that can estimate the depth of objects in an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode-221116-104421\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3736, \"Mae\": 0.3079, \"Rmse\": 0.4321, \"Abs Rel\": 0.3666, \"Log Mae\": 0.1288, \"Log Rmse\": 0.1794, \"Delta1\": 0.4929, \"Delta2\": 0.7934, \"Delta3\": 0.9234}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 162, "text": " We are developing an autonomous driving system, and we need an algorithm to estimate the depth of objects using a single image captured from the car's camera.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221121-063504\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.24.0\", \"torch==1.12.1+cu116\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3533, \"Mae\": 0.2668, \"Rmse\": 0.3716, \"Abs Rel\": 0.3427, \"Log Mae\": 0.1167, \"Log Rmse\": 0.1703, \"Delta1\": 0.5522, \"Delta2\": 0.8362, \"Delta3\": 0.9382}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation.\"}}", "category": "generic"}
{"question_id": 163, "text": " A mobile app development company is trying to build a feature to overlay filters on photos. We need to estimate depth in images for this feature.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221116-110652\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4018, \"Mae\": 0.3272, \"Rmse\": 0.4546, \"Abs Rel\": 0.3934, \"Log Mae\": 0.138, \"Log Rmse\": 0.1907, \"Delta1\": 0.4598, \"Delta2\": 0.7659, \"Delta3\": 0.9082}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.\"}}", "category": "generic"}
{"question_id": 164, "text": " I want to use this model to estimate the depth within an image file, what should I do?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221121-113853\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.24.0\", \"torch==1.12.1\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3384, \"Mae\": 0.2739, \"Rmse\": 0.3959, \"Abs Rel\": 0.323, \"Log Mae\": 0.1148, \"Log Rmse\": 0.1651, \"Delta1\": 0.5576, \"Delta2\": 0.8345, \"Delta3\": 0.9398}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 165, "text": " I need an algorithm to implement depth estimation for given an image in Python.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221122-014502\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3476, \"Mae\": 0.2763, \"Rmse\": 0.4088, \"Abs Rel\": 0.3308, \"Log Mae\": 0.1161, \"Log Rmse\": 0.17, \"Delta1\": 0.5682, \"Delta2\": 0.8301, \"Delta3\": 0.9279}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics.\"}}", "category": "generic"}
{"question_id": 166, "text": " Let's find the depth estimation for an image captured in a warehouse.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221122-044810\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-044810')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, torch==1.12.1, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.369, \"Mae\": 0.2909, \"Rmse\": 0.4208, \"Abs Rel\": 0.3635, \"Log Mae\": 0.1224, \"Log Rmse\": 0.1793, \"Delta1\": 0.5323, \"Delta2\": 0.8179, \"Delta3\": 0.9258}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 167, "text": " We are creating a scene renderer and we'd like to estimate the depth of each object in the 3D reconstruction.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221122-082237\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')\", \"api_arguments\": \"pretrained_model_name\", \"python_environment_requirements\": \"transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3421, \"Mae\": 0.27, \"Rmse\": 0.4042, \"Abs Rel\": 0.3279, \"Log Mae\": 0.1132, \"Log Rmse\": 0.1688, \"Delta1\": 0.5839, \"Delta2\": 0.8408, \"Delta3\": 0.9309}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks.\"}}", "category": "generic"}
{"question_id": 168, "text": " Design a code that can detect the depth in images of cars driving on the road, without any sensor.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-kitti-finetuned-diode-221214-123047\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers==4.24.0\", \"torch==1.12.1+cu116\", \"tokenizers==0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.3497, \"Mae\": 0.2847, \"Rmse\": 0.3977, \"Abs Rel\": 0.3477, \"Log Mae\": 0.1203, \"Log Rmse\": 0.1726, \"Delta1\": 0.5217, \"Delta2\": 0.8246, \"Delta3\": 0.9436}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.\"}}", "category": "generic"}
{"question_id": 169, "text": " Develop a new robotic application to control a robot to avoid obstacle by applying computer vision depth estimation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221221-102136\", \"api_call\": \"pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136').model\", \"api_arguments\": [], \"python_environment_requirements\": [\"Transformers 4.24.0\", \"Pytorch 1.12.1+cu116\", \"Datasets 2.8.0\", \"Tokenizers 0.13.2\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4222, \"Mae\": 0.411, \"Rmse\": 0.6292, \"Abs Rel\": 0.3778, \"Log Mae\": 0.1636, \"Log Rmse\": 0.224, \"Delta1\": 0.432, \"Delta2\": 0.6806, \"Delta3\": 0.8068}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 170, "text": " A construction company works on a project where automating the process of obtaining depth maps from images is required. Help them by providing an initial solution.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"glpn-nyu-finetuned-diode-221228-072509\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.4012, \"Mae\": 0.403, \"Rmse\": 0.6173, \"Abs Rel\": 0.3487, \"Log Mae\": 0.1574, \"Log Rmse\": 0.211, \"Delta1\": 0.4308, \"Delta2\": 0.6997, \"Delta3\": 0.8249}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\"}}", "category": "generic"}
{"question_id": 171, "text": " Help me estimate the depth of various objects in an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Depth Estimation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Depth Estimation\", \"api_name\": \"glpn-nyu-finetuned-diode-230103-091356\", \"api_call\": \"AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"diode-subset\", \"accuracy\": {\"Loss\": 0.436, \"Mae\": 0.4251, \"Rmse\": 0.6169, \"Abs Rel\": 0.45, \"Log Mae\": 0.1721, \"Log Rmse\": 0.2269, \"Delta1\": 0.3828, \"Delta2\": 0.6326, \"Delta3\": 0.8051}}, \"description\": \"This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks.\"}}", "category": "generic"}
{"question_id": 172, "text": " We need to create a tool to classify images of either a cat or a dog.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/resnet-50\", \"api_call\": \"ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\", \"api_arguments\": {\"from_pretrained\": \"microsoft/resnet-50\"}, \"python_environment_requirements\": {\"transformers\": \"AutoImageProcessor, ResNetForImageClassification\", \"torch\": \"torch\", \"datasets\": \"load_dataset\"}, \"example_code\": \"from transformers import AutoImageProcessor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\\ninputs = processor(image, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"~0.5% top1\"}, \"description\": \"ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.\"}}", "category": "generic"}
{"question_id": 173, "text": " I am building an AI tool to support building analysis for a construction company. The tool needs to classify images of buildings as residential, commercial, or industrial.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/convnext-large-224\", \"api_call\": \"ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/convnext-large-224\"}, \"python_environment_requirements\": {\"transformers\": \"Hugging Face Transformers\", \"torch\": \"PyTorch\", \"datasets\": \"Hugging Face Datasets\"}, \"example_code\": {\"import\": [\"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\", \"import torch\", \"from datasets import load_dataset\"], \"load_dataset\": \"dataset = load_dataset('huggingface/cats-image')\", \"image\": \"image = dataset['test']['image'][0]\", \"feature_extractor\": \"feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\", \"model\": \"model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\", \"inputs\": \"inputs = feature_extractor(image, return_tensors='pt')\", \"logits\": \"with torch.no_grad():\\n  logits = model(**inputs).logits\", \"predicted_label\": \"predicted_label = logits.argmax(-1).item()\", \"print\": \"print(model.config.id2label[predicted_label])\"}, \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not specified\"}, \"description\": \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.\"}}", "category": "generic"}
{"question_id": 174, "text": " Create an AI app that recognizes and identifies images of cats using a pretrained model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/resnet-18\", \"api_call\": \"ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\", \"api_arguments\": [\"image\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"datasets\"], \"example_code\": \"from transformers import AutoFeatureExtractor, ResNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\"}, \"description\": \"ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.\"}}", "category": "generic"}
{"question_id": 175, "text": " I am the owner of an online pet store. I need a solution that categorizes the uploaded photos of pets into cat or dog.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/convnext-base-224\", \"api_call\": \"ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\", \"api_arguments\": [\"image\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"datasets\"], \"example_code\": \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": null}, \"description\": \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.\"}}", "category": "generic"}
{"question_id": 176, "text": " We are a web platform for sports programming. Images are received from users and thus it's crucial to recognize the sport in each image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/beit-base-patch16-224-pt22k-ft22k\", \"api_call\": \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"ImageNet-22k\", \"accuracy\": \"Not specified\"}, \"description\": \"BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.\"}}", "category": "generic"}
{"question_id": 177, "text": " I need to create a powerful detecting software to identify the stage of diabetic retinopathy from a patient's retina image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"martinezomg/vit-base-patch16-224-diabetic-retinopathy\", \"api_call\": \"pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy').model\", \"api_arguments\": {\"model_name\": \"martinezomg/vit-base-patch16-224-diabetic-retinopathy\"}, \"python_environment_requirements\": {\"transformers\": \"4.28.1\", \"pytorch\": \"2.0.0+cu118\", \"datasets\": \"2.11.0\", \"tokenizers\": \"0.13.3\"}, \"example_code\": \"from transformers import pipeline\\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\\nresult = image_classifier('path/to/image.jpg')\", \"performance\": {\"dataset\": \"None\", \"accuracy\": 0.7744}, \"description\": \"This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.\"}}", "category": "generic"}
{"question_id": 178, "text": " Detect the content of an image and classify it into one of the 1000 classes in the ImageNet dataset.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"google/vit-base-patch16-224\", \"api_call\": \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"google/vit-base-patch16-224\", \"from_tf\": \"False\", \"config\": \"None\", \"cache_dir\": \"None\", \"revision\": \"None\", \"use_auth_token\": \"False\"}, \"python_environment_requirements\": {\"transformers\": \"4.0.0\", \"torch\": \"1.9.0\", \"PIL\": \"8.3.2\", \"requests\": \"2.26.0\"}, \"example_code\": {\"1\": \"from transformers import ViTImageProcessor, ViTForImageClassification\", \"2\": \"from PIL import Image\", \"3\": \"import requests\", \"4\": \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", \"5\": \"image = Image.open(requests.get(url, stream=True).raw)\", \"6\": \"processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\", \"7\": \"model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\", \"8\": \"inputs = processor(images=image, return_tensors='pt')\", \"9\": \"outputs = model(**inputs)\", \"10\": \"logits = outputs.logits\", \"11\": \"predicted_class_idx = logits.argmax(-1).item()\", \"12\": \"print('Predicted class:', model.config.id2label[predicted_class_idx])\"}, \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.\"}}", "category": "generic"}
{"question_id": 179, "text": " The company wants a model to estimate the age of customers in their store from a picture so they can offer the best-customized service.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Age Classification\", \"api_name\": \"nateraw/vit-age-classifier\", \"api_call\": \"ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nateraw/vit-age-classifier\"}, \"python_environment_requirements\": [\"requests\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nfrom PIL import Image\\nfrom io import BytesIO\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\\nim = Image.open(BytesIO(r.content))\\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\\ninputs = transforms(im, return_tensors='pt')\\noutput = model(**inputs)\\nproba = output.logits.softmax(1)\\npreds = proba.argmax(1)\", \"performance\": {\"dataset\": \"fairface\", \"accuracy\": null}, \"description\": \"A vision transformer finetuned to classify the age of a given person's face.\"}}", "category": "generic"}
{"question_id": 180, "text": " We are creating an app that identifies the objects in photos. The app should classify the images correctly so that the user can better understand the content of their photos.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"google/vit-base-patch16-384\", \"api_call\": \"ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"google/vit-base-patch16-384\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import ViTFeatureExtractor, ViTForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"ImageNet\", \"accuracy\": \"Refer to tables 2 and 5 of the original paper\"}, \"description\": \"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\"}}", "category": "generic"}
{"question_id": 181, "text": " I am designing a website for a real estate firm, I need to classify images of houses, apartments, and land plots.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/beit-base-patch16-224\", \"api_call\": \"BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"microsoft/beit-base-patch16-224\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import BeitImageProcessor, BeitForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224')\\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"ImageNet\", \"accuracy\": \"Refer to tables 1 and 2 of the original paper\"}, \"description\": \"BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224.\"}}", "category": "generic"}
{"question_id": 182, "text": " I have an image gallery and want to make sure it contains only pictures of dogs and no food. Can you set up a classification system for me?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"abhishek/autotrain-dog-vs-food\", \"api_call\": \"pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')\", \"api_arguments\": \"image_path\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"sasha/dog-food\", \"accuracy\": 0.998}, \"description\": \"A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework.\"}}", "category": "generic"}
{"question_id": 183, "text": " Please map an image with 720*720 dimensions to one of these animal categories for my animal species recognition app: dog, cat, bird, fish, lizard.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lysandre/tiny-vit-random\", \"api_call\": \"ViTForImageClassification.from_pretrained('lysandre/tiny-vit-random')\", \"api_arguments\": \"image_path\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny-vit-random model for image classification using Hugging Face Transformers.\"}}", "category": "generic"}
{"question_id": 184, "text": " Classify the given beans image file for identifying different types of beans.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"fxmarty/resnet-tiny-beans\", \"api_call\": \"pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\", \"api_arguments\": {\"model\": \"fxmarty/resnet-tiny-beans\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": \"from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')\", \"performance\": {\"dataset\": \"beans\", \"accuracy\": \"Not provided\"}, \"description\": \"A model trained on the beans dataset, just for testing and having a really tiny model.\"}}", "category": "generic"}
{"question_id": 185, "text": " We are performing quality assessment on an industrial conveyor belt, can you please classify products in the factory images?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"nvidia/mit-b0\", \"api_call\": \"SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nvidia/mit-b0\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"PIL\": \"latest\", \"requests\": \"latest\"}, \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet_1k\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.\"}}", "category": "generic"}
{"question_id": 186, "text": " Our company is working on an advertising platform. We need to understand the type of content in an image so that we can show relevant ads based on user preferences.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"google/mobilenet_v1_0.75_192\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"google/mobilenet_v1_0.75_192\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.\"}}", "category": "generic"}
{"question_id": 187, "text": " There is a new order from an online shopping platform to classify an image of a fashion product that was just uploaded.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/convnext-tiny-224\", \"api_call\": \"ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/convnext-tiny-224\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"datasets\"], \"example_code\": \"from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset('huggingface/cats-image')\\nimage = dataset['test']['image'][0]\\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\\ninputs = feature_extractor(image, return_tensors='pt')\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not specified\"}, \"description\": \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.\"}}", "category": "generic"}
{"question_id": 188, "text": " Imagine you are an artist in a drawing studio, and need a classifier to verify whether the drawn image is an animal or not.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"vit_base_patch16_224.augreg2_in21k_ft_in1k\", \"api_call\": \"ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"timm/vit_base_patch16_224.augreg2_in21k_ft_in1k\"}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 189, "text": " Determine the objects in an image to categorize it.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k\", \"api_call\": \"pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')\", \"api_arguments\": {\"model\": \"timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k\", \"framework\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI.\"}}", "category": "generic"}
{"question_id": 190, "text": " To create a photo gallery app, we need a way to categorize images according to their content.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"google/mobilenet_v2_1.0_224\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v2_1.0_224)\\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v2_1.0_224)\\ninputs = preprocessor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not specified\"}, \"description\": \"MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.\"}}", "category": "generic"}
{"question_id": 191, "text": " Design an image classifier to recognize objects in images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"vit_tiny_patch16_224.augreg_in21k_ft_in1k\", \"api_call\": \"create_model('vit_tiny_patch16_224.augreg_in21k_ft_in1k', pretrained=True)\", \"api_arguments\": \"pretrained\", \"python_environment_requirements\": \"timm\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization.\"}}", "category": "generic"}
{"question_id": 192, "text": " I am building an app that detects flower species from uploaded images. What model can I use?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/swin-tiny-patch4-window7-224\", \"api_call\": \"SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"AutoFeatureExtractor\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": \"from transformers import AutoFeatureExtractor, SwinForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not specified\"}, \"description\": \"Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.\"}}", "category": "generic"}
{"question_id": 193, "text": " What's that: A hotdog or not a hotdog? I need a system to clarify.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"julien-c/hotdog-not-hotdog\", \"api_call\": \"pipeline('image-classification', model='julien-c/hotdog-not-hotdog')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": 0.825}, \"description\": \"A model that classifies images as hotdog or not hotdog.\"}}", "category": "generic"}
{"question_id": 194, "text": " While browsing an anime forum, find out if a certain picture was drawn by an AI or a human.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"saltacc/anime-ai-detect\", \"api_call\": \"pipeline('image-classification', model='saltacc/anime-ai-detect')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"aibooru and imageboard sites\", \"accuracy\": \"96%\"}, \"description\": \"A BEiT classifier to see if anime art was made by an AI or a human.\"}}", "category": "generic"}
{"question_id": 195, "text": " I provide drone inspection services. I would like to identify objects in the images captured by my drone.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"microsoft/swinv2-tiny-patch4-window8-256\", \"api_call\": \"AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')\", \"api_arguments\": {\"image\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import AutoImageProcessor, AutoModelForImageClassification\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.\"}}", "category": "generic"}
{"question_id": 196, "text": " Our marketing team wants to extract data from tables in various reports. We need to identify the structure (rows and columns) of the tables within the reports.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/table-transformer-structure-recognition\", \"api_call\": \"pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"PubTables1M\", \"accuracy\": \"\"}, \"description\": \"Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\"}}", "category": "generic"}
{"question_id": 197, "text": " We're working with a startup that's developing an app for detecting damaged products in its warehouse, based on images. We want a solution for classification.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"swin-tiny-patch4-window7-224-bottom_cleaned_data\", \"api_call\": \"Output: AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')\", \"api_arguments\": [\"learning_rate\", \"train_batch_size\", \"eval_batch_size\", \"seed\", \"gradient_accumulation_steps\", \"total_train_batch_size\", \"optimizer\", \"lr_scheduler_type\", \"lr_scheduler_warmup_ratio\", \"num_epochs\"], \"python_environment_requirements\": [\"Transformers 4.28.1\", \"Pytorch 2.0.0+cu118\", \"Datasets 2.11.0\", \"Tokenizers 0.13.3\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"imagefolder\", \"accuracy\": 0.9726}, \"description\": \"This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.\"}}", "category": "generic"}
{"question_id": 198, "text": " Create a program that uses image input and returns the most probable classification of the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"facebook/regnet-y-008\", \"api_call\": \"RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"zuppif/regnet-y-040\"}, \"python_environment_requirements\": {\"transformers\": \"AutoFeatureExtractor, RegNetForImageClassification\", \"torch\": \"torch\", \"datasets\": \"load_dataset\"}, \"example_code\": \"from transformers import AutoFeatureExtractor, RegNetForImageClassification\\nimport torch\\nfrom datasets import load_dataset\\ndataset = load_dataset(huggingface/cats-image)\\nimage = dataset[test][image][0]\\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\\ninputs = feature_extractor(image, return_tensors=pt)\\nwith torch.no_grad():\\n... logits = model(**inputs).logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.\"}}", "category": "generic"}
{"question_id": 199, "text": " Our organization is working on an image catalog. We need to classify the images to find the top 5 categories for each image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"convnextv2_huge.fcmae_ft_in1k\", \"api_call\": \"Output: timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\"}, \"python_environment_requirements\": [\"timm\"], \"example_code\": \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": 86.256}, \"description\": \"A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 200, "text": " Design an artificial intelligence engine able to identify what kind of food is displayed in a picture.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification, Feature Map Extraction, Image Embeddings\", \"api_name\": \"convnext_base.fb_in1k\", \"api_call\": \"timm.create_model('convnext_base.fb_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\", \"features_only\": \"True\", \"num_classes\": \"0\"}, \"python_environment_requirements\": [\"timm\"], \"example_code\": [\"from urllib.request import urlopen\", \"from PIL import Image\", \"import timm\", \"img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\", \"model = timm.create_model('convnext_base.fb_in1k', pretrained=True)\", \"model = model.eval()\", \"data_config = timm.data.resolve_model_data_config(model)\", \"transforms = timm.data.create_transform(**data_config, is_training=False)\", \"output = model(transforms(img).unsqueeze(0))\"], \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"83.82%\"}, \"description\": \"A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings.\"}}", "category": "generic"}
{"question_id": 201, "text": " We are trying to develop an app that recommends food by analyzing images. We need a model that can classify various types of food.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Classification\", \"api_name\": \"timm/mobilenetv3_large_100.ra_in1k\", \"api_call\": \"Output: timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\", \"api_arguments\": {\"pretrained\": \"True\"}, \"python_environment_requirements\": {\"timm\": \"latest\"}, \"example_code\": \"from urllib.request import urlopen\\nfrom PIL import Image\\nimport timm\\nimg = Image.open(urlopen(\\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\\n))\\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\\nmodel = model.eval()\\ndata_config = timm.data.resolve_model_data_config(model)\\ntransforms = timm.data.create_transform(**data_config, is_training=False)\\noutput = model(transforms(img).unsqueeze(0))\", \"performance\": {\"dataset\": \"imagenet-1k\", \"accuracy\": \"Not provided\"}, \"description\": \"A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup.\"}}", "category": "generic"}
{"question_id": 202, "text": " The company requires an application that can automatically identify tables in financial reports from images. \\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/table-transformer-detection\", \"api_call\": \"TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')\", \"api_arguments\": \"image\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)\", \"performance\": {\"dataset\": \"PubTables1M\", \"accuracy\": \"Not provided\"}, \"description\": \"Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\"}}", "category": "generic"}
{"question_id": 203, "text": " Locate different objects in a picture.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"facebook/detr-resnet-50\", \"api_call\": \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\", \"api_arguments\": {\"pretrained_model_name\": \"facebook/detr-resnet-50\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"42.0 AP\"}, \"description\": \"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 204, "text": " Analyze real-time traffic images with object detection by visualizing the objects detected in the image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"hustvl/yolos-tiny\", \"api_call\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"28.7 AP\"}, \"description\": \"YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\"}}", "category": "generic"}
{"question_id": 205, "text": " The company wants to automatically detect objects in images from their website to improve the user experience. Please provide a solution.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"facebook/detr-resnet-101\", \"api_call\": \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"COCO 2017\", \"accuracy\": \"43.5 AP\"}, \"description\": \"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 206, "text": " Our company has been working on a new robotic prototype. Test the OwlViT for object detection with input text queries.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"zero-shot-object-detection\", \"api_name\": \"google/owlvit-base-patch32\", \"api_call\": \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\", \"api_arguments\": {\"texts\": \"List of text queries\", \"images\": \"Image to be processed\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"import requests\\nfrom PIL import Image\\nimport torch\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\", \"performance\": {\"dataset\": \"COCO and OpenImages\", \"accuracy\": \"Not specified\"}, \"description\": \"OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\"}}", "category": "generic"}
{"question_id": 207, "text": " We have a new office document management software and want to extract tables from the scanned documents. \\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Extraction\", \"api_name\": \"keremberke/yolov8m-table-extraction\", \"api_call\": \"YOLO('keremberke/yolov8m-table-extraction')\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"table-extraction\", \"accuracy\": 0.952}, \"description\": \"A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\"}}", "category": "generic"}
{"question_id": 208, "text": " In a document management application, we need to find both bordered and borderless tables.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Detect Bordered and Borderless tables in documents\", \"api_name\": \"TahaDouaji/detr-doc-table-detection\", \"api_call\": \"DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\", \"api_arguments\": [\"images\", \"return_tensors\", \"threshold\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import DetrImageProcessor, DetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nimage = Image.open(IMAGE_PATH)\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\ninputs = processor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.tensor([image.size[::-1]])\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\n box = [round(i, 2) for i in box.tolist()]\\n print(\\n fDetected {model.config.id2label[label.item()]} with confidence \\n f{round(score.item(), 3)} at location {box}\\n )\", \"performance\": {\"dataset\": \"ICDAR2019 Table Dataset\", \"accuracy\": \"Not provided\"}, \"description\": \"detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\"}}", "category": "generic"}
{"question_id": 209, "text": " As a company specialized in security systems, we need to detect objects in CCTV footage.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"hustvl/yolos-small\", \"api_call\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", \"api_arguments\": {\"model_name\": \"hustvl/yolos-small\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": {\"import\": [\"from transformers import YolosFeatureExtractor, YolosForObjectDetection\", \"from PIL import Image\", \"import requests\"], \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\", \"image\": \"Image.open(requests.get(url, stream=True).raw)\", \"feature_extractor\": \"YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\", \"model\": \"YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\", \"inputs\": \"feature_extractor(images=image, return_tensors='pt')\", \"outputs\": \"model(**inputs)\", \"logits\": \"outputs.logits\", \"bboxes\": \"outputs.pred_boxes\"}, \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"36.1 AP\"}, \"description\": \"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\"}}", "category": "generic"}
{"question_id": 210, "text": " We need to build a security system with cameras. It is important to identify suspicious individuals entering restricted areas.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"facebook/detr-resnet-101-dc5\", \"api_call\": \"DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\", \"api_arguments\": {\"image\": \"Image.open(requests.get(url, stream=True).raw)\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import DetrFeatureExtractor, DetrForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\\nbboxes = outputs.pred_boxes\", \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": \"AP 44.9\"}, \"description\": \"DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\"}}", "category": "generic"}
{"question_id": 211, "text": " We need an object detection system for tracking and identifying vehicles in a smart city.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"deformable-detr\", \"api_call\": \"DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\\nimport torch\\nfrom PIL import Image\\nimport requests\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\\ninputs = processor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"COCO 2017\", \"accuracy\": \"Not provided\"}, \"description\": \"Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 212, "text": " Recommend me an object detection library that can detect hard hats in construction areas to ensure safety compliance.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-hard-hat-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-hard-hat-detection')\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.24\", \"ultralytics==8.0.23\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"hard-hat-detection\", \"accuracy\": 0.811}, \"description\": \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\"}}", "category": "generic"}
{"question_id": 213, "text": " The local police department is investigating a crime scene and they want a program to detect license plates in images to help them identify suspects.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"License Plate Detection\", \"api_name\": \"keremberke/yolov5m-license-plate\", \"api_call\": \"yolov5.load('keremberke/yolov5m-license-plate')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic\": false, \"multi_label\": false, \"max_det\": 1000, \"img\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\", \"size\": 640, \"augment\": true}, \"python_environment_requirements\": \"pip install -U yolov5\", \"example_code\": [\"import yolov5\", \"model = yolov5.load('keremberke/yolov5m-license-plate')\", \"model.conf = 0.25\", \"model.iou = 0.45\", \"model.agnostic = False\", \"model.multi_label = False\", \"model.max_det = 1000\", \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model(img, size=640)\", \"results = model(img, augment=True)\", \"predictions = results.pred[0]\", \"boxes = predictions[:, :4]\", \"scores = predictions[:, 4]\", \"categories = predictions[:, 5]\", \"results.show()\", \"results.save(save_dir='results/')\"], \"performance\": {\"dataset\": \"keremberke/license-plate-object-detection\", \"accuracy\": 0.988}, \"description\": \"A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\"}}", "category": "generic"}
{"question_id": 214, "text": " Our customer is creating an application to automatically analyze video game streams for real-time performance analysis. They need to detect objects in the game.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-valorant-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-valorant-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"valorant-object-detection\", \"accuracy\": 0.965}, \"description\": \"A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\"}}", "category": "generic"}
{"question_id": 215, "text": " We are creating an automated tool to analyze Counter-Strike: Global Offensive matches. Detect players in a provided image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-csgo-player-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-csgo-player-detection').predict(image)\", \"api_arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"python_environment_requirements\": \"ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"csgo-object-detection\", \"accuracy\": 0.892}, \"description\": \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\"}}", "category": "generic"}
{"question_id": 216, "text": " Can you help me extract tables from images?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Extraction\", \"api_name\": \"keremberke/yolov8s-table-extraction\", \"api_call\": \"YOLO('keremberke/yolov8s-table-extraction').predict(image)\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-table-extraction')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"table-extraction\", \"accuracy\": 0.984}, \"description\": \"A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\"}}", "category": "generic"}
{"question_id": 217, "text": " We need to implement a zero-shot object detection system to detect cats and dogs in images for a pet adoption application.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"zero-shot-object-detection\", \"api_name\": \"google/owlvit-large-patch14\", \"api_call\": \"OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')\", \"api_arguments\": {\"model_name\": \"google/owlvit-large-patch14\"}, \"python_environment_requirements\": [\"torch\", \"transformers\", \"PIL\", \"requests\"], \"example_code\": [\"import requests\", \"from PIL import Image\", \"import torch\", \"from transformers import OwlViTProcessor, OwlViTForObjectDetection\", \"processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)\", \"model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)\", \"url = http://images.cocodataset.org/val2017/000000039769.jpg\", \"image = Image.open(requests.get(url, stream=True).raw)\", \"texts = [[a photo of a cat, a photo of a dog]\", \"inputs = processor(text=texts, images=image, return_tensors=pt)\", \"outputs = model(**inputs)\", \"target_sizes = torch.Tensor([image.size[::-1]])\", \"results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\", \"i = 0\", \"text = texts[i]\", \"boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]\", \"score_threshold = 0.1\", \"for box, score, label in zip(boxes, scores, labels):\", \" box = [round(i, 2) for i in box.tolist()]\", \" if score >= score_threshold:\", \" print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})\"], \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not specified\"}, \"description\": \"OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\"}}", "category": "generic"}
{"question_id": 218, "text": " We are analyzing an American football game video. Our goal is to detect as many players as possible by recognizing their helmets.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-nlf-head-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-nlf-head-detection').predict(image)\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000, \"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.24 ultralytics==8.0.23\", \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8m-nlf-head-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"nfl-object-detection\", \"accuracy\": 0.287}, \"description\": \"A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\"}}", "category": "generic"}
{"question_id": 219, "text": " I am designing an app for warehouse safety. Implement a model that detects forklifts and persons in images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-forklift-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-forklift-detection').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8m-forklift-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"forklift-object-detection\", \"accuracy\": 0.846}, \"description\": \"A YOLOv8 model for detecting forklifts and persons in images.\"}}", "category": "generic"}
{"question_id": 220, "text": " Our client is a company that develops smart home technology. Design a system to detect specific objects in a live video feed.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"zero-shot-object-detection\", \"api_name\": \"google/owlvit-base-patch16\", \"api_call\": \"OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')\", \"api_arguments\": [\"texts\", \"images\"], \"python_environment_requirements\": [\"requests\", \"PIL\", \"torch\", \"transformers\"], \"example_code\": \"processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [[a photo of a cat, a photo of a dog]]\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\noutputs = model(**inputs)\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not provided\"}, \"description\": \"OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\"}}", "category": "generic"}
{"question_id": 221, "text": " We have an agency tracking aircraft in the skies to detect unauthorized planes. We need a reliable tool for aircraft detection in images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-plane-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-plane-detection').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8m-plane-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"plane-detection\", \"accuracy\": \"0.995\"}, \"description\": \"A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\"}}", "category": "generic"}
{"question_id": 222, "text": " We are designing a Counter-Strike: Global Offensive (CS:GO) AI assistant. We need the assistant to detect players through real-time images taken during the game.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8s-csgo-player-detection\", \"api_call\": \"YOLO('keremberke/yolov8s-csgo-player-detection').predict(image)\", \"api_arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"csgo-object-detection\", \"accuracy\": 0.886}, \"description\": \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\"}}", "category": "generic"}
{"question_id": 223, "text": " I want an app that identifies the types of blood cells present in a given blood sample image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8m-blood-cell-detection\", \"api_call\": \"YOLO('keremberke/yolov8m-blood-cell-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.24\", \"ultralytics==8.0.23\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8m-blood-cell-detection')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"blood-cell-object-detection\", \"accuracy\": 0.927}, \"description\": \"A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\"}}", "category": "generic"}
{"question_id": 224, "text": " We want to develop an AI that can monitor the construction workers to make sure they are wearing hard hats for their safety.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8s-hard-hat-detection\", \"api_call\": \"YOLO('keremberke/yolov8s-hard-hat-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"hard-hat-detection\", \"accuracy\": 0.834}, \"description\": \"An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\"}}", "category": "generic"}
{"question_id": 225, "text": " We need to monitor people who enter the security room. Detect the people in the given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"fcakyon/yolov5s-v7.0\", \"api_call\": \"yolov5.load('fcakyon/yolov5s-v7.0')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic\": false, \"multi_label\": false, \"max_det\": 1000, \"img\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\", \"size\": 640, \"augment\": true}, \"python_environment_requirements\": \"pip install -U yolov5\", \"example_code\": \"import yolov5\\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\\nmodel.conf = 0.25\\nmodel.iou = 0.45\\nmodel.agnostic = False\\nmodel.multi_label = False\\nmodel.max_det = 1000\\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model(img)\\nresults = model(img, size=640)\\nresults = model(img, augment=True)\\npredictions = results.pred[0]\\nboxes = predictions[:, :4]\\nscores = predictions[:, 4]\\ncategories = predictions[:, 5]\\nresults.show()\\nresults.save(save_dir='results/')\", \"performance\": {\"dataset\": \"detection-datasets/coco\", \"accuracy\": null}, \"description\": \"Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\"}}", "category": "generic"}
{"question_id": 226, "text": " Our organization wants to analyze documents and extract tables from them for further data processing. We need a solution to automatically detect tables in the images of the documents.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Extraction\", \"api_name\": \"keremberke/yolov8n-table-extraction\", \"api_call\": \"YOLO('keremberke/yolov8n-table-extraction')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8n-table-extraction')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"table-extraction\", \"accuracy\": 0.967}, \"description\": \"An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\"}}", "category": "generic"}
{"question_id": 227, "text": " We wish to develop an application that can use image and text prompts to perform image segmentation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"clipseg-rd64-refined\", \"api_call\": \"pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')\", \"api_arguments\": {\"model\": \"CIDAS/clipseg-rd64-refined\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L\\u00fcddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\"}}", "category": "generic"}
{"question_id": 228, "text": " Detect players in the image of the popular first-person shooter game Counter-Strike: Global Offensive.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Object Detection\", \"api_name\": \"keremberke/yolov8n-csgo-player-detection\", \"api_call\": \"YOLO('keremberke/yolov8n-csgo-player-detection').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to image\"}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"csgo-object-detection\", \"accuracy\": 0.844}, \"description\": \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\"}}", "category": "generic"}
{"question_id": 229, "text": " Our company needs to detect license plates of vehicles in our parking lot. Please provide object detection solution for this purpose.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"License Plate Detection\", \"api_name\": \"keremberke/yolov5s-license-plate\", \"api_call\": \"model(img, size=640)\", \"api_arguments\": {\"img\": \"image url or path\", \"size\": \"image resize dimensions\", \"augment\": \"optional, test time augmentation\"}, \"python_environment_requirements\": \"pip install -U yolov5\", \"example_code\": [\"import yolov5\", \"model = yolov5.load('keremberke/yolov5s-license-plate')\", \"model.conf = 0.25\", \"model.iou = 0.45\", \"model.agnostic = False\", \"model.multi_label = False\", \"model.max_det = 1000\", \"img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model(img, size=640)\", \"results = model(img, augment=True)\", \"predictions = results.pred[0]\", \"boxes = predictions[:, :4]\", \"scores = predictions[:, 4]\", \"categories = predictions[:, 5]\", \"results.show()\", \"results.save(save_dir='results/')\"], \"performance\": {\"dataset\": \"keremberke/license-plate-object-detection\", \"accuracy\": 0.985}, \"description\": \"A YOLOv5 based license plate detection model trained on a custom dataset.\"}}", "category": "generic"}
{"question_id": 230, "text": " We are working on a smart camera for recognizing people in crowded areas, please segment the given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"openmmlab/upernet-convnext-small\", \"api_call\": \"UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\"}}", "category": "generic"}
{"question_id": 231, "text": " Our team at a blood test center is analyzing blood cell images. Find a way to detect blood cells in the images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Object Detection\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Blood Cell Detection\", \"api_name\": \"keremberke/yolov8n-blood-cell-detection\", \"api_call\": \"YOLO('keremberke/yolov8n-blood-cell-detection')\", \"api_arguments\": {\"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": \"ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"blood-cell-object-detection\", \"accuracy\": 0.893}, \"description\": \"This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\"}}", "category": "generic"}
{"question_id": 232, "text": " We are an interior design company. We need a tool that identifies different classes of objects in various interior photos.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Semantic Segmentation\", \"api_name\": \"nvidia/segformer-b0-finetuned-ade-512-512\", \"api_call\": \"'SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)'\", \"api_arguments\": {\"images\": \"Image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"SegformerImageProcessor, SegformerForSemanticSegmentation\", \"PIL\": \"Image\", \"requests\": \"requests\"}, \"example_code\": \"from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nprocessor = SegformerImageProcessor.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 233, "text": " Our company is developing an indoor navigation system for visually impaired people. We want to integrate image segmentation into our system.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"nvidia/segformer-b5-finetuned-ade-640-640\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-ade-512-512)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"ADE20K\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 234, "text": " We are building an autonomous driving system and need to segment images of urban areas.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Semantic Segmentation\", \"api_name\": \"nvidia/segformer-b2-finetuned-cityscapes-1024-1024\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"PIL\": \"latest\", \"requests\": \"latest\"}, \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"Cityscapes\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 235, "text": " We are building a traffic monitoring system that requires image segmentation of street scenes.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-cityscapes-1024-1024')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"CityScapes\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 236, "text": " We are in a robotics project, and we want our robot to understand the environment through the images provided, using the facebook/detr-resnet-50-panoptic pre-trained model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"facebook/detr-resnet-50-panoptic\", \"api_call\": \"DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"torch\", \"numpy\", \"transformers\", \"PIL\", \"requests\", \"io\"], \"example_code\": [\"import io\", \"import requests\", \"from PIL import Image\", \"import torch\", \"import numpy\", \"from transformers import DetrFeatureExtractor, DetrForSegmentation\", \"from transformers.models.detr.feature_extraction_detr import rgb_to_id\", \"url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\", \"image = Image.open(requests.get(url, stream=True).raw)\", \"feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"model = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\", \"inputs = feature_extractor(images=image, return_tensors='pt')\", \"outputs = model(**inputs)\", \"processed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\", \"result = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]\", \"panoptic_seg = Image.open(io.BytesIO(result['png_string']))\", \"panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8)\", \"panoptic_seg_id = rgb_to_id(panoptic_seg)\"], \"performance\": {\"dataset\": \"COCO 2017 validation\", \"accuracy\": {\"box_AP\": 38.8, \"segmentation_AP\": 31.1, \"PQ\": 43.4}}, \"description\": \"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 237, "text": " As a factory owner, we want to segment objects in images to improve our production process.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-base-coco\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\"}}", "category": "generic"}
{"question_id": 238, "text": " We need a system to analyze pictures and recognize where clothes are on a person.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"mattmdjaga/segformer_b2_clothes\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\", \"matplotlib\", \"torch\"], \"example_code\": \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nimport matplotlib.pyplot as plt\\nimport torch.nn as nn\\nextractor = AutoFeatureExtractor.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nmodel = SegformerForSemanticSegmentation.from_pretrained('mattmdjaga/segformer_b2_clothes')\\nurl = 'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nlogits = outputs.logits.cpu()\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode='bilinear', align_corners=False)\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\nplt.imshow(pred_seg)\", \"performance\": {\"dataset\": \"mattmdjaga/human_parsing_dataset\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on ATR dataset for clothes segmentation.\"}}", "category": "generic"}
{"question_id": 239, "text": " We are an agricultural company aiming to detect and classify segments of crops in aerial images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-base-coco-panoptic\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/mask2former-swin-base-coco-panoptic\"}, \"python_environment_requirements\": {\"packages\": [\"requests\", \"torch\", \"PIL\", \"transformers\"]}, \"example_code\": \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\\nprocessor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO panoptic segmentation\", \"accuracy\": null}, \"description\": \"Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\"}}", "category": "generic"}
{"question_id": 240, "text": " We need a computer vision model capable of segmenting images for urban planning purposes.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-large-cityscapes-semantic\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/mask2former-swin-large-cityscapes-semantic\"}, \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"Cityscapes\", \"accuracy\": \"Not specified\"}, \"description\": \"Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\"}}", "category": "generic"}
{"question_id": 241, "text": " We require an AI model to identify and segment different objects in different contexts.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"shi-labs/oneformer_coco_swin_large\", \"api_call\": \"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)'\", \"api_arguments\": {\"images\": \"image\", \"task_inputs\": [\"semantic\", \"instance\", \"panoptic\"], \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/coco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_coco_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"ydshieh/coco_dataset_script\", \"accuracy\": \"Not provided\"}, \"description\": \"OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\"}}", "category": "generic"}
{"question_id": 242, "text": " I need to detect and segment objects in images taken by cameras set up around the building.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-large-ade\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')\", \"api_arguments\": {\"from_pretrained\": \"facebook/maskformer-swin-large-ade\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": \"from transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = MaskFormerImageProcessor.from_pretrained('facebook/maskformer-swin-large-ade')\\ninputs = processor(images=image, return_tensors='pt')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\"}}", "category": "generic"}
{"question_id": 243, "text": " I have an image that I want to segment into semantic, instance, and panoptic categories.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"shi-labs/oneformer_ade20k_swin_large\", \"api_call\": \"OneFormerForUniversalSegmentation.from_pretrained(\\\\shi-labs/oneformer_ade20k_swin_large\\\\)\", \"api_arguments\": [\"images\", \"task_inputs\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"scene_parse_150\", \"accuracy\": null}, \"description\": \"OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\"}}", "category": "generic"}
{"question_id": 244, "text": " We want to create an application to help enhance the security of buildings. It needs to detect and segment people who enter the building.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-large-coco-panoptic\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-coco-panoptic')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"requests\", \"torch\", \"PIL\", \"transformers\"], \"example_code\": \"processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result[segmentation]\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not provided\"}, \"description\": \"Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\"}}", "category": "generic"}
{"question_id": 245, "text": " The app being developed is meant to recognize the different objects within an image boundary. So far it includes cars, bikes, and bicycles. Help us by providing a proper implementation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-small-coco-instance\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/mask2former-swin-small-coco-instance\"}, \"python_environment_requirements\": [\"requests\", \"torch\", \"PIL\", \"transformers\"], \"example_code\": \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not provided\"}, \"description\": \"Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\"}}", "category": "generic"}
{"question_id": 246, "text": " We have to create a system that detects and segments buildings in an aerial image of a city.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8m-building-segmentation\", \"api_call\": \"Output: YOLO('keremberke/yolov8m-building-segmentation').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": \"pip install ultralyticsplus==0.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-building-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"satellite-building-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.623, \"mAP@0.5(mask)\": 0.613}}, \"description\": \"A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\"}}", "category": "generic"}
{"question_id": 247, "text": " Develop a solution to segment a given image into semantic, instance, and panoptic segmentation.\\n###Input: {\\\"image_url\\\": \\\"https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"shi-labs/oneformer_ade20k_swin_tiny\", \"api_call\": \"'OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)'\", \"api_arguments\": {\"images\": \"image\", \"task_inputs\": [\"semantic\", \"instance\", \"panoptic\"], \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\nsemantic_outputs = model(**semantic_inputs)\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\ninstance_outputs = model(**instance_inputs)\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\npanoptic_outputs = model(**panoptic_inputs)\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\"}}", "category": "generic"}
{"question_id": 248, "text": " An automation company needs to develop a system for detecting cityscape semantic segmentation. They need help suggesting an API.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Semantic Segmentation\", \"api_name\": \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\", \"api_call\": \"SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b5-finetuned-cityscapes-1024-1024')\", \"api_arguments\": {\"images\": \"image\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": \"from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b5-finetuned-cityscapes-1024-1024)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\noutputs = model(**inputs)\\nlogits = outputs.logits\", \"performance\": {\"dataset\": \"CityScapes\", \"accuracy\": \"Not provided\"}, \"description\": \"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 249, "text": " I am an wildlife conservation team member, and I want to identify animals and their boundaries in images. I am looking for a suitable model for it.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/mask2former-swin-tiny-coco-instance\", \"api_call\": \"Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"facebook/mask2former-swin-tiny-coco-instance\"}, \"python_environment_requirements\": [\"torch\", \"transformers\", \"PIL\", \"requests\"], \"example_code\": \"processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(images=image, return_tensors='pt')\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_instance_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not specified\"}, \"description\": \"Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\"}}", "category": "generic"}
{"question_id": 250, "text": " Help us design a solution to segment objects inside images and classify them.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-base-ade\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade')\", \"api_arguments\": {\"from_pretrained\": \"facebook/maskformer-swin-base-ade\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"PIL\": \"latest\", \"requests\": \"latest\"}, \"example_code\": \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nurl = https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(facebook/maskformer-swin-base-ade)\\ninputs = feature_extractor(images=image, return_tensors=pt)\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(facebook/maskformer-swin-base-ade)\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\npredicted_semantic_map = feature_extractor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\", \"performance\": {\"dataset\": \"ADE20k\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\"}}", "category": "generic"}
{"question_id": 251, "text": " Our company provides quality control services for electronic manufacturers. Develop an AI solution to identify defects in printed circuit boards.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8m-pcb-defect-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8m-pcb-defect-segmentation').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.24\", \"ultralytics==8.0.23\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"print(results[0].masks)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"pcb-defect-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.568, \"mAP@0.5(mask)\": 0.557}}, \"description\": \"A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\"}}", "category": "generic"}
{"question_id": 252, "text": " The company has recently launched an image classification software which recognizes all the objects in an image. How the software can be improved by a more sophisticated instance segmentation model to recognize individual objects and their boundaries within the image?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"facebook/maskformer-swin-tiny-coco\", \"api_call\": \"MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\", \"api_arguments\": [\"image\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\\nfrom PIL import Image\\nimport requests\\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nmodel = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco')\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = feature_extractor(images=image, return_tensors='pt')\\noutputs = model(**inputs)\\nclass_queries_logits = outputs.class_queries_logits\\nmasks_queries_logits = outputs.masks_queries_logits\\nresult = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\npredicted_panoptic_map = result['segmentation']\", \"performance\": {\"dataset\": \"COCO panoptic segmentation\", \"accuracy\": \"Not provided\"}, \"description\": \"MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\"}}", "category": "generic"}
{"question_id": 253, "text": " We have installed a city surveillance camera in every single main street junction, we want to detect potholes and automatically report them to the municipality.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8m-pothole-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8m-pothole-segmentation')\", \"api_arguments\": {\"image\": \"URL or local image path\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8m-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pothole-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.858, \"mAP@0.5(mask)\": 0.895}}, \"description\": \"A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\"}}", "category": "generic"}
{"question_id": 254, "text": " Our city planning department needs to analyze satellite imagery for building segmentation. We need to use an appropriate model to complete the task.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8s-building-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8s-building-segmentation')\", \"api_arguments\": [\"conf\", \"iou\", \"agnostic_nms\", \"max_det\", \"image\"], \"python_environment_requirements\": [\"ultralyticsplus==0.0.21\"], \"example_code\": [\"from ultralyticsplus import YOLO, render_result\", \"model = YOLO('keremberke/yolov8s-building-segmentation')\", \"model.overrides['conf'] = 0.25\", \"model.overrides['iou'] = 0.45\", \"model.overrides['agnostic_nms'] = False\", \"model.overrides['max_det'] = 1000\", \"image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\", \"results = model.predict(image)\", \"print(results[0].boxes)\", \"print(results[0].masks)\", \"render = render_result(model=model, image=image, result=results[0])\", \"render.show()\"], \"performance\": {\"dataset\": \"satellite-building-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.661, \"mAP@0.5(mask)\": 0.651}}, \"description\": \"A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\"}}", "category": "generic"}
{"question_id": 255, "text": " Our company is responsible for repairing the road surface. Please help us detect the areas damaged by potholes.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8s-pothole-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8s-pothole-segmentation').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to the image\"}, \"python_environment_requirements\": {\"ultralyticsplus\": \"0.0.23\", \"ultralytics\": \"8.0.21\"}, \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pothole-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.928, \"mAP@0.5(mask)\": 0.928}}, \"description\": \"A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\"}}", "category": "generic"}
{"question_id": 256, "text": " We are trying to create a safe navigation app that warns drivers about pot holes on the road ahead.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8n-pothole-segmentation\", \"api_call\": \"'model.predict(image)'\", \"api_arguments\": {\"image\": \"https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\", \"conf\": 0.25, \"iou\": 0.45, \"agnostic_nms\": false, \"max_det\": 1000}, \"python_environment_requirements\": {\"ultralyticsplus\": \"0.0.23\", \"ultralytics\": \"8.0.21\"}, \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pothole-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pothole-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.995, \"mAP@0.5(mask)\": 0.995}}, \"description\": \"A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\"}}", "category": "generic"}
{"question_id": 257, "text": " Our factory needs to identify defects in printed circuit boards (PCBs), including segmentation of the defects for further analysis.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8n-pcb-defect-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8n-pcb-defect-segmentation').predict(image)\", \"api_arguments\": {\"image\": \"URL or local path to image\"}, \"python_environment_requirements\": \"ultralyticsplus==0.0.23 ultralytics==8.0.21\", \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pcb-defect-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.512, \"mAP@0.5(mask)\": 0.517}}, \"description\": \"A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\"}}", "category": "generic"}
{"question_id": 258, "text": " Create a program that generates various artistic image variations for an online art gallery.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Variations\", \"api_name\": \"lambdalabs/sd-image-variations-diffusers\", \"api_call\": \"StableDiffusionImageVariationPipeline.from_pretrained('lambdalabs/sd-image-variations-diffusers', revision='v2.0')\", \"api_arguments\": {\"revision\": \"v2.0\"}, \"python_environment_requirements\": \"Diffusers >=0.8.0\", \"example_code\": \"from diffusers import StableDiffusionImageVariationPipeline\\nfrom PIL import Image\\ndevice = cuda:0\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\n lambdalabs/sd-image-variations-diffusers,\\n revision=v2.0,\\n)\\nsd_pipe = sd_pipe.to(device)\\nim = Image.open(path/to/image.jpg)\\ntform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Resize(\\n  (224, 224),\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\n  antialias=False,\\n ),\\n transforms.Normalize(\\n  [0.48145466, 0.4578275, 0.40821073],\\n  [0.26862954, 0.26130258, 0.27577711]),\\n])\\ninp = tform(im).to(device).unsqueeze(0)\\nout = sd_pipe(inp, guidance_scale=3)\\nout[images][0].save(result.jpg)\", \"performance\": {\"dataset\": \"ChristophSchuhmann/improved_aesthetics_6plus\", \"accuracy\": \"N/A\"}, \"description\": \"This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 259, "text": " We have a production line creating computer chips. We need to identify and segment any defects in the printed circuit boards(PCB) of these products.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image Segmentation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Segmentation\", \"api_name\": \"keremberke/yolov8s-pcb-defect-segmentation\", \"api_call\": \"YOLO('keremberke/yolov8s-pcb-defect-segmentation')\", \"api_arguments\": {\"image\": \"URL or local path to image\"}, \"python_environment_requirements\": [\"ultralyticsplus==0.0.23\", \"ultralytics==8.0.21\"], \"example_code\": \"from ultralyticsplus import YOLO, render_result\\nmodel = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\\nmodel.overrides['conf'] = 0.25\\nmodel.overrides['iou'] = 0.45\\nmodel.overrides['agnostic_nms'] = False\\nmodel.overrides['max_det'] = 1000\\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\\nresults = model.predict(image)\\nprint(results[0].boxes)\\nprint(results[0].masks)\\nrender = render_result(model=model, image=image, result=results[0])\\nrender.show()\", \"performance\": {\"dataset\": \"pcb-defect-segmentation\", \"accuracy\": {\"mAP@0.5(box)\": 0.515, \"mAP@0.5(mask)\": 0.491}}, \"description\": \"YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\"}}", "category": "generic"}
{"question_id": 260, "text": " Our customer company deals with travelling business, they are looking to improve the quality of images of locations using edge image generation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"lllyasviel/sd-controlnet-canny\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny', torch_dtype=torch.float16)\", \"api_arguments\": {\"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": {\"opencv\": \"pip install opencv-contrib-python\", \"diffusers\": \"pip install diffusers transformers accelerate\"}, \"example_code\": \"import cv2\\nfrom PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nimport numpy as np\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\nimage = np.array(image)\\nlow_threshold = 100\\nhigh_threshold = 200\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\nimage.save('images/bird_canny_out.png')\", \"performance\": {\"dataset\": \"3M edge-image, caption pairs\", \"accuracy\": \"600 GPU-hours with Nvidia A100 80G\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 261, "text": " Implement a solution to visualize human pose estimation in an image of a chef in the kitchen.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Human Pose Estimation\", \"api_name\": \"lllyasviel/sd-controlnet-openpose\", \"api_call\": \"pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\", \"api_arguments\": {\"text\": \"chef in the kitchen\", \"image\": \"image\", \"num_inference_steps\": 20}, \"python_environment_requirements\": {\"diffusers\": \"pip install diffusers\", \"transformers\": \"pip install transformers\", \"accelerate\": \"pip install accelerate\", \"controlnet_aux\": \"pip install controlnet_aux\"}, \"example_code\": \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import OpenposeDetector\\nfrom diffusers.utils import load_image\\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\nimage = openpose(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\nimage.save('images/chef_pose_out.png')\", \"performance\": {\"dataset\": \"200k pose-image, caption pairs\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 262, "text": " We need to segment images of houses for a real estate platform. Help us extract detailed information from the images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Segmentation\", \"api_name\": \"lllyasviel/sd-controlnet-seg\", \"api_call\": \"Output: ControlNetModel.from_pretrained(\\\\lllyasviel/sd-controlnet-seg\\\\, torch_dtype=torch.float16)\", \"api_arguments\": [\"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"image = pipe(house, image, num_inference_steps=20).images[0]\\nimage.save('./images/house_seg_out.png')\", \"performance\": {\"dataset\": \"ADE20K\", \"accuracy\": \"Trained on 164K segmentation-image, caption pairs\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 263, "text": " Create a masterpiece painting of a handsome old man using the provided image file.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"lllyasviel/sd-controlnet-hed\", \"api_call\": \"ControlNetModel.from_pretrained(\\\\lllyasviel/sd-controlnet-hed\\\\, torch_dtype=torch.float16)\", \"api_arguments\": [\"image\", \"text\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux\"], \"example_code\": \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/man.png)\\nimage = hed(image)\\ncontrolnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-hed, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(oil painting of handsome old man, masterpiece, image, num_inference_steps=20).images[0]\\nimage.save('images/man_hed_out.png')\", \"performance\": {\"dataset\": \"3M edge-image, caption pairs\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 264, "text": " I want to create a depth estimation model of images using the ControlNet model 'lllyasviel/sd-controlnet-depth'.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Depth Estimation\", \"api_name\": \"lllyasviel/sd-controlnet-depth\", \"api_call\": \"ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)\", \"api_arguments\": {\"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"PIL\", \"numpy\", \"torch\"], \"example_code\": {\"install_packages\": \"pip install diffusers transformers accelerate\", \"code\": [\"from transformers import pipeline\", \"from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\", \"from PIL import Image\", \"import numpy as np\", \"import torch\", \"from diffusers.utils import load_image\", \"depth_estimator = pipeline('depth-estimation')\", \"image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png)\", \"image = depth_estimator(image)['depth']\", \"image = np.array(image)\", \"image = image[:, :, None]\", \"image = np.concatenate([image, image, image], axis=2)\", \"image = Image.fromarray(image)\", \"controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-depth, torch_dtype=torch.float16)\", \"pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\", \"pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\", \"pipe.enable_xformers_memory_efficient_attention()\", \"pipe.enable_model_cpu_offload()\", \"image = pipe(Stormtrooper's lecture, image, num_inference_steps=20).images[0]\", \"image.save('./images/stormtrooper_depth_out.png')\"]}, \"performance\": {\"dataset\": \"3M depth-image, caption pairs\", \"accuracy\": \"500 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 265, "text": " Draw/create a picture of a cat playing with a toy on a blue background, based on the provided text.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Diffusion Models\", \"api_name\": \"lllyasviel/sd-controlnet-scribble\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\", \"api_arguments\": [\"image\", \"text\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux\"], \"example_code\": \"from PIL import Image\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom controlnet_aux import HEDdetector\\nfrom diffusers.utils import load_image\\nhed = HEDdetector.from_pretrained('lllyasviel/ControlNet')\\nimage = load_image('https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png')\\nimage = hed(image, scribble=True)\\ncontrolnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-scribble', torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe('bag', image, num_inference_steps=20).images[0]\\nimage.save('images/bag_scribble_out.png')\", \"performance\": {\"dataset\": \"500k scribble-image, caption pairs\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 266, "text": " Design an AI based tool to transform the photo into a painting of a blue paradise bird.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"lllyasviel/control_v11p_sd15_canny\", \"api_call\": \"pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]\", \"api_arguments\": {\"text\": \"a blue paradise bird in the jungle\", \"num_inference_steps\": 20, \"generator\": \"torch.manual_seed(33)\", \"image\": \"control_image\"}, \"python_environment_requirements\": [\"pip install opencv-contrib-python\", \"pip install diffusers transformers accelerate\"], \"example_code\": [\"import torch\", \"import os\", \"from huggingface_hub import HfApi\", \"from pathlib import Path\", \"from diffusers.utils import load_image\", \"import numpy as np\", \"import cv2\", \"from PIL import Image\", \"from diffusers import (\", \" ControlNetModel,\", \" StableDiffusionControlNetPipeline,\", \" UniPCMultistepScheduler,\", \")\", \"checkpoint = lllyasviel/control_v11p_sd15_canny\", \"image = load_image(\", \" https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png\", \")\", \"image = np.array(image)\", \"low_threshold = 100\", \"high_threshold = 200\", \"image = cv2.Canny(image, low_threshold, high_threshold)\", \"image = image[:, :, None]\", \"image = np.concatenate([image, image, image], axis=2)\", \"control_image = Image.fromarray(image)\", \"control_image.save(./images/control.png)\", \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"pipe = StableDiffusionControlNetPipeline.from_pretrained(\", \" runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\", \")\", \"pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\", \"pipe.enable_model_cpu_offload()\", \"generator = torch.manual_seed(33)\", \"image = pipe(a blue paradise bird in the jungle, num_inference_steps=20, generator=generator, image=control_image).images[0]\", \"image.save('images/image_out.png')\"], \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}", "category": "generic"}
{"question_id": 267, "text": " Our company wants to create an art generator utilizing a ControlNet trained model that uses lineart images as input.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"ControlNet\", \"api_name\": \"lllyasviel/control_v11p_sd15_lineart\", \"api_call\": \"Output: ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"ControlNet-1-1-preview/control_v11p_sd15_lineart\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": \"pip install diffusers transformers accelerate controlnet_aux==0.3.0\", \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import LineartDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = ControlNet-1-1-preview/control_v11p_sd15_lineart\\nimage = load_image(\\n https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart/resolve/main/images/input.png\\n)\\nimage = image.resize((512, 512))\\nprompt = michael jackson concert\\nprocessor = LineartDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"ControlNet-1-1-preview\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\"}}", "category": "generic"}
{"question_id": 268, "text": " We need to process the walls of the room given an input image, such as adjusting their color or tone.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"ControlNet - M-LSD Straight Line Version\", \"api_name\": \"lllyasviel/sd-controlnet-mlsd\", \"api_call\": \"ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)\", \"api_arguments\": {\"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": {\"diffusers\": \"pip install diffusers\", \"transformers\": \"pip install transformers\", \"accelerate\": \"pip install accelerate\", \"controlnet_aux\": \"pip install controlnet_aux\"}, \"example_code\": {\"import\": [\"from PIL import Image\", \"from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\", \"import torch\", \"from controlnet_aux import MLSDdetector\", \"from diffusers.utils import load_image\"], \"setup\": [\"mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\", \"image = load_image(https://huggingface.co/lllyasviel/sd-controlnet-mlsd/resolve/main/images/room.png)\", \"image = mlsd(image)\", \"controlnet = ControlNetModel.from_pretrained(lllyasviel/sd-controlnet-mlsd, torch_dtype=torch.float16)\", \"pipe = StableDiffusionControlNetPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)\", \"pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\"], \"execution\": [\"pipe.enable_xformers_memory_efficient_attention()\", \"pipe.enable_model_cpu_offload()\", \"image = pipe(room, image, num_inference_steps=20).images[0]\", \"image.save('images/room_mlsd_out.png')\"]}, \"performance\": {\"dataset\": \"600k edge-image, caption pairs generated from Places2\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 269, "text": " In order to provide better visual content for our clients, we need to generate images using their textual descriptions.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Diffusers\", \"functionality\": \"Text-to-Image\", \"api_name\": \"lllyasviel/control_v11p_sd15_scribble\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_scribble\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux==0.3.0\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_scribble\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, scribble=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"Stable Diffusion v1-5\", \"accuracy\": \"Not specified\"}, \"description\": \"Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}", "category": "generic"}
{"question_id": 270, "text": " I want a tool to estimate normal maps from images, which will be applied to some 3D models.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Normal Map Estimation\", \"api_name\": \"lllyasviel/sd-controlnet-normal\", \"api_call\": \"ControlNetModel.from_pretrained(fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16)\", \"api_arguments\": [\"image\", \"num_inference_steps\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"from PIL import Image\\nfrom transformers import pipeline\\nimport numpy as np\\nimport cv2\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\nimport torch\\nfrom diffusers.utils import load_image\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-normal/resolve/main/images/toy.png).convert(RGB)\\ndepth_estimator = pipeline(depth-estimation, model =Intel/dpt-hybrid-midas )\\nimage = depth_estimator(image)['predicted_depth'][0]\\nimage = image.numpy()\\nimage_depth = image.copy()\\nimage_depth -= np.min(image_depth)\\nimage_depth /= np.max(image_depth)\\nbg_threhold = 0.4\\nx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\\nx[image_depth &lt; bg_threhold] = 0\\ny = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\\ny[image_depth &lt; bg_threhold] = 0\\nz = np.ones_like(x) * np.pi * 2.0\\nimage = np.stack([x, y, z], axis=2)\\nimage /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\\nimage = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\\nimage = Image.fromarray(image)\\ncontrolnet = ControlNetModel.from_pretrained(\\n fusing/stable-diffusion-v1-5-controlnet-normal, torch_dtype=torch.float16\\n)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_xformers_memory_efficient_attention()\\npipe.enable_model_cpu_offload()\\nimage = pipe(cute toy, image, num_inference_steps=20).images[0]\\nimage.save('images/toy_normal_out.png')\", \"performance\": {\"dataset\": \"DIODE\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\"}}", "category": "generic"}
{"question_id": 271, "text": " Find a solution to create coherent animations from two different photos by controlling diffusion models using openpose images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Diffusers\", \"functionality\": \"Text-to-Image Diffusion Models\", \"api_name\": \"lllyasviel/control_v11p_sd15_openpose\", \"api_call\": \"Output: ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_openpose\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux==0.3.0\"], \"example_code\": {\"import_libraries\": [\"import torch\", \"import os\", \"from huggingface_hub import HfApi\", \"from pathlib import Path\", \"from diffusers.utils import load_image\", \"from PIL import Image\", \"import numpy as np\", \"from controlnet_aux import OpenposeDetector\", \"from diffusers import (\", \" ControlNetModel,\", \" StableDiffusionControlNetPipeline,\", \" UniPCMultistepScheduler,\", \")\"], \"load_model\": [\"checkpoint = lllyasviel/control_v11p_sd15_openpose\", \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\"], \"example_usage\": [\"image = load_image(https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/input.png)\", \"prompt = chef in the kitchen\", \"processor = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\", \"control_image = processor(image, hand_and_face=True)\", \"control_image.save(./images/control.png)\", \"pipe = StableDiffusionControlNetPipeline.from_pretrained(\", \" runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\", \")\", \"pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\", \"pipe.enable_model_cpu_offload()\", \"generator = torch.manual_seed(0)\", \"image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\", \"image.save('images/image_out.png')\"]}, \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\"}}", "category": "generic"}
{"question_id": 272, "text": " Develop a photo album application that will provide an option for enhancing image resolution.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Super-Resolution\", \"api_name\": \"caidas/swin2SR-classical-sr-x2-64\", \"api_call\": \"Swin2SRForImageSuperResolution.from_pretrained('caidas/swin2sr-classical-sr-x2-64')\", \"api_arguments\": \"image, model, feature_extractor\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"Refer to the documentation.\", \"performance\": {\"dataset\": \"arxiv: 2209.11345\", \"accuracy\": \"Not provided\"}, \"description\": \"Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 273, "text": " Create a new custom logo from an existing logo with a specific theme.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation model\", \"api_name\": \"lllyasviel/control_v11e_sd15_ip2p\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": [\"checkpoint\", \"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11e_sd15_ip2p\\ncontrol_image = load_image(https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p/resolve/main/images/input.png).convert('RGB')\\nprompt = make it on fire\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"Stable Diffusion v1-5\", \"accuracy\": \"Not provided\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\"}}", "category": "generic"}
{"question_id": 274, "text": " As an advertising agency, we need to create graphics based on text ideas provided by our clients. The text will be about their brands and products.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation\", \"api_name\": \"lllyasviel/control_v11p_sd15_seg\", \"api_call\": \"ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_seg', torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_seg\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\"], \"example_code\": \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"COCO\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}", "category": "generic"}
{"question_id": 275, "text": " Help me upscale an image of my nanotechnology prototype so I can share it with my colleagues.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"swin2SR-lightweight-x2-64\", \"api_call\": \"Swin2SRForConditionalGeneration.from_pretrained('condef/Swin2SR-lightweight-x2-64').\", \"api_arguments\": \"feature_extractor, model\", \"python_environment_requirements\": \"transformers, torch\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.\"}}", "category": "generic"}
{"question_id": 276, "text": " I want to generate an image of a \\\"sunset over the ocean\\\" with detailed shapes and distinct colors.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation\", \"api_name\": \"lllyasviel/control_v11p_sd15_softedge\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_softedge\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux==0.3.0\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import PidiNetDetector, HEDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_softedge\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = HEDdetector.from_pretrained('lllyasviel/Annotators')\\nprocessor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\\ncontrol_image = processor(image, safe=True)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"ControlNet\", \"accuracy\": \"Not provided\"}, \"description\": \"Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}", "category": "generic"}
{"question_id": 277, "text": " Design a large banner for our online store by synthesizing an image of a control room decorated with balloons and colorful lights.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Diffusion Models\", \"api_name\": \"lllyasviel/control_v11p_sd15_mlsd\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": [\"checkpoint\", \"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import MLSDdetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_mlsd\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_mlsd/resolve/main/images/input.png\\n)\\nprompt = royal chamber with fancy bed\\nprocessor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"MLSD\", \"accuracy\": \"Not provided\"}, \"description\": \"Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\"}}", "category": "generic"}
{"question_id": 278, "text": " I want to automatically generate art for my blog based on text descriptions. Find a way to generate an image of a \\\"sunrise over a mountain range\\\".\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Diffusion-based text-to-image generation model\", \"api_name\": \"lllyasviel/control_v11p_sd15_normalbae\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": [\"checkpoint\", \"torch_dtype\"], \"python_environment_requirements\": [\"diffusers\", \"transformers\", \"accelerate\", \"controlnet_aux\"], \"example_code\": \"import torch\\nimport os\\nfrom huggingface_hub import HfApi\\nfrom pathlib import Path\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom controlnet_aux import NormalBaeDetector\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_normalbae\\nimage = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_normalbae/resolve/main/images/input.png\\n)\\nprompt = A head full of roses\\nprocessor = NormalBaeDetector.from_pretrained(lllyasviel/Annotators)\\ncontrol_image = processor(image)\\ncontrol_image.save(./images/control.png)\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(33)\\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\\nimage.save('images/image_out.png')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\"}}", "category": "generic"}
{"question_id": 279, "text": " I am creating a website for my photography business where I need to improve the resolution of some images before uploading them.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"swin2SR-classical-sr-x4-64\", \"api_call\": \"pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\", \"api_arguments\": [\"input_image\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\"}}", "category": "generic"}
{"question_id": 280, "text": " You are an interior designer, and you wish to propose a room modification based on an existing photo.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"GreeneryScenery/SheepsControlV3\", \"api_call\": \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"api_arguments\": {\"image\": \"Path to image file\", \"text_guidance\": \"Optional text guidance for the model\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": [\"from transformers import pipeline\", \"model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\", \"result = model({'image': 'path/to/image.jpg', 'text_guidance': 'Optional text guidance'})\"], \"performance\": {\"dataset\": \"GreeneryScenery/SheepsControlV3\", \"accuracy\": \"Not provided\"}, \"description\": \"GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\"}}", "category": "generic"}
{"question_id": 281, "text": " We want to create visuals for our company profile. Design an image in a different style or representation using our logo as input.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image-to-Image\", \"api_name\": \"GreeneryScenery/SheepsControlV5\", \"api_call\": \"pipeline('image-to-image', model='GreeneryScenery/SheepsControlV5')\", \"api_arguments\": {\"input_image\": \"path/to/image/file\"}, \"python_environment_requirements\": {\"huggingface_hub\": \">=0.0.17\", \"transformers\": \">=4.13.0\", \"torch\": \">=1.10.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"poloclub/diffusiondb\", \"accuracy\": \"Not provided\"}, \"description\": \"SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\"}}", "category": "generic"}
{"question_id": 282, "text": " I have an automatic drone and need to deblur the images it captures when moving fast.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Keras\", \"functionality\": \"Image Deblurring\", \"api_name\": \"google/maxim-s3-deblurring-gopro\", \"api_call\": \"from_pretrained_keras('google/maxim-s3-deblurring-gopro')\", \"api_arguments\": [\"image\"], \"python_environment_requirements\": [\"huggingface_hub\", \"PIL\", \"tensorflow\", \"numpy\", \"requests\"], \"example_code\": \"from huggingface_hub import from_pretrained_keras\\nfrom PIL import Image\\nimport tensorflow as tf\\nimport numpy as np\\nimport requests\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage = np.array(image)\\nimage = tf.convert_to_tensor(image)\\nimage = tf.image.resize(image, (256, 256))\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\npredictions = model.predict(tf.expand_dims(image, 0))\", \"performance\": {\"dataset\": \"GoPro\", \"accuracy\": {\"PSNR\": 32.86, \"SSIM\": 0.961}}, \"description\": \"MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\"}}", "category": "generic"}
{"question_id": 283, "text": " Craft a powerful image of a pirate ship sailing at night during a storm, using text-to-image diffusion.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Text-to-Image Diffusion Models\", \"api_name\": \"lllyasviel/control_v11p_sd15s2_lineart_anime\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15s2_lineart_anime\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": [\"pip install diffusers transformers accelerate\", \"pip install controlnet_aux==0.3.0\"], \"example_code\": [\"import torch\", \"import os\", \"from huggingface_hub import HfApi\", \"from pathlib import Path\", \"from diffusers.utils import load_image\", \"from PIL import Image\", \"import numpy as np\", \"from controlnet_aux import LineartAnimeDetector\", \"from transformers import CLIPTextModel\", \"from diffusers import (\", \" ControlNetModel,\", \" StableDiffusionControlNetPipeline,\", \" UniPCMultistepScheduler,\", \")\", \"checkpoint = lllyasviel/control_v11p_sd15s2_lineart_anime\", \"image = load_image(\", \" https://huggingface.co/lllyasviel/control_v11p_sd15s2_lineart_anime/resolve/main/images/input.png\", \")\", \"image = image.resize((512, 512))\", \"prompt = A warrior girl in the jungle\", \"processor = LineartAnimeDetector.from_pretrained(lllyasviel/Annotators)\", \"control_image = processor(image)\", \"control_image.save(./images/control.png)\", \"text_encoder = CLIPTextModel.from_pretrained(runwayml/stable-diffusion-v1-5, subfolder=text_encoder, num_hidden_layers=11, torch_dtype=torch.float16)\", \"controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"pipe = StableDiffusionControlNetPipeline.from_pretrained(\", \" runwayml/stable-diffusion-v1-5, text_encoder=text_encoder, controlnet=controlnet, torch_dtype=torch.float16\", \")\", \"pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\", \"pipe.enable_model_cpu_offload()\", \"generator = torch.manual_seed(0)\", \"image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\", \"image.save('images/image_out.png')\"], \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\"}}", "category": "generic"}
{"question_id": 284, "text": " I have an incomplete drawing of a house underconstruction, I would like to see what it looks like when its completed, so I can decide what external design options to use.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Image-to-Image\", \"framework\": \"Hugging Face\", \"functionality\": \"Image Inpainting\", \"api_name\": \"lllyasviel/control_v11p_sd15_inpaint\", \"api_call\": \"ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\", \"api_arguments\": {\"checkpoint\": \"lllyasviel/control_v11p_sd15_inpaint\", \"torch_dtype\": \"torch.float16\"}, \"python_environment_requirements\": \"pip install diffusers transformers accelerate\", \"example_code\": \"import torch\\nimport os\\nfrom diffusers.utils import load_image\\nfrom PIL import Image\\nimport numpy as np\\nfrom diffusers import (\\n ControlNetModel,\\n StableDiffusionControlNetPipeline,\\n UniPCMultistepScheduler,\\n)\\ncheckpoint = lllyasviel/control_v11p_sd15_inpaint\\noriginal_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/original.png\\n)\\nmask_image = load_image(\\n https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/images/mask.png\\n)\\ndef make_inpaint_condition(image, image_mask):\\n image = np.array(image.convert(RGB)).astype(np.float32) / 255.0\\n image_mask = np.array(image_mask.convert(L))\\n assert image.shape[0:1] == image_mask.shape[0:1], image and image_mask must have the same image size\\n image[image_mask < 128] = -1.0 # set as masked pixel \\n image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\\n image = torch.from_numpy(image)\\n return image\\ncontrol_image = make_inpaint_condition(original_image, mask_image)\\nprompt = best quality\\nnegative_prompt=lowres, bad anatomy, bad hands, cropped, worst quality\\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, torch_dtype=torch.float16\\n)\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\npipe.enable_model_cpu_offload()\\ngenerator = torch.manual_seed(2)\\nimage = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=30, \\n generator=generator, image=control_image).images[0]\\nimage.save('images/output.png')\", \"performance\": {\"dataset\": \"Stable Diffusion v1-5\", \"accuracy\": \"Not specified\"}, \"description\": \"ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\"}}", "category": "generic"}
{"question_id": 285, "text": " We are a video game developer working on a new game. Let's synthesize high-quality images for the game's background.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Synthesis\", \"api_name\": \"google/ddpm-cifar10-32\", \"api_call\": \"Output: DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", \"api_arguments\": \"None\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.\"}}", "category": "generic"}
{"question_id": 286, "text": " Our design team wants to generate artwork for an online gallery. Create images using AI.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"google/ddpm-celebahq-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-celebahq-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm()[sample]\\nimage[0].save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.\"}}", "category": "generic"}
{"question_id": 287, "text": " My daughter loves cats, so I want to create a high-quality image of a cat for her.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ddpm-cat-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.\"}}", "category": "generic"}
{"question_id": 288, "text": " We are a real estate company, and we want to generate hypothetical images of churches for our marketing campaigns.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ddpm-church-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-church-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.\"}}", "category": "generic"}
{"question_id": 289, "text": " We need an image generator that will generate faces for a virtual fashion show.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ncsnpp-celebahq-256\", \"api_call\": \"sde_ve = DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')\", \"api_arguments\": {\"model_id\": \"google/ncsnpp-celebahq-256\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR-10\", \"accuracy\": {\"Inception_score\": 9.89, \"FID\": 2.2, \"likelihood\": 2.99}}, \"description\": \"Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.\"}}", "category": "generic"}
{"question_id": 290, "text": " Let's create an AI-driven interior design concept for a residential bedroom.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Denoising Diffusion Probabilistic Models (DDPM)\", \"api_name\": \"google/ddpm-bedroom-256\", \"api_call\": \"Output: DDPMPipeline.from_pretrained('google/ddpm-bedroom-256')()\", \"api_arguments\": \"None\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception score\": 9.46, \"FID score\": 3.17}}, \"description\": \"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\"}}", "category": "generic"}
{"question_id": 291, "text": " I am a wildlife enthusiast, and I am making a learning app about butterflies. Generate some realistic images of butterflies to use as examples.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ceyda/butterfly_cropped_uniq1K_512\", \"api_call\": \"LightweightGAN.from_pretrained('ceyda/butterfly_cropped_uniq1K_512')\", \"api_arguments\": [\"pretrained_model_name_or_path\"], \"python_environment_requirements\": [\"torch\", \"huggan.pytorch.lightweight_gan.lightweight_gan\"], \"example_code\": \"import torch\\nfrom huggan.pytorch.lightweight_gan.lightweight_gan import LightweightGAN\\ngan = LightweightGAN.from_pretrained(ceyda/butterfly_cropped_uniq1K_512)\\ngan.eval()\\nbatch_size = 1\\nwith torch.no_grad():\\n ims = gan.G(torch.randn(batch_size, gan.latent_dim)).clamp_(0., 1.)*255\\n ims = ims.permute(0,2,3,1).detach().cpu().numpy().astype(np.uint8)\\n # ims is [BxWxHxC] call Image.fromarray(ims[0])\", \"performance\": {\"dataset\": \"huggan/smithsonian_butterflies_subset\", \"accuracy\": \"FID score on 100 images\"}, \"description\": \"Butterfly GAN model based on the paper 'Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis'. The model is intended for fun and learning purposes. It was trained on 1000 images from the huggan/smithsonian_butterflies_subset dataset, with a focus on low data training as mentioned in the paper. The model generates high-quality butterfly images.\"}}", "category": "generic"}
{"question_id": 292, "text": " As an art gallery director, we would like to generate computer-generated artwork to discover new painting styles.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ncsnpp-church-256\", \"api_call\": \"sde_ve = DiffusionPipeline.from_pretrained('google/ncsnpp-church-256')\", \"api_arguments\": \"model_id\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-church-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR-10\", \"accuracy\": {\"Inception_score\": 9.89, \"FID\": 2.2, \"likelihood\": 2.99}}, \"description\": \"Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.\"}}", "category": "generic"}
{"question_id": 293, "text": " We are working on an art website that recommends beautiful artworks. We want to use a model that generates images similar to the ones from WikiArt.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"johnowhitaker/sd-class-wikiart-from-bedrooms\", \"api_call\": \"DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"https://huggingface.co/datasets/huggan/wikiart\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.\"}}", "category": "generic"}
{"question_id": 294, "text": " Create a program that can generate an original image to use as a social media profile picture.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ddpm-cifar10-32\", \"api_call\": \"Output: DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').\", \"api_arguments\": \"None\", \"python_environment_requirements\": \"!pip install diffusers\", \"example_code\": \"from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-cifar10-32\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception score\": 9.46, \"FID score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.\"}}", "category": "generic"}
{"question_id": 295, "text": " I want to create a visual representation of my dream house, which can be generated with some help of Google/DDPM-EMA pretrained model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Denoising Diffusion Probabilistic Models (DDPM)\", \"api_name\": \"google/ddpm-ema-bedroom-256\", \"api_call\": \"DDPMPipeline.from_pretrained(model_id)\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-bedroom-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics, capable of producing high-quality image synthesis results. The model can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. It obtains an Inception score of 9.46 and a state-of-the-art FID score of 3.17 on the unconditional CIFAR10 dataset.\"}}", "category": "generic"}
{"question_id": 296, "text": " We have an application that generates random images of portraits from samples, is there a model that can generate high quality images?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Inference\", \"api_name\": \"google/ncsnpp-ffhq-1024\", \"api_call\": \"DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-1024')\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"!pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-ffhq-1024\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR-10\", \"accuracy\": {\"Inception_score\": 9.89, \"FID\": 2.2, \"likelihood\": 2.99}}, \"description\": \"Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.\"}}", "category": "generic"}
{"question_id": 297, "text": " Imagine yourself as an upcoming science fiction writer. Design a cover for your next novel with the help of the latest AI-powered technology.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ocariz/universe_1400\", \"api_call\": \"DDPMPipeline.from_pretrained('ocariz/universe_1400')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs.\"}}", "category": "generic"}
{"question_id": 298, "text": " I am building a game server for which I want the model to generate custom skins for Minecraft characters.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"WiNE-iNEFF/Minecraft-Skin-Diffusion-V2\", \"api_call\": \"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\", \"api_arguments\": [], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2')\\nimage = pipeline().images[0].convert('RGBA')\\nimage\", \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"An unconditional image generation model for generating Minecraft skin images using the diffusion model.\"}}", "category": "generic"}
{"question_id": 299, "text": " Develop a program or so for a gaming community that enjoy mincraft. The reason is to generate unique and cool skin designs for the user's characters.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"Minecraft-Skin-Diffusion\", \"api_call\": \"DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion')\", \"api_arguments\": {}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion')\\nimage = pipeline().images[0].convert('RGBA')\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods.\"}}", "category": "generic"}
{"question_id": 300, "text": " Present a code for generating a colorful butterfly using A.I. technology.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\", \"api_arguments\": {\"model_id\": \"clp/sd-class-butterflies-32\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies.\"}}", "category": "generic"}
{"question_id": 301, "text": " Develop an application that generates images of butterflies.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"MFawad/sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('MFawad/sd-class-butterflies-32')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('MFawad/sd-class-butterflies-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute \\ud83e\\udd8b.\"}}", "category": "generic"}
{"question_id": 302, "text": " We need to generate unique, high-quality 2D human faces for a mobile game that can be played offline.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"google/ncsnpp-ffhq-256\", \"api_call\": \"sde_ve = DiffusionPipeline.from_pretrained('google/ncsnpp-ffhq-256')\", \"api_arguments\": {\"model_id\": \"google/ncsnpp-ffhq-256\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": [\"!pip install diffusers\", \"from diffusers import DiffusionPipeline\", \"model_id = google/ncsnpp-ffhq-256\", \"sde_ve = DiffusionPipeline.from_pretrained(model_id)\", \"image = sde_ve()[sample]\", \"image[0].save(sde_ve_generated_image.png)\"], \"performance\": {\"dataset\": \"CIFAR-10\", \"accuracy\": {\"Inception score\": 9.89, \"FID\": 2.2, \"Likelihood\": 2.99}}, \"description\": \"Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.\"}}", "category": "generic"}
{"question_id": 303, "text": " Can you generate a cat image with a 256x256 resolution?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Denoising Diffusion Probabilistic Models (DDPM)\", \"api_name\": \"google/ddpm-ema-cat-256\", \"api_call\": \"Output: DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256')()\", \"api_arguments\": [\"model_id\"], \"python_environment_requirements\": [\"!pip install diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\nmodel_id = google/ddpm-ema-cat-256\\nddpm = DDPMPipeline.from_pretrained(model_id)\\nimage = ddpm().images[0]\\nimage.save(ddpm_generated_image.png)\", \"performance\": {\"dataset\": \"CIFAR10\", \"accuracy\": {\"Inception_score\": 9.46, \"FID_score\": 3.17}}, \"description\": \"Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17.\"}}", "category": "generic"}
{"question_id": 304, "text": " A greeting card publisher wants to generate pictures of the butterflies for their new collection.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ocariz/butterfly_200\", \"api_call\": \"DDPMPipeline.from_pretrained('ocariz/butterfly_200')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.\"}}", "category": "generic"}
{"question_id": 305, "text": " I am working on a new shoe design and need to generate a variety of shoe images as inspiration.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"Apocalypse-19/shoe-generator\", \"api_call\": \"DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\", \"api_arguments\": [], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"custom dataset\", \"accuracy\": \"128x128 resolution\"}, \"description\": \"This model is a diffusion model for unconditional image generation of shoes trained on a custom dataset at 128x128 resolution.\"}}", "category": "generic"}
{"question_id": 306, "text": " The compnay needs a random butterfly design to be printed on the company's T-shirts.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"ntrant7/sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\", \"api_arguments\": [], \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies.\"}}", "category": "generic"}
{"question_id": 307, "text": " I have videos of sports events and would like to identify the type of sport happening in a video without knowing any labels.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/xclip-base-patch32\", \"api_call\": \"XClipModel.from_pretrained('microsoft/xclip-base-patch32')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"For code examples, we refer to the documentation.\", \"performance\": {\"dataset\": \"Kinetics 400\", \"accuracy\": {\"top-1\": 80.4, \"top-5\": 95.0}}, \"description\": \"X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\"}}", "category": "generic"}
{"question_id": 308, "text": " Our client wants to create vintage-style images for their social media to showcase their retro-inspired products. Provide a solution to achieve this.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs\", \"api_call\": \"DDPMPipeline.from_pretrained('pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"diffusers\", \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Example Fine-Tuned Model for Unit 2 of the Diffusion Models Class\"}}", "category": "generic"}
{"question_id": 309, "text": " As an environmentalist organization, we want to generate images of butterflies to use in our promotional content.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Diffusers\", \"api_name\": \"myunus1/diffmodels_galaxies_scratchbook\", \"api_call\": \"DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\", \"api_arguments\": {\"from_pretrained\": \"myunus1/diffmodels_galaxies_scratchbook\"}, \"python_environment_requirements\": {\"package\": \"diffusers\", \"import\": \"from diffusers import DDPMPipeline\"}, \"example_code\": {\"initialize_pipeline\": \"pipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook')\", \"generate_image\": \"image = pipeline().images[0]\", \"display_image\": \"image\"}, \"performance\": {\"dataset\": \"Not provided\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute \\ud83e\\udd8b.\"}}", "category": "generic"}
{"question_id": 310, "text": " We are tasked to make a presentation on the types of butterflies for a biology class. We want to generate images to showcase butterfly variety.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"utyug1/sd-class-butterflies-32\", \"api_call\": \"DDPMPipeline.from_pretrained('utyug1/sd-class-butterflies-32')\", \"api_arguments\": {\"pretrained_model\": \"utyug1/sd-class-butterflies-32\"}, \"python_environment_requirements\": [\"diffusers\"], \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('utyug1/sd-class-butterflies-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute butterflies.\"}}", "category": "generic"}
{"question_id": 311, "text": " Let's develop a software to generate images of cute animals automatically.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Unconditional Image Generation\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Unconditional Image Generation\", \"api_name\": \"sd-class-pandas-32\", \"api_call\": \"DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\", \"api_arguments\": {\"pretrained_model\": \"schdoel/sd-class-AFHQ-32\"}, \"python_environment_requirements\": {\"package\": \"diffusers\", \"import\": \"from diffusers import DDPMPipeline\"}, \"example_code\": \"from diffusers import DDPMPipeline\\npipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\\nimage = pipeline().images[0]\\nimage\", \"performance\": {\"dataset\": \"AFHQ\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a diffusion model for unconditional image generation of cute \\ud83e\\udd8b.\"}}", "category": "generic"}
{"question_id": 312, "text": " I am part of a research team aiming to create a video analysis tool, we would like to classify different video activities.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-base-finetuned-k400\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k400')\", \"api_arguments\": \"video, return_tensors\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(8, 3, 224, 224))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k400)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k400)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\n  logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.\"}}", "category": "generic"}
{"question_id": 313, "text": " As a video editor, I'd like smooth scene changes in a footage with some action detection.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base)\\npixel_values = processor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"To be provided\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.\"}}", "category": "generic"}
{"question_id": 314, "text": " Create a software to classify video clips for a security system.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-base-finetuned-k600\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600')\", \"api_arguments\": [\"images\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(8, 3, 224, 224))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k600)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k600)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-600\", \"accuracy\": null}, \"description\": \"TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 315, "text": " I have a video. I want to see if this video can be used for advertisement or not by our AI.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-hr-finetuned-k600\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k600')\", \"api_arguments\": {\"images\": \"video\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 448, 448))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k600)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\n  logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-600\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels.\"}}", "category": "generic"}
{"question_id": 316, "text": " A dance studio wants to host a competition that requires contestants to submit videos. The studio wants to automatically categorize the dances into genres to help with organizing the event.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base-finetuned-kinetics\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-kinetics')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-kinetics)\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-kinetics)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": {\"top-1\": 80.9, \"top-5\": 94.7}}, \"description\": \"VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 317, "text": " Develop an AI actor that can recognize scenes from a trailer and identify the specific movie genre.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-hr-finetuned-k400\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 448, 448))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-k400)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\n  logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not specified\"}, \"description\": \"TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al.\"}}", "category": "generic"}
{"question_id": 318, "text": " To improve the customer experience on our video streaming platform, we need to classify videos into proper categories based on content. Develop a video classification model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-base-finetuned-ssv2\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-ssv2')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(8, 3, 224, 224))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-ssv2)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-ssv2)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something Something v2\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 319, "text": " Develop a security system for our business site that can detect unauthorized people by analyzing video clips.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"videomae-large\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-large')\", \"api_arguments\": \"pixel_values, bool_masked_pos\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-large)\\npixel_values = processor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not provided\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.\"}}", "category": "generic"}
{"question_id": 320, "text": " The marketing team requires automated classification of video clips to target advertisements to customers. Please generate a solution.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"facebook/timesformer-hr-finetuned-ssv2\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 448, 448))\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-hr-finetuned-ssv2)\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-hr-finetuned-ssv2)\\ninputs = feature_extractor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something Something v2\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 321, "text": " The management requires an AI software to categorize the video types.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base-finetuned-ssv2\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\", \"api_arguments\": \"video\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n    logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": {\"top-1\": 70.6, \"top-5\": 92.6}}, \"description\": \"VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 322, "text": " I own a security company, I want our surveillance cameras to send me an alert when it captures a burglar.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base-short\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"MCG-NJU/videomae-base-short\"}, \"python_environment_requirements\": {\"packages\": [\"transformers\"]}, \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short)\\npixel_values = processor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not provided\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.\"}}", "category": "generic"}
{"question_id": 323, "text": " Find the class of the action in video content related to sports and physical activities.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-large-finetuned-kinetics\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-large-finetuned-kinetics)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": {\"top-1\": 84.7, \"top-5\": 96.5}}, \"description\": \"VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 324, "text": " I have a series of short videos about people performing various actions. Now I want to know which action is performed in each video.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"MCG-NJU/videomae-base-short-finetuned-kinetics\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\\ninputs = processor(video, return_tensors='pt')\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": {\"top-1\": 79.4, \"top-5\": 94.1}}, \"description\": \"VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 325, "text": " Our surveillance system needs to alert us when it detects violent situations in real-time video feeds.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"videomae-base-finetuned-RealLifeViolenceSituations-subset\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset')\", \"api_arguments\": {\"model_name\": \"dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset\"}, \"python_environment_requirements\": {\"transformers\": \"4.27.2\", \"pytorch\": \"1.13.1\", \"datasets\": \"2.10.1\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 0.9533}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is trained for video classification task, specifically for RealLifeViolenceSituations.\"}}", "category": "generic"}
{"question_id": 326, "text": " In order to optimize marketing videos, I want to classify video content into possible Kinetics-400 labels to distinguish them from others.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"fcakyon/timesformer-large-finetuned-k400\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('fcakyon/timesformer-large-finetuned-k400')\", \"api_arguments\": [\"video\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(96, 3, 224, 224))\\nprocessor = AutoImageProcessor.from_pretrained(fcakyon/timesformer-large-finetuned-k400)\\nmodel = TimesformerForVideoClassification.from_pretrained(fcakyon/timesformer-large-finetuned-k400)\\ninputs = processor(video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper 'TimeSformer: Is Space-Time Attention All You Need for Video Understanding?' by Tong et al.\"}}", "category": "generic"}
{"question_id": 327, "text": " We are an AI consulting firm, and our client requires a video classification solution for their business.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"videomae-base-short-ssv2\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\", \"api_arguments\": [\"video\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\npixel_values = processor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": \"N/A\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\"}}", "category": "generic"}
{"question_id": 328, "text": " Create a solution to classify videos automatically into predetermined categories.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kb')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers, torch, tokenizers, datasets\", \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 0.7298}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.5482, Accuracy: 0.7298.\"}}", "category": "generic"}
{"question_id": 329, "text": " We are working on a project to classify human-video interaction in real-time\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"videomae-small-finetuned-ssv2\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-ssv2')\", \"api_arguments\": {\"model_name\": \"MCG-NJU/videomae-small-finetuned-ssv2\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\", \"numpy\": \"import numpy as np\", \"torch\": \"import torch\"}, \"example_code\": \"video = list(np.random.randn(16, 3, 224, 224))\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\ninputs = feature_extractor(video, return_tensors=pt)\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\n  logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Something-Something V2\", \"accuracy\": {\"top-1\": 66.8, \"top-5\": 90.3}}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\"}}", "category": "generic"}
{"question_id": 330, "text": " Analyze a video and provide a classification based on the content.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset')\", \"api_arguments\": {\"learning_rate\": 5e-05, \"train_batch_size\": 8, \"eval_batch_size\": 8, \"seed\": 42, \"optimizer\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\", \"lr_scheduler_type\": \"linear\", \"lr_scheduler_warmup_ratio\": 0.1, \"training_steps\": 111}, \"python_environment_requirements\": {\"transformers\": \"4.24.0\", \"pytorch\": \"1.12.1+cu113\", \"datasets\": \"2.6.1\", \"tokenizers\": \"0.13.2\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 1.0}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 331, "text": " Identify the activities in a video supplied as input for security monitoring purposes.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 0.7453}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 332, "text": " Our media company wants to analyze different videos into different categories. Develop an AI model to process and classify the videos.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"videomae-base-ssv2\", \"api_call\": \"VideoMAEForPreTraining.from_pretrained('MCG-NJU/videomae-base-short-ssv2')\", \"api_arguments\": \"video\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\\nimport numpy as np\\nimport torch\\nnum_frames = 16\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\nmodel = VideoMAEForPreTraining.from_pretrained(MCG-NJU/videomae-base-short-ssv2)\\npixel_values = feature_extractor(video, return_tensors=pt).pixel_values\\nnum_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\\nseq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\\nbool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\\noutputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\\nloss = outputs.loss\", \"performance\": {\"dataset\": \"Something-Something-v2\", \"accuracy\": \"\"}, \"description\": \"VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\"}}", "category": "generic"}
{"question_id": 333, "text": " A client needs to create a video playback software that can automatically classify the genre of a video clip as it is played.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"fcakyon/timesformer-hr-finetuned-k400\", \"api_call\": \"TimesformerForVideoClassification.from_pretrained('fcakyon/timesformer-hr-finetuned-k400')\", \"api_arguments\": [\"images\", \"return_tensors\"], \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import AutoImageProcessor, TimesformerForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 448, 448))\\nprocessor = AutoImageProcessor.from_pretrained(fcakyon/timesformer-hr-finetuned-k400)\\nmodel = TimesformerForVideoClassification.from_pretrained(fcakyon/timesformer-hr-finetuned-k400)\\ninputs = processor(images=video, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": \"Not provided\"}, \"description\": \"TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper 'TimeSformer: Is Space-Time Attention All You Need for Video Understanding?' by Tong et al.\"}}", "category": "generic"}
{"question_id": 334, "text": " I want to improve my home security system by categorizing the type of activities seen in the security footage.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"videomae-small-finetuned-kinetics\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\", \"api_arguments\": {\"video\": \"list(np.random.randn(16, 3, 224, 224))\"}, \"python_environment_requirements\": [\"transformers\", \"numpy\", \"torch\"], \"example_code\": \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\nimport numpy as np\\nimport torch\\nvideo = list(np.random.randn(16, 3, 224, 224))\\nprocessor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\nmodel = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\\ninputs = processor(video, return_tensors='pt')\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_class_idx = logits.argmax(-1).item()\\nprint('Predicted class:', model.config.id2label[predicted_class_idx])\", \"performance\": {\"dataset\": \"Kinetics-400\", \"accuracy\": {\"top-1\": 79.0, \"top-5\": 93.8}}, \"description\": \"VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\"}}", "category": "generic"}
{"question_id": 335, "text": " I want to predict appropriate categories of videos to better understand users video choices.+\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('lmazzon70/videomae-large-finetuned-kinetics-finetuned-rwf2000-epochs8-batch8-kl-torch2')\", \"api_arguments\": \"video_path\", \"python_environment_requirements\": \"transformers==4.27.4, torch==2.0.0+cu117, datasets==2.11.0, tokenizers==0.13.2\", \"example_code\": \"\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 0.7212}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-large-finetuned-kinetics on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 336, "text": " We are building an app to recognize human activity in videos. We need to process a video file and get a categorized list of actions.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"tiny-random-VideoMAEForVideoClassification\", \"api_call\": \"VideoClassificationPipeline(model='hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification')\", \"api_arguments\": \"model\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random VideoMAE model for video classification.\"}}", "category": "generic"}
{"question_id": 337, "text": " We want to add a feature to our video platform that can automatically recognize the activities in the uploaded videos.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"videomae-base-finetuned-ucf101-subset\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('zahrav/videomae-base-finetuned-ucf101-subset')\", \"api_arguments\": \"N/A\", \"python_environment_requirements\": \"transformers==4.25.1, torch==1.10.0, datasets==2.7.1, tokenizers==0.12.1\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 0.8968}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is used for video classification tasks.\"}}", "category": "generic"}
{"question_id": 338, "text": " Imagine you want to classify videos from the UCF101 dataset. We need to use the pre-trained AutoModelForVideoClassification model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Classification\", \"api_name\": \"sayakpaul/videomae-base-finetuned-ucf101-subset\", \"api_call\": \"AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')\", \"api_arguments\": {\"learning_rate\": 5e-05, \"train_batch_size\": 8, \"eval_batch_size\": 8, \"seed\": 42, \"optimizer\": \"Adam with betas=(0.9,0.999) and epsilon=1e-08\", \"lr_scheduler_type\": \"linear\", \"lr_scheduler_warmup_ratio\": 0.1, \"training_steps\": 148}, \"python_environment_requirements\": {\"Transformers\": \"4.24.0\", \"Pytorch\": \"1.12.1+cu113\", \"Datasets\": \"2.6.1\", \"Tokenizers\": \"0.13.2\"}, \"example_code\": \"from transformers import AutoModelForVideoClassification, AutoTokenizer\\nmodel = AutoModelForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')\\ntokenizer = AutoTokenizer.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subset')\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": 0.8645}, \"description\": \"This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645.\"}}", "category": "generic"}
{"question_id": 339, "text": " Our team is developing a surveillance system for public spaces, and we require a model to identify people's actions in the video footage.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Video Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Video Action Recognition\", \"api_name\": \"videomae-base-finetuned-ucf101\", \"api_call\": \"VideoMAEForVideoClassification.from_pretrained('nateraw/videomae-base-finetuned-ucf101')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"nateraw/videomae-base-finetuned-ucf101\"}, \"python_environment_requirements\": [\"transformers\", \"decord\", \"huggingface_hub\"], \"example_code\": \"from decord import VideoReader, cpu\\nimport torch\\nimport numpy as np\\nfrom transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\nfrom huggingface_hub import hf_hub_download\\nnp.random.seed(0)\\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\\n converted_len = int(clip_len * frame_sample_rate)\\n end_idx = np.random.randint(converted_len, seg_len)\\n start_idx = end_idx - converted_len\\n indices = np.linspace(start_idx, end_idx, num=clip_len)\\n indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\\n return indices\\nfile_path = hf_hub_download(\\n repo_id=nateraw/dino-clips, filename=archery.mp4, repo_type=space\\n)\\nvideoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\\nvideoreader.seek(0)\\nindices = sample_frame_indices(clip_len=16, frame_sample_rate=4, seg_len=len(videoreader))\\nvideo = videoreader.get_batch(indices).asnumpy()\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\nmodel = VideoMAEForVideoClassification.from_pretrained(nateraw/videomae-base-finetuned-ucf101)\\ninputs = feature_extractor(list(video), return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\npredicted_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted_label])\", \"performance\": {\"dataset\": \"UCF101\", \"accuracy\": 0.758209764957428}, \"description\": \"VideoMAE Base model fine tuned on UCF101 for Video Action Recognition\"}}", "category": "generic"}
{"question_id": 340, "text": " Help detect whether there is a cat or a dog in the given image.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-large-patch14-336\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-large-patch14').\", \"api_arguments\": \"image_path, tokenizer, model\", \"python_environment_requirements\": \"Transformers 4.21.3, TensorFlow 2.8.2, Tokenizers 0.12.1\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"unknown\", \"accuracy\": \"N/A\"}, \"description\": \"This model was trained from scratch on an unknown dataset.\"}}", "category": "generic"}
{"question_id": 341, "text": " My client is a real estate agency looking to categorize housing images so that they can provide potential buyers with relevant recommendations. Can you help us with the image classification?\\n###Input: image='path/to/housing/image', class_names=['apartment', 'house', 'townhouse', 'studio', 'duplex', 'penthouse']\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", \"api_call\": \"pipeline('zero-shot-classification', model='laion/CLIP-ViT-B-32-laion2B-s34B-b79K')\", \"api_arguments\": {\"image\": \"path/to/image\", \"class_names\": [\"class1\", \"class2\", \"class3\"]}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"from transformers import pipeline; classifier = pipeline('zero-shot-classification', model='laion/CLIP-ViT-B-32-laion2B-s34B-b79K'); classifier(image='path/to/image', class_names=['class1', 'class2', 'class3'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": 66.6}, \"description\": \"A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. It enables researchers to better understand and explore zero-shot, arbitrary image classification. The model can be used for zero-shot image classification, image and text retrieval, among others.\"}}", "category": "generic"}
{"question_id": 342, "text": " We are working on building an AI that can judge a programmer's ability based on their portfolio website. We require a way to generate categories based on website images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-base-patch32\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\", \"api_arguments\": [\"text\", \"images\", \"return_tensors\", \"padding\"], \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-base-patch32)\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-base-patch32)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}", "category": "generic"}
{"question_id": 343, "text": " A new zoo just opened in our town. We want to know if the animals in the provided image are related to animals.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-large-patch14\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\", \"api_arguments\": {\"text\": [\"a photo of a cat\", \"a photo of a dog\"], \"images\": \"image\", \"return_tensors\": \"pt\", \"padding\": \"True\"}, \"python_environment_requirements\": {\"packages\": [\"PIL\", \"requests\", \"transformers\"]}, \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-large-patch14)\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-large-patch14)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies depending on the dataset\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}", "category": "generic"}
{"question_id": 344, "text": " We are building an AI-powered art platform that recommends artwork for users based on their preferences. Find a way to classify the images according to different categories without any annotation.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-ViT-L-14-laion2B-s32B-b82K')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": 75.3}, \"description\": \"A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model.\"}}", "category": "generic"}
{"question_id": 345, "text": " I run an online store and I want to automatically sort the uploaded product images into categories: electronics, clothing, and furniture.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-g-14-laion2B-s34B-b88K\", \"api_call\": \"pipeline('zero-shot-image-classification', model='laion/CLIP-ViT-g-14-laion2B-s34B-b88K')\", \"api_arguments\": {\"image\": \"path/to/image/file\", \"class_names\": \"list_of_class_names\"}, \"python_environment_requirements\": {\"huggingface_hub\": \"0.0.17\", \"transformers\": \"4.11.3\", \"torch\": \"1.9.0\", \"torchvision\": \"0.10.0\"}, \"example_code\": null, \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\"}}", "category": "generic"}
{"question_id": 346, "text": " As someone who loves photography, I want my app to be able to automatically identify what the subject is in my uploaded images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\", \"api_arguments\": [\"image\", \"possible_class_names\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'); classifier(image, possible_class_names=['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"80.1\"}, \"description\": \"A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 347, "text": " We want to build a mobile app where you take a picture of a bird and it tells you which species it is.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-ViT-B-16-laion2B-s34B-b88K\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-ViT-B-16-laion2B-s34B-b88K')\", \"api_arguments\": {\"image\": \"Path to image file or URL\", \"class_names\": \"List of possible class names (comma-separated)\"}, \"python_environment_requirements\": {\"transformers\": \">=4.11.0\"}, \"example_code\": \"from transformers import pipeline; classify = pipeline('image-classification', model='laion/CLIP-ViT-B-16-laion2B-s34B-b88K'); classify('/path/to/image.jpg', ['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.2%\"}, \"description\": \"A CLIP ViT-B/16 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. This model is intended for research purposes and can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}", "category": "generic"}
{"question_id": 348, "text": " We are creating an app for designing posters, and it is required to suggest the most relevant images for a specific description.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"openai/clip-vit-base-patch16\", \"api_call\": \"CLIPModel.from_pretrained('openai/clip-vit-base-patch16')\", \"api_arguments\": [\"text\", \"images\", \"return_tensors\", \"padding\"], \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-base-patch16)\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-base-patch16)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [\"Food101\", \"CIFAR10\", \"CIFAR100\", \"Birdsnap\", \"SUN397\", \"Stanford Cars\", \"FGVC Aircraft\", \"VOC2007\", \"DTD\", \"Oxford-IIIT Pet dataset\", \"Caltech101\", \"Flowers102\", \"MNIST\", \"SVHN\", \"IIIT5K\", \"Hateful Memes\", \"SST-2\", \"UCF101\", \"Kinetics700\", \"Country211\", \"CLEVR Counting\", \"KITTI Distance\", \"STL-10\", \"RareAct\", \"Flickr30\", \"MSCOCO\", \"ImageNet\", \"ImageNet-A\", \"ImageNet-R\", \"ImageNet Sketch\", \"ObjectNet (ImageNet Overlap)\", \"Youtube-BB\", \"ImageNet-Vid\"], \"accuracy\": \"varies depending on the dataset\"}, \"description\": \"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\"}}", "category": "generic"}
{"question_id": 349, "text": " A dog training company wants to automatically categorize dog breeds in images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup')\", \"api_arguments\": \"image_path, class_names\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"results = model(image_path, class_names='cat, dog, bird')\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"76.9\"}, \"description\": \"A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).\"}}", "category": "generic"}
{"question_id": 350, "text": " We are an e-commerce company that wants to classify products such as blue shoes and leather jackets using a zero-shot image classification model.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"patrickjohncyh/fashion-clip\", \"api_call\": \"CLIPModel.from_pretrained('patrickjohncyh/fashion-clip')\", \"api_arguments\": {\"image\": \"File\", \"class_names\": \"String (comma-separated)\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained('patrickjohncyh/fashion-clip'); processor = CLIPProcessor.from_pretrained('patrickjohncyh/fashion-clip'); inputs = processor(text='blue shoes', images=image, return_tensors='pt', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \"performance\": {\"dataset\": [{\"name\": \"FMNIST\", \"accuracy\": 0.83}, {\"name\": \"KAGL\", \"accuracy\": 0.73}, {\"name\": \"DEEP\", \"accuracy\": 0.62}]}, \"description\": \"FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\"}}", "category": "generic"}
{"question_id": 351, "text": " Build an image classifier for wildlife monitoring photocaptures. The software will be installed on cameras monitoring private properties and national parks.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\", \"api_arguments\": {\"image\": \"path to image file\", \"class_names\": \"list of possible class names (comma-separated)\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; model = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K'); model('path/to/image.jpg', ['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8% to 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. These models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. They can be used for zero-shot image classification, image and text retrieval, and other tasks.\"}}", "category": "generic"}
{"question_id": 352, "text": " As a curious pet owner, I'd like to know what goes on around in my house when I'm not home, so I'd like to automatically classify creatures captured in my security camera.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup')\", \"api_arguments\": \"image, class_names\", \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"results = classifier(image, class_names='cat, dog')\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"79.1-79.4\"}, \"description\": \"A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 353, "text": " We are working on an AI-powered animal encyclopedia. Can you create a model for identifying unknown animal species in a given image?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w-laion2B-s13B-b82K\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\", \"api_arguments\": {\"image_path\": \"path to the image file\", \"labels\": \"list of possible class names\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K'); clip('path/to/image.jpg', ['cat', 'dog'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8 - 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}", "category": "generic"}
{"question_id": 354, "text": " A party planner organizer needs to classify the uploaded images into categories like food, decoration, and fun activities.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augreg')\", \"api_arguments\": {\"image\": \"path_to_image\", \"candidate_labels\": [\"list\", \"of\", \"candidate\", \"labels\"]}, \"python_environment_requirements\": {\"transformers\": \">=4.13.0\"}, \"example_code\": \"classifier(image='path_to_image', candidate_labels=['cat', 'dog', 'car'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8 - 71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Base model (convnext_base) as the image tower, and the same text tower as the RN50x4 (depth 12, embed dim 640) model from OpenAI CLIP. The base models are trained at 256x256 image resolution and roughly match the RN50x4 models on FLOPs and activation counts.\"}}", "category": "generic"}
{"question_id": 355, "text": " We are a team of medical researchers developing an app to help people identify health issues based on images of their symptoms. We need a model to categorize symptom images.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Zero-Shot Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\", \"api_call\": \"pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\", \"api_arguments\": \"image, possible_class_names\", \"python_environment_requirements\": \"transformers, torch, torchvision\", \"example_code\": \"from transformers import pipeline\\nclip = pipeline('zero-shot-image-classification', model='microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\\nimage = 'path/to/image.png'\\npossible_class_names = ['class1', 'class2', 'class3']\\nresult = clip(image, possible_class_names)\", \"performance\": {\"dataset\": \"PMC-15M\", \"accuracy\": \"State of the art\"}, \"description\": \"BiomedCLIP is a biomedical vision-language foundation model pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations. It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering.\"}}", "category": "generic"}
{"question_id": 356, "text": " A platform for selling animal photos needs to know what animal is in each image so they can better categorize them.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\", \"api_arguments\": {\"image_path\": \"Path to the image\", \"class_names\": \"Comma-separated list of possible class names\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nimage_classification = pipeline('image-classification', model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\\nimage_path = 'path/to/image.jpg'\\nclass_names = 'dog, cat'\\nresult = image_classification(image_path, class_names)\\nprint(result)\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"70.8-71.7%\"}, \"description\": \"A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Base model (convnext_base) as the image tower, and the same text tower as the RN50x4 (depth 12, embed dim 640) model from OpenAI CLIP.\"}}", "category": "generic"}
{"question_id": 357, "text": " Our company is working on a project to identify different areas in satellite images. We want to use a zero-shot approach for classifying images.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"flax-community/clip-rsicd-v2\", \"api_call\": \"CLIPModel.from_pretrained('flax-community/clip-rsicd-v2')\", \"api_arguments\": {\"text\": [\"a photo of a residential area\", \"a photo of a playground\", \"a photo of a stadium\", \"a photo of a forest\", \"a photo of an airport\"], \"images\": \"image\", \"return_tensors\": \"pt\", \"padding\": \"True\"}, \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(flax-community/clip-rsicd-v2)\\nprocessor = CLIPProcessor.from_pretrained(flax-community/clip-rsicd-v2)\\nurl = https://raw.githubusercontent.com/arampacha/CLIP-rsicd/master/data/stadium_1.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nlabels = [residential area, playground, stadium, forest, airport]\\ninputs = processor(text=[fa photo of a {l} for l in labels], images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\\nfor l, p in zip(labels, probs[0]):\\n print(f{l:&lt;16} {p:.4f})\", \"performance\": {\"dataset\": {\"RSICD\": {\"original CLIP\": {\"k=1\": 0.572, \"k=3\": 0.745, \"k=5\": 0.837, \"k=10\": 0.939}, \"clip-rsicd-v2 (this model)\": {\"k=1\": 0.883, \"k=3\": 0.968, \"k=5\": 0.982, \"k=10\": 0.998}}}}, \"description\": \"This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.\"}}", "category": "generic"}
{"question_id": 358, "text": " A museum curator asked to develop an automated tool that will identify and categorize a collection of images.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"kakaobrain/align-base\", \"api_call\": \"AlignModel.from_pretrained('kakaobrain/align-base')\", \"api_arguments\": [\"text\", \"images\", \"return_tensors\"], \"python_environment_requirements\": [\"requests\", \"torch\", \"PIL\", \"transformers\"], \"example_code\": \"import requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AlignProcessor, AlignModel\\nprocessor = AlignProcessor.from_pretrained(kakaobrain/align-base)\\nmodel = AlignModel.from_pretrained(kakaobrain/align-base)\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ncandidate_labels = [an image of a cat, an image of a dog]\\ninputs = processor(text=candidate_labels, images=image, return_tensors=pt)\\nwith torch.no_grad():\\n outputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\\nprint(probs)\", \"performance\": {\"dataset\": \"COYO-700M\", \"accuracy\": \"on-par or outperforms Google ALIGN's reported metrics\"}, \"description\": \"The ALIGN model is a dual-encoder architecture with EfficientNet as its vision encoder and BERT as its text encoder. It learns to align visual and text representations with contrastive learning. This implementation is trained on the open source COYO dataset and can be used for zero-shot image classification and multi-modal embedding retrieval.\"}}", "category": "generic"}
{"question_id": 359, "text": " Let's say we are building a mobile-based health assistant and we want to recognize different fruits from images taken by the user. How can we classify the images?\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"tiny-random-CLIPSegModel\", \"api_call\": \"pipeline('zero-shot-image-classification', model='hf-tiny-model-private/tiny-random-CLIPSegModel')\", \"api_arguments\": {\"image\": \"File or URL\", \"class_names\": \"List of strings\"}, \"python_environment_requirements\": {\"transformers\": \">=4.13.0\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A tiny random CLIPSegModel for zero-shot image classification.\"}}", "category": "generic"}
{"question_id": 360, "text": " We are a fashion boutique, and we want to categorize the photos of dresses that we have in our inventory.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k\", \"api_call\": \"clip.load('timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k')\", \"api_arguments\": \"image, class_names\", \"python_environment_requirements\": \"huggingface_hub, openai, transformers\", \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"This model is a zero-shot image classification model based on OpenCLIP. It can be used for classifying images into various categories without any additional training.\"}}", "category": "generic"}
{"question_id": 361, "text": " I need to build a model to automatically classify images of insects into three categories: ant, bee, and butterfly.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')\", \"api_arguments\": {\"image_path\": \"./path/to/image.jpg\", \"class_names\": \"class1,class2,class3\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nclip = pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')\\nclip('./path/to/image.jpg', 'class1,class2,class3')\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"75.9%\"}, \"description\": \"A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on LAION-2B (english), a subset of LAION-5B, using OpenCLIP. The models are trained at 256x256 image resolution and achieve a 75.9 top-1 zero-shot accuracy on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 362, "text": " We run a social media app and want to automatically classify images uploaded by users.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind\", \"api_call\": \"CLIPModel.from_pretrained('laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind')\", \"api_arguments\": \"image, class_names\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; clip = pipeline('zero-shot-classification', model='laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-rewind'); clip(image, class_names=['cat', 'dog', 'fish'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"79.1 - 79.4\"}, \"description\": \"A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.\"}}", "category": "generic"}
{"question_id": 363, "text": " I am running an online store, and I would like to automatically categorize images of clothing items.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft\", \"api_call\": \"pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\", \"api_arguments\": {\"image_path\": \"Path to the image file\", \"class_names\": \"List of comma-separated class names\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; classifier = pipeline('image-classification', model='laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft'); classifier('path/to/image.jpg', ['class1', 'class2'])\", \"performance\": {\"dataset\": \"ImageNet-1k\", \"accuracy\": \"75.9-76.9%\"}, \"description\": \"A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.\"}}", "category": "generic"}
{"question_id": 364, "text": " Tell me how to check whether an image contains a list of animals in Chinese.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"OFA-Sys/chinese-clip-vit-base-patch16\", \"api_call\": \"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-base-patch16')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"OFA-Sys/chinese-clip-vit-base-patch16\"}, \"python_environment_requirements\": {\"transformers\": \"ChineseCLIPProcessor, ChineseCLIPModel\"}, \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-base-patch16)\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [\\u6770\\u5c3c\\u9f9f, \\u5999\\u86d9\\u79cd\\u5b50, \\u5c0f\\u706b\\u9f99, \\u76ae\\u5361\\u4e18]\\ninputs = processor(images=image, return_tensors=pt)\\nimage_features = model.get_image_features(**inputs)\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\ntext_features = model.get_text_features(**inputs)\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": {\"MUGE Text-to-Image Retrieval\": {\"accuracy\": {\"Zero-shot R@1\": 63.0, \"Zero-shot R@5\": 84.1, \"Zero-shot R@10\": 89.2, \"Finetune R@1\": 68.9, \"Finetune R@5\": 88.7, \"Finetune R@10\": 93.1}}, \"Flickr30K-CN Retrieval\": {\"accuracy\": {\"Zero-shot Text-to-Image R@1\": 71.2, \"Zero-shot Text-to-Image R@5\": 91.4, \"Zero-shot Text-to-Image R@10\": 95.5, \"Finetune Text-to-Image R@1\": 83.8, \"Finetune Text-to-Image R@5\": 96.9, \"Finetune Text-to-Image R@10\": 98.6, \"Zero-shot Image-to-Text R@1\": 81.6, \"Zero-shot Image-to-Text R@5\": 97.5, \"Zero-shot Image-to-Text R@10\": 98.8, \"Finetune Image-to-Text R@1\": 95.3, \"Finetune Image-to-Text R@5\": 99.7, \"Finetune Image-to-Text R@10\": 100.0}}, \"COCO-CN Retrieval\": {\"accuracy\": {\"Zero-shot Text-to-Image R@1\": 69.2, \"Zero-shot Text-to-Image R@5\": 89.9, \"Zero-shot Text-to-Image R@10\": 96.1, \"Finetune Text-to-Image R@1\": 81.5, \"Finetune Text-to-Image R@5\": 96.9, \"Finetune Text-to-Image R@10\": 99.1, \"Zero-shot Image-to-Text R@1\": 63.0, \"Zero-shot Image-to-Text R@5\": 86.6, \"Zero-shot Image-to-Text R@10\": 92.9, \"Finetune Image-to-Text R@1\": 83.5, \"Finetune Image-to-Text R@5\": 97.3, \"Finetune Image-to-Text R@10\": 99.2}}, \"Zero-shot Image Classification\": {\"accuracy\": {\"CIFAR10\": 96.0, \"CIFAR100\": 79.7, \"DTD\": 51.2, \"EuroSAT\": 52.0, \"FER\": 55.1, \"FGVC\": 26.2, \"KITTI\": 49.9, \"MNIST\": 79.4, \"PC\": 63.5, \"VOC\": 84.9}}}}, \"description\": \"Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder.\"}}", "category": "generic"}
{"question_id": 365, "text": " Our company wants to analyze images to determine the number of cats in the picture, please provide a solution for this in Korean.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"clip-vit-base-patch32-ko\", \"api_call\": \"pipeline('zero-shot-image-classification', model='Bingsu/clip-vit-base-patch32-ko')\", \"api_arguments\": {\"images\": \"url\", \"candidate_labels\": \"Array of strings\", \"hypothesis_template\": \"String\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"PIL\", \"requests\"], \"example_code\": \"from transformers import pipeline\\nrepo = 'Bingsu/clip-vit-base-patch32-ko'\\npipe = pipeline('zero-shot-image-classification', model=repo)\\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\\nresult = pipe(images=url, candidate_labels=['\\uace0\\uc591\\uc774 \\ud55c \\ub9c8\\ub9ac', '\\uace0\\uc591\\uc774 \\ub450 \\ub9c8\\ub9ac', '\\ubd84\\ud64d\\uc0c9 \\uc18c\\ud30c\\uc5d0 \\ub4dc\\ub7ec\\ub204\\uc6b4 \\uace0\\uc591\\uc774 \\uce5c\\uad6c\\ub4e4'], hypothesis_template='{}')\\nresult\", \"performance\": {\"dataset\": \"AIHUB\", \"accuracy\": \"Not provided\"}, \"description\": \"Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data.\"}}", "category": "generic"}
{"question_id": 366, "text": " We want to share an Image Classification model with your mobile app in AI photocomparison, enabling users to access different categories of breed.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Zero-Shot Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"OFA-Sys/chinese-clip-vit-large-patch14-336px\", \"api_call\": \"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14-336px')\", \"api_arguments\": {\"images\": \"image\", \"text\": \"texts\", \"return_tensors\": \"pt\", \"padding\": \"True\"}, \"python_environment_requirements\": [\"PIL\", \"requests\", \"transformers\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14-336px)\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [\\u6770\\u5c3c\\u9f9f, \\u5999\\u86d9\\u79cd\\u5b50, \\u5c0f\\u706b\\u9f99, \\u76ae\\u5361\\u4e18]\\ninputs = processor(images=image, return_tensors=pt)\\nimage_features = model.get_image_features(**inputs)\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\ntext_features = model.get_text_features(**inputs)\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": {\"CIFAR10\": 96.0, \"CIFAR100\": 79.75, \"DTD\": 51.2, \"EuroSAT\": 52.0, \"FER\": 55.1, \"FGVC\": 26.2, \"KITTI\": 49.9, \"MNIST\": 79.4, \"PC\": 63.5, \"VOC\": 84.9}, \"accuracy\": \"various\"}, \"description\": \"Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-L/14@336px as the image encoder and RoBERTa-wwm-base as the text encoder.\"}}", "category": "generic"}
{"question_id": 367, "text": " I am building an AI sales manager, and I would like to know how happy customers feel about our products from their reviews.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"sentiment_analysis_generic_dataset\", \"api_call\": \"pipeline('text-classification', model='Seethal/sentiment_analysis_generic_dataset')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"sentiment_analysis('I love this product!')\", \"performance\": {\"dataset\": \"generic_dataset\", \"accuracy\": \"Not specified\"}, \"description\": \"This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.\"}}", "category": "generic"}
{"question_id": 368, "text": " I'm an English teacher and I want to automatically classify the sentiment of a student's essay.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"distilbert-base-uncased-finetuned-sst-2-english\", \"api_call\": \"DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\", \"api_arguments\": [\"inputs\"], \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\\ninputs = tokenizer('Hello, my dog is cute', return_tensors='pt')\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\npredicted_class_id = logits.argmax().item()\\nmodel.config.id2label[predicted_class_id]\", \"performance\": {\"dataset\": \"glue\", \"accuracy\": 0.911}, \"description\": \"This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\"}}", "category": "generic"}
{"question_id": 369, "text": " We want to build a sentiment analysis system for a variety of social media posts like Twitter and Instagram.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"cardiffnlp/twitter-roberta-base-sentiment\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained(MODEL)\", \"api_arguments\": [\"MODEL\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoModelForSequenceClassification\\nfrom transformers import TFAutoModelForSequenceClassification\\nfrom transformers import AutoTokenizer\\nimport numpy as np\\nfrom scipy.special import softmax\\nimport csv\\nimport urllib.request\\ndef preprocess(text):\\n new_text = []\\nfor t in text.split( ):\\n t = '@user' if t.startswith('@') and len(t) > 1 else t\\n t = 'http' if t.startswith('http') else t\\n new_text.append(t)\\nreturn  .join(new_text)\\ntask='sentiment'\\nMODEL = fcardiffnlp/twitter-roberta-base-{task}\\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\\nlabels=[]\\nmapping_link = fhttps://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\\nwith urllib.request.urlopen(mapping_link) as f:\\n html = f.read().decode('utf-8').split(\\\\n)\\n csvreader = csv.reader(html, delimiter='\\\\t')\\nlabels = [row[1] for row in csvreader if len(row) > 1]\\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\\nmodel.save_pretrained(MODEL)\\ntext = Good night \\ud83d\\ude0a\\ntext = preprocess(text)\\nencoded_input = tokenizer(text, return_tensors='pt')\\noutput = model(**encoded_input)\\nscores = output[0][0].detach().numpy()\\nscores = softmax(scores)\\nranking = np.argsort(scores)\\nranking = ranking[::-1]\\nfor i in range(scores.shape[0]):\\n l = labels[ranking[i]]\\n s = scores[ranking[i]]\\n print(f{i+1}) {l} {np.round(float(s), 4)})\", \"performance\": {\"dataset\": \"tweet_eval\", \"accuracy\": \"Not provided\"}, \"description\": \"Twitter-roBERTa-base for Sentiment Analysis. This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English.\"}}", "category": "generic"}
{"question_id": 370, "text": " I want to build a conversational agent that is able to detect the sentiment of text messages regardless of the language.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\", \"api_call\": \"sentiment_task = pipeline(sentiment-analysis, model=XLMRobertaForSequenceClassification.from_pretrained('cardiffnlp/twitter-xlm-roberta-base-sentiment'), tokenizer=XLMRobertaTokenizer.from_pretrained('cardiffnlp/twitter-xlm-roberta-base-sentiment'))\", \"api_arguments\": [\"model_path\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nmodel_path = cardiffnlp/twitter-xlm-roberta-base-sentiment\\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\\nsentiment_task(T'estimo!)\", \"performance\": {\"dataset\": \"Twitter\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\"}}", "category": "generic"}
{"question_id": 371, "text": " We are dealing with a project related to self-driving cars. In order to improve the navigation, we need to classify the location of an image.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Zero-Shot Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Image Geolocalization\", \"api_name\": \"geolocal/StreetCLIP\", \"api_call\": \"CLIPModel.from_pretrained('geolocal/StreetCLIP')\", \"api_arguments\": {\"pretrained_model_name_or_path\": \"geolocal/StreetCLIP\"}, \"python_environment_requirements\": [\"transformers\", \"PIL\", \"requests\"], \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import CLIPProcessor, CLIPModel\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobs = logits_per_image.softmax(dim=1)\", \"performance\": {\"dataset\": [{\"name\": \"IM2GPS\", \"accuracy\": {\"25km\": 28.3, \"200km\": 45.1, \"750km\": 74.7, \"2500km\": 88.2}}, {\"name\": \"IM2GPS3K\", \"accuracy\": {\"25km\": 22.4, \"200km\": 37.4, \"750km\": 61.3, \"2500km\": 80.4}}]}, \"description\": \"StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\"}}", "category": "generic"}
{"question_id": 372, "text": " Develop a Chinese language prototype to classify images into various categories such as Pok\\u00e9mon, plants, and animals.\\n \n Use this API documentation for reference:  {\"domain\": \"Computer Vision Zero-Shor Image Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Zero-Shot Image Classification\", \"api_name\": \"chinese-clip-vit-large-patch14\", \"api_call\": \"ChineseCLIPModel.from_pretrained('OFA-Sys/chinese-clip-vit-large-patch14')\", \"api_arguments\": {\"model_name\": \"OFA-Sys/chinese-clip-vit-large-patch14\"}, \"python_environment_requirements\": {\"libraries\": [\"transformers\", \"PIL\", \"requests\"]}, \"example_code\": \"from PIL import Image\\nimport requests\\nfrom transformers import ChineseCLIPProcessor, ChineseCLIPModel\\nmodel = ChineseCLIPModel.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\nprocessor = ChineseCLIPProcessor.from_pretrained(OFA-Sys/chinese-clip-vit-large-patch14)\\nurl = https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\\nimage = Image.open(requests.get(url, stream=True).raw)\\ntexts = [\\u6770\\u5c3c\\u9f9f, \\u5999\\u86d9\\u79cd\\u5b50, \\u5c0f\\u706b\\u9f99, \\u76ae\\u5361\\u4e18]\\ninputs = processor(images=image, return_tensors=pt)\\nimage_features = model.get_image_features(**inputs)\\nimage_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # normalize\\ninputs = processor(text=texts, padding=True, return_tensors=pt)\\ntext_features = model.get_text_features(**inputs)\\ntext_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) # normalize\\ninputs = processor(text=texts, images=image, return_tensors=pt, padding=True)\\noutputs = model(**inputs)\\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\\nprobs = logits_per_image.softmax(dim=1) # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]\", \"performance\": {\"dataset\": \"MUGE Text-to-Image Retrieval, Flickr30K-CN Retrieval, COCO-CN Retrieval, CIFAR10, CIFAR100, DTD, EuroSAT, FER, FGV, KITTI, MNIST, PASCAL VOC\", \"accuracy\": \"Varies depending on the dataset\"}, \"description\": \"Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.\"}}", "category": "generic"}
{"question_id": 373, "text": " Analyze a text and determine the language it is written in.\\n###Input: {\\\"text\\\": \\\"C'est incroyable \\u00e0 quel point les langues peuvent \\u00eatre diff\\u00e9rentes les unes des autres.\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Language Detection\", \"api_name\": \"papluca/xlm-roberta-base-language-detection\", \"api_call\": \"pipeline('text-classification', model='papluca/xlm-roberta-base-language-detection')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"language_detection('Hello, how are you?')\", \"performance\": {\"dataset\": \"Language Identification\", \"accuracy\": 0.996}, \"description\": \"This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset. It is an XLM-RoBERTa transformer model with a classification head on top, and can be used as a language detector for sequence classification tasks. It supports 20 languages including Arabic, Bulgarian, German, Greek, English, Spanish, French, Hindi, Italian, Japanese, Dutch, Polish, Portuguese, Russian, Swahili, Thai, Turkish, Urdu, Vietnamese, and Chinese.\"}}", "category": "generic"}
{"question_id": 374, "text": " We want to process financial news headlines before investing our money. What is your advice?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"financial-sentiment-analysis\", \"api_name\": \"ProsusAI/finbert\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline; classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert'); classifier('your_text_here')\", \"performance\": {\"dataset\": \"Financial PhraseBank\", \"accuracy\": \"Not provided\"}, \"description\": \"FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\"}}", "category": "generic"}
{"question_id": 375, "text": " Our bank is relying on AI technology to determine the sentiment of customer communication with companies. We need a model to automate this.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"financial-sentiment-analysis\", \"api_name\": \"yiyanghkust/finbert-tone\", \"api_call\": \"Output: BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\", \"api_arguments\": [\"sentences\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import BertTokenizer, BertForSequenceClassification\\nfrom transformers import pipeline\\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\nsentences = [there is a shortage of capital, and we need extra financing,\\n growth is strong and we have plenty of liquidity,\\n there are doubts about our finances,\\n profits are flat]\\nresults = nlp(sentences)\\nprint(results)\", \"performance\": {\"dataset\": \"10,000 manually annotated sentences from analyst reports\", \"accuracy\": \"superior performance on financial tone analysis task\"}, \"description\": \"FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\"}}", "category": "generic"}
{"question_id": 376, "text": " We are working on a smart assistant that analyzes social media content for public relations purposes. We would like to detect the sentiment of a given tweet.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"api_call\": \"sentiment_task = pipeline(sentiment-analysis, model=AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest'), tokenizer=AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest'))\", \"api_arguments\": {\"model\": \"model_path\", \"tokenizer\": \"model_path\"}, \"python_environment_requirements\": [\"transformers\", \"numpy\", \"scipy\"], \"example_code\": \"from transformers import pipeline\\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\\nsentiment_task(Covid cases are increasing fast!)\", \"performance\": {\"dataset\": \"tweet_eval\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. The model is suitable for English.\"}}", "category": "generic"}
{"question_id": 377, "text": " My company is developing a customer support chatbot. We need to identify and respond to customer emotions in their text queries.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Emotion Classification\", \"api_name\": \"j-hartmann/emotion-english-distilroberta-base\", \"api_call\": \"text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True\", \"api_arguments\": {\"text\": \"string\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": \"from transformers import pipeline\\nclassifier = pipeline(text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True)\\nclassifier(I love this!)\", \"performance\": {\"dataset\": \"Balanced subset from 6 diverse datasets\", \"accuracy\": \"66%\"}, \"description\": \"This model classifies emotions in English text data. It predicts Ekman's 6 basic emotions, plus a neutral class: anger, disgust, fear, joy, neutral, sadness, and surprise. The model is a fine-tuned checkpoint of DistilRoBERTa-base.\"}}", "category": "generic"}
{"question_id": 378, "text": " We are writing a blog post about AI-generated news and need a way to identify if the text was produced by GPT-2.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Detect GPT-2 generated text\", \"api_name\": \"roberta-base-openai-detector\", \"api_call\": \"pipeline('text-classification', model='roberta-base-openai-detector')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\nprint(pipe(Hello world! Is this content AI-generated?))\", \"performance\": {\"dataset\": \"WebText\", \"accuracy\": \"95%\"}, \"description\": \"RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.\"}}", "category": "generic"}
{"question_id": 379, "text": " I am building an application that refines users' language. It should improve the adequacy of sentences.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"prithivida/parrot_adequacy_model\", \"api_call\": \"pipeline('text-classification', model='prithivida/parrot_adequacy_model')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. This model is an ancillary model for Parrot paraphraser.\"}}", "category": "generic"}
{"question_id": 380, "text": " Write a review about a movie and get the sentiment of the review.\\n###Input: \\\"Interstellar is a stunning sci-fi masterpiece with visually spectacular special effects and an emotionally engaging storyline.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"bert-base-multilingual-uncased-sentiment\", \"api_call\": \"pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"result = sentiment_pipeline('I love this product!')\", \"performance\": {\"dataset\": [{\"language\": \"English\", \"accuracy\": {\"exact\": \"67%\", \"off-by-1\": \"95%\"}}, {\"language\": \"Dutch\", \"accuracy\": {\"exact\": \"57%\", \"off-by-1\": \"93%\"}}, {\"language\": \"German\", \"accuracy\": {\"exact\": \"61%\", \"off-by-1\": \"94%\"}}, {\"language\": \"French\", \"accuracy\": {\"exact\": \"59%\", \"off-by-1\": \"94%\"}}, {\"language\": \"Italian\", \"accuracy\": {\"exact\": \"59%\", \"off-by-1\": \"95%\"}}, {\"language\": \"Spanish\", \"accuracy\": {\"exact\": \"58%\", \"off-by-1\": \"95%\"}}]}, \"description\": \"This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\"}}", "category": "generic"}
{"question_id": 381, "text": " The company wants to analyze the public's sentiment about a specific stock through social media comments.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Inferencing for stock-related comments\", \"api_name\": \"zhayunduo/roberta-base-stocktwits-finetuned\", \"api_call\": \"Output: RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\", \"api_arguments\": {\"model\": \"RobertaForSequenceClassification\", \"tokenizer\": \"RobertaTokenizer\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\nfrom transformers import pipeline\\nimport pandas as pd\\nimport emoji\\ntokenizer_loaded = RobertaTokenizer.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained('zhayunduo/roberta-base-stocktwits-finetuned')\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\nsentences = pd.Series(['just buy','just sell it','entity rocket to the sky!','go down','even though it is going up, I still think it will not keep this trend in the near future'])\\nsentences = list(sentences)\\nresults = nlp(sentences)\\nprint(results)\", \"performance\": {\"dataset\": \"stocktwits\", \"accuracy\": 0.9343}, \"description\": \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags 'Bullish' or 'Bearish'.\"}}", "category": "generic"}
{"question_id": 382, "text": " We are creating an app to analyze emotions of given inputs. We need to classify the emotion of a text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"emotion\", \"api_name\": \"bhadresh-savani/distilbert-base-uncased-emotion\", \"api_call\": \"pipeline('text-classification', model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"prediction = classifier('I love using transformers. The best part is wide range of support and its easy to use')\", \"performance\": {\"dataset\": \"Twitter-Sentiment-Analysis\", \"accuracy\": 0.938}, \"description\": \"Distilbert is created with knowledge distillation during the pre-training phase which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding. It's smaller, faster than Bert and any other Bert-based model. Distilbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer.\"}}", "category": "generic"}
{"question_id": 383, "text": " As an opinion writer, my users posted comments to my Twitter account. I want to understand the sentiment from those tweets.\\n###Input: Estoy de acuerdo con tu opini\\u00f3n. Los cambios en las pol\\u00edticas deber\\u00edan ser planeados cuidadosamente.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"finiteautomata/beto-sentiment-analysis\", \"api_call\": \"pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"Hugging Face Transformers library\", \"example_code\": \"\", \"performance\": {\"dataset\": \"TASS 2020 corpus\", \"accuracy\": \"\"}, \"description\": \"Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\"}}", "category": "generic"}
{"question_id": 384, "text": " Design an online customer query support system which can retrieve the most relevant answer from a set of possible answers when a customer asks a specific question.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Information Retrieval\", \"api_name\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\", \"api_arguments\": {\"model_name\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\"}, \"python_environment_requirements\": {\"transformers\": \"latest\", \"torch\": \"latest\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\nimport torch\\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\\ntokenizer = AutoTokenizer.from_pretrained('model_name')\\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\\nmodel.eval()\\nwith torch.no_grad():\\n scores = model(**features).logits\\n print(scores)\", \"performance\": {\"dataset\": \"MS Marco Passage Reranking\", \"accuracy\": \"MRR@10: 39.01%\"}, \"description\": \"This model was trained on the MS Marco Passage Ranking task and can be used for Information Retrieval. Given a query, encode the query with all possible passages, then sort the passages in a decreasing order.\"}}", "category": "generic"}
{"question_id": 385, "text": " We're a company that collects customer reviews on our website. We want to analyze the sentiment of their comments.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"finiteautomata/bertweet-base-sentiment-analysis\", \"api_call\": \"pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\", \"api_arguments\": \"text\", \"python_environment_requirements\": \"Transformers\", \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('text-classification', model='finiteautomata/bertweet-base-sentiment-analysis')\\nresult = nlp('I love this movie!')\", \"performance\": {\"dataset\": \"SemEval 2017\", \"accuracy\": null}, \"description\": \"Model trained with SemEval 2017 corpus (around ~40k tweets). Base model is BERTweet, a RoBERTa model trained on English tweets. Uses POS, NEG, NEU labels.\"}}", "category": "generic"}
{"question_id": 386, "text": " I run a blog and I want to classify each comment by their sentiment, positive or negative.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"lvwerra/distilbert-imdb\", \"api_call\": \"pipeline('sentiment-analysis', model='lvwerra/distilbert-imdb')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\", \"pytorch\"], \"example_code\": \"classifier('I love this movie!')\", \"performance\": {\"dataset\": \"imdb\", \"accuracy\": 0.928}, \"description\": \"This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\"}}", "category": "generic"}
{"question_id": 387, "text": " In a new social media platform called \\\"ParrotTalk\\\", we need a function to paraphrase user messages keeping the fluency.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Paraphrase-based utterance augmentation\", \"api_name\": \"prithivida/parrot_fluency_model\", \"api_call\": \"pipeline('text-classification', model='prithivida/parrot_fluency_model')\", \"api_arguments\": \"text\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"parrot('your input text')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model.\"}}", "category": "generic"}
{"question_id": 388, "text": " I have a trivia app where users can upvote the most accurate answers. Can you build a device that helps me find the best answer according to the context given?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Information Retrieval\", \"api_name\": \"cross-encoder/ms-marco-MiniLM-L-12-v2\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('model_name')\", \"api_arguments\": {\"padding\": \"True\", \"truncation\": \"True\", \"return_tensors\": \"pt\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\", \"torch\": \"import torch\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\nimport torch\\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\\ntokenizer = AutoTokenizer.from_pretrained('model_name')\\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors=pt)\\nmodel.eval()\\nwith torch.no_grad():\\n scores = model(**features).logits\\n print(scores)\", \"performance\": {\"dataset\": {\"TREC Deep Learning 2019\": {\"NDCG@10\": 74.31}, \"MS Marco Passage Reranking\": {\"MRR@10\": 39.02, \"accuracy\": \"960 Docs / Sec\"}}}, \"description\": \"This model was trained on the MS Marco Passage Ranking task. The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco\"}}", "category": "generic"}
{"question_id": 389, "text": " We need to filter out inappropriate content or toxic comments from user feedback.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"martin-ha/toxic-comment-model\", \"api_call\": \"pipeline(model=AutoModelForSequenceClassification.from_pretrained('martin-ha/toxic-comment-model'), tokenizer=AutoTokenizer.from_pretrained('martin-ha/toxic-comment-model')).__call__('This is a test text.')\", \"api_arguments\": {\"model_path\": \"martin-ha/toxic-comment-model\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\nmodel_path = martin-ha/toxic-comment-model\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\nprint(pipeline('This is a test text.'))\", \"performance\": {\"dataset\": \"held-out test set\", \"accuracy\": 0.94, \"f1-score\": 0.59}, \"description\": \"This model is a fine-tuned version of the DistilBERT model to classify toxic comments.\"}}", "category": "generic"}
{"question_id": 390, "text": " I'm trying to determine if a list of German sentences are positive, negative, or neutral in sentiment.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"German Sentiment Classification\", \"api_name\": \"oliverguhr/german-sentiment-bert\", \"api_call\": \"Output: SentimentModel()\", \"api_arguments\": [\"texts\"], \"python_environment_requirements\": \"pip install germansentiment\", \"example_code\": [\"from germansentiment import SentimentModel\", \"model = SentimentModel()\", \"texts = [\", \" Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,\", \" Total awesome!,nicht so schlecht wie erwartet,\", \" Der Test verlief positiv.,Sie f\\u00e4hrt ein gr\\u00fcnes Auto.]\", \"result = model.predict_sentiment(texts)\", \"print(result)\"], \"performance\": {\"dataset\": [\"holidaycheck\", \"scare\", \"filmstarts\", \"germeval\", \"PotTS\", \"emotions\", \"sb10k\", \"Leipzig Wikipedia Corpus 2016\", \"all\"], \"accuracy\": [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, \"description\": \"This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.\"}}", "category": "generic"}
{"question_id": 391, "text": " Our company requires a tool to provide us with an understanding of users' sentiments in their reviews.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"siebert/sentiment-roberta-large-english\", \"api_call\": \"pipeline('sentiment-analysis', model='siebert/sentiment-roberta-large-english')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nsentiment_analysis = pipeline(sentiment-analysis, model=siebert/sentiment-roberta-large-english)\\nprint(sentiment_analysis(I love this!))\", \"performance\": {\"dataset\": [{\"name\": \"McAuley and Leskovec (2013) (Reviews)\", \"accuracy\": 98.0}, {\"name\": \"McAuley and Leskovec (2013) (Review Titles)\", \"accuracy\": 87.0}, {\"name\": \"Yelp Academic Dataset\", \"accuracy\": 96.5}, {\"name\": \"Maas et al. (2011)\", \"accuracy\": 96.0}, {\"name\": \"Kaggle\", \"accuracy\": 96.0}, {\"name\": \"Pang and Lee (2005)\", \"accuracy\": 91.0}, {\"name\": \"Nakov et al. (2013)\", \"accuracy\": 88.5}, {\"name\": \"Shamma (2009)\", \"accuracy\": 87.0}, {\"name\": \"Blitzer et al. (2007) (Books)\", \"accuracy\": 92.5}, {\"name\": \"Blitzer et al. (2007) (DVDs)\", \"accuracy\": 92.5}, {\"name\": \"Blitzer et al. (2007) (Electronics)\", \"accuracy\": 95.0}, {\"name\": \"Blitzer et al. (2007) (Kitchen devices)\", \"accuracy\": 98.5}, {\"name\": \"Pang et al. (2002)\", \"accuracy\": 95.5}, {\"name\": \"Speriosu et al. (2011)\", \"accuracy\": 85.5}, {\"name\": \"Hartmann et al. (2019)\", \"accuracy\": 98.0}], \"average_accuracy\": 93.2}, \"description\": \"This model ('SiEBERT', prefix for 'Sentiment in English') is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below.\"}}", "category": "generic"}
{"question_id": 392, "text": " We are a movie streaming platform, and we want a model to classify user reviews to better understand user emotions while watching movies.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"joeddav/distilbert-base-uncased-go-emotions-student\", \"api_call\": \"pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\", \"api_arguments\": \"text\", \"python_environment_requirements\": [\"transformers\", \"torch\", \"tensorflow\"], \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('text-classification', model='joeddav/distilbert-base-uncased-go-emotions-student')\\nresult = nlp('I am so happy today!')\", \"performance\": {\"dataset\": \"go_emotions\"}, \"description\": \"This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data.\"}}", "category": "generic"}
{"question_id": 393, "text": " Create a function to classify a given sentence as a question or statement to control the flow of conversation so that the assistant can respond accordingly.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Text Classification\", \"api_name\": \"shahrukhx01/question-vs-statement-classifier\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('shahrukhx01/question-vs-statement-classifier')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\\nmodel = AutoModelForSequenceClassification.from_pretrained(shahrukhx01/question-vs-statement-classifier)\", \"performance\": {\"dataset\": \"Haystack\", \"accuracy\": \"Not provided\"}, \"description\": \"Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack\"}}", "category": "generic"}
{"question_id": 394, "text": " Can you tell me if the following restaurant review has a positive or negative sentiment?\\n###Input: \\\"The food was absolutely amazing, and the service was top-notch. I will definitely be coming back to this place.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"results-yelp\", \"api_call\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained('bert-base-uncased')\", \"config\": \"AutoConfig.from_pretrained('potatobunny/results-yelp')\"}, \"python_environment_requirements\": {\"Transformers\": \"4.18.0\", \"Pytorch\": \"1.10.0+cu111\", \"Datasets\": \"2.0.0\", \"Tokenizers\": \"0.12.1\"}, \"example_code\": \"\", \"performance\": {\"dataset\": \"Yelp\", \"accuracy\": 0.9302}, \"description\": \"This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\"}}", "category": "generic"}
{"question_id": 395, "text": " Create a system that can identify if a piece of text is gibberish or not.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"madhurjindal/autonlp-Gibberish-Detector-492513457\", \"api_call\": \"Output: AutoModelForSequenceClassification.from_pretrained('madhurjindal/autonlp-Gibberish-Detector-492513457', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoNLP\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForSequenceClassification\", \"AutoTokenizer\": \"from_pretrained\"}, \"example_code\": \"from transformers import AutoModelForSequenceClassification, AutoTokenizer\\nmodel = AutoModelForSequenceClassification.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\ninputs = tokenizer(I love AutoNLP, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"madhurjindal/autonlp-data-Gibberish-Detector\", \"accuracy\": 0.9735624586913417}, \"description\": \"A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT.\"}}", "category": "generic"}
{"question_id": 396, "text": " One of our customers is managing a forum and we have to detect any offensive comments.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Transformers\", \"functionality\": \"Sentiment Analysis\", \"api_name\": \"michellejieli/NSFW_text_classifier\", \"api_call\": \"pipeline('sentiment-analysis', model='michellejieli/NSFW_text_classification')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"classifier(I see you\\u2019ve set aside this special time to humiliate yourself in public.)\", \"performance\": {\"dataset\": \"Reddit posts\", \"accuracy\": \"Not specified\"}, \"description\": \"DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\"}}", "category": "generic"}
{"question_id": 397, "text": " We are planning to build a chatbot and we need to know whether a piece of text represents a specific emotion.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"michellejieli/emotion_text_classifier\", \"api_call\": \"pipeline('sentiment-analysis', model='michellejieli/emotion_text_classifier')\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"classifier(I love this!)\", \"performance\": {\"dataset\": [\"Crowdflower (2016)\", \"Emotion Dataset, Elvis et al. (2018)\", \"GoEmotions, Demszky et al. (2020)\", \"ISEAR, Vikash (2018)\", \"MELD, Poria et al. (2019)\", \"SemEval-2018, EI-reg, Mohammad et al. (2018)\", \"Emotion Lines (Friends)\"], \"accuracy\": \"Not provided\"}, \"description\": \"DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\"}}", "category": "generic"}
{"question_id": 398, "text": " I need to choose the right answer from a set of options for a given question.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Text Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Information Retrieval\", \"api_name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\", \"api_call\": \"AutoModelForSequenceClassification.from_pretrained('model_name')\", \"api_arguments\": {\"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('model_name')\", \"features\": \"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\"}, \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": {\"import\": \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\", \"model\": \"model = AutoModelForSequenceClassification.from_pretrained('model_name')\", \"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained('model_name')\", \"features\": \"features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'], padding=True, truncation=True, return_tensors='pt')\", \"scores\": \"with torch.no_grad():\\n    scores = model(**features).logits\\n    print(scores)\"}, \"performance\": {\"dataset\": \"TREC Deep Learning 2019\", \"accuracy\": \"69.84 (NDCG@10)\"}, \"description\": \"This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\"}}", "category": "generic"}
{"question_id": 399, "text": " The theme of our research conference is the COVID-19 pandemic. We need to extract names of organizations and locations mentioned in the text of the submitted papers.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"dslim/bert-base-NER\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('dslim/bert-base-NER')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(dslim/bert-base-NER)\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\nfrom transformers import pipeline\\ntokenizer = AutoTokenizer.from_pretrained(dslim/bert-base-NER)\\nmodel = AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\nexample = My name is Wolfgang and I live in Berlin\\nner_results = nlp(example)\\nprint(ner_results)\", \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": {\"f1\": 91.3, \"precision\": 90.7, \"recall\": 91.9}}, \"description\": \"bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\"}}", "category": "generic"}
{"question_id": 400, "text": " We are a law firm, and we have many documents that contain personal information. Help us retrieve the names of the individuals involved.\\n###Input: My name is John Doe, and I am the attorney representing Jane Smith in her case against Acme Corporation.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-fast\", \"api_call\": \"SequenceTagger.load('flair/ner-english-fast')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": \"flair (pip install flair)\", \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\ntagger = SequenceTagger.load('flair/ner-english-fast')\\nsentence = Sentence('George Washington went to Washington')\\ntagger.predict(sentence)\\nprint(sentence)\\nfor entity in sentence.get_spans('ner'):\\n    print(entity)\", \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": \"F1-Score: 92.92\"}, \"description\": \"This is the fast 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 401, "text": " A doctor asked us to find all the biomedical entities in a patient's case report. \\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"d4data/biomedical-ner-all\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('d4data/biomedical-ner-all')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)\", \"aggregation_strategy\": \"simple\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\"}, \"example_code\": \"pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)\", \"performance\": {\"dataset\": \"Maccrobat\", \"accuracy\": \"Not provided\"}, \"description\": \"An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.\"}}", "category": "generic"}
{"question_id": 402, "text": " Our company need a text analysis system to separate different entities like person name, location, organization in french text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"Jean-Baptiste/camembert-ner\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/camembert-ner')\", \"api_arguments\": {\"model\": \"model\", \"tokenizer\": \"tokenizer\", \"aggregation_strategy\": \"simple\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForTokenClassification, pipeline\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\\nfrom transformers import pipeline\\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\\nnlp(Apple est cr\\u00e9\\u00e9e le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs \\u00e0 Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitu\\u00e9e sous forme de soci\\u00e9t\\u00e9 le 3 janvier 1977 \\u00e0 l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refl\\u00e9ter la diversification de ses produits, le mot \\u00ab computer \\u00bb est retir\\u00e9 le 9 janvier 2015.)\", \"performance\": {\"dataset\": \"wikiner-fr\", \"accuracy\": {\"overall_f1\": 0.8914, \"PER_f1\": 0.9483, \"ORG_f1\": 0.8181, \"LOC_f1\": 0.8955, \"MISC_f1\": 0.8146}}, \"description\": \"camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\"}}", "category": "generic"}
{"question_id": 403, "text": " A voice transcription program is generating strings of text. It would be great to have proper punctuation inserted within the text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"punctuation prediction\", \"api_name\": \"oliverguhr/fullstop-punctuation-multilang-large\", \"api_call\": \"Output: PunctuationModel()\", \"api_arguments\": [\"text\"], \"python_environment_requirements\": [\"pip install deepmultilingualpunctuation\"], \"example_code\": \"from deepmultilingualpunctuation import PunctuationModel\\nmodel = PunctuationModel()\\ntext = My name is Clara and I live in Berkeley California Ist das eine Frage Frau M\\u00fcller\\nresult = model.restore_punctuation(text)\\nprint(result)\", \"performance\": {\"dataset\": \"wmt/europarl\", \"EN_accuracy\": 0.775, \"DE_accuracy\": 0.814, \"FR_accuracy\": 0.782, \"IT_accuracy\": 0.762}, \"description\": \"This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :.\"}}", "category": "generic"}
{"question_id": 404, "text": " Develop a system that can recognize names, locations and organizations in text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"distilbert-base-multilingual-cased-ner-hrl\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl')\", \"api_arguments\": {\"model\": \"Davlan/distilbert-base-multilingual-cased-ner-hrl\", \"tokenizer\": \"Davlan/distilbert-base-multilingual-cased-ner-hrl\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\nfrom transformers import pipeline\\ntokenizer = AutoTokenizer.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\\nmodel = AutoModelForTokenClassification.from_pretrained(Davlan/distilbert-base-multilingual-cased-ner-hrl)\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\nexample = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\\nner_results = nlp(example)\\nprint(ner_results)\", \"performance\": {\"dataset\": [{\"name\": \"ANERcorp\", \"language\": \"Arabic\"}, {\"name\": \"conll 2003\", \"language\": \"German\"}, {\"name\": \"conll 2003\", \"language\": \"English\"}, {\"name\": \"conll 2002\", \"language\": \"Spanish\"}, {\"name\": \"Europeana Newspapers\", \"language\": \"French\"}, {\"name\": \"Italian I-CAB\", \"language\": \"Italian\"}, {\"name\": \"Latvian NER\", \"language\": \"Latvian\"}, {\"name\": \"conll 2002\", \"language\": \"Dutch\"}, {\"name\": \"Paramopama + Second Harem\", \"language\": \"Portuguese\"}, {\"name\": \"MSRA\", \"language\": \"Chinese\"}], \"accuracy\": \"Not specified\"}, \"description\": \"distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).\"}}", "category": "generic"}
{"question_id": 405, "text": " Summarize a news article and find the named entities (persons, organizations, and locations) mentioned in the summary.\\n###Input: International human rights organizations are urging Brazilian President Jair Bolsonaro to protect the rights of indigenous people in Brazil. The call comes ahead of the United Nations climate summit, where Bolsonaro has been widely criticized for his handling of indigenous issues and the Amazon rainforest.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"Davlan/bert-base-multilingual-cased-ner-hrl\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Davlan/bert-base-multilingual-cased-ner-hrl')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(Davlan/bert-base-multilingual-cased-ner-hrl)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(Davlan/bert-base-multilingual-cased-ner-hrl)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\"}, \"example_code\": \"example = Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.; ner_results = nlp(example); print(ner_results)\", \"performance\": {\"dataset\": {\"Arabic\": \"ANERcorp\", \"German\": \"conll 2003\", \"English\": \"conll 2003\", \"Spanish\": \"conll 2002\", \"French\": \"Europeana Newspapers\", \"Italian\": \"Italian I-CAB\", \"Latvian\": \"Latvian NER\", \"Dutch\": \"conll 2002\", \"Portuguese\": \"Paramopama + Second Harem\", \"Chinese\": \"MSRA\"}, \"accuracy\": \"Not provided\"}, \"description\": \"bert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned mBERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER).\"}}", "category": "generic"}
{"question_id": 406, "text": " We would like a system to identify the names of people and locations in the articles that we retrieve.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"dslim/bert-base-NER-uncased\", \"api_call\": \"pipeline('ner', model='dslim/bert-base-NER-uncased')\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"nlp('My name is John and I live in New York.')\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.\"}}", "category": "generic"}
{"question_id": 407, "text": " Protect user privacy! Remove sensitive data from a document using a trained model.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"De-identification\", \"api_name\": \"StanfordAIMI/stanford-deidentifier-base\", \"api_call\": \"pipeline('ner', model='StanfordAIMI/stanford-deidentifier-base')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"deidentifier('Your input text here')\", \"performance\": {\"dataset\": \"radreports\", \"accuracy\": {\"known_institution_F1\": 97.9, \"new_institution_F1\": 99.6, \"i2b2_2006_F1\": 99.5, \"i2b2_2014_F1\": 98.9}}, \"description\": \"Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.\"}}", "category": "generic"}
{"question_id": 408, "text": " In a dataset about mountain climbers, find the names of the climbers and the places they climbed in the text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Token Classification\", \"api_name\": \"xlm-roberta-large-finetuned-conll03-english\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('xlm-roberta-large-finetuned-conll03-english')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(xlm-roberta-large-finetuned-conll03-english)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(xlm-roberta-large-finetuned-conll03-english)\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"classifier(Hello I'm Omar and I live in Z\\u00fcrich.)\", \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": \"More information needed\"}, \"description\": \"The XLM-RoBERTa model is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data. This model is XLM-RoBERTa-large fine-tuned with the conll2003 dataset in English. It can be used for token classification tasks such as Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging.\"}}", "category": "generic"}
{"question_id": 409, "text": " As a researcher, I'm analyzing an interview for a divierstity study, and I want to extract locations, organizations, and personal names mentioned in the text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"dslim/bert-large-NER\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('dslim/bert-large-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('dslim/bert-large-NER')\"}, \"python_environment_requirements\": {\"transformers\": \"4.0.1\"}, \"example_code\": {\"example\": \"My name is Wolfgang and I live in Berlin\", \"ner_results\": \"nlp(example)\"}, \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": {\"f1\": 0.92, \"precision\": 0.92, \"recall\": 0.919}}, \"description\": \"bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\"}}", "category": "generic"}
{"question_id": 410, "text": " I need to extract all company names from a list of news articles.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Entity Extraction\", \"api_name\": \"903429548\", \"api_call\": \"Output: AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-company_all-903429548', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoTrain\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTokenClassification, AutoTokenizer\"}, \"example_code\": \"from transformers import AutoModelForTokenClassification, AutoTokenizer\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"ismail-lucifer011/autotrain-data-company_all\", \"accuracy\": 0.9979930566588805}, \"description\": \"A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.\"}}", "category": "generic"}
{"question_id": 411, "text": " I need to extract organizations' names from a given text. Please suggest a token classification model to perform this task.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Entity Extraction\", \"api_name\": \"904029577\", \"api_call\": \"Output: AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-name_all-904029577', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoTrain\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTokenClassification\", \"tokenizer\": \"AutoTokenizer\"}, \"example_code\": \"from transformers import AutoModelForTokenClassification, AutoTokenizer\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-name_all-904029577, use_auth_token=True)\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"ismail-lucifer011/autotrain-data-name_all\", \"accuracy\": 0.9989316041363876}, \"description\": \"This model is trained using AutoTrain for entity extraction. It is based on the DistilBert architecture and has a CO2 Emissions of 0.8375653425894861 grams.\"}}", "category": "generic"}
{"question_id": 412, "text": " I need to build a recommendation engine for a meal planning app. It should be able to analyze social media posts and extract food-related entities.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"Dizex/InstaFoodRoBERTa-NER\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Dizex/InstaFoodRoBERTa-NER')\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\"}, \"example_code\": \"example = Today's meal: Fresh olive pok\\u00e9 bowl topped with chia seeds. Very delicious!\\nner_entity_results = pipe(example, aggregation_strategy='simple')\\nprint(ner_entity_results)\", \"performance\": {\"dataset\": \"Dizex/InstaFoodSet\", \"accuracy\": {\"f1\": 0.91, \"precision\": 0.89, \"recall\": 0.93}}, \"description\": \"InstaFoodRoBERTa-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition of Food entities on informal text (social media like). It has been trained to recognize a single entity: food (FOOD). Specifically, this model is a roberta-base model that was fine-tuned on a dataset consisting of 400 English Instagram posts related to food.\"}}", "category": "generic"}
{"question_id": 413, "text": " Provide a tool for extracting the names of the people and the organizations mentioned in a given financial news article.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"Jean-Baptiste/roberta-large-ner-english\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/roberta-large-ner-english')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained('Jean-Baptiste/roberta-large-ner-english')\", \"tokenizer\": \"AutoTokenizer.from_pretrained('Jean-Baptiste/roberta-large-ner-english')\", \"aggregation_strategy\": \"simple\"}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"nlp(Apple was founded in 1976 by Steve Jobs, Steve Wozniak and Ronald Wayne to develop and sell Wozniak's Apple I personal computer)\", \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": {\"PER\": {\"precision\": 0.9914, \"recall\": 0.9927, \"f1\": 0.992}, \"ORG\": {\"precision\": 0.9627, \"recall\": 0.9661, \"f1\": 0.9644}, \"LOC\": {\"precision\": 0.9795, \"recall\": 0.9862, \"f1\": 0.9828}, \"MISC\": {\"precision\": 0.9292, \"recall\": 0.9262, \"f1\": 0.9277}, \"Overall\": {\"precision\": 0.974, \"recall\": 0.9766, \"f1\": 0.9753}}}, \"description\": \"roberta-large-ner-english is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. Model was validated on emails/chat data and outperformed other models on this type of data specifically. In particular, the model seems to work better on entities that don't start with an upper case.\"}}", "category": "generic"}
{"question_id": 414, "text": " Analyze the following text and identify the named entities associated with it: \\\"Yesterday, John Doe visited the Statue of Liberty and spent $30 on the trip.\\\"\\n###Input: \\\"Yesterday, John Doe visited the Statue of Liberty and spent $30 on the trip.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-ontonotes\", \"api_call\": \"Output: SequenceTagger.load(\\\\flair/ner-english-ontonotes\\\\)\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\ntagger = SequenceTagger.load(flair/ner-english-ontonotes)\\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\\ntagger.predict(sentence)\\nprint(sentence)\\nfor entity in sentence.get_spans('ner'):\\n    print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": \"89.27\"}, \"description\": \"This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 415, "text": " Our company is working on a customer support tool that automatically extracts relevant information mentioned by users in their messages. Analyze customer messages to extract entities.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Entity Extraction\", \"api_name\": \"903929564\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('ismail-lucifer011/autotrain-job_all-903929564', use_auth_token=True)\", \"api_arguments\": {\"inputs\": \"I love AutoTrain\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTokenClassification, AutoTokenizer\"}, \"example_code\": \"from transformers import AutoModelForTokenClassification, AutoTokenizer\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-job_all-903929564, use_auth_token=True)\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-job_all-903929564, use_auth_token=True)\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\noutputs = model(**inputs)\", \"performance\": {\"dataset\": \"ismail-lucifer011/autotrain-data-job_all\", \"accuracy\": 0.9989412009896035}, \"description\": \"A Token Classification model trained using AutoTrain for Entity Extraction. The model is based on distilbert and achieves high accuracy, precision, recall, and F1 score.\"}}", "category": "generic"}
{"question_id": 416, "text": " I am building an app that transcribes voice recordings into text. I want to add punctuation to these transcriptions to make it more readable.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"kredor/punctuate-all\", \"api_call\": \"pipeline('token-classification', model='kredor/punctuate-all')\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"multilingual\", \"accuracy\": 0.98}, \"description\": \"A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.\"}}", "category": "generic"}
{"question_id": 417, "text": " We are developing a system to identify the named entities in a given sentence.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Token Classification\", \"api_name\": \"dbmdz/bert-large-cased-finetuned-conll03-english\", \"api_call\": \"Output: AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\", \"api_arguments\": [\"model_name\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\\nmodel = AutoModelForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\", \"performance\": {\"dataset\": \"CoNLL-03\", \"accuracy\": \"Not provided\"}, \"description\": \"This is a BERT-large-cased model fine-tuned on the CoNLL-03 dataset for token classification tasks.\"}}", "category": "generic"}
{"question_id": 418, "text": " In a news article, you are asked to identify part-of-speech tags.\\n###Input: The economy is slowly bouncing back from the pandemic and creating more job opportunities.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Flair\", \"functionality\": \"Part-of-Speech Tagging\", \"api_name\": \"flair/upos-english\", \"api_call\": \"SequenceTagger.load('flair/upos-english')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": \"pip install flair\", \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\ntagger = SequenceTagger.load('flair/upos-english')\\nsentence = Sentence('I love Berlin.')\\ntagger.predict(sentence)\\nprint(sentence)\\nfor entity in sentence.get_spans('pos'):\\n    print(entity)\", \"performance\": {\"dataset\": \"ontonotes\", \"accuracy\": 98.6}, \"description\": \"This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 419, "text": " Implement a text classifier for Chinese text to identify tokens and sentences.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Token Classification\", \"api_name\": \"ckiplab/bert-base-chinese-ws\", \"api_call\": \"AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\", \"api_arguments\": {\"pretrained_model\": \"ckiplab/bert-base-chinese-ws\"}, \"python_environment_requirements\": {\"transformers\": \"BertTokenizerFast, AutoModel\"}, \"example_code\": \"from transformers import (\\n BertTokenizerFast,\\n AutoModel,\\n)\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\", \"performance\": {\"dataset\": \"Not specified\", \"accuracy\": \"Not specified\"}, \"description\": \"This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\"}}", "category": "generic"}
{"question_id": 420, "text": " Assume you are working with a Chinese literature company. They need your expertise to identify the parts of speech for traditional Chinese texts.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Part-of-speech tagging\", \"api_name\": \"ckiplab/bert-base-chinese-pos\", \"api_call\": \"AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\", \"api_arguments\": {\"tokenizer\": \"BertTokenizerFast.from_pretrained('bert-base-chinese')\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import (\\n  BertTokenizerFast,\\n  AutoModel,\\n)\\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\"}}", "category": "generic"}
{"question_id": 421, "text": " I am working on a news article summarization project, and I need to extract the named entities present in the articles.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-large\", \"api_call\": \"Output: SequenceTagger.load('flair/ner-english-large')\", \"api_arguments\": \"sentence\", \"python_environment_requirements\": \"Flair (pip install flair)\", \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\n# load tagger\\ntagger = SequenceTagger.load(flair/ner-english-large)\\n# make example sentence\\nsentence = Sentence(George Washington went to Washington)\\n# predict NER tags\\ntagger.predict(sentence)\\n# print sentence\\nprint(sentence)\\n# print predicted NER spans\\nprint('The following NER tags are found:')\\n# iterate over entities and print\\nfor entity in sentence.get_spans('ner'):\\n print(entity)\", \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": \"94.36\"}, \"description\": \"This is the large 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on document-level XLM-R embeddings and FLERT.\"}}", "category": "generic"}
{"question_id": 422, "text": " As a blog editor, I need to analyze the grammatical structure of a sentence to improve the quality.\\n###Input: \\\"He went to the grocery store to buy some vegetables.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Part-of-Speech Tagging\", \"api_name\": \"flair/pos-english\", \"api_call\": \"'tagger = SequenceTagger.load(flair/pos-english)'\", \"api_arguments\": \"sentence\", \"python_environment_requirements\": \"flair (pip install flair)\", \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\ntagger = SequenceTagger.load(flair/pos-english)\\nsentence = Sentence(I love Berlin.)\\ntagger.predict(sentence)\\nprint(sentence)\\nfor entity in sentence.get_spans('pos'):\\n print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": \"98.19\"}, \"description\": \"This is the standard part-of-speech tagging model for English that ships with Flair. It predicts fine-grained POS tags based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 423, "text": " Discover the named entities in an English text.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english\", \"api_call\": \"SequenceTagger.load('flair/ner-english')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\n# load tagger\\ntagger = SequenceTagger.load('flair/ner-english')\\n# make example sentence\\nsentence = Sentence('George Washington went to Washington')\\n# predict NER tags\\ntagger.predict(sentence)\\n# print sentence\\nprint(sentence)\\n# print predicted NER spans\\nprint('The following NER tags are found:')\\n# iterate over entities and print\\nfor entity in sentence.get_spans('ner'):\\n    print(entity)\", \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": \"93.06\"}, \"description\": \"This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 424, "text": " Detect named entities within a news article to understand what the main subjects are.\\n###Input: On July 15th, Joe Biden announced that the United States and countries in the European Union agreed to a series of measures to counter cyberattacks, mostly to target Russia.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-ontonotes-fast\", \"api_call\": \"Output: SequenceTagger.load('flair/ner-english-ontonotes-fast')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\ntagger = SequenceTagger.load(flair/ner-english-ontonotes-fast)\\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\\ntagger.predict(sentence)\\nprint(sentence)\\nfor entity in sentence.get_spans('ner'):\\n  print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": \"F1-Score: 89.3\"}, \"description\": \"This is the fast version of the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 425, "text": " Analyze the given sentence to figure out the role each word plays in the sentence grammatically.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Transformers\", \"functionality\": \"Token Classification\", \"api_name\": \"vblagoje/bert-english-uncased-finetuned-pos\", \"api_call\": \"vblagoje/bert-english-uncased-finetuned-pos\", \"api_arguments\": \"model\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"nlp('Hello world!')\", \"performance\": {\"dataset\": \"N/A\", \"accuracy\": \"N/A\"}, \"description\": \"A BERT model fine-tuned for Part-of-Speech (POS) tagging in English text.\"}}", "category": "generic"}
{"question_id": 426, "text": " As a German news agency, we want to extract names of persons, locations, and organizations from German articles.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-german\", \"api_call\": \"SequenceTagger.load('flair/ner-german')\", \"api_arguments\": [\"Sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\n# load tagger\\ntagger = SequenceTagger.load('flair/ner-german')\\n# make example sentence\\nsentence = Sentence('George Washington ging nach Washington')\\n# predict NER tags\\ntagger.predict(sentence)\\n# print sentence\\nprint(sentence)\\n# print predicted NER spans\\nprint('The following NER tags are found:')\\n# iterate over entities and print\\nfor entity in sentence.get_spans('ner'):\\n    print(entity)\", \"performance\": {\"dataset\": \"conll2003\", \"accuracy\": \"87.94\"}, \"description\": \"This is the standard 4-class NER model for German that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.\"}}", "category": "generic"}
{"question_id": 427, "text": " Our accounting department requires a pre-trained language model able to understand table structures for easy and rapid data interpretation.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/tapex-large\", \"api_call\": \"Output: AutoModelForSeq2SeqLM.from_pretrained('microsoft/tapex-large')\", \"api_arguments\": [\"model_name = 'microsoft/tapex-large'\", \"tokenizer = AutoTokenizer.from_pretrained(model_name)\", \"model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\"], \"python_environment_requirements\": [\"transformers\", \"torch\"], \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.\"}}", "category": "generic"}
{"question_id": 428, "text": " We are part of a research team that needs to analyze the relationship between specific data from a table and answer a given question.\\n###Input: \\\"<data_table>\\\": \\\"### Country & Population (2021)      \\\\n| Country      | Continent     | 2021 Population |                                               \\\\n|--------------|---------------|-----------------|                                               \\\\n| China        | Asia          | 1,444,216,107   |                                               \\\\n| India        | Asia          | 1,393,409,038   |                                               \\\\n| United States | North America | 332,915,073     |                                               \\\\n| Indonesia    | Asia          | 276,361,783     |\\\\n<question>\\\": \\\"What is the population of Indonesia in 2021?\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-base-finetuned-wtq\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\", \"api_arguments\": {\"model_name\": \"google/tapas-base-finetuned-wtq\"}, \"python_environment_requirements\": {\"transformers\": \"4.12.0\"}, \"example_code\": \"from transformers import TapasTokenizer, TapasForQuestionAnswering\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-base-finetuned-wtq')\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": 0.4638}, \"description\": \"TAPAS base model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion, and then fine-tuned on SQA, WikiSQL, and finally WTQ. It can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 429, "text": " Our client manages a school and would like to be able to find specific information within tables to quickly answer questions from parents and teachers.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-large-finetuned-wtq\", \"api_call\": \"pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\", \"api_arguments\": {\"model\": \"google/tapas-large-finetuned-wtq\", \"task\": \"table-question-answering\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\\nresult = qa_pipeline(table=table, query=query)\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": 0.5097}, \"description\": \"TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\"}}", "category": "generic"}
{"question_id": 430, "text": " We have a large dataset about sales and financial information. We want to extract the total revenue from the table.\\n###Input: table={[[\\\"Month\\\", \\\"Revenue\\\", \\\"Expense\\\"], [\\\"Jan\\\", 10000, 8000], [\\\"Feb\\\", 12000, 9000], [\\\"Mar\\\", 8000, 7000]]\\nquery='What is the total revenue?'\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-medium-finetuned-sqa\", \"api_call\": \"pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\", \"api_arguments\": \"table, query\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\n# Initialize the pipeline\\ntable_qa_pipeline = pipeline('table-question-answering', model='google/tapas-medium-finetuned-sqa')\\n# Provide the table and query\\nresult = table_qa_pipeline(table=table, query='What is the total revenue?')\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.6561}, \"description\": \"TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.\"}}", "category": "generic"}
{"question_id": 431, "text": " A teacher asks her students to create a small application that can answer questions about data stored in tables without writing customized code.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-small-finetuned-wtq\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wtq'), TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wtq')\", \"api_arguments\": \"model_name_or_path, table, query\", \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import TapasForQuestionAnswering, TapasTokenizer\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wtq')\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wtq')\\ninputs = tokenizer(table=table, queries=query, return_tensors='pt')\\noutputs = model(**inputs)\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": 0.3762}, \"description\": \"TAPAS small model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\"}}", "category": "generic"}
{"question_id": 432, "text": " We need a way to identify important proper nouns from user reviews on our website like names, cities, organizations, etc.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"Babelscape/wikineural-multilingual-ner\", \"api_call\": \"AutoModelForTokenClassification.from_pretrained('Babelscape/wikineural-multilingual-ner')\", \"api_arguments\": {\"model\": \"AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\", \"tokenizer\": \"AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\"}, \"python_environment_requirements\": {\"transformers\": \"from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\"}, \"example_code\": \"tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\nexample = My name is Wolfgang and I live in Berlin\\nner_results = nlp(example)\\nprint(ner_results)\", \"performance\": {\"dataset\": \"Babelscape/wikineural-multilingual-ner\", \"accuracy\": \"span-based F1-score up to 6 points over previous state-of-the-art systems for data creation\"}, \"description\": \"A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.\"}}", "category": "generic"}
{"question_id": 433, "text": " Identify the named entities in the given news article.\\n###Input: On August 21st, Apple Inc. announced their new product, the iPhone 13, will be released on September 14th. CEO Tim Cook led the event and highlighted the new features, including an improved camera and longer battery life. The starting price for the device is $699.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Token Classification\", \"framework\": \"Flair\", \"functionality\": \"Named Entity Recognition\", \"api_name\": \"flair/ner-english-ontonotes-large\", \"api_call\": \"SequenceTagger.load('flair/ner-english-ontonotes-large')\", \"api_arguments\": [\"sentence\"], \"python_environment_requirements\": [\"flair\"], \"example_code\": \"from flair.data import Sentence\\nfrom flair.models import SequenceTagger\\n# load tagger\\ntagger = SequenceTagger.load('flair/ner-english-ontonotes-large')\\n# make example sentence\\nsentence = Sentence('On September 1st George won 1 dollar while watching Game of Thrones.')\\n# predict NER tags\\ntagger.predict(sentence)\\n# print sentence\\nprint(sentence)\\n# print predicted NER spans\\nprint('The following NER tags are found:')\\n# iterate over entities and print\\nfor entity in sentence.get_spans('ner'):\\n    print(entity)\", \"performance\": {\"dataset\": \"Ontonotes\", \"accuracy\": 90.93}, \"description\": \"English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.\"}}", "category": "generic"}
{"question_id": 434, "text": " Design bedside nurse report for COVID-19 patients with easy comprehension. We need to provide an overview of a patient's condition to the nurse.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lysandre/tiny-tapas-random-sqa\", \"api_call\": \"TapasForCovid.from_pretrained('lysandre/tiny-tapas-random-sqa')\", \"api_arguments\": null, \"python_environment_requirements\": \"transformers\", \"example_code\": null, \"performance\": {\"dataset\": null, \"accuracy\": null}, \"description\": \"A tiny TAPAS model for table question answering tasks.\"}}", "category": "generic"}
{"question_id": 435, "text": " Create a system to help users find specific information from a given table of sales data.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-base-finetuned-sqa\", \"api_call\": \"TapasTokenizer.from_pretrained('google/tapas-base-finetuned-sqa')\", \"api_arguments\": [\"question\", \"table\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\n# Initialize the pipeline\\ntapas_pipeline = pipeline('table-question-answering', model='google/tapas-base-finetuned-sqa')\\n# Define the table and question\\nquestion = 'How many goals did player A score?'\\ntable = [['Player', 'Goals'], ['Player A', 5], ['Player B', 3]]\\n# Get the answer\\nresult = tapas_pipeline(question=question, table=table)\\nprint(result)\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.6874}, \"description\": \"TAPAS base model fine-tuned on Sequential Question Answering (SQA). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia and fine-tuned on SQA. It can be used for answering questions related to a table in a conversational set-up.\"}}", "category": "generic"}
{"question_id": 436, "text": " I have a large dataset of data in a table format. I want to analyze and retrieve the answers to the questions related to the data.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"lysandre/tiny-tapas-random-wtq\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('lysandre/tiny-tapas-random-wtq')\", \"api_arguments\": \"model = TapasForQuestionAnswering.from_pretrained('lysandre/tiny-tapas-random-wtq')\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": \"WTQ\", \"accuracy\": \"Not provided\"}, \"description\": \"A tiny TAPAS model trained on the WikiTableQuestions dataset for table question answering tasks.\"}}", "category": "generic"}
{"question_id": 437, "text": " As a company manager, I want a summary of yearly revenue and expenses in a table. Then, I want the model to tell me in which year our company had the highest revenue.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/tapex-base\", \"api_call\": \"Output: BartForConditionalGeneration.from_pretrained('microsoft/tapex-base')\", \"api_arguments\": {\"table\": \"pd.DataFrame\", \"query\": \"str\"}, \"python_environment_requirements\": {\"libraries\": [\"transformers\", \"pandas\"]}, \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = select year where city = beijing\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"arxiv:2107.07653\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.\"}}", "category": "generic"}
{"question_id": 438, "text": " We need to enable the ability to answer queries about events in a historical timeline table.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/tapex-base-finetuned-wtq\", \"api_call\": \"BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wtq')\", \"api_arguments\": {\"tokenizer\": \"TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)\", \"model\": \"BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)\", \"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"query\"}, \"python_environment_requirements\": [\"transformers\", \"pandas\"], \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wtq)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wtq)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.\"}}", "category": "generic"}
{"question_id": 439, "text": " We are creating a software for reading tabular data and answering questions based on it. Let's include the relevant API calls.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/tapex-large-finetuned-wtq\", \"api_call\": \"BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wtq')\", \"api_arguments\": {\"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"str\"}, \"python_environment_requirements\": [\"transformers\", \"pandas\"], \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-finetuned-wtq)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-finetuned-wtq)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiTableQuestions dataset.\"}}", "category": "generic"}
{"question_id": 440, "text": " Compare the number of COVID-19 cases across different countries and figure out which country has the highest number of cases.\\n###Input: Singapore has 124653 cases. South Korea has 199470. Vietnam has 12791. New Zealand has 1670.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"dsba-lab/koreapas-finetuned-korwikitq\", \"api_call\": \"pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq')\", \"api_arguments\": {}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"from transformers import pipeline; table_qa = pipeline('table-question-answering', model='dsba-lab/koreapas-finetuned-korwikitq'); table_qa(table=table, query='\\uc9c8\\ubb38')\", \"performance\": {\"dataset\": \"korwikitq\", \"accuracy\": null}, \"description\": \"A Korean Table Question Answering model finetuned on the korwikitq dataset.\"}}", "category": "generic"}
{"question_id": 441, "text": " We have a database with employee information, and we need to know the net salary of an employee named \\\"John Doe.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"microsoft/tapex-large-finetuned-wikisql\", \"api_call\": \"Output: BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-finetuned-wikisql')\", \"api_arguments\": {\"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"query\"}, \"python_environment_requirements\": {\"transformers\": \"TapexTokenizer, BartForConditionalGeneration\", \"pandas\": \"pd\"}, \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"N/A\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiSQL dataset.\"}}", "category": "generic"}
{"question_id": 442, "text": " We have a dataset containing the cities and years of events, I need to find out the year when the event took place in Beijing.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/tapex-large-sql-execution\", \"api_call\": \"BartForConditionalGeneration.from_pretrained('microsoft/tapex-large-sql-execution')\", \"api_arguments\": {\"table\": \"pd.DataFrame\", \"query\": \"str\"}, \"python_environment_requirements\": [\"transformers\", \"pandas\"], \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-sql-execution)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-sql-execution)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = select year where city = beijing\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"synthetic corpus\", \"accuracy\": \"not specified\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.\"}}", "category": "generic"}
{"question_id": 443, "text": " Our executive is going on a business trip to Europe. Find the capital city of France for them.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-large-finetuned-wikisql-supervised\", \"api_call\": \"pipeline('table-question-answering', model='google/tapas-large-finetuned-wikisql-supervised')\", \"api_arguments\": {\"model\": \"google/tapas-large-finetuned-wikisql-supervised\"}, \"python_environment_requirements\": [\"transformers\", \"torch\", \"tensorflow\"], \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wikisql-supervised')\\nresult = qa_pipeline(question='What is the capital of France?', table=table)\\nprint(result)\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 444, "text": " Our marketing team has a table with sales data for each month. They would like to know the total revenue for February.\\n###Input: \\n\\\"question\\\": \\\"What is the total revenue for February?\\\",\\n\\\"table\\\": {\\n  \\\"header\\\": [\\\"Month\\\", \\\"Sales\\\", \\\"Revenue\\\"],\\n  \\\"data\\\": [\\n    [\\\"January\\\", 1500, 15000],\\n    [\\\"February\\\", 1800, 18000],\\n    [\\\"March\\\", 2000, 23000],\\n    [\\\"April\\\", 2500, 25000],\\n    [\\\"May\\\", 2250, 24000]\\n  ]\\n}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-large-finetuned-sqa\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-sqa')\", \"api_arguments\": [\"question\", \"table\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"https://huggingface.co/google/tapas-large-finetuned-sqa\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.7289}, \"description\": \"TAPAS large model fine-tuned on Sequential Question Answering (SQA). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\"}}", "category": "generic"}
{"question_id": 445, "text": " I want to have a question-answering system for solving queries present in tabular data to make quick decisions.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-small-finetuned-wikisql-supervised\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\", \"api_arguments\": \"model = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import TapasTokenizer, TapasForQuestionAnswering\\ntokenizer = TapasTokenizer.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\\nmodel = TapasForQuestionAnswering.from_pretrained('google/tapas-small-finetuned-wikisql-supervised')\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"Not specified\"}, \"description\": \"TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 446, "text": " We have a large dataset of tables and we need a suitable model to query it in natural language.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"table-question-answering-tapas\", \"api_call\": \"pipeline('table-question-answering', model='Meena/table-question-answering-tapas').model\", \"api_arguments\": [], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"This model can be loaded on the Inference API on-demand.\", \"performance\": {\"dataset\": [{\"name\": \"SQA (Sequential Question Answering by Microsoft)\", \"accuracy\": null}, {\"name\": \"WTQ (Wiki Table Questions by Stanford University)\", \"accuracy\": null}, {\"name\": \"WikiSQL (by Salesforce)\", \"accuracy\": null}]}, \"description\": \"TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\"}}", "category": "generic"}
{"question_id": 447, "text": " Our team needs to work on an app that can answer questions about online shopping items based on tables. Provide assistance.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-mini-finetuned-wtq\", \"api_call\": \"AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\", \"api_arguments\": \"tokenizer = AutoTokenizer.from_pretrained('google/tapas-mini-finetuned-wtq'); model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-wtq')\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nnlp = pipeline('table-question-answering', model='google/tapas-mini-finetuned-wtq', tokenizer='google/tapas-mini-finetuned-wtq')\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": 0.2854}, \"description\": \"TAPAS mini model fine-tuned on WikiTable Questions (WTQ). It is pretrained on a large corpus of English data from Wikipedia and can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 448, "text": " My company needs to answer questions about sales data on a monthly basis.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"lysandre/tapas-temporary-repo\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\", \"api_arguments\": [\"model_name\", \"question\", \"table\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"tokenizer = TapasTokenizer.from_pretrained('lysandre/tapas-temporary-repo')\\nmodel = TapasForQuestionAnswering.from_pretrained('lysandre/tapas-temporary-repo')\\ninputs = tokenizer(table=table, queries=question, return_tensors='pt')\\noutputs = model(**inputs)\\npredicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(inputs, outputs.logits.detach(), outputs.logits_aggregation.detach())\", \"performance\": {\"dataset\": \"SQA\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS base model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion and can be used for answering questions related to a table in a conversational set-up.\"}}", "category": "generic"}
{"question_id": 449, "text": " I want to create a tool that takes a table with data about Olympic Games years and cities and allows me to ask questions like, \\\"In which year did Beijing host the Olympic Games?\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table-based QA\", \"api_name\": \"neulab/omnitab-large\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large')\", \"api_arguments\": {\"table\": \"pd.DataFrame\", \"query\": \"str\"}, \"python_environment_requirements\": {\"transformers\": \"AutoTokenizer, AutoModelForSeq2SeqLM\", \"pandas\": \"pd\"}, \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nimport pandas as pd\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": null}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data.\"}}", "category": "generic"}
{"question_id": 450, "text": " We work for the tourism department and have access to a data table with information about Olympic Games host cities. We want to find out when Beijing hosted the Olympics.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Transformers\", \"api_name\": \"microsoft/tapex-base-finetuned-wikisql\", \"api_call\": \"BartForConditionalGeneration.from_pretrained('microsoft/tapex-base-finetuned-wikisql')\", \"api_arguments\": {\"tokenizer\": \"TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wikisql)\", \"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"str\"}, \"python_environment_requirements\": [\"transformers\", \"pandas\"], \"example_code\": \"from transformers import TapexTokenizer, BartForConditionalGeneration\\nimport pandas as pd\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-base-finetuned-wikisql)\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-base-finetuned-wikisql)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikisql\"}, \"description\": \"TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.\"}}", "category": "generic"}
{"question_id": 451, "text": " For our city, we want to create a database of public facilities and answer questions related to these facilities.\\n###Input: {\\\"table\\\": \\\"Name,Type,Address,Operating Hours\\nPark,Public Park,123 Park St,8am-10pm\\nLibrary,Public Library,456 Library Rd,9am-9pm\\\", \\\"queries\\\": \\\"What type of facility is at address 123 Park St?\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-mini-finetuned-sqa\", \"api_call\": \"TapasForQuestionAnswering.from_pretrained('google/tapas-mini-finetuned-sqa')\", \"api_arguments\": [\"model_name\", \"table\", \"queries\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"N/A\", \"performance\": {\"dataset\": \"msr_sqa\", \"accuracy\": 0.5148}, \"description\": \"TAPAS mini model fine-tuned on Sequential Question Answering (SQA)\"}}", "category": "generic"}
{"question_id": 452, "text": " I have a table of sales reports of my company's different departments. I want to have the exact number of sales of the Software department in this table.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"google/tapas-medium-finetuned-wtq\", \"api_call\": \"pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\", \"api_arguments\": \"table, query\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\n# Initialize the pipeline\\ntable_qa = pipeline('table-question-answering', model='google/tapas-medium-finetuned-wtq')\\n# Define the table and the query\\ntable = {...}\\nquery = '...'\\n# Get the answer\\nanswer = table_qa(table=table, query=query)\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": 0.4324}, \"description\": \"TAPAS medium model fine-tuned on WikiTable Questions (WTQ). This model is pretrained on a large corpus of English data from Wikipedia and is used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 453, "text": " Can you help me with a code that will extract information from a data table about Olympic Games host cities and years? \\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Table-based QA\", \"api_name\": \"neulab/omnitab-large-1024shot\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot')\", \"api_arguments\": {\"table\": \"pd.DataFrame.from_dict(data)\", \"query\": \"str\"}, \"python_environment_requirements\": [\"transformers\", \"pandas\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nimport pandas as pd\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting).\"}}", "category": "generic"}
{"question_id": 454, "text": " Help me find the answer to my questions about Python libraries.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/roberta-base-squad2\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\", \"api_arguments\": {\"model_name\": \"deepset/roberta-base-squad2\", \"tokenizer\": \"deepset/roberta-base-squad2\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": {\"code\": \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\nmodel_name = deepset/roberta-base-squad2\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\nQA_input = {\\n 'question': 'Why is model conversion important?',\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\n}\\nres = nlp(QA_input)\"}, \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"exact\": 79.87029394424324, \"f1\": 82.91251169582613}}, \"description\": \"This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset for the task of Question Answering. It's been trained on question-answer pairs, including unanswerable questions.\"}}", "category": "generic"}
{"question_id": 455, "text": " Our customer is a student with queries about certain tables. We need to build a model to answer their questions based on the data provided in the table.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"text2text-generation\", \"api_name\": \"neulab/omnitab-large-1024shot-finetuned-wtq-1024shot\", \"api_call\": \"AutoModelForSeq2SeqLM.from_pretrained('neulab/omnitab-large-1024shot-finetuned-wtq-1024shot')\", \"api_arguments\": {\"table\": \"pd.DataFrame\", \"query\": \"string\"}, \"python_environment_requirements\": [\"pandas\", \"transformers\"], \"example_code\": \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nimport pandas as pd\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot-finetuned-wtq-1024shot)\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot-finetuned-wtq-1024shot)\\ndata = {\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\n city: [athens, paris, st. louis, athens, beijing, london]\\n}\\ntable = pd.DataFrame.from_dict(data)\\nquery = In which year did beijing host the Olympic Games?\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\noutputs = model.generate(**encoding)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\", \"performance\": {\"dataset\": \"wikitablequestions\", \"accuracy\": \"Not provided\"}, \"description\": \"OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab. neulab/omnitab-large-1024shot-finetuned-wtq-1024shot (based on BART architecture) is initialized with neulab/omnitab-large-1024shot and fine-tuned on WikiTableQuestions in the 1024-shot setting.\"}}", "category": "generic"}
{"question_id": 456, "text": " We want to calculate the average number of stars for these open-source repositories. Create a table with repository, stars, and extract the data from it.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Table Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Table Question Answering\", \"api_name\": \"navteca/tapas-large-finetuned-wtq\", \"api_call\": \"AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\", \"api_arguments\": {\"table\": \"table_data\", \"query\": \"query\"}, \"python_environment_requirements\": {\"transformers\": \"AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\"}, \"example_code\": \"from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\\n# Load model & tokenizer\\ntapas_model = AutoModelForTableQuestionAnswering.from_pretrained('navteca/tapas-large-finetuned-wtq')\\ntapas_tokenizer = AutoTokenizer.from_pretrained('navteca/tapas-large-finetuned-wtq')\\n# Get predictions\\nnlp = pipeline('table-question-answering', model=tapas_model, tokenizer=tapas_tokenizer)\\nresult = nlp({'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'], 'Stars': ['36542', '4512', '3934'], 'Contributors': ['651', '77', '34'], 'Programming language': ['Python', 'Python', 'Rust, Python and NodeJS']}, 'query': 'How many stars does the transformers repository have?'})\\nprint(result)\", \"performance\": {\"dataset\": \"wikisql\", \"accuracy\": \"Not provided\"}, \"description\": \"TAPAS large model fine-tuned on WikiTable Questions (WTQ). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.\"}}", "category": "generic"}
{"question_id": 457, "text": " How can we extract answers from medical articles using natural language processing?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"sultan/BioM-ELECTRA-Large-SQuAD2\", \"api_call\": \"sultan/BioM-ELECTRA-Large-SQuAD2\", \"api_arguments\": null, \"python_environment_requirements\": [\"transformers\", \"sentencepiece\"], \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='sultan/BioM-ELECTRA-Large-SQuAD2')\\nresult = qa_pipeline({'context': 'your_context', 'question': 'your_question'})\", \"performance\": {\"dataset\": \"SQuAD2.0 Dev\", \"accuracy\": {\"exact\": 84.33420365535248, \"f1\": 87.49354241889522}}, \"description\": \"BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.\"}}", "category": "generic"}
{"question_id": 458, "text": " Create a system to analyze customers' questions in a company's email correspondence and automatically answer them.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"distilbert-base-uncased-distilled-squad\", \"api_call\": \"pipeline(\\\\question-answering\\\\, model='distilbert-base-uncased-distilled-squad')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nquestion_answerer = pipeline(question-answering, model='distilbert-base-uncased-distilled-squad')\\ncontext = r\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\n... \\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\nprint(\\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\\n...)\", \"performance\": {\"dataset\": \"SQuAD v1.1\", \"accuracy\": \"86.9 F1 score\"}, \"description\": \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\"}}", "category": "generic"}
{"question_id": 459, "text": " I need to know how to find my favorite podcast on Spotify in several languages.\\n###Input: Senza alcuna conoscenza, come posso trovare il mio podcast preferito su Spotify?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Multilingual Question Answering\", \"api_name\": \"mrm8488/bert-multi-cased-finetuned-xquadv1\", \"api_call\": \"pipeline('question-answering', model='mrm8488/bert-multi-cased-finetuned-xquadv1', tokenizer='mrm8488/bert-multi-cased-finetuned-xquadv1')\", \"api_arguments\": {\"context\": \"string\", \"question\": \"string\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"qa_pipeline({\\n 'context': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\n 'question': Who has been working hard for hugginface/transformers lately?\\n})\", \"performance\": {\"dataset\": \"XQuAD\", \"accuracy\": \"Not provided\"}, \"description\": \"This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.\"}}", "category": "generic"}
{"question_id": 460, "text": " A high school student wonders about the admission process of Oxford University. Help him find the answer.\\n###Input: {\\\"question\\\": \\\"What is the admission process of Oxford University?\\\", \\\"context\\\": \\\"Oxford University has a competitive admission process. Students must first apply through the Universities and Colleges Admissions Service (UCAS). Then, they need to take an admissions test and submit their written work as part of their application. Shortlisted candidates are usually invited for an interview, which plays a critical role in the final decision. Successful candidates receive offers, typically conditional on examination results.\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"bert-large-uncased-whole-word-masking-finetuned-squad\", \"api_call\": \"AutoModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\", \"api_arguments\": {\"model_name_or_path\": \"bert-large-uncased-whole-word-masking\", \"dataset_name\": \"squad\", \"do_train\": true, \"do_eval\": true, \"learning_rate\": 3e-05, \"num_train_epochs\": 2, \"max_seq_length\": 384, \"doc_stride\": 128, \"output_dir\": \"./examples/models/wwm_uncased_finetuned_squad/\", \"per_device_eval_batch_size\": 3, \"per_device_train_batch_size\": 3}, \"python_environment_requirements\": {\"torch\": \"1.9.0\", \"transformers\": \"4.9.2\"}, \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\\nresult = qa_pipeline({'question': 'What is the capital of France?', 'context': 'Paris is the capital of France.'})\\nprint(result)\", \"performance\": {\"dataset\": \"SQuAD\", \"accuracy\": {\"f1\": 93.15, \"exact_match\": 86.91}}, \"description\": \"BERT large model (uncased) whole word masking finetuned on SQuAD. The model was pretrained on BookCorpus and English Wikipedia. It was trained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This model should be used as a question-answering model.\"}}", "category": "generic"}
{"question_id": 461, "text": " We are building an application for high school students. Part of the application would be answering questions from their textbooks. We need to integrate an approach that provides accurate answers.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"distilbert-base-cased-distilled-squad\", \"api_call\": \"DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline\\nquestion_answerer = pipeline(question-answering, model='distilbert-base-cased-distilled-squad')\\ncontext = r\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\n... \\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\nprint(\\n... fAnswer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\\n...)\", \"performance\": {\"dataset\": \"SQuAD v1.1\", \"accuracy\": {\"Exact Match\": 79.6, \"F1\": 86.996}}, \"description\": \"DistilBERT base cased distilled SQuAD is a fine-tuned checkpoint of DistilBERT-base-cased, trained using knowledge distillation on SQuAD v1.1 dataset. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. This model can be used for question answering.\"}}", "category": "generic"}
{"question_id": 462, "text": " I am a student and I have a question about why we need a certain feature in a programming tool. Can you find the relevant answer?\\n###Input: {\\\"question\\\": \\\"Why is model conversion important?\\\", \\\"context\\\": \\\"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"bert-large-uncased-whole-word-masking-squad2\", \"api_call\": \"pipeline('question-answering', model=AutoModel.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'), tokenizer=AutoTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2'))\", \"api_arguments\": {\"model_name\": \"deepset/bert-large-uncased-whole-word-masking-squad2\", \"tokenizer\": \"deepset/bert-large-uncased-whole-word-masking-squad2\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"QA_input = {\\n 'question': 'Why is model conversion important?',\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\n}\\nres = nlp(QA_input)\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"Exact Match\": 80.885, \"F1\": 83.876}}, \"description\": \"This is a bert-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. It is designed for extractive question answering and supports English language.\"}}", "category": "generic"}
{"question_id": 463, "text": " We need to extract answers from legal documents. The user asks questions like, \\\"How much is the termination fee?\\\".\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"Rakib/roberta-base-on-cuad\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('Rakib/roberta-base-on-cuad')\", \"api_arguments\": {\"tokenizer\": \"AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"python_environment_requirements\": {\"transformers\": \"latest\"}, \"example_code\": {\"import\": \"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\", \"tokenizer\": \"tokenizer = AutoTokenizer.from_pretrained(Rakib/roberta-base-on-cuad)\", \"model\": \"model = AutoModelForQuestionAnswering.from_pretrained(Rakib/roberta-base-on-cuad)\"}, \"performance\": {\"dataset\": \"cuad\", \"accuracy\": \"46.6%\"}, \"description\": \"This model is trained for the task of Question Answering on Legal Documents using the CUAD dataset. It is based on the RoBERTa architecture and can be used to extract answers from legal contracts and documents.\"}}", "category": "generic"}
{"question_id": 464, "text": " I will be in South Korea on a business trip and I need a specialized assistant to help me with interpreting information in the Korean language.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"monologg/koelectra-small-v2-distilled-korquad-384\", \"api_call\": \"pipeline('question-answering', model='monologg/koelectra-small-v2-distilled-korquad-384')\", \"api_arguments\": {\"model\": \"monologg/koelectra-small-v2-distilled-korquad-384\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"nlp(question='your_question', context='your_context')\", \"performance\": {\"dataset\": \"KorQuAD\", \"accuracy\": \"Not provided\"}, \"description\": \"A Korean Question Answering model based on Electra and trained on the KorQuAD dataset.\"}}", "category": "generic"}
{"question_id": 465, "text": " We are building a knowledge base management system with AI assistance, and we want to ask it to explain concepts for us.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/roberta-large-squad2\", \"api_call\": \"pipeline('question-answering', model='deepset/roberta-large-squad2')\", \"api_arguments\": [\"question\", \"context\"], \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import pipeline; nlp = pipeline('question-answering', model='deepset/roberta-large-squad2'); nlp({'question': 'What is the capital of Germany?', 'context': 'Berlin is the capital of Germany.'})\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": \"Not provided\"}, \"description\": \"A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\"}}", "category": "generic"}
{"question_id": 466, "text": " With the overwhelming quantity of information about COVID-19 on the internet, we want to make it easier for users to find accurate answers to their questions.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/roberta-base-squad2-covid\", \"api_call\": \"pipeline('question-answering', model=RobertaForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2-covid'), tokenizer=RobertaTokenizer.from_pretrained('deepset/roberta-base-squad2-covid'))\", \"api_arguments\": {\"model_name\": \"deepset/roberta-base-squad2-covid\", \"tokenizer\": \"deepset/roberta-base-squad2-covid\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": {\"QA_input\": {\"question\": \"Why is model conversion important?\", \"context\": \"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\"}, \"res\": \"nlp(QA_input)\"}, \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"XVAL_EM\": 0.17890995260663506, \"XVAL_f1\": 0.49925444207319924, \"XVAL_top_3_recall\": 0.8021327014218009}}, \"description\": \"This model is a Roberta-based model fine-tuned on SQuAD-style CORD-19 annotations for the task of extractive question answering in the context of COVID-19. It can be used with the Hugging Face Transformers library for question answering tasks.\"}}", "category": "generic"}
{"question_id": 467, "text": " Write a query for a legal document to detect who are the parties involved in the contract.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"valhalla/longformer-base-4096-finetuned-squadv1\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\", \"api_arguments\": {\"input_ids\": \"encoding['input_ids']\", \"attention_mask\": \"encoding['attention_mask']\"}, \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"import torch\\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\\ntokenizer = AutoTokenizer.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\\nmodel = AutoModelForQuestionAnswering.from_pretrained('valhalla/longformer-base-4096-finetuned-squadv1')\\ntext = 'Huggingface has democratized NLP. Huge thanks to Huggingface for this.'\\nquestion = 'What has Huggingface done ?'\\nencoding = tokenizer(question, text, return_tensors='pt')\\ninput_ids = encoding['input_ids']\\nattention_mask = encoding['attention_mask']\\nstart_scores, end_scores = model(input_ids, attention_mask=attention_mask)\\nall_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\\nanswer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\\nanswer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\", \"performance\": {\"dataset\": \"squad_v1\", \"accuracy\": {\"Exact Match\": 85.1466, \"F1\": 91.5415}}, \"description\": \"This is longformer-base-4096 model fine-tuned on SQuAD v1 dataset for question answering task. Longformer model created by Iz Beltagy, Matthew E. Peters, Arman Coha from AllenAI. As the paper explains it, Longformer is a BERT-like model for long documents. The pre-trained model can handle sequences with up to 4096 tokens.\"}}", "category": "generic"}
{"question_id": 468, "text": " Build a question-answering system that uses a distilled model to provide quick and accurate responses.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/tinyroberta-squad2\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('deepset/tinyroberta-squad2')\", \"api_arguments\": {\"model_name_or_path\": \"deepset/tinyroberta-squad2\", \"question\": \"Why is model conversion important?\", \"context\": \"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\nmodel_name = deepset/tinyroberta-squad2\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\nQA_input = {\\n 'question': 'Why is model conversion important?',\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\n}\\nres = nlp(QA_input)\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"exact\": 78.69114798281817, \"f1\": 81.9198998536977}}, \"description\": \"This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.\"}}", "category": "generic"}
{"question_id": 469, "text": " Our team is developing a virtual assistant that can answer questions based on the given context. Can you help us find the right model?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"philschmid/distilbert-onnx\", \"api_call\": \"pipeline('question-answering', model='philschmid/distilbert-onnx')\", \"api_arguments\": {\"model\": \"philschmid/distilbert-onnx\"}, \"python_environment_requirements\": [\"transformers\", \"onnx\"], \"example_code\": {\"Compute\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='philschmid/distilbert-onnx')\\nqa_pipeline({'context': 'This is a context', 'question': 'What is this?'})\"}, \"performance\": {\"dataset\": \"squad\", \"accuracy\": \"F1 score: 87.1\"}, \"description\": \"This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.\"}}", "category": "generic"}
{"question_id": 470, "text": " I am running an e-commerce clothing store and I have a large amount of customer reviews. Extract answers to specific questions about product quality from these reviews.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/bert-base-cased-squad2\", \"api_call\": \"deepset/bert-base-cased-squad2\", \"api_arguments\": \"context, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"qa_pipeline({'context': 'This model can be loaded on the Inference API on-demand.', 'question': 'Where can this model be loaded?'})\", \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"exact_match\": 71.152, \"f1\": 74.671}}, \"description\": \"This is a BERT base cased model trained on SQuAD v2\"}}", "category": "generic"}
{"question_id": 471, "text": " A person wants to know what's mentioned about the capital of France in a document about European countries.\\n###Input: {\\\"question\\\": \\\"What is the capital of France?\\\", \\\"context\\\": \\\"France, a popular tourist destination, is also a leading European country with a rich history and diverse landscapes. Its capital is Paris, which is known for its architecture, culture and fashion.\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"ahotrod/electra_large_discriminator_squad2_512\", \"api_call\": \"ahotrod/electra_large_discriminator_squad2_512\", \"api_arguments\": \"question, context\", \"python_environment_requirements\": [\"transformers\", \"torch\", \"tensorflow\"], \"example_code\": \"qa_pipeline({'question': 'What is the capital of France?', 'context': 'France is a country in Europe. Its capital is Paris.'})\", \"performance\": {\"dataset\": \"SQuAD2.0\", \"accuracy\": {\"exact\": 87.09677419354838, \"f1\": 89.98343832723452}}, \"description\": \"ELECTRA_large_discriminator language model fine-tuned on SQuAD2.0 for question answering tasks.\"}}", "category": "generic"}
{"question_id": 472, "text": " Collect information about global warming and answer this question: \\\"What are the main causes of global warming?\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"PyTorch Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/bert-medium-squad2-distilled\", \"api_call\": \"AutoModel.from_pretrained('deepset/bert-medium-squad2-distilled')\", \"api_arguments\": {\"model\": \"deepset/bert-medium-squad2-distilled\"}, \"python_environment_requirements\": {\"transformers\": \">=4.0.0\"}, \"example_code\": \"qa({'context': 'This is an example context.', 'question': 'What is this example about?'})\", \"performance\": {\"dataset\": \"squad_v2\", \"exact\": 68.6431398972458, \"f1\": 72.7637083790805}, \"description\": \"This model is a distilled version of deepset/bert-large-uncased-whole-word-masking-squad2, trained on the SQuAD 2.0 dataset for question answering tasks. It is based on the BERT-medium architecture and uses the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 473, "text": " I want my app to answer questions about a given text like a wikipedia article.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/minilm-uncased-squad2\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\", \"api_arguments\": {\"model_name\": \"deepset/minilm-uncased-squad2\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": {\"a\": {\"code\": \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\nmodel_name = deepset/minilm-uncased-squad2\\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\nQA_input = {\\n 'question': 'Why is model conversion important?',\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\n}\\nres = nlp(QA_input)\"}, \"b\": {\"code\": \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\nmodel_name = deepset/minilm-uncased-squad2\\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\"}}, \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"exact\": 76.13071675229513, \"f1\": 79.49786500219953}}, \"description\": \"MiniLM-L12-H384-uncased is a language model fine-tuned for extractive question answering on the SQuAD 2.0 dataset. It is based on the microsoft/MiniLM-L12-H384-uncased model and can be used with the Hugging Face Transformers library.\"}}", "category": "generic"}
{"question_id": 474, "text": " To help a student with their history project, create a function that will find answers to specific questions related to a given text from their book.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/roberta-base-squad2-distilled\", \"api_call\": \"AutoModel.from_pretrained('deepset/roberta-base-squad2-distilled')\", \"api_arguments\": {\"context\": \"string\", \"question\": \"string\"}, \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2-distilled')\\nresult = qa_pipeline({'context': 'This is a context.', 'question': 'What is this?'})\\nprint(result)\", \"performance\": {\"dataset\": \"squad_v2\", \"exact\": 79.8366040596311, \"f1\": 83.916407079888}, \"description\": \"This model is a distilled version of deepset/roberta-large-squad2, trained on SQuAD 2.0 dataset for question answering tasks. It is based on the Roberta architecture and has been fine-tuned using Haystack's distillation feature.\"}}", "category": "generic"}
{"question_id": 475, "text": " I am working on a project to analyze novels. Can you provide a question-answering pipeline to help me find specific information from a large text?\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"thatdramebaazguy/roberta-base-squad\", \"api_call\": \"pipeline(model=RobertaModel.from_pretrained('thatdramebaazguy/roberta-base-squad'), tokenizer=RobertaTokenizer.from_pretrained('thatdramebaazguy/roberta-base-squad'), revision='v1.0', task='question-answering')\", \"api_arguments\": {\"model_name\": \"thatdramebaazguy/roberta-base-squad\", \"tokenizer\": \"thatdramebaazguy/roberta-base-squad\", \"revision\": \"v1.0\", \"task\": \"question-answering\"}, \"python_environment_requirements\": {\"huggingface\": {\"transformers\": \"latest\"}}, \"example_code\": \"model_name = 'thatdramebaazguy/roberta-base-squad'\\nqa_pipeline = pipeline(model=model_name, tokenizer=model_name, revision='v1.0', task='question-answering')\", \"performance\": {\"dataset\": [{\"name\": \"SQuADv1\", \"accuracy\": {\"exact_match\": 83.6045, \"f1\": 91.1709}}, {\"name\": \"MoviesQA\", \"accuracy\": {\"exact_match\": 51.6494, \"f1\": 68.2615}}]}, \"description\": \"This is Roberta Base trained to do the SQuAD Task. This makes a QA model capable of answering questions.\"}}", "category": "generic"}
{"question_id": 476, "text": " I encountered a text passage that might contain the answer to an important question. Could you help me refine the answer through deep learning models?\\n###Input: {\\\"question\\\": \\\"What are the benefits of model conversion?\\\", \\\"context\\\": \\\"The benefits of model conversion include providing flexibility to the user, allowing people to easily switch between different frameworks, and reducing the need to retrain models across various platforms.\\\"}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/deberta-v3-base-squad2\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-base-squad2')\", \"api_arguments\": {\"model_name\": \"deepset/deberta-v3-base-squad2\", \"tokenizer\": \"deepset/deberta-v3-base-squad2\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": {\"import\": \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\", \"initialize\": [\"model_name = 'deepset/deberta-v3-base-squad2'\", \"nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\"], \"example_input\": {\"question\": \"Why is model conversion important?\", \"context\": \"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\"}, \"example_output\": \"res = nlp(QA_input)\"}, \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"Exact Match\": 83.825, \"F1\": 87.41}}, \"description\": \"This is the deberta-v3-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.\"}}", "category": "generic"}
{"question_id": 477, "text": " I am creating a mobile app that answers questions related to images shown on-screen. I would prefer a transformer model to be used.\\n \n Use this API documentation for reference:  {\"domain\": \"Multimodal Visual Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"uclanlp/visualbert-vqa\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('uclanlp/visualbert-vqa')\", \"api_arguments\": \"\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\": \"A VisualBERT model for Visual Question Answering.\"}}", "category": "generic"}
{"question_id": 478, "text": " Help us find the answer to the question: \\\"What day was the game played on?\\\" with the following context: \\\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\"\\n###Input: \\\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\"\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"csarron/bert-base-uncased-squad-v1\", \"api_call\": \"pipeline('question-answering', model='csarron/bert-base-uncased-squad-v1', tokenizer='csarron/bert-base-uncased-squad-v1')\", \"api_arguments\": {\"model\": \"csarron/bert-base-uncased-squad-v1\", \"tokenizer\": \"csarron/bert-base-uncased-squad-v1\"}, \"python_environment_requirements\": \"Python 3.7.5\", \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline(\\n question-answering,\\n model=csarron/bert-base-uncased-squad-v1,\\n tokenizer=csarron/bert-base-uncased-squad-v1\\n)\\npredictions = qa_pipeline({\\n 'context': The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.,\\n 'question': What day was the game played on?\\n})\\nprint(predictions)\", \"performance\": {\"dataset\": \"SQuAD1.1\", \"accuracy\": {\"EM\": 80.9, \"F1\": 88.2}}, \"description\": \"BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\"}}", "category": "generic"}
{"question_id": 479, "text": " Assist me in finding the answer to a question written in traditional Chinese regarding a historical article.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\", \"api_call\": \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large.from_pretrained()\", \"api_arguments\": \"context, question\", \"python_environment_requirements\": \"transformers\", \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='luhua/chinese_pretrain_mrc_roberta_wwm_ext_large')\\nresult = qa_pipeline({'context': 'your_context_here', 'question': 'your_question_here'})\", \"performance\": {\"dataset\": \"Dureader-2021\", \"accuracy\": \"83.1\"}, \"description\": \"A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition.\"}}", "category": "generic"}
{"question_id": 480, "text": " Assistance needed in retrieving an answer for a user input question.\\n### Input: {'context': 'A pandemic is an epidemic occurring on a scale which crosses international boundaries, usually affecting people on a worldwide scale. The most well-known example of a pandemic is the Spanish flu (also known as the 1918 flu pandemic) causing 50 million deaths.', 'question': 'What is an example of a well-known pandemic?'}\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"bert-large-cased-whole-word-masking-finetuned-squad\", \"api_call\": \"AutoModel.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\", \"api_arguments\": {\"model_name_or_path\": \"bert-large-cased-whole-word-masking\", \"dataset_name\": \"squad\", \"do_train\": true, \"do_eval\": true, \"learning_rate\": 3e-05, \"num_train_epochs\": 2, \"max_seq_length\": 384, \"doc_stride\": 128, \"output_dir\": \"./examples/models/wwm_cased_finetuned_squad/\", \"per_device_eval_batch_size\": 3, \"per_device_train_batch_size\": 3}, \"python_environment_requirements\": [\"torch\", \"transformers\"], \"example_code\": \"from transformers import pipeline\\nqa_pipeline = pipeline('question-answering', model='bert-large-cased-whole-word-masking-finetuned-squad')\\nresult = qa_pipeline({'context': 'This is a context example.', 'question': 'What is this example for?'})\\nprint(result)\", \"performance\": {\"dataset\": [{\"name\": \"BookCorpus\", \"accuracy\": \"N/A\"}, {\"name\": \"English Wikipedia\", \"accuracy\": \"N/A\"}]}, \"description\": \"BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset.\"}}", "category": "generic"}
{"question_id": 481, "text": " I am a student learning NLP. I want to build a system for answering questions about articles on the internet.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Hugging Face Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/deberta-v3-large-squad2\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('deepset/deberta-v3-large-squad2')\", \"api_arguments\": {\"model_name_or_path\": \"deepset/deberta-v3-large-squad2\", \"tokenizer\": \"deepset/deberta-v3-large-squad2\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": {\"a\": {\"code\": \"nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\\nQA_input = {\\n 'question': 'Why is model conversion important?',\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\n}\\nres = nlp(QA_input)\"}, \"b\": {\"code\": \"model = AutoModelForQuestionAnswering.from_pretrained(model_name)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\"}}, \"performance\": {\"dataset\": \"squad_v2\", \"accuracy\": {\"exact\": 87.6105449338836, \"f1\": 90.75307008866517}}, \"description\": \"This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.\"}}", "category": "generic"}
{"question_id": 482, "text": " I'm a student studying for my exams. I need to answer questions from my study materials in multiple languages.\\n \n Use this API documentation for reference:  {\"domain\": \"Natural Language Processing Question Answering\", \"framework\": \"Transformers\", \"functionality\": \"Question Answering\", \"api_name\": \"deepset/xlm-roberta-large-squad2\", \"api_call\": \"AutoModelForQuestionAnswering.from_pretrained('deepset/xlm-roberta-large-squad2')\", \"api_arguments\": {\"model_name\": \"deepset/xlm-roberta-large-squad2\", \"tokenizer\": \"deepset/xlm-roberta-large-squad2\"}, \"python_environment_requirements\": [\"transformers\"], \"example_code\": {\"import_example\": \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\", \"usage_example\": \"QA_input = {\\n 'question': 'Why is model conversion important?',\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\n}\\nres = nlp(QA_input)\"}, \"performance\": {\"squad_v2\": {\"exact_match\": 81.828, \"f1\": 84.889}}, \"description\": \"Multilingual XLM-RoBERTa large model for extractive question answering on various languages. Trained on SQuAD 2.0 dataset and evaluated on SQuAD dev set, German MLQA, and German XQuAD.\"}}", "category": "generic"}
