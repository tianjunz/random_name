{"question_id": 1, "text": " We are a medical research institute and want to extract features from biomedical texts to improve literature-based predictions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'dmis-lab/biobert-v1.1\\', \\'api_call\\': \"pipeline(\\'feature-extraction\\', model=\\'dmis-lab/biobert-v1.1\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.\\'}', metadata={})]", "category": "generic"}
{"question_id": 2, "text": " I'm organizing an online conference with participants from different countries who speak different languages. I want to find a tool to extract information from their forum discussions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Zixtrauce/BaekBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'Zixtrauce/BaekBot\\')\", \\'api_arguments\\': \\'input_message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"conversational_pipeline(\\'input_message\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'BaekBot is a conversational model based on the GPT-2 architecture for text generation. It can be used for generating human-like responses in a chat-like environment.\\'}', metadata={})]", "category": "generic"}
{"question_id": 3, "text": " Our company plans to build a video-sharing platform that automatically generates video descriptions. We need a model that extracts video features and understands human language.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': 'N/A (There is no model instantiation provided in the `api_call` field)', 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.'}\", metadata={})]", "category": "generic"}
{"question_id": 4, "text": " We are working on a project that can generate images from textual description for an online art gallery. We need to create diverse images from a given text prompt.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 5, "text": " I want to create a document digitization tool that converts printed documents into text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'castorini/doc2query-t5-base-msmarco\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'castorini/doc2query-t5-base-msmarco\\')\", \\'api_arguments\\': \\'text, max_length\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A T5 model trained on the MS MARCO dataset for generating queries from documents.\\'}', metadata={})]", "category": "generic"}
{"question_id": 6, "text": " We would like to present images of our product to create a caption for each image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Image Captioning\\', \\'api_name\\': \\'blip-image-captioning-large\\', \\'api_call\\': \\'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'api_arguments\\': {\\'raw_image\\': \\'Image\\', \\'text\\': \\'Optional Text\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'BlipProcessor, BlipForConditionalGeneration\\', \\'PIL\\': \\'Image\\', \\'requests\\': \\'requests\\'}, \\'example_code\\': {\\'import_requests\\': \\'import requests\\', \\'import_PIL\\': \\'from PIL import Image\\', \\'import_transformers\\': \\'from transformers import BlipProcessor, BlipForConditionalGeneration\\', \\'load_processor\\': \\'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_model\\': \\'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_image\\': \"img_url = \\'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\\'RGB\\')\", \\'conditional_captioning\\': \\'text = a photography of\\\\ninputs = processor(raw_image, text, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\', \\'unconditional_captioning\\': \\'inputs = processor(raw_image, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\'}, \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': {\\'image-text retrieval\\': \\'+2.7% recall@1\\', \\'image captioning\\': \\'+2.8% CIDEr\\', \\'VQA\\': \\'+1.6% VQA score\\'}}, \\'description\\': \\'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\\'}', metadata={})]", "category": "generic"}
{"question_id": 7, "text": " I need to build an interactive robot that can answer questions using an image input, process questions in natural language, and generate a response.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 8, "text": " I need assistance to describe an image of a stop sign in Australia.\\n###Input: The image URL is https://www.ilankelman.org/stopsigns/australia.jpg\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = stabilityai/stable-diffusion-2-base\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.'}\", metadata={})]", "category": "generic"}
{"question_id": 9, "text": " We have an old dataset of charts from previous reports, and we want to generate tables from these charts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'microsoft/tapex-large-finetuned-wikisql\\', \\'api_call\\': \"Output: BartForConditionalGeneration.from_pretrained(\\'microsoft/tapex-large-finetuned-wikisql\\')\", \\'api_arguments\\': {\\'table\\': \\'pd.DataFrame.from_dict(data)\\', \\'query\\': \\'query\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'TapexTokenizer, BartForConditionalGeneration\\', \\'pandas\\': \\'pd\\'}, \\'example_code\\': \\'from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-finetuned-wikisql)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'wikisql\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. This model is the tapex-base model fine-tuned on the WikiSQL dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 10, "text": " I am a clothing designer and I need to analyze an image of a new fabric pattern with a model. Can you suggest how to do that to get the description of the fabric pattern?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = nitrosocke/nitro-diffusion\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = archer arcane style magical princess with golden hair\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.'}\", metadata={})]", "category": "generic"}
{"question_id": 11, "text": " We want to create an application that allows users to upload photos of tourist attractions and provides them with information about the images. Implement a system that can answer questions about these images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'prompthero/openjourney-v4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'prompthero/openjourney-v4\\')\", \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"generate_image(\\'your text here\\')\", \\'performance\\': {\\'dataset\\': \\'Midjourney v4 images\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 12, "text": " We are creating a virtual tour guide. The guide will answer the users' questions about the places they visit based on the images of those places.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': 'Not specified.', 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = prompthero/openjourney\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.'}\", metadata={})]", "category": "generic"}
{"question_id": 13, "text": " A bank provides us with account statements of their clients in PDF format. We need to extract information such as transaction date, transaction amount, and balance after each transaction.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'PegasusForConditionalGeneration.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 14, "text": " I need a personal assistant to help me analyze the financial reports and answer the question: What is the total revenue for the company?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'PegasusForConditionalGeneration.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 15, "text": " Develop an AI model for a local art gallery accepting scanned artwork, to help the staff easily find information about any piece of art by asking questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 16, "text": " Working in a government agency, I need to extract specific information from a variety of unstructured forms and documents. Could you help me extract the answer to my question from the given context?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 17, "text": " We are building a virtual makeover app and we need to estimate the depth of various surfaces in an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 18, "text": " The digital marketing company needs to create a virtual reality environment for their product display. They asked us to estimate the depth of input images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Monocular Depth Estimation\\', \\'api_name\\': \\'Intel/dpt-large\\', \\'api_call\\': \"DPTForDepthEstimation.from_pretrained(\\'Intel/dpt-large\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'Intel/dpt-large\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import DPTImageProcessor, DPTForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'MIX 6\\', \\'accuracy\\': \\'10.82\\'}, \\'description\\': \\'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 19, "text": " I am working on an indoor robot project that needs depth estimation for the robot's vision. Help sequence the proper code for depth estimation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 20, "text": " We are developing a smart mobility solution for the visually impaired. We want to estimate the depth of surrounding objects in real-time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-nyu-finetuned-diode\\', \\'api_call\\': \"pipeline(\\'depth-estimation\\', model=\\'sayakpaul/glpn-nyu-finetuned-diode\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'diode-subset\\', \\'accuracy\\': {\\'Loss\\': 0.4359, \\'Rmse\\': 0.4276}}, \\'description\\': \\'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 21, "text": " Let's find the depth estimation for an image captured in a warehouse.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 22, "text": " I am the owner of an online pet store. I need a solution that categorizes the uploaded photos of pets into cat or dog.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 23, "text": " To create a photo gallery app, we need a way to categorize images according to their content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 24, "text": " I am building an app that detects flower species from uploaded images. What model can I use?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ntrant7/sd-class-butterflies-32\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'Not specified\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies.\\'}', metadata={})]", "category": "generic"}
{"question_id": 25, "text": " Recommend me an object detection library that can detect hard hats in construction areas to ensure safety compliance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-hard-hat-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\\\\n\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'hard-hat-detection\\', \\'accuracy\\': 0.811}, \\'description\\': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between \\'Hardhat\\' and \\'NO-Hardhat\\' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\"}', metadata={})]", "category": "generic"}
{"question_id": 26, "text": " We need to implement a zero-shot object detection system to detect cats and dogs in images for a pet adoption application.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 27, "text": " Our company needs to detect license plates of vehicles in our parking lot. Please provide object detection solution for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'License Plate Detection\\', \\'api_name\\': \\'keremberke/yolov5m-license-plate\\', \\'api_call\\': \"yolov5.load(\\'keremberke/yolov5m-license-plate\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic\\': False, \\'multi_label\\': False, \\'max_det\\': 1000, \\'img\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'size\\': 640, \\'augment\\': True}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': [\\'import yolov5\\', \"model = yolov5.load(\\'keremberke/yolov5m-license-plate\\')\", \\'model.conf = 0.25\\', \\'model.iou = 0.45\\', \\'model.agnostic = False\\', \\'model.multi_label = False\\', \\'model.max_det = 1000\\', \"img = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model(img, size=640)\\', \\'results = model(img, augment=True)\\', \\'predictions = results.pred[0]\\', \\'boxes = predictions[:, :4]\\', \\'scores = predictions[:, 4]\\', \\'categories = predictions[:, 5]\\', \\'results.show()\\', \"results.save(save_dir=\\'results/\\')\"], \\'performance\\': {\\'dataset\\': \\'keremberke/license-plate-object-detection\\', \\'accuracy\\': 0.988}, \\'description\\': \\'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\\'}', metadata={})]", "category": "generic"}
{"question_id": 28, "text": " We are working on a smart camera for recognizing people in crowded areas, please segment the given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-pcb-defect-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-pcb-defect-segmentation\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-pcb-defect-segmentation\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'print(results[0].masks)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'pcb-defect-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.568, \\'mAP@0.5(mask)\\': 0.557}}, \\'description\\': \\'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\\'}', metadata={})]", "category": "generic"}
{"question_id": 29, "text": " Our team at a blood test center is analyzing blood cell images. Find a way to detect blood cells in the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Blood Cell Detection\\', \\'api_name\\': \\'keremberke/yolov8n-blood-cell-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'blood-cell-object-detection\\', \\'accuracy\\': 0.893}, \\'description\\': \\'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 30, "text": " We are an agricultural company aiming to detect and classify segments of crops in aerial images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 31, "text": " We want to create an application to help enhance the security of buildings. It needs to detect and segment people who enter the building.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8s-building-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8s-building-segmentation\\')\", \\'api_arguments\\': [\\'conf\\', \\'iou\\', \\'agnostic_nms\\', \\'max_det\\', \\'image\\'], \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.21\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8s-building-segmentation\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'print(results[0].masks)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'satellite-building-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.661, \\'mAP@0.5(mask)\\': 0.651}}, \\'description\\': \\'A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\\'}', metadata={})]", "category": "generic"}
{"question_id": 32, "text": " Help us design a solution to segment objects inside images and classify them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-pcb-defect-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-pcb-defect-segmentation\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-pcb-defect-segmentation\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'print(results[0].masks)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'pcb-defect-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.568, \\'mAP@0.5(mask)\\': 0.557}}, \\'description\\': \\'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\\'}', metadata={})]", "category": "generic"}
{"question_id": 33, "text": " The company has recently launched an image classification software which recognizes all the objects in an image. How the software can be improved by a more sophisticated instance segmentation model to recognize individual objects and their boundaries within the image?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-pcb-defect-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-pcb-defect-segmentation\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-pcb-defect-segmentation\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'print(results[0].masks)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'pcb-defect-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.568, \\'mAP@0.5(mask)\\': 0.557}}, \\'description\\': \\'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\\'}', metadata={})]", "category": "generic"}
{"question_id": 34, "text": " We have a production line creating computer chips. We need to identify and segment any defects in the printed circuit boards(PCB) of these products.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8s-pcb-defect-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8s-pcb-defect-segmentation\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8s-pcb-defect-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pcb-defect-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.515, \\'mAP@0.5(mask)\\': 0.491}}, \\'description\\': \\'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\\'}', metadata={})]", "category": "generic"}
{"question_id": 35, "text": " The art gallery is exploring innovative ways to display their collections. We are helping them to generate abstract artworks with the help of AI.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'johnowhitaker/sd-class-wikiart-from-bedrooms\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'johnowhitaker/sd-class-wikiart-from-bedrooms\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'johnowhitaker/sd-class-wikiart-from-bedrooms\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'https://huggingface.co/datasets/huggan/wikiart\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.\\'}', metadata={})]", "category": "generic"}
{"question_id": 36, "text": " Present a code for generating a colorful butterfly using A.I. technology.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ceyda/butterfly_cropped_uniq1K_512\\', \\'api_call\\': \"LightweightGAN.from_pretrained(\\'ceyda/butterfly_cropped_uniq1K_512\\')\", \\'api_arguments\\': [\\'pretrained_model_name_or_path\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'huggan.pytorch.lightweight_gan.lightweight_gan\\'], \\'example_code\\': \\'import torch\\\\nfrom huggan.pytorch.lightweight_gan.lightweight_gan import LightweightGAN\\\\ngan = LightweightGAN.from_pretrained(ceyda/butterfly_cropped_uniq1K_512)\\\\ngan.eval()\\\\nbatch_size = 1\\\\nwith torch.no_grad():\\\\n ims = gan.G(torch.randn(batch_size, gan.latent_dim)).clamp_(0., 1.)*255\\\\n ims = ims.permute(0,2,3,1).detach().cpu().numpy().astype(np.uint8)\\\\n # ims is [BxWxHxC] call Image.fromarray(ims[0])\\', \\'performance\\': {\\'dataset\\': \\'huggan/smithsonian_butterflies_subset\\', \\'accuracy\\': \\'FID score on 100 images\\'}, \\'description\\': \"Butterfly GAN model based on the paper \\'Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis\\'. The model is intended for fun and learning purposes. It was trained on 1000 images from the huggan/smithsonian_butterflies_subset dataset, with a focus on low data training as mentioned in the paper. The model generates high-quality butterfly images.\"}', metadata={})]", "category": "generic"}
{"question_id": 37, "text": " Develop an AI actor that can recognize scenes from a trailer and identify the specific movie genre.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 38, "text": " To improve the customer experience on our video streaming platform, we need to classify videos into proper categories based on content. Develop a video classification model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'tiny-random-VideoMAEForVideoClassification\\', \\'api_call\\': \"VideoClassificationPipeline(model=\\'hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification\\')\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A tiny random VideoMAE model for video classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 39, "text": " Analyze a video and provide a classification based on the content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-kinetics\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-kinetics\\')\", \\'api_arguments\\': [\\'video\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-kinetics)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-kinetics)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': {\\'top-1\\': 80.9, \\'top-5\\': 94.7}}, \\'description\\': \\'VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 40, "text": " As someone who loves photography, I want my app to be able to automatically identify what the subject is in my uploaded images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 41, "text": " We are dealing with a project related to self-driving cars. In order to improve the navigation, we need to classify the location of an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 42, "text": " I'm trying to determine if a list of German sentences are positive, negative, or neutral in sentiment.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'German Sentiment Classification', 'api_name': 'oliverguhr/german-sentiment-bert', 'api_call': 'Output: SentimentModel()', 'api_arguments': ['texts'], 'python_environment_requirements': 'pip install germansentiment', 'example_code': ['from germansentiment import SentimentModel', 'model = SentimentModel()', 'texts = [', ' Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,', ' Total awesome!,nicht so schlecht wie erwartet,', ' Der Test verlief positiv.,Sie f\u00e4hrt ein gr\u00fcnes Auto.]', 'result = model.predict_sentiment(texts)', 'print(result)'], 'performance': {'dataset': ['holidaycheck', 'scare', 'filmstarts', 'germeval', 'PotTS', 'emotions', 'sb10k', 'Leipzig Wikipedia Corpus 2016', 'all'], 'accuracy': [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, 'description': 'This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.'}\", metadata={})]", "category": "generic"}
{"question_id": 43, "text": " I need to choose the right answer from a set of options for a given question.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'optimum/roberta-base-squad2\\', \\'api_call\\': \"AutoModelForQuestionAnswering.from_pretrained(\\'deepset/roberta-base-squad2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'deepset/roberta-base-squad2\\', \\'tokenizer\\': \\'deepset/roberta-base-squad2\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': {\\'a\\': {\\'import\\': \\'from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\', \\'example\\': \"nlp = pipeline(\\'question-answering\\', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n \\'question\\': \\'Why is model conversion important?\\',\\\\n \\'context\\': \\'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\\'\\\\n}\\\\nres = nlp(QA_input)\"}, \\'b\\': {\\'import\\': \\'from transformers import AutoModelForQuestionAnswering, AutoTokenizer\\', \\'example\\': \\'model = AutoModelForQuestionAnswering.from_pretrained(model_name)\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\'}}, \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': {\\'exact\\': 79.87029394424324, \\'f1\\': 82.91251169582613}}, \\'description\\': \\'This is an ONNX conversion of the deepset/roberta-base-squad2 model for extractive question answering. It is trained on the SQuAD 2.0 dataset and is compatible with the Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 44, "text": " I need to extract all company names from a list of news articles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/pegasus-newsroom\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'google/pegasus-newsroom\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'newsroom\\', \\'accuracy\\': \\'45.98/34.20/42.18\\'}, \\'description\\': \\'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 45, "text": " Our company is working on a customer support tool that automatically extracts relevant information mentioned by users in their messages. Analyze customer messages to extract entities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'finiteautomata/beto-sentiment-analysis\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'finiteautomata/beto-sentiment-analysis\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'Hugging Face Transformers library\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TASS 2020 corpus\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 46, "text": " We are developing a system to identify the named entities in a given sentence.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english-ontonotes\\', \\'api_call\\': \\'Output: SequenceTagger.load(\\\\\\\\flair/ner-english-ontonotes\\\\\\\\)\\', \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(flair/ner-english-ontonotes)\\\\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'Ontonotes\\', \\'accuracy\\': \\'89.27\\'}, \\'description\\': \\'This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 47, "text": " Identify the named entities in the given news article.\\n###Input: On August 21st, Apple Inc. announced their new product, the iPhone 13, will be released on September 14th. CEO Tim Cook led the event and highlighted the new features, including an improved camera and longer battery life. The starting price for the device is $699.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'facebook/bart-large-cnn\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'facebook/bart-large-cnn\\')\", \\'api_arguments\\': [\\'ARTICLE\\', \\'max_length\\', \\'min_length\\', \\'do_sample\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\\\\nARTICLE = ...\\\\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\\', \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 42.949, \\'ROUGE-2\\': 20.815, \\'ROUGE-L\\': 30.619, \\'ROUGE-LSUM\\': 40.038}}, \\'description\\': \\'BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 48, "text": " Design bedside nurse report for COVID-19 patients with easy comprehension. We need to provide an overview of a patient's condition to the nurse.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-base-squad2-covid\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=RobertaForQuestionAnswering.from_pretrained(\\'deepset/roberta-base-squad2-covid\\'), tokenizer=RobertaTokenizer.from_pretrained(\\'deepset/roberta-base-squad2-covid\\'))\", \\'api_arguments\\': {\\'model_name\\': \\'deepset/roberta-base-squad2-covid\\', \\'tokenizer\\': \\'deepset/roberta-base-squad2-covid\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': {\\'QA_input\\': {\\'question\\': \\'Why is model conversion important?\\', \\'context\\': \\'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\\'}, \\'res\\': \\'nlp(QA_input)\\'}, \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': {\\'XVAL_EM\\': 0.17890995260663506, \\'XVAL_f1\\': 0.49925444207319924, \\'XVAL_top_3_recall\\': 0.8021327014218009}}, \\'description\\': \\'This model is a Roberta-based model fine-tuned on SQuAD-style CORD-19 annotations for the task of extractive question answering in the context of COVID-19. It can be used with the Hugging Face Transformers library for question answering tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 49, "text": " My company needs to answer questions about sales data on a monthly basis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 50, "text": " We are building a knowledge base management system with AI assistance, and we want to ask it to explain concepts for us.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 51, "text": " Collect information about global warming and answer this question: \\\"What are the main causes of global warming?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 52, "text": " Our website contains news articles in French, and we need to categorize them into sports, politics, and science.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 53, "text": " We are a news outlet, and our editors need to categorize the incoming articles into politics, sports, and entertainment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 54, "text": " We would like to identify if the following hypothesis \\\"A woman is walking a dog\\\" represents a contradiction, entailment, or neutral relationship with the given premise \\\"A woman is strolling with her pet.\\\"\\n###Input: premise = \\\"A woman is strolling with her pet.\\\", hypothesis = \\\"A woman is walking a dog.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'prithivida/parrot_adequacy_model\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'prithivida/parrot_adequacy_model\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. This model is an ancillary model for Parrot paraphraser.\\'}', metadata={})]", "category": "generic"}
{"question_id": 55, "text": " My software needs to improve in conversational understanding. I want it to predict the logical relationship of two short texts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 56, "text": " We are a multinational company, and our website received a review in Russian. We want to understand the content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-en-ru\\', \\'api_call\\': \\'Helsinki-NLP/opus-mt-en-ru\\', \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'tensorflow\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_en_to_ru\\', model=\\'Helsinki-NLP/opus-mt-en-ru\\')\\\\ntranslated_text = translation(\\'Hello, how are you?\\')\", \\'performance\\': {\\'dataset\\': \\'newstest2019-enru\\', \\'accuracy\\': \\'27.1\\'}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 57, "text": " We are working on a research paper and one of the paragraphs needs translation from Italian to English. Can you help me to do it?\\n###Input: During my trip to Italy, I found out Italians are incredibly passionate about food. \\\"essenziale per la salute e il benessere, coscome il piacere che si ottiene da un pasto delizioso.\\\" The Italian cuisine is famous for its simplicity, with most dishes comprising only a few high-quality ingredients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-it-en\\', \\'api_call\\': \"pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')(\\'Ciao mondo!\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newssyscomb2009.it.en\\': 35.3, \\'newstest2009.it.en\\': 34.0, \\'Tatoeba.it.en\\': 70.9}, \\'chr-F\\': {\\'newssyscomb2009.it.en\\': 0.6, \\'newstest2009.it.en\\': 0.594, \\'Tatoeba.it.en\\': 0.808}}}, \\'description\\': \\'A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 58, "text": " Create an email draft for a German colleague, but I need the mail to be available in Spanish since they requested it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-de-es\\', \\'api_call\\': \\'Helsinki-NLP/opus-mt-de-es\\', \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_de_to_es\\', model=\\'Helsinki-NLP/opus-mt-de-es\\')\\\\ntranslated_text = translation(\\'Guten Tag\\')\", \\'performance\\': {\\'dataset\\': \\'Tatoeba.de.es\\', \\'accuracy\\': {\\'BLEU\\': 48.5, \\'chr-F\\': 0.676}}, \\'description\\': \\'A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 59, "text": " Prepare a software that extracts the main points of a German text.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'German Sentiment Classification', 'api_name': 'oliverguhr/german-sentiment-bert', 'api_call': 'Output: SentimentModel()', 'api_arguments': ['texts'], 'python_environment_requirements': 'pip install germansentiment', 'example_code': ['from germansentiment import SentimentModel', 'model = SentimentModel()', 'texts = [', ' Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,', ' Total awesome!,nicht so schlecht wie erwartet,', ' Der Test verlief positiv.,Sie f\u00e4hrt ein gr\u00fcnes Auto.]', 'result = model.predict_sentiment(texts)', 'print(result)'], 'performance': {'dataset': ['holidaycheck', 'scare', 'filmstarts', 'germeval', 'PotTS', 'emotions', 'sb10k', 'Leipzig Wikipedia Corpus 2016', 'all'], 'accuracy': [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, 'description': 'This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.'}\", metadata={})]", "category": "generic"}
{"question_id": 60, "text": " I have a long Chinese article about sports news. I want to get a summary of the article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 61, "text": " Generate a story for kids about space traveling with the Big Red Button that could change everything.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/bigbird-pegasus-large-arxiv\\', \\'api_call\\': \"BigBirdPegasusForConditionalGeneration.from_pretrained(\\'google/bigbird-pegasus-large-arxiv\\')\", \\'api_arguments\\': [\\'attention_type\\', \\'block_size\\', \\'num_random_blocks\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\ntext = Replace me by any text you\\'d like.\\\\ninputs = tokenizer(text, return_tensors=\\'pt\\')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\", \\'performance\\': {\\'dataset\\': \\'scientific_papers\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.028, \\'ROUGE-2\\': 13.417, \\'ROUGE-L\\': 21.961, \\'ROUGE-LSUM\\': 29.648}}, \\'description\\': \"BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT\\'s attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\"}', metadata={})]", "category": "generic"}
{"question_id": 62, "text": " I want to start a new novel about my life. Can you please help me generate the first few lines?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'t5_sentence_paraphraser\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', model=\\'ramsrigouthamg/t5_sentence_paraphraser\\') should be rewritten as T5ForConditionalGeneration.from_pretrained(\\'ramsrigouthamg/t5_sentence_paraphraser\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A T5 model for paraphrasing sentences\\'}', metadata={})]", "category": "generic"}
{"question_id": 63, "text": " We need a powerful model to generate statistical analysis from a paragraph.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'PegasusForConditionalGeneration.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 64, "text": " Let the program suggest the next word in the sentence \\\"The AI model can complete any unfinished task by [MASK].\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Masked Language Modeling, Next Sentence Prediction\\', \\'api_name\\': \\'bert-base-uncased\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'bert-base-uncased\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nunmasker = pipeline(\\'fill-mask\\', model=\\'bert-base-uncased\\')\\\\nunmasker(Hello I\\'m a [MASK] model.)\", \\'performance\\': {\\'dataset\\': \\'GLUE\\', \\'accuracy\\': 79.6}, \\'description\\': \\'BERT base model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It can be used for masked language modeling, next sentence prediction, and fine-tuning on downstream tasks such as sequence classification, token classification, or question answering.\\'}', metadata={})]", "category": "generic"}
{"question_id": 65, "text": " Write a code that helps me autocompleting an input text.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 66, "text": " We want to search for relevant content in customer feedback to improve our product. Identify the most similar feedback to a given query.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 67, "text": " Our company is creating a chatbot. Can you go through user messages and find the ones that are most similar to each other?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 68, "text": " I want to build an AI to answer if Sentences A and B are similar or not in nature.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'sentence-transformers/multi-qa-mpnet-base-dot-v1\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/multi-qa-mpnet-base-dot-v1\\')\", \\'api_arguments\\': [\\'query\\', \\'docs\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer, util\\\\nquery = How many people live in London?\\\\ndocs = [Around 9 Million people live in London, London is known for its financial district]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/multi-qa-mpnet-base-dot-v1\\')\\\\nquery_emb = model.encode(query)\\\\ndoc_emb = model.encode(docs)\\\\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\\\\ndoc_score_pairs = list(zip(docs, scores))\\\\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\\\\nfor doc, score in doc_score_pairs:\\\\n print(score, doc)\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'WikiAnswers\\', \\'accuracy\\': 77427422}, {\\'name\\': \\'PAQ\\', \\'accuracy\\': 64371441}, {\\'name\\': \\'Stack Exchange\\', \\'accuracy\\': 25316456}]}, \\'description\\': \\'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.\\'}', metadata={})]", "category": "generic"}
{"question_id": 69, "text": " I am an employee in a big corporation, I need to find similar sentences in a document with many sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'sentence-transformers/all-roberta-large-v1\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/all-roberta-large-v1\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/all-roberta-large-v1\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'Automated evaluation\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 70, "text": " Design a notification system that reads out upcoming events from a user's calendar using synthesized speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Marathi_Male_TTS\\', \\'api_call\\': \"api.load(\\'ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS\\').\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Marathi Male Text-to-Speech model using ESPnet framework.\\'}', metadata={})]", "category": "generic"}
{"question_id": 71, "text": " My startup works within India, primarily in the Marathi language. Help me convert text to speech suited for a male voice in Marathi.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Marathi_Male_TTS\\', \\'api_call\\': \"api.load(\\'ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS\\').\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Marathi Male Text-to-Speech model using ESPnet framework.\\'}', metadata={})]", "category": "generic"}
{"question_id": 72, "text": " In our mobile application, we need a function to read the users messages in Telugu Male voice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Telugu_Male_TTS\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'SYSPIN/Telugu_Male_TTS\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.\\'}', metadata={})]", "category": "generic"}
{"question_id": 73, "text": " Create a system that translates the phrase \\\"Welcome to the team. We are excited to have you\\\" into speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Marathi_Male_TTS\\', \\'api_call\\': \"api.load(\\'ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS\\').\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Marathi Male Text-to-Speech model using ESPnet framework.\\'}', metadata={})]", "category": "generic"}
{"question_id": 74, "text": " Recently, a hearing-impaired colleague requested an app to convert audio meetings to written text. Develop a solution that transcribes audio files.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 75, "text": " Develop a tool that can identify and differentiate speakers in a conversation for our transcription service.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 76, "text": " We are building an app for transcribing audio calls between multiple users. It's essential to divide overlapping speech into separate audio recordings for each user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 77, "text": " We want to transcribe speech from a podcast into text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Telugu_Male_TTS\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'SYSPIN/Telugu_Male_TTS\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.\\'}', metadata={})]", "category": "generic"}
{"question_id": 78, "text": " Play this Chinese video and provide transcriptions.\\n###Input: /path/to/chinese_video.mp4\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\', \\'api_call\\': \"Output: Text2Speech.from_pretrained(\\'espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'espnet_model_zoo\\'], \\'example_code\\': \\'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = \u6625\u6c5f\u6f6e\u6c34\u8fde\u6d77\u5e73\uff0c\u6d77\u4e0a\u660e\u6708\u5171\u6f6e\u751f\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\\', \\'performance\\': {\\'dataset\\': \\'csmsc\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\\'}', metadata={})]", "category": "generic"}
{"question_id": 79, "text": " I have an audio file containing the sound of a machine and a person talking in the background. I want to separate the two sounds for better analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Enhancement\\', \\'api_name\\': \\'sepformer-wham-enhancement\\', \\'api_call\\': \"model.separate_file(path=\\'speechbrain/sepformer-wham-enhancement/example_wham.wav\\')\", \\'api_arguments\\': [\\'path\\'], \\'python_environment_requirements\\': [\\'pip install speechbrain\\'], \\'example_code\\': \"from speechbrain.pretrained import SepformerSeparation as separator\\\\nimport torchaudio\\\\nmodel = separator.from_hparams(source=\\'speechbrain/sepformer-wham-enhancement\\', savedir=\\'pretrained_models/sepformer-wham-enhancement\\')\\\\nest_sources = model.separate_file(path=\\'speechbrain/sepformer-wham-enhancement/example_wham.wav\\')\\\\ntorchaudio.save(\\'enhanced_wham.wav\\', est_sources[:, :, 0].detach().cpu(), 8000)\", \\'performance\\': {\\'dataset\\': \\'WHAM!\\', \\'accuracy\\': \\'14.35 dB SI-SNR\\'}, \\'description\\': \\'This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k.\\'}', metadata={})]", "category": "generic"}
{"question_id": 80, "text": " Our customer is English-speaking and needs to communicate with Hokkien-speaking locals. We need to provide speech-to-speech translation from English to Hokkien.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'xm_transformer_s2ut_en-hk\\', \\'api_call\\': \"load_model_ensemble_and_task_from_hf_hub(\\'facebook/xm_transformer_s2ut_en-hk\\')\", \\'api_arguments\\': {\\'arg_overrides\\': {\\'config_yaml\\': \\'config.yaml\\', \\'task\\': \\'speech_to_text\\'}, \\'cache_dir\\': \\'cache_dir\\'}, \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\', \\'torchaudio\\'], \\'example_code\\': {\\'import_modules\\': [\\'import json\\', \\'import os\\', \\'from pathlib import Path\\', \\'import IPython.display as ipd\\', \\'from fairseq import hub_utils\\', \\'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\', \\'from fairseq.models.speech_to_text.hub_interface import S2THubInterface\\', \\'from fairseq.models.text_to_speech import CodeHiFiGANVocoder\\', \\'from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\', \\'from huggingface_hub import snapshot_download\\', \\'import torchaudio\\'], \\'load_model\\': [\"cache_dir = os.getenv(\\'HUGGINGFACE_HUB_CACHE\\')\", \"models, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\'facebook/xm_transformer_s2ut_en-hk\\', arg_overrides={\\'config_yaml\\': \\'config.yaml\\', \\'task\\': \\'speech_to_text\\'}, cache_dir=cache_dir)\", \\'model = models[0].cpu()\\', \"cfg[\\'task\\'].cpu = True\"], \\'generate_prediction\\': [\\'generator = task.build_generator([model], cfg)\\', \"audio, _ = torchaudio.load(\\'/path/to/an/audio/file\\')\", \\'sample = S2THubInterface.get_model_input(task, audio)\\', \\'unit = S2THubInterface.get_prediction(task, model, generator, sample)\\'], \\'speech_synthesis\\': [\"library_name = \\'fairseq\\'\", \"cache_dir = (cache_dir or (Path.home() / \\'.cache\\' / library_name).as_posix())\", \"cache_dir = snapshot_download(\\'facebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\', cache_dir=cache_dir, library_name=library_name)\", \"x = hub_utils.from_pretrained(cache_dir, \\'model.pt\\', \\'.\\', archive_map=CodeHiFiGANVocoder.hub_models(), config_yaml=\\'config.json\\', fp16=False, is_vocoder=True)\", \"with open(f\\'{x[\\'args\\'][\\'data\\']}/config.json\\') as f:\", \\'  vocoder_cfg = json.load(f)\\', \"assert (len(x[\\'args\\'][\\'model_path\\']) == 1), \\'Too many vocoder models in the input\\'\", \"vocoder = CodeHiFiGANVocoder(x[\\'args\\'][\\'model_path\\'][0], vocoder_cfg)\", \\'tts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\', \\'tts_sample = tts_model.get_model_input(unit)\\', \\'wav, sr = tts_model.get_prediction(tts_sample)\\', \\'ipd.Audio(wav, rate=sr)\\']}, \\'performance\\': {\\'dataset\\': \\'MuST-C\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Speech-to-speech translation model with single-pass decoder (S2UT) from fairseq: English-Hokkien. Trained with supervised data in TED domain, and weakly supervised data in TED and Audiobook domain.\\'}', metadata={})]", "category": "generic"}
{"question_id": 81, "text": " We have an app that records sounds and we need to extract multiple voices from the environment. We'd like to try separating three speakers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 82, "text": " As a tourist to Romania, I just recorded a native speaker explaining directions, and I need a translation of this speech into English.\\n###Input: Romanian_speaker_audio_recording.wav\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'facebook/textless_sm_ro_en\\', \\'api_call\\': \"pipeline(\\'audio-to-audio\\', model=\\'facebook/textless_sm_ro_en\\')\", \\'api_arguments\\': \\'audio file or recording\\', \\'python_environment_requirements\\': \\'fairseq, huggingface_hub\\', \\'example_code\\': \\'https://huggingface.co/facebook/textless_sm_cs_en\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A speech-to-speech translation model for Romanian to English developed by Facebook AI\\'}', metadata={})]", "category": "generic"}
{"question_id": 83, "text": " There's a noisy background sound in the recorded meeting we had today. Can you please enhance the speech signal?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Enhancement\\', \\'api_name\\': \\'speechbrain/mtl-mimic-voicebank\\', \\'api_call\\': \"WaveformEnhancement.from_hparams(\\'speechbrain/mtl-mimic-voicebank\\', \\'pretrained_models/mtl-mimic-voicebank\\').enhance_file(\\'speechbrain/mtl-mimic-voicebank/example.wav\\')\", \\'api_arguments\\': [\\'source\\', \\'savedir\\'], \\'python_environment_requirements\\': \\'pip install speechbrain\\', \\'example_code\\': \"import torchaudio\\\\nfrom speechbrain.pretrained import WaveformEnhancement\\\\nenhance_model = WaveformEnhancement.from_hparams(\\\\n source=speechbrain/mtl-mimic-voicebank,\\\\n savedir=pretrained_models/mtl-mimic-voicebank,\\\\n)\\\\nenhanced = enhance_model.enhance_file(speechbrain/mtl-mimic-voicebank/example.wav)\\\\ntorchaudio.save(\\'enhanced.wav\\', enhanced.unsqueeze(0).cpu(), 16000)\", \\'performance\\': {\\'dataset\\': \\'Voicebank\\', \\'accuracy\\': {\\'Test PESQ\\': 3.05, \\'Test COVL\\': 3.74, \\'Valid WER\\': 2.89, \\'Test WER\\': 2.8}}, \\'description\\': \\'This repository provides all the necessary tools to perform enhancement and\\\\nrobust ASR training (EN) within\\\\nSpeechBrain. For a better experience we encourage you to learn more about\\\\nSpeechBrain. The model performance is:\\\\nRelease\\\\nTest PESQ\\\\nTest COVL\\\\nValid WER\\\\nTest WER\\\\n22-06-21\\\\n3.05\\\\n3.74\\\\n2.89\\\\n2.80\\\\nWorks with SpeechBrain v0.5.12\\'}', metadata={})]", "category": "generic"}
{"question_id": 84, "text": " We're developing a voice assistant with emotion recognition. Help us classify emotions in spoken language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'SpeechBrain\\', \\'functionality\\': \\'Emotion Recognition\\', \\'api_name\\': \\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP\\', \\'api_call\\': \"classifier = foreign_class(source=\\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP\\', pymodule_file=\\'custom_interface.py\\', classname=\\'CustomEncoderWav2vec2Classifier\\')\\\\\\\\nclassifier.classify_file(\\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav\\')\", \\'api_arguments\\': [\\'file_path\\'], \\'python_environment_requirements\\': \\'pip install speechbrain\\', \\'example_code\\': \"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source=\\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP\\', pymodule_file=\\'custom_interface.py\\', classname=\\'CustomEncoderWav2vec2Classifier\\')\\\\nout_prob, score, index, text_lab = classifier.classify_file(\\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav\\')\\\\nprint(text_lab)\", \\'performance\\': {\\'dataset\\': \\'IEMOCAP\\', \\'accuracy\\': \\'78.7%\\'}, \\'description\\': \\'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 85, "text": " I am developing an AI tool to spot specific keywords present in different audio files to categorize them. Provide me with a solution to detect keywords in the given audio files.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ast-finetuned-speech-commands-v2\\', \\'api_call\\': \\'MIT/ast-finetuned-speech-commands-v2\\', \\'api_arguments\\': \\'audio file\\', \\'python_environment_requirements\\': \\'transformers library\\', \\'example_code\\': \"result = audio_classifier(\\'path/to/audio/file.wav\\')\", \\'performance\\': {\\'dataset\\': \\'Speech Commands v2\\', \\'accuracy\\': \\'98.120\\'}, \\'description\\': \\'Audio Spectrogram Transformer (AST) model fine-tuned on Speech Commands v2. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 86, "text": " A customer support center needs a tool to analyze customers' satisfaction based on their voice messages. Recommend a solution for evaluating sentiment in Spanish voice messages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'finiteautomata/beto-sentiment-analysis\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'finiteautomata/beto-sentiment-analysis\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'Hugging Face Transformers library\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TASS 2020 corpus\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 87, "text": " Your company is building voice assistants for smartphones, and they must be able to recognize and execute spoken commands efficiently.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 88, "text": " A finance company needs a way to predict whether potential clients will have an income above $50,000 per year.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 89, "text": " We are a data science agency, we are helping people to predict if a passenger of titanic will survive based on different features.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'harithapliyal/autotrain-tatanic-survival-51030121311\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'harithapliyal/autotrain-data-tatanic-survival\\', \\'accuracy\\': 0.872}, \\'description\\': \\'A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.\\'}', metadata={})]", "category": "generic"}
{"question_id": 90, "text": " We are a museum and we need to classify the survivers from Titanic according to their age, gender, and passenger class.\\n###Input: data = pd.read_csv('data.csv')\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'harithapliyal/autotrain-tatanic-survival-51030121311\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'harithapliyal/autotrain-data-tatanic-survival\\', \\'accuracy\\': 0.872}, \\'description\\': \\'A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.\\'}', metadata={})]", "category": "generic"}
{"question_id": 91, "text": " Estimate the carbon emissions of a vehicle using its underlying features.\\n###Input: {\\\"data\\\": {\\\"Engine Size(L)\\\": 1.5, \\\"Cylinders\\\": 4, \\\"Transmission\\\": \\\"Manual\\\", \\\"Fuel Type\\\": \\\"gas\\\", \\\"Fuel Consumption Comb (L/100 km)\\\": 6.0}}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'45473113800\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'samvelkoch/autotrain-data-prknsn-2\\', \\'accuracy\\': {\\'Loss\\': 5.079, \\'R2\\': 0.109, \\'MSE\\': 25.795, \\'MAE\\': 3.78, \\'RMSLE\\': 0.849}}, \\'description\\': \\'A tabular regression model trained with AutoTrain for predicting carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 92, "text": " A real estate company needs a prediction tool to estimate housing prices for different regions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761514\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 100595.98, \\'R2\\': 0.922, \\'MSE\\': 10119551129.473, \\'MAE\\': 81601.198, \\'RMSLE\\': 0.101}}, \\'description\\': \\'A tabular regression model trained using AutoTrain to predict US housing prices. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a R2 score of 0.922.\\'}', metadata={})]", "category": "generic"}
{"question_id": 93, "text": " I want to predict the carbon emissions of a specific car with given features.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'45473113800\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'samvelkoch/autotrain-data-prknsn-2\\', \\'accuracy\\': {\\'Loss\\': 5.079, \\'R2\\': 0.109, \\'MSE\\': 25.795, \\'MAE\\': 3.78, \\'RMSLE\\': 0.849}}, \\'description\\': \\'A tabular regression model trained with AutoTrain for predicting carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 94, "text": " As an environmental organization, we want to estimate the carbon emissions from various factories given specific input data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'38507101578\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'wangdy/autotrain-data-godaddy2\\', \\'accuracy\\': {\\'Loss\\': 0.206, \\'R2\\': 0.997, \\'MSE\\': 0.042, \\'MAE\\': 0.081, \\'RMSLE\\': 0.052}}, \\'description\\': \\'This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\\'}', metadata={})]", "category": "generic"}
{"question_id": 95, "text": " I am a data scientist working on a project to predict carbon emissions based on a set of input data. Provide instructions to load and use the model to make predictions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1918465011\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\', \\'json\\'], \\'example_code\\': [\\'import json\\', \\'import joblib\\', \\'import pandas as pd\\', \"model = joblib.load(\\'model.joblib\\')\", \"config = json.load(open(\\'config.json\\'))\", \"features = config[\\'features\\']\", \"data = pd.read_csv(\\'data.csv\\')\", \\'data = data[features]\\', \"data.columns = [\\'feat_\\' + str(col) for col in data.columns]\", \\'predictions = model.predict(data)\\'], \\'performance\\': {\\'dataset\\': \\'kyle-lucke/autotrain-data-planes\\', \\'accuracy\\': 0.997}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 96, "text": " A restaurant wants to predict tips given by customers based on factors like bill amount, day, time, and other attributes. We can use this to optimize staff allocation during peak hours.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Scikit-learn\\', \\'functionality\\': \\'baseline-trainer\\', \\'api_name\\': \\'merve/tips9y0jvt5q-tip-regression\\', \\'api_call\\': \"pipeline(\\'tabular-regression\\', model=\\'merve/tips9y0jvt5q-tip-regression\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'dabl\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'tips9y0jvt5q\\', \\'accuracy\\': {\\'r2\\': 0.41524, \\'neg_mean_squared_error\\': -1.098792}}, \\'description\\': \\'Baseline Model trained on tips9y0jvt5q to apply regression on tip. The model uses Ridge(alpha=10) and is trained with dabl library as a baseline. For better results, use AutoTrain.\\'}', metadata={})]", "category": "generic"}
{"question_id": 97, "text": " We need to predict fish's weight based on different measurements, as part of a fish farm management software.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Scikit-learn\\', \\'functionality\\': \\'GradientBoostingRegressor\\', \\'api_name\\': \\'Fish-Weight\\', \\'api_call\\': \"load(\\'path_to_folder/example.pkl\\')\", \\'api_arguments\\': {\\'model_path\\': \\'path_to_folder/example.pkl\\'}, \\'python_environment_requirements\\': {\\'skops.hub_utils\\': \\'download\\', \\'skops.io\\': \\'load\\'}, \\'example_code\\': \"from skops.hub_utils import download\\\\nfrom skops.io import load\\\\ndownload(\\'brendenc/Fish-Weight\\', \\'path_to_folder\\')\\\\nmodel = load(\\'path_to_folder/example.pkl\\')\", \\'performance\\': {\\'dataset\\': \\'Fish dataset\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a GradientBoostingRegressor on a fish dataset. This model is intended for educational purposes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 98, "text": " We are making an application that predicts stock closing prices using a regression model on financial data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761513\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 100581.032, \\'R2\\': 0.922, \\'MSE\\': 10116543945.03, \\'MAE\\': 81586.656, \\'RMSLE\\': 0.101}}, \\'description\\': \\'A single column regression model for predicting US housing prices, trained with AutoTrain and using the Joblib framework.\\'}', metadata={})]", "category": "generic"}
{"question_id": 99, "text": " Create an AI model that can play CartPole-v1 game and store the results.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Stable-Baselines3\\', \\'functionality\\': \\'LunarLander-v2\\', \\'api_name\\': \\'araffin/ppo-LunarLander-v2\\', \\'api_call\\': \"Output: model = PPO.load_from_hub(\\'araffin/ppo-LunarLander-v2\\', \\'ppo-LunarLander-v2.zip\\')\", \\'api_arguments\\': {\\'checkpoint\\': \\'araffin/ppo-LunarLander-v2.zip\\'}, \\'python_environment_requirements\\': [\\'huggingface_sb3\\', \\'stable_baselines3\\'], \\'example_code\\': {\\'load_model\\': \\'from huggingface_sb3 import load_from_hub\\\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\\\nmodel = PPO.load(checkpoint)\\', \\'evaluate\\': \\'from stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})\\'}, \\'performance\\': {\\'dataset\\': \\'LunarLander-v2\\', \\'accuracy\\': \\'283.49 +/- 13.74\\'}, \\'description\\': \\'This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 100, "text": " A soccer match is being co-organized by a couple of sports organizations. The sports organizers want to prepare a learning-based simulation for the match.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 101, "text": " The game development team needs a learning-based agent to create a character that can learn to hop in a game environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 102, "text": " Create a game application that requires the interaction between two soccer-playing agents. The agents will learn how to play from each interaction.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 103, "text": " A new robotic toy is being developed for kids. It recognizes objects and interacts with them accordingly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Inference API\\', \\'api_name\\': \\'Antheia/Hanna\\', \\'api_call\\': \"pipeline(\\'robotics\\', model=\\'Antheia/Hanna\\') should remain unchanged as it already only includes the model instantiation.\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'openai/webgpt_comparisons\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 104, "text": " A home robotics company requires object manipulation and indoor navigation capabilities. How can we achieve this using available APIs?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Inference API\\', \\'api_name\\': \\'Antheia/Hanna\\', \\'api_call\\': \"pipeline(\\'robotics\\', model=\\'Antheia/Hanna\\') should remain unchanged as it already only includes the model instantiation.\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'openai/webgpt_comparisons\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 105, "text": " Create a new story to be read to a child by starting the story with \\\"Once upon a time in a land far away...\\\"\\n###Input: \\\"Once upon a time in a land far away...\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 106, "text": " In the next product launch, we want to showcase the company mascot. We need a high-quality anime character with specific features.\\n###Input: 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 107, "text": " I need a script to create photorealistic images from a description I provide, so I can use the generated images for my art projects.\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 108, "text": " Develop a piece of code that generates a high-resolution image of a landscape using a given prompt and low-resolution latents in the provided AI model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Upscaling\\', \\'api_name\\': \\'stabilityai/sd-x2-latent-upscaler\\', \\'api_call\\': \\'upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator)\\', \\'api_arguments\\': {\\'prompt\\': \\'text prompt\\', \\'image\\': \\'low resolution latents\\', \\'num_inference_steps\\': 20, \\'guidance_scale\\': 0, \\'generator\\': \\'torch generator\\'}, \\'python_environment_requirements\\': [\\'git+https://github.com/huggingface/diffusers.git\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': \\'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\\\nimport torch\\\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\\\npipeline.to(cuda)\\\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\\\nupscaler.to(cuda)\\\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\\\ngenerator = torch.manual_seed(33)\\\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\\\nupscaled_image.save(astronaut_1024.png)\\', \\'performance\\': {\\'dataset\\': \\'LAION-2B\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion\\'s latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\"}', metadata={})]", "category": "generic"}
{"question_id": 109, "text": " We have a scanned Japanese manga image and we want to extract the text from it.\\n###Input: image_path = \\\"scanned_manga_page.jpg\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 110, "text": " Please help us with describing the image, which we have submitted for our advertisement campaign.\\n###Input: \\\"<the image provided>\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'prompthero/openjourney-v4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'prompthero/openjourney-v4\\')\", \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"generate_image(\\'your text here\\')\", \\'performance\\': {\\'dataset\\': \\'Midjourney v4 images\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 111, "text": " Our company is designing a virtual assistant that will answer customer questions based on images they submit. Implement a function that finds out how many people are in a picture submitted by the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 112, "text": " I would like to create an AI system that can provide a detailed textual description for a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 113, "text": " A children's publisher requested a system to generate stories based on images. For this purpose, we need to find a solution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 114, "text": " I want to analyze and answer questions about a chart using image and text data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 115, "text": " I want to develop an application to read traffic signboards and provide output text in real-time. Can you tell me the model I can use and code to apply the model?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 116, "text": " We want to provide video summaries of news articles on our platform. We would like to turn our article's text into video summaries.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 117, "text": " As a children's author, I need my story to be converted to video clips. The text is in English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 118, "text": " Produce a short movie scene showing the protagonist breaking up with their partner while on a walk in the city under the rain.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 119, "text": " Please generate a code snippet for me to create an AI algorithm that answers questions based on the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'blip-vqa-base\\', \\'api_call\\': \"Output: BlipForQuestionAnswering.from_pretrained(\\'Salesforce/blip-vqa-base\\').generate(**inputs)\", \\'api_arguments\\': {\\'raw_image\\': \\'Image\\', \\'question\\': \\'String\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'BlipProcessor, BlipForQuestionAnswering\\', \\'PIL\\': \\'Image\\', \\'requests\\': \\'requests\\'}, \\'example_code\\': \"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-base)\\\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-base)\\\\nimg_url = \\'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\\'RGB\\')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \\'performance\\': {\\'dataset\\': \\'VQA\\', \\'accuracy\\': \\'+1.6% in VQA score\\'}, \\'description\\': \\'BLIP is a Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is trained on visual question answering with a base architecture (using ViT base backbone).\\'}', metadata={})]", "category": "generic"}
{"question_id": 120, "text": " We are an online study group specializing in history. Our members can upload an image of the text, and they ask questions related to the text. We need a Q&A model to answer these questions automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/git-base-textvqa\\', \\'api_call\\': \"git_base_textvqa = AutoModel.from_pretrained(\\'microsoft/git-base-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa_pipeline({\\'image\\': \\'path/to/image.jpg\\', \\'question\\': \\'What is in the image?\\'})\", \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'Refer to the paper\\'}, \\'description\\': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}', metadata={})]", "category": "generic"}
{"question_id": 121, "text": " Our company needs a solution to automatically extract specific information such as invoice numbers and dates from invoices in various formats.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 122, "text": " I have a document containing information about a company's products, and I want to find answers to my questions about these products.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 123, "text": " I need an electronic document examiner that can answer a question about my document content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 124, "text": " I need a molecule prediction model to predict the properties of different molecules in my research.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1918465011\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\', \\'json\\'], \\'example_code\\': [\\'import json\\', \\'import joblib\\', \\'import pandas as pd\\', \"model = joblib.load(\\'model.joblib\\')\", \"config = json.load(open(\\'config.json\\'))\", \"features = config[\\'features\\']\", \"data = pd.read_csv(\\'data.csv\\')\", \\'data = data[features]\\', \"data.columns = [\\'feat_\\' + str(col) for col in data.columns]\", \\'predictions = model.predict(data)\\'], \\'performance\\': {\\'dataset\\': \\'kyle-lucke/autotrain-data-planes\\', \\'accuracy\\': 0.997}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 125, "text": " As an autonomous car company, we want to estimate the depth of objects in front of the car's cameras.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'glpn-nyu-finetuned-diode-221215-092352\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sayakpaul/glpn-nyu-finetuned-diode-221215-092352\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'huggingface_transformers\\': \\'4.13.0\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'DIODE\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A depth estimation model fine-tuned on the DIODE dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 126, "text": " I am building a robot that needs to navigate through a room. I need to estimate the depth of the room from an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 127, "text": " Our environmental agency wants to estimate the depth of plant cover in a forest from the given dataset of images to understand their growth patterns.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-nyu-finetuned-diode\\', \\'api_call\\': \"pipeline(\\'depth-estimation\\', model=\\'sayakpaul/glpn-nyu-finetuned-diode\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'diode-subset\\', \\'accuracy\\': {\\'Loss\\': 0.4359, \\'Rmse\\': 0.4276}}, \\'description\\': \\'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 128, "text": " A local grocery store is developing an AI application to identify fruits and vegetables from their images. What kind of code could they use for image classification?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 129, "text": " We are developing an app that identifies objects in pictures. In order to classify images, I must find a suitable image classification model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 130, "text": " The security team at our company is working on improving camera surveillance in parking lots. We need a model to track objects and cars in the parking lots.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'License Plate Detection\\', \\'api_name\\': \\'keremberke/yolov5s-license-plate\\', \\'api_call\\': \\'model(img, size=640)\\', \\'api_arguments\\': {\\'img\\': \\'image url or path\\', \\'size\\': \\'image resize dimensions\\', \\'augment\\': \\'optional, test time augmentation\\'}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': [\\'import yolov5\\', \"model = yolov5.load(\\'keremberke/yolov5s-license-plate\\')\", \\'model.conf = 0.25\\', \\'model.iou = 0.45\\', \\'model.agnostic = False\\', \\'model.multi_label = False\\', \\'model.max_det = 1000\\', \"img = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model(img, size=640)\\', \\'results = model(img, augment=True)\\', \\'predictions = results.pred[0]\\', \\'boxes = predictions[:, :4]\\', \\'scores = predictions[:, 4]\\', \\'categories = predictions[:, 5]\\', \\'results.show()\\', \"results.save(save_dir=\\'results/\\')\"], \\'performance\\': {\\'dataset\\': \\'keremberke/license-plate-object-detection\\', \\'accuracy\\': 0.985}, \\'description\\': \\'A YOLOv5 based license plate detection model trained on a custom dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 131, "text": " Our construction safety management team needs to detect whether workers are using hard hats in a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-hard-hat-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\\\\n\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'hard-hat-detection\\', \\'accuracy\\': 0.811}, \\'description\\': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between \\'Hardhat\\' and \\'NO-Hardhat\\' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\"}', metadata={})]", "category": "generic"}
{"question_id": 132, "text": " As a company specialized in smart cities, we need to develop roadway maintenance tools. Recently we got a requirement for detecting potholes in road images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8s-pothole-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8s-pothole-segmentation\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': {\\'ultralyticsplus\\': \\'0.0.23\\', \\'ultralytics\\': \\'8.0.21\\'}, \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8s-pothole-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pothole-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.928, \\'mAP@0.5(mask)\\': 0.928}}, \\'description\\': \\'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 133, "text": " Design a virtual assistant to help users create an effective Instagram Story image. The assistant should modify the input image by generating a stylized version as output.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 134, "text": " How can I insert a text prompt for creating an image which shows the text's meaning?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 135, "text": " I want to create a new image of a beautiful mountain scenery by providing input of a plain field picture.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': 'pipe(prompt=prompt, image=image, mask_image=mask_image)', 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.'}\", metadata={})]", "category": "generic"}
{"question_id": 136, "text": " I am the owner of a photography website and want to automatically generate pictures of cats in order to add content to my website.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': 'DDPMPipeline.from_pretrained(model_id)', 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.'}\", metadata={})]", "category": "generic"}
{"question_id": 137, "text": " We need to generate a high-quality image for the AI to analyze some human face expression so the algorithms should be improved.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 138, "text": " We are an entertainment company specializing in creating realistic virtual characters. We want to generate synthetic images of human faces.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 139, "text": " Can you make a butterfly-themed birthday card for my niece?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ntrant7/sd-class-butterflies-32\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'Not specified\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies.\\'}', metadata={})]", "category": "generic"}
{"question_id": 140, "text": " We need to come up with some creative and innovative facial designs for our new android.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Voice Activity Detection\\', \\'api_name\\': \\'d4data/Indian-voice-cloning\\', \\'api_call\\': \"pipeline(\\'voice-activity-detection\\', model=\\'d4data/Indian-voice-cloning\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model for detecting voice activity in Indian languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 141, "text": " Can you make it work to generate an image of a cute butterfly for us?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ntrant7/sd-class-butterflies-32\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'Not specified\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies.\\'}', metadata={})]", "category": "generic"}
{"question_id": 142, "text": " Animate a video based on the past images we have from the surveillance camera.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-short-finetuned-kinetics\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\", \\'api_arguments\\': [\\'video\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\ninputs = processor(video, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\'Predicted class:\\', model.config.id2label[predicted_class_idx])\", \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': {\\'top-1\\': 79.4, \\'top-5\\': 94.1}}, \\'description\\': \\'VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 143, "text": " I need help to classify the videos online for my content platform.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'facebook/timesformer-base-finetuned-ssv2\\', \\'api_call\\': \"TimesformerForVideoClassification.from_pretrained(\\'facebook/timesformer-base-finetuned-ssv2\\')\", \\'api_arguments\\': [\\'images\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'numpy\\', \\'torch\\'], \\'example_code\\': \\'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-ssv2)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-ssv2)\\\\ninputs = processor(images=video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something Something v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 144, "text": " Look at this visual poster design and find out if it is suitable for use in a bakery.\\n###Input: <url to the poster>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 145, "text": " We are building an app to provide information about objects around us. The user can take a picture of anything in front of them, and the app will tell them what it is.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 146, "text": " I am building a machine learning powered app that can classify food into three categories: fruit, vegetable, and junk food just by analyzing images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 147, "text": " Classify an image of food and determine if it's a burger or a pizza with a pre-trained model.\\n###Input: \\\"path/to/food_image.jpg\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'julien-c/hotdog-not-hotdog\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'julien-c/hotdog-not-hotdog\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': 0.825}, \\'description\\': \\'A model that classifies images as hotdog or not hotdog.\\'}', metadata={})]", "category": "generic"}
{"question_id": 148, "text": " We have a box of photos for old family trips, but we can't identify the place. Can you help us estimate which country or region the pictures were taken in?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 149, "text": " A client of ours wants to classify images of animals when they upload them to their website to better categorize the content. We are looking for a solution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 150, "text": " We are a startup focusing on content creation for a Chinese audience. We want to recommend images related to poke categories automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 151, "text": " I run a review website where users submit their reviews of various products. I want to classify positive and negative reviews.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 152, "text": " We need to keep our platform clean from unsuitable content. Build a system that can identify and flag NSFW (not safe for work) text in user-generated content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 153, "text": " An AI application development company wants to develop an application that can classify emotions from a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Emotion Classification\\', \\'api_name\\': \\'j-hartmann/emotion-english-distilroberta-base\\', \\'api_call\\': \\'text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True\\', \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \\'from transformers import pipeline\\\\nclassifier = pipeline(text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True)\\\\nclassifier(I love this!)\\', \\'performance\\': {\\'dataset\\': \\'Balanced subset from 6 diverse datasets\\', \\'accuracy\\': \\'66%\\'}, \\'description\\': \"This model classifies emotions in English text data. It predicts Ekman\\'s 6 basic emotions, plus a neutral class: anger, disgust, fear, joy, neutral, sadness, and surprise. The model is a fine-tuned checkpoint of DistilRoBERTa-base.\"}', metadata={})]", "category": "generic"}
{"question_id": 154, "text": " Explain how to find out the date and location of a meeting mentioned in a sentence.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Embeddings\\', \\'api_name\\': \\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'Automated evaluation\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 155, "text": " I am the editor of a political blog, and we often use quotes from different politicians. I want to extract names of politicians from the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 156, "text": " Your manager needs a detailed report on a given text. Ensure that you include the part-of-speech tags for each word in the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'finiteautomata/beto-sentiment-analysis\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'finiteautomata/beto-sentiment-analysis\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'Hugging Face Transformers library\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TASS 2020 corpus\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 157, "text": " I live in Berlin and I want to develop a multilingual named entity recognition pipeline for processing German, English, and Russian texts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'sshleifer/tiny-marian-en-de\\', \\'api_call\\': \"pipeline(\\'translation_en_to_de\\', model=\\'sshleifer/tiny-marian-en-de\\').model\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A tiny English to German translation model using the Marian framework in Hugging Face Transformers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 158, "text": " As an online tutor, I need to prepare a list of frequently asked questions about my subjects. I require a tool that can quickly answer questions related to a spreadsheet containing data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 159, "text": " I work for a tour agency and I need to answer customer questions using a database of historical Olympic Games. I need to know the host city of the 1904 Olympics.\\n###Input: {\\\"year\\\": [1896, 1900, 1904, 2004, 2008, 2012], \\\"city\\\": [\\\"athens\\\", \\\"paris\\\", \\\"st. louis\\\", \\\"athens\\\", \\\"beijing\\\", \\\"london\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table-based QA\\', \\'api_name\\': \\'neulab/omnitab-large-1024shot\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'neulab/omnitab-large-1024shot\\')\", \\'api_arguments\\': {\\'table\\': \\'pd.DataFrame.from_dict(data)\\', \\'query\\': \\'str\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'pandas\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport pandas as pd\\\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'wikitablequestions\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting).\\'}', metadata={})]", "category": "generic"}
{"question_id": 160, "text": " I want to prepare an AI model for an interactive storybook for kids. It should be able to answer questions related to the story.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 161, "text": " I want to classify text between three subjects. The subjects are art, science and history.\\n###Input: {\\\"text\\\": \\\"The invention of the telescope allowed astronomers to observe the solar system more effectively and led to the discovery of new planets and celestial objects.\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 162, "text": " Suppose my company wants to hire a candidate for a technical writer position. How could we determine whether an applicant's cover letter is aligned with the job expectations or not?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 163, "text": " To assist in efficient communication, we need a tool to help us translate comments on our multilingual social media posts to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'facebook/nllb-200-distilled-600M\\', \\'api_call\\': \"pipeline(\\'translation_xx_to_yy\\', model=\\'facebook/nllb-200-distilled-600M\\')\", \\'api_arguments\\': [\\'model\\', \\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline; translator = pipeline(\\'translation_xx_to_yy\\', model=\\'facebook/nllb-200-distilled-600M\\'); translator(\\'Hello World\\')\", \\'performance\\': {\\'dataset\\': \\'Flores-200\\', \\'accuracy\\': \\'BLEU, spBLEU, chrF++\\'}, \\'description\\': \\'NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 164, "text": " I need to translate an English paragraph to Arabic for my website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-ar\\', \\'api_call\\': \"pipeline(\\'translation_en_to_ar\\', model=\\'Helsinki-NLP/opus-mt-en-ar\\')\", \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_en_to_ar\\', model=\\'Helsinki-NLP/opus-mt-en-ar\\')\\\\ntranslated_text = translation(\\'Hello World\\')\", \\'performance\\': {\\'dataset\\': \\'Tatoeba-test.eng.ara\\', \\'accuracy\\': {\\'BLEU\\': 14.0, \\'chr-F\\': 0.437}}, \\'description\\': \"A Hugging Face Transformers model for English to Arabic translation, trained on the Tatoeba dataset. It uses a transformer architecture and requires a sentence initial language token in the form of \\'>>id<<\\' (id = valid target language ID).\"}', metadata={})]", "category": "generic"}
{"question_id": 165, "text": " I need a tool that can translate Dutch text to English language for an upcoming project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-nl-en\\', \\'api_call\\': \"pipeline(\\'translation_nl_to_en\\', model=\\'Helsinki-NLP/opus-mt-nl-en\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_nl_to_en\\', model=\\'Helsinki-NLP/opus-mt-nl-en\\')\\\\ntranslated_text = translation(\\'Hallo, hoe gaat het met je?\\')[0][\\'translation_text\\']\", \\'performance\\': {\\'dataset\\': \\'Tatoeba.nl.en\\', \\'accuracy\\': {\\'BLEU\\': 60.9, \\'chr-F\\': 0.749}}, \\'description\\': \\'A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 166, "text": " Can you automatically summarize a long article in French language for me?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Abstractive Text Summarization', 'api_name': 'plguillou/t5-base-fr-sum-cnndm', 'api_call': 'T5ForConditionalGeneration.from_pretrained(\\\\\\\\plguillou/t5-base-fr-sum-cnndm\\\\\\\\)', 'api_arguments': {'input_text': 'summarize: ARTICLE'}, 'python_environment_requirements': {'transformers': 'from transformers import T5Tokenizer, T5ForConditionalGeneration'}, 'example_code': 'tokenizer = T5Tokenizer.from_pretrained(plguillou/t5-base-fr-sum-cnndm)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(plguillou/t5-base-fr-sum-cnndm)', 'performance': {'dataset': 'cnn_dailymail', 'ROUGE-1': 44.5252, 'ROUGE-2': 22.652, 'ROUGE-L': 29.8866}, 'description': 'This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 167, "text": " I am a German management consulting firm. I need to summarize the content of business documents for my clients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'Einmalumdiewelt/T5-Base_GNAD\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'Einmalumdiewelt/T5-Base_GNAD\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': {\\'Loss\\': 2.1025, \\'Rouge1\\': 27.5357, \\'Rouge2\\': 8.5623, \\'Rougel\\': 19.1508, \\'Rougelsum\\': 23.9029, \\'Gen Len\\': 52.7253}}, \\'description\\': \\'This model is a fine-tuned version of Einmalumdiewelt/T5-Base_GNAD on an unknown dataset. It is intended for German text summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 168, "text": " Implement a communication method for our employee chatbot for quick and simple responses.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 169, "text": " Design a text summarizer for a Russian news website that will automatically condense the lengthy articles for the readers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 170, "text": " We are creating a chatbot that can participate in a live Dungeons and Dragons game. Bring Pygmalion-6B into the chat session to play a character.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'pygmalion-6b\\', \\'api_call\\': \"Output: model = AutoModel.from_pretrained(\\'uft-6b\\')\", \\'api_arguments\\': [\\'input_prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"[CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]\\\\n<START>\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI\\'s GPT-J-6B. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model was initialized from the uft-6b ConvoGPT model and fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.\"}', metadata={})]", "category": "generic"}
{"question_id": 171, "text": " We have an online platform where psychologists chat with anonymous users. We want a virtual agent that will play the role of a psychologist for some of our users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 172, "text": " We are designing a conversational AI agent for our customers. We want the agent to respond naturally to users' questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 173, "text": " I need help generating a short piece of Python code that calculates the sum of square numbers for any input list.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Abstractive Russian Summarization\\', \\'api_name\\': \\'cointegrated/rut5-base-absum\\', \\'api_call\\': \"Output: T5ForConditionalGeneration.from_pretrained(\\'cointegrated/rut5-base-absum\\')\", \\'api_arguments\\': {\\'n_words\\': \\'int\\', \\'compression\\': \\'float\\', \\'max_length\\': \\'int\\', \\'num_beams\\': \\'int\\', \\'do_sample\\': \\'bool\\', \\'repetition_penalty\\': \\'float\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\\\\nMODEL_NAME = \\'cointegrated/rut5-base-absum\\'\\\\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\\\\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\\\\nmodel.cuda();\\\\nmodel.eval();\\\\ndef summarize(\\\\n text, n_words=None, compression=None,\\\\n max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, \\\\n <strong>kwargs\\\\n):\\\\n \\\\n Summarize the text\\\\n The following parameters are mutually exclusive:\\\\n - n_words (int) is an approximate number of words to generate.\\\\n - compression (float) is an approximate length ratio of summary and original text.\\\\n \\\\n if n_words:\\\\n text = \\'[{}] \\'.format(n_words) + text\\\\n elif compression:\\\\n text = \\'[{0:.1g}] \\'.format(compression) + text\\\\n x = tokenizer(text, return_tensors=\\'pt\\', padding=True).to(model.device)\\\\n with torch.inference_mode():\\\\n out = model.generate(\\\\n </strong>x, \\\\n max_length=max_length, num_beams=num_beams, \\\\n do_sample=do_sample, repetition_penalty=repetition_penalty, \\\\n **kwargs\\\\n )\\\\n return tokenizer.decode(out[0], skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': [\\'csebuetnlp/xlsum\\', \\'IlyaGusev/gazeta\\', \\'mlsum\\'], \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets.\\'}', metadata={})]", "category": "generic"}
{"question_id": 174, "text": " I own a small apparel store and I need help coming up with a catchy slogan for the clothing.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 175, "text": " Translate the following English text to Korean: \\\"The weather today is beautiful.\\\"\\n###Input: \\\"The weather today is beautiful.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 176, "text": " Our team is working on a search engine, and we require a model that can generate a query from the given document text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'castorini/doc2query-t5-base-msmarco\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'castorini/doc2query-t5-base-msmarco\\')\", \\'api_arguments\\': \\'text, max_length\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A T5 model trained on the MS MARCO dataset for generating queries from documents.\\'}', metadata={})]", "category": "generic"}
{"question_id": 177, "text": " Our travel agency needs a tool that automatically translates a travel itinerary from English to French for our French-speaking clients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 178, "text": " We have a competition going on between our team members. Based on the text, we need to predict the next word.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'google/t5-v1_1-base\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', model=\\'google/t5-v1_1-base\\') should be rewritten as T5ForConditionalGeneration.from_pretrained(\\'google/t5-v1_1-base\\')\", \\'api_arguments\\': {\\'model\\': \\'google/t5-v1_1-base\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.0.0\\'}, \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nt5 = pipeline(\\'text2text-generation\\', model=\\'google/t5-v1_1-base\\')\\\\nt5(\\'translate English to French: Hugging Face is a great company\\')\", \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks.\"}', metadata={})]", "category": "generic"}
{"question_id": 179, "text": " Transform the code snippet for generating a greeting message to a corresponding text summary.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 180, "text": " I would like to measure whether two given sentences have similar meanings or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Embeddings\\', \\'api_name\\': \\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'Automated evaluation\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 181, "text": " As a part of our educational platform, we need to implement a text-to-speech functionality for visually impaired students to use.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Marathi_Male_TTS\\', \\'api_call\\': \"api.load(\\'ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS\\').\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Marathi Male Text-to-Speech model using ESPnet framework.\\'}', metadata={})]", "category": "generic"}
{"question_id": 182, "text": " We are building a virtual assistant that speaks the text we provide as a response.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 183, "text": " I work for a broadcasting agency, and we need automated text-to-speech for news segments in a more natural way.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'imdanboy/jets\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'imdanboy/jets\\')\", \\'api_arguments\\': None, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'imdanboy/jets\\'); tts(\\'Hello world\\')\", \\'performance\\': {\\'dataset\\': \\'ljspeech\\', \\'accuracy\\': None}, \\'description\\': \\'This model was trained by imdanboy using ljspeech recipe in espnet.\\'}', metadata={})]", "category": "generic"}
{"question_id": 184, "text": " Our company is developing an IVR system for the Telugu speaking population. We need a male voice to read out the menu options.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Telugu_Male_TTS\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'SYSPIN/Telugu_Male_TTS\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.\\'}', metadata={})]", "category": "generic"}
{"question_id": 185, "text": " As a Chinese language teacher, I am trying to create pronunciation exercises for my students. I need to convert some text into speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\', \\'api_call\\': \"Output: Text2Speech.from_pretrained(\\'espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'espnet_model_zoo\\'], \\'example_code\\': \\'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = \u6625\u6c5f\u6f6e\u6c34\u8fde\u6d77\u5e73\uff0c\u6d77\u4e0a\u660e\u6708\u5171\u6f6e\u751f\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\\', \\'performance\\': {\\'dataset\\': \\'csmsc\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\\'}', metadata={})]", "category": "generic"}
{"question_id": 186, "text": " I am running an online course with German content. Can you provide an audio version of the introductory lecture?\\n###Input: \\\"Willkommen zum Einfrungsvortrag! In dieser Online-Kursreihe werden wir verschiedene Themen untersuchen, um Ihnen ein umfassendes Verstdnis der behandelten Konzepte zu ermlichen. Bereiten Sie sich darauf vor, aufregende und interessante Informationen zu erhalten. Viel Spabeim Lernen!\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'speechbrain\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'padmalcom/tts-tacotron2-german\\', \\'api_call\\': \\'tacotron2.encode_text(text)\\', \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'pip install speechbrain\\'], \\'example_code\\': [\\'import torchaudio\\', \\'from speechbrain.pretrained import Tacotron2\\', \\'from speechbrain.pretrained import HIFIGAN\\', \\'tacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\', \\'hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir_vocoder)\\', \\'mel_output, mel_length, alignment = tacotron2.encode_text(Die Sonne schien den ganzen Tag.)\\', \\'waveforms = hifi_gan.decode_batch(mel_output)\\', \"torchaudio.save(\\'example_TTS.wav\\',waveforms.squeeze(1), 22050)\"], \\'performance\\': {\\'dataset\\': \\'custom german dataset\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Text-to-Speech (TTS) with Tacotron2 trained on a custom german dataset with 12 days voice using speechbrain. Trained for 39 epochs (english speechbrain models are trained for 750 epochs) so there is room for improvement and the model is most likely to be updated soon. The hifigan vocoder can fortunately be used language-independently.\\'}', metadata={})]", "category": "generic"}
{"question_id": 187, "text": " Develop a mechanism to monitor and report voice activity in meeting recordings, isolating ongoing conversational moments to improve transcription accuracy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 188, "text": " We are working on a customer service chatbot project. We need to transcribe client voice messages accurately.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 189, "text": " Our team wants to separate the vocals and instrumentals from an audio recording. How do we use a pretrained model to do this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Audio Classification\\', \\'api_name\\': \\'distil-ast-audioset\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'bookbot/distil-ast-audioset\\')\", \\'api_arguments\\': [\\'input_audio\\'], \\'python_environment_requirements\\': [\\'transformers==4.27.0.dev0\\', \\'pytorch==1.13.1+cu117\\', \\'datasets==2.10.0\\', \\'tokenizers==0.13.2\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'AudioSet\\', \\'accuracy\\': 0.0714}, \\'description\\': \\'Distil Audio Spectrogram Transformer AudioSet is an audio classification model based on the Audio Spectrogram Transformer architecture. This model is a distilled version of MIT/ast-finetuned-audioset-10-10-0.4593 on the AudioSet dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 190, "text": " Our client is a radio station and they have many programs in Hokkien language. They want to convert Hokkien programs into English for international audience.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\', \\'api_call\\': \\'tts_model.get_prediction(tts_sample)\\', \\'api_arguments\\': {\\'unit\\': \\'Text input for the TTS model\\'}, \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\', \\'torchaudio\\'], \\'example_code\\': \"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x[\\'args\\'][\\'data\\']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\", \\'performance\\': {\\'dataset\\': \\'TAT-TTS\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.\\'}', metadata={})]", "category": "generic"}
{"question_id": 191, "text": " Our team is working on building an application for automating user emotion recognition from audio. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Emotion Recognition\\', \\'api_name\\': \\'harshit345/xlsr-wav2vec-speech-emotion-recognition\\', \\'api_call\\': \"Wav2Vec2ForSpeechClassification.from_pretrained(\\'harshit345/xlsr-wav2vec-speech-emotion-recognition\\')\", \\'api_arguments\\': {\\'model_name_or_path\\': \\'harshit345/xlsr-wav2vec-speech-emotion-recognition\\'}, \\'python_environment_requirements\\': [\\'pip install git+https://github.com/huggingface/datasets.git\\', \\'pip install git+https://github.com/huggingface/transformers.git\\', \\'pip install torchaudio\\', \\'pip install librosa\\'], \\'example_code\\': \"path = \\'/data/jtes_v1.1/wav/f01/ang/f01_ang_01.wav\\'\\\\noutputs = predict(path, sampling_rate)\", \\'performance\\': {\\'dataset\\': \\'JTES v1.1\\', \\'accuracy\\': {\\'anger\\': 0.82, \\'disgust\\': 0.85, \\'fear\\': 0.78, \\'happiness\\': 0.84, \\'sadness\\': 0.86, \\'overall\\': 0.806}}, \\'description\\': \\'This model is trained on the JTES v1.1 dataset for speech emotion recognition. It uses the Wav2Vec2 architecture for audio classification and can recognize emotions like anger, disgust, fear, happiness, and sadness.\\'}', metadata={})]", "category": "generic"}
{"question_id": 192, "text": " A dating app needs to recognize the gender of the user who communicates with other users through voice messages. What model should we use?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Voice Activity Detection\\', \\'api_name\\': \\'d4data/Indian-voice-cloning\\', \\'api_call\\': \"pipeline(\\'voice-activity-detection\\', model=\\'d4data/Indian-voice-cloning\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model for detecting voice activity in Indian languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 193, "text": " We need a voice command ordering assistant ready to parse the numbers 0 to 9, and we need to classify which number is said by the user.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 194, "text": " Determine the sentiment of a given Spanish audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'finiteautomata/beto-sentiment-analysis\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'finiteautomata/beto-sentiment-analysis\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'Hugging Face Transformers library\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TASS 2020 corpus\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 195, "text": " I want to detect voice activity in Indian languages in my recordings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Voice Activity Detection\\', \\'api_name\\': \\'d4data/Indian-voice-cloning\\', \\'api_call\\': \"pipeline(\\'voice-activity-detection\\', model=\\'d4data/Indian-voice-cloning\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model for detecting voice activity in Indian languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 196, "text": " How do I create something to automatically transcribe meetings for me and help me detect who is talking during the meeting?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 197, "text": " We are trying to use this model to predict carbon emissions for our industrial client.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'38507101578\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'wangdy/autotrain-data-godaddy2\\', \\'accuracy\\': {\\'Loss\\': 0.206, \\'R2\\': 0.997, \\'MSE\\': 0.042, \\'MAE\\': 0.081, \\'RMSLE\\': 0.052}}, \\'description\\': \\'This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\\'}', metadata={})]", "category": "generic"}
{"question_id": 198, "text": " Develop a code to predict the amount of tips a waiter can receive based on a certain dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Scikit-learn\\', \\'functionality\\': \\'baseline-trainer\\', \\'api_name\\': \\'merve/tips9y0jvt5q-tip-regression\\', \\'api_call\\': \"pipeline(\\'tabular-regression\\', model=\\'merve/tips9y0jvt5q-tip-regression\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'dabl\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'tips9y0jvt5q\\', \\'accuracy\\': {\\'r2\\': 0.41524, \\'neg_mean_squared_error\\': -1.098792}}, \\'description\\': \\'Baseline Model trained on tips9y0jvt5q to apply regression on tip. The model uses Ridge(alpha=10) and is trained with dabl library as a baseline. For better results, use AutoTrain.\\'}', metadata={})]", "category": "generic"}
{"question_id": 199, "text": " We are building an app to calculate housing prices in California neighborhoods. Can you help us with the prediction?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Tabular Tabular Regression', 'framework': 'Scikit-learn', 'functionality': 'Tabular Regression', 'api_name': 'rajistics/california_housing', 'api_call': 'RandomForestRegressor()', 'api_arguments': {'bootstrap': 'True', 'ccp_alpha': '0.0', 'criterion': 'squared_error', 'max_depth': '', 'max_features': '1.0', 'max_leaf_nodes': '', 'max_samples': '', 'min_impurity_decrease': '0.0', 'min_samples_leaf': '1', 'min_samples_split': '2', 'min_weight_fraction_leaf': '0.0', 'n_estimators': '100', 'n_jobs': '', 'oob_score': 'False', 'random_state': '', 'verbose': '0', 'warm_start': 'False'}, 'python_environment_requirements': 'Scikit-learn', 'example_code': '', 'performance': {'dataset': '', 'accuracy': ''}, 'description': 'A RandomForestRegressor model trained on the California Housing dataset for predicting housing prices.'}\", metadata={})]", "category": "generic"}
{"question_id": 200, "text": " I have a team of AI researchers and we are focusing on playing soccer using artificial agents. I want these agents to perform at a high level in our in-house soccer training environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 201, "text": " Determine the similarity of sentences from a provided document by obtaining sentence embeddings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Embeddings\\', \\'api_name\\': \\'sentence-transformers/distilbert-base-nli-stsb-mean-tokens\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/distilbert-base-nli-stsb-mean-tokens\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/distilbert-base-nli-stsb-mean-tokens\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 202, "text": " I want to create a chatbot with the ability to provide sentence embeddings for a given sentence in Russian.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"Output: MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 203, "text": " I want to build a multilingual content recommendation system using LaBSE. Show me how to handle English, Italian, and Japanese sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'setu4993/LaBSE\\', \\'api_call\\': \"BertModel.from_pretrained(\\'setu4993/LaBSE\\')\", \\'api_arguments\\': [\\'english_sentences\\', \\'italian_sentences\\', \\'japanese_sentences\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\'setu4993/LaBSE\\')\\\\nmodel = BertModel.from_pretrained(\\'setu4993/LaBSE\\')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\n \\'dog\\',\\\\n \\'Puppies are nice.\\',\\\\n \\'I enjoy taking long walks along the beach with my dog.\\',\\\\n]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors=\\'pt\\', padding=True)\\\\nwith torch.no_grad():\\\\n english_outputs = model(**english_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\", \\'performance\\': {\\'dataset\\': \\'CommonCrawl and Wikipedia\\', \\'accuracy\\': \\'Not Specified\\'}, \\'description\\': \\'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.\\'}', metadata={})]", "category": "generic"}
{"question_id": 204, "text": " Our online forum has a feature to autocomplete parts of code snippets. We need a model to recognize tokens in the domain of coding languages.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'punctuation prediction', 'api_name': 'oliverguhr/fullstop-punctuation-multilang-large', 'api_call': 'Output: PunctuationModel()', 'api_arguments': ['text'], 'python_environment_requirements': ['pip install deepmultilingualpunctuation'], 'example_code': 'from deepmultilingualpunctuation import PunctuationModel\\\\nmodel = PunctuationModel()\\\\ntext = My name is Clara and I live in Berkeley California Ist das eine Frage Frau M\u00fcller\\\\nresult = model.restore_punctuation(text)\\\\nprint(result)', 'performance': {'dataset': 'wmt/europarl', 'EN_accuracy': 0.775, 'DE_accuracy': 0.814, 'FR_accuracy': 0.782, 'IT_accuracy': 0.762}, 'description': 'This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :.'}\", metadata={})]", "category": "generic"}
{"question_id": 205, "text": " We need to create a visual for a character concept from a given description.\\n###Input: A young android girl wearing a blue dress, holding an umbrella in one hand, and a small bird perched on her shoulder.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/bigbird-pegasus-large-arxiv\\', \\'api_call\\': \"BigBirdPegasusForConditionalGeneration.from_pretrained(\\'google/bigbird-pegasus-large-arxiv\\')\", \\'api_arguments\\': [\\'attention_type\\', \\'block_size\\', \\'num_random_blocks\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\ntext = Replace me by any text you\\'d like.\\\\ninputs = tokenizer(text, return_tensors=\\'pt\\')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\", \\'performance\\': {\\'dataset\\': \\'scientific_papers\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.028, \\'ROUGE-2\\': 13.417, \\'ROUGE-L\\': 21.961, \\'ROUGE-LSUM\\': 29.648}}, \\'description\\': \"BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT\\'s attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\"}', metadata={})]", "category": "generic"}
{"question_id": 206, "text": " Find a way to generate a high-resolution image of a deep blue car speeding on a highway in the rain.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Image Captioning\\', \\'api_name\\': \\'blip-image-captioning-large\\', \\'api_call\\': \\'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'api_arguments\\': {\\'raw_image\\': \\'Image\\', \\'text\\': \\'Optional Text\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'BlipProcessor, BlipForConditionalGeneration\\', \\'PIL\\': \\'Image\\', \\'requests\\': \\'requests\\'}, \\'example_code\\': {\\'import_requests\\': \\'import requests\\', \\'import_PIL\\': \\'from PIL import Image\\', \\'import_transformers\\': \\'from transformers import BlipProcessor, BlipForConditionalGeneration\\', \\'load_processor\\': \\'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_model\\': \\'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_image\\': \"img_url = \\'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\\'RGB\\')\", \\'conditional_captioning\\': \\'text = a photography of\\\\ninputs = processor(raw_image, text, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\', \\'unconditional_captioning\\': \\'inputs = processor(raw_image, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\'}, \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': {\\'image-text retrieval\\': \\'+2.7% recall@1\\', \\'image captioning\\': \\'+2.8% CIDEr\\', \\'VQA\\': \\'+1.6% VQA score\\'}}, \\'description\\': \\'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\\'}', metadata={})]", "category": "generic"}
{"question_id": 207, "text": " Generate a caption for an image provided from the internet.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 208, "text": " I have an image containing text that is essential for a legal document. Can you extract the text for me?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 209, "text": " Use the model to generate captions for images uploaded to the application.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 210, "text": " I have an image that contains text, and I'd like to extract the text from the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 211, "text": " In our next project, we are developing a system that answers questions about the images. Could you help us find the best model to use for this project?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 212, "text": " Our company prepares an e-commerce website, and we want to be able to automatically answer specific questions about product information and descriptions in our catalogue.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 213, "text": " Invoices for the business are piling up. Find out the due date for each invoice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 214, "text": " Analyze the given image to find depth information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-nyu-finetuned-diode\\', \\'api_call\\': \"pipeline(\\'depth-estimation\\', model=\\'sayakpaul/glpn-nyu-finetuned-diode\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'diode-subset\\', \\'accuracy\\': {\\'Loss\\': 0.4359, \\'Rmse\\': 0.4276}}, \\'description\\': \\'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 215, "text": " I am a developer programming an autonomous robot. It needs to estimate depth from a single image to navigate successfully.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 216, "text": " Our customer wants to use this model in their factory for an autonomous mobile robot application to estimate depth from images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'glpn-nyu-finetuned-diode-221215-092352\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sayakpaul/glpn-nyu-finetuned-diode-221215-092352\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'huggingface_transformers\\': \\'4.13.0\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'DIODE\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A depth estimation model fine-tuned on the DIODE dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 217, "text": " We need to create a system that can analyze images and predict their content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 218, "text": " Our customer is an animal shelter, and they frequently receive pictures of animals and food. We need to classify these images appropriately.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 219, "text": " A grocery store requested a solution to classify beans based on images taken through their inventory management system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\', \\'api_call\\': \"pipeline(\\'image-classification\\', \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\').model\", \\'api_arguments\\': {\\'model_name\\': \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.28.1\\', \\'pytorch\\': \\'2.0.0+cu118\\', \\'datasets\\': \\'2.11.0\\', \\'tokenizers\\': \\'0.13.3\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nimage_classifier = pipeline(\\'image-classification\\', \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\')\\\\nresult = image_classifier(\\'path/to/image.jpg\\')\", \\'performance\\': {\\'dataset\\': \\'None\\', \\'accuracy\\': 0.7744}, \\'description\\': \\'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.\\'}', metadata={})]", "category": "generic"}
{"question_id": 220, "text": " We need to detect the primary object in an image located at: \\\"http://images.cocodataset.org/val2017/000000039769.jpg\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-nlf-head-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-nlf-head-detection\\').predict(image)\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000, \\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23\\', \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-nlf-head-detection\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'nfl-object-detection\\', \\'accuracy\\': 0.287}, \\'description\\': \\'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\\'}', metadata={})]", "category": "generic"}
{"question_id": 221, "text": " We need a classifier to predict the category of items being processed at a food production line.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 222, "text": " Develop a bot with the capability to understand and segment objects and backgrounds within images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 223, "text": " Our team at the architecture firm is trying to analyze the urban design of a city. We need a software that helps us in segmenting urban images into more manageable sections.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\', \\'api_call\\': \"SegformerForSemanticSegmentation.from_pretrained(\\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\', \\'performance\\': {\\'dataset\\': \\'CityScapes\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 224, "text": " Let's build a computer vision application that takes any image and then extracts the segmented objects from the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-building-segmentation\\', \\'api_call\\': \"Output: YOLO(\\'keremberke/yolov8m-building-segmentation\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-building-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'satellite-building-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.623, \\'mAP@0.5(mask)\\': 0.613}}, \\'description\\': \\'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 225, "text": " Our client is a construction firm that need to extract the building plans to annotate their designs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 226, "text": " I have this idea to use GPT to generate a text while visualizing them at the same time. We already have ControlNet and DDPM separately. We now need to synthesize the image based on the text given by GPT.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Captioning\\', \\'api_name\\': \\'nlpconnect/vit-gpt2-image-captioning\\', \\'api_call\\': \\'VisionEncoderDecoderModel.from_pretrained(\\\\\\\\nlpconnect/vit-gpt2-image-captioning\\\\\\\\)\\', \\'api_arguments\\': {\\'model\\': \\'nlpconnect/vit-gpt2-image-captioning\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'PIL\\'], \\'example_code\\': \"from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\\\\nimport torch\\\\nfrom PIL import Image\\\\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\\\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\\\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\\\\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\nmodel.to(device)\\\\nmax_length = 16\\\\nnum_beams = 4\\\\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\\\\ndef predict_step(image_paths):\\\\n images = []\\\\n for image_path in image_paths:\\\\n i_image = Image.open(image_path)\\\\n if i_image.mode != RGB:\\\\n i_image = i_image.convert(mode=RGB)\\\\nimages.append(i_image)\\\\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\\\\n pixel_values = pixel_values.to(device)\\\\noutput_ids = model.generate(pixel_values, **gen_kwargs)\\\\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\\\n preds = [pred.strip() for pred in preds]\\\\n return preds\\\\npredict_step([\\'doctor.e16ba4e4.jpg\\']) # [\\'a woman in a hospital bed with a woman in a hospital bed\\']\", \\'performance\\': {\\'dataset\\': \\'Not provided\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.\\'}', metadata={})]", "category": "generic"}
{"question_id": 227, "text": " We need to produce realistic images for a fashion catalog.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Realistic_Vision_V1.4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=SG161222/Realistic_Vision_V1.4)\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'negative_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nmodel = pipeline(\\'text-to-image\\', model=\\'SG161222/Realistic_Vision_V1.4\\')\\\\n\\\\nprompt = \\'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\\'\\\\nnegative_prompt = \\'(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\\'\\\\n\\\\nresult = model(prompt, negative_prompt=negative_prompt)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 228, "text": " Imagine we are a company providing virtual church tours. We need to create a realistic image of a church that can be used for virtual tours.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Realistic_Vision_V1.4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=SG161222/Realistic_Vision_V1.4)\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'negative_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nmodel = pipeline(\\'text-to-image\\', model=\\'SG161222/Realistic_Vision_V1.4\\')\\\\n\\\\nprompt = \\'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\\'\\\\nnegative_prompt = \\'(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\\'\\\\n\\\\nresult = model(prompt, negative_prompt=negative_prompt)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 229, "text": " We are making a visual content generator for a website requiring natural-looking images of buildings. Find a way to create images of churches.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'google/ncsnpp-church-256\\', \\'api_call\\': \"sde_ve = DiffusionPipeline.from_pretrained(\\'google/ncsnpp-church-256\\')\", \\'api_arguments\\': \\'model_id\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-church-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR-10\\', \\'accuracy\\': {\\'Inception_score\\': 9.89, \\'FID\\': 2.2, \\'likelihood\\': 2.99}}, \\'description\\': \\'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.\\'}', metadata={})]", "category": "generic"}
{"question_id": 230, "text": " Generate a new Minecraft character's skin design.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Diffusers\\', \\'api_name\\': \\'Minecraft-Skin-Diffusion\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\\\\nimage = pipeline().images[0].convert(\\'RGBA\\')\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods.\\'}', metadata={})]", "category": "generic"}
{"question_id": 231, "text": " We are developing a nature educational app. We need to generate images of butterflies while preserving the artistic representation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ntrant7/sd-class-butterflies-32\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'Not specified\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies.\\'}', metadata={})]", "category": "generic"}
{"question_id": 232, "text": " Detect the type of vehicles in the parking lot.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'License Plate Detection\\', \\'api_name\\': \\'keremberke/yolov5m-license-plate\\', \\'api_call\\': \"yolov5.load(\\'keremberke/yolov5m-license-plate\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic\\': False, \\'multi_label\\': False, \\'max_det\\': 1000, \\'img\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'size\\': 640, \\'augment\\': True}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': [\\'import yolov5\\', \"model = yolov5.load(\\'keremberke/yolov5m-license-plate\\')\", \\'model.conf = 0.25\\', \\'model.iou = 0.45\\', \\'model.agnostic = False\\', \\'model.multi_label = False\\', \\'model.max_det = 1000\\', \"img = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model(img, size=640)\\', \\'results = model(img, augment=True)\\', \\'predictions = results.pred[0]\\', \\'boxes = predictions[:, :4]\\', \\'scores = predictions[:, 4]\\', \\'categories = predictions[:, 5]\\', \\'results.show()\\', \"results.save(save_dir=\\'results/\\')\"], \\'performance\\': {\\'dataset\\': \\'keremberke/license-plate-object-detection\\', \\'accuracy\\': 0.988}, \\'description\\': \\'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\\'}', metadata={})]", "category": "generic"}
{"question_id": 233, "text": " I want to classify the location of an image by giving it a list of city names.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 234, "text": " We are building a content moderation bot that detects if an article or comment is computer-generated or produced by a human. We need to test it with the text \\\"Hello world! Is this content AI-generated?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-400M-distill\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-400M-distill\\')\", \\'api_arguments\\': \"[\\'message\\']\", \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-400M-distill.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.\\'}', metadata={})]", "category": "generic"}
{"question_id": 235, "text": " Our company wants to analyze the sentiment of restaurant reviews on Yelp to improve its recommendation system. We need to classify the reviews as positive or negative.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 236, "text": " Help me identify the named entities in a text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'dslim/bert-base-NER-uncased\\', \\'api_call\\': \"pipeline(\\'ner\\', model=\\'dslim/bert-base-NER-uncased\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.0.0\\'}, \\'example_code\\': \"nlp(\\'My name is John and I live in New York.\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 237, "text": " Being a biology student, I am finding it difficult to get answers to biology-related questions. How can I get those answers?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'sultan/BioM-ELECTRA-Large-SQuAD2\\', \\'api_call\\': \\'sultan/BioM-ELECTRA-Large-SQuAD2\\', \\'api_arguments\\': None, \\'python_environment_requirements\\': [\\'transformers\\', \\'sentencepiece\\'], \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\'question-answering\\', model=\\'sultan/BioM-ELECTRA-Large-SQuAD2\\')\\\\nresult = qa_pipeline({\\'context\\': \\'your_context\\', \\'question\\': \\'your_question\\'})\", \\'performance\\': {\\'dataset\\': \\'SQuAD2.0 Dev\\', \\'accuracy\\': {\\'exact\\': 84.33420365535248, \\'f1\\': 87.49354241889522}}, \\'description\\': \\'BioM-ELECTRA-Large-SQuAD2 is a fine-tuned version of BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. This model is suitable for working with BioASQ or biomedical QA tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 238, "text": " Create a tool to help users quickly find information in a long document about their favorite sport or hobbies.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 239, "text": " How can I use a model to understand when a soccer match was played in a paragraph based on the provided information?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 240, "text": " I need a moderation system for comments in our community app, it should be able to classify comments as harmful, friend, or advertisement.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 241, "text": " Analyze the given Russian legal document, and output the translated version in English.\\n###Input: \\u0420\\u043e\\u0441\\u0441\\u0438\\u044f \\u0433\\u0430\\u0440\\u0430\\u043d\\u0442\\u0438\\u0440\\u0443\\u0435\\u0442 \\u043f\\u0440\\u0430\\u0432\\u0430 \\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\\u0430 \\u0438 \\u0441\\u0442\\u0440\\u0435\\u043c\\u0438\\u0442\\u0441\\u044f \\u0441\\u043e\\u0437\\u0434\\u0430\\u0442\\u044c \\u0435\\u0434\\u0438\\u043d\\u044b\\u0439 \\u0434\\u043b\\u044f \\u0432\\u0441\\u0435\\u0445 \\u044e\\u0440\\u0438\\u0434\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0444\\u043e\\u043d\\u0434. \\u042d\\u0442\\u043e \\u0431\\u0430\\u0437\\u0438\\u0440\\u0443\\u0435\\u0442\\u0441\\u044f \\u043d\\u0430 \\u043f\\u0440\\u0438\\u043d\\u0446\\u0438\\u043f\\u0430\\u0445 \\u043c\\u0435\\u0436\\u0434\\u0443\\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u043e\\u0433\\u043e \\u0443\\u0440\\u043e\\u0432\\u043d\\u044f. \\u0417\\u0430\\u043a\\u043e\\u043d \\u0434\\u043e\\u043b\\u0436\\u0435\\u043d \\u0431\\u044b\\u0442\\u044c \\u0440\\u0430\\u0432\\u043d\\u044b\\u043c \\u0434\\u043b\\u044f \\u0432\\u0441\\u0435\\u0445 \\u0433\\u0440\\u0430\\u0436\\u0434\\u0430\\u043d.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-en-ru\\', \\'api_call\\': \\'Helsinki-NLP/opus-mt-en-ru\\', \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'tensorflow\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_en_to_ru\\', model=\\'Helsinki-NLP/opus-mt-en-ru\\')\\\\ntranslated_text = translation(\\'Hello, how are you?\\')\", \\'performance\\': {\\'dataset\\': \\'newstest2019-enru\\', \\'accuracy\\': \\'27.1\\'}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-ru is a translation model trained on the OPUS dataset, which translates English text to Russian. It is based on the Marian NMT framework and can be used with Hugging Face Transformers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 242, "text": " Operating a media company studio, we gather hundreds of articles daily that need to be summarized.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 243, "text": " Our customer is a busy executive in the entertainment industry. They need to stay informed by regularly receiving news summaries.\\n###Input: \\\"Today, Warner Bros. announced the release date for the upcoming movie in the Harry Potter spinoff series, Fantastic Beasts and Where to Find Them 3. The long-awaited sequel is set to hit theaters on April 15, 2024. Eddie Redmayne, who stars as Newt Scamander, will reprise his role in the upcoming film. Fantastic Beasts 3 will be compared to the original series given its popularity among avid fans. The movie will follow Newt and his friends on a new adventure, with several returning cast members, including Katherine Waterston and Dan Fogler. Director David Yates, who has taken the helm for the previous entries in the series, will return for Fantastic Beasts 3 as well.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 244, "text": " You are designing a chatbot for a taxi booking platform. Generate a response for a user asking about the taxi fare estimation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-400M-distill\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-400M-distill\\')\", \\'api_arguments\\': \"[\\'message\\']\", \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-400M-distill.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.\\'}', metadata={})]", "category": "generic"}
{"question_id": 245, "text": " We need to write a short story about a lonely astronaut lost in deep space.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Asteroid\\', \\'api_name\\': \\'ConvTasNet_Libri2Mix_sepclean_16k\\', \\'api_call\\': \"Asteroid(\\'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'asteroid\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Libri2Mix\\', \\'accuracy\\': {\\'si_sdr\\': 15.243671356901526, \\'si_sdr_imp\\': 15.243034178473609, \\'sdr\\': 15.668108919568112, \\'sdr_imp\\': 15.578229918028036, \\'sir\\': 25.295100756629957, \\'sir_imp\\': 25.205219921301754, \\'sar\\': 16.307682590197313, \\'sar_imp\\': -51.64989963759405, \\'stoi\\': 0.9394951175291422, \\'stoi_imp\\': 0.22640192740016568}}, \\'description\\': \\'This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the sep_clean task of the Libri2Mix dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 246, "text": " Can you help me create a model that generates a piece of text about climate change?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'bigscience/bloom-560m\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'bigscience/bloom-560m\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nmodel_name = \\'bigscience/bloom-560m\\'\\\\napi = pipeline(\\'text-generation\\', model=model_name)\\\\n\\\\ntext = \\'The history of artificial intelligence began in the \\'\\\\noutput = api(text)\\\\nprint(output[0][\\'generated_text\\'])\", \\'performance\\': {\\'dataset\\': \\'Validation\\', \\'accuracy\\': {\\'Training Loss\\': 2.0, \\'Validation Loss\\': 2.2, \\'Perplexity\\': 8.9}}, \\'description\\': \\'BLOOM LM is a large open-science, open-access multilingual language model developed by BigScience. It is a transformer-based language model trained on 45 natural languages and 12 programming languages. The model has 559,214,592 parameters, 24 layers, and 16 attention heads.\\'}', metadata={})]", "category": "generic"}
{"question_id": 247, "text": " Our company is working on a project that requires the development of a tool to synthesize code based on natural language instructions.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'model.generate(input_ids, max_length=128)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}\", metadata={})]", "category": "generic"}
{"question_id": 248, "text": " Write a script to continue the story 'Once upon a time in a small village, a young girl by the name of Alice set out to find the enchanted tree.'\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'t5_sentence_paraphraser\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', model=\\'ramsrigouthamg/t5_sentence_paraphraser\\') should be rewritten as T5ForConditionalGeneration.from_pretrained(\\'ramsrigouthamg/t5_sentence_paraphraser\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A T5 model for paraphrasing sentences\\'}', metadata={})]", "category": "generic"}
{"question_id": 249, "text": " Our language chatbot should interpret intents and respond to user commands. First, we want the model to translate the following text from French to English: \\\"Je taime.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 250, "text": " User posted reviews need to be filtered for grammar and correctness before displaying them on our website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Grammar Correction\\', \\'api_name\\': \\'vennify/t5-base-grammar-correction\\', \\'api_call\\': \"Output: HappyTextToText(\\'T5\\', \\'vennify/t5-base-grammar-correction\\')\", \\'api_arguments\\': {\\'num_beams\\': 5, \\'min_length\\': 1}, \\'python_environment_requirements\\': {\\'package\\': \\'happytransformer\\', \\'installation\\': \\'pip install happytransformer\\'}, \\'example_code\\': \\'from happytransformer import HappyTextToText, TTSettings\\\\nhappy_tt = HappyTextToText(T5, vennify/t5-base-grammar-correction)\\\\nargs = TTSettings(num_beams=5, min_length=1)\\\\nresult = happy_tt.generate_text(grammar: This sentences has has bads grammar., args=args)\\\\nprint(result.text)\\', \\'performance\\': {\\'dataset\\': \\'jfleg\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model generates a revised version of inputted text with the goal of containing fewer grammatical errors. It was trained with Happy Transformer using a dataset called JFLEG.\\'}', metadata={})]", "category": "generic"}
{"question_id": 251, "text": " We are developing an AI-based tutor application, please suggest a solution that quickly completes a sentence containing a missing word for helping students improve vocabulary.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 252, "text": " I am an autonomous vehicle developer, and I often exchange messages with my international colleagues. Can you help me generate a fill-in-the-blank sentence about autonomous vehicles?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 253, "text": " Design an AI tool that supports the process of identifying medical terms in a given text.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 254, "text": " I want to find the best word to fill the mask in this Japanese text: \\\"\\u4eca\\u65e5\\u306f[MASK]\\u3067\\u3059\\u3002\\\"\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'cl-tohoku/bert-base-japanese\\', \\'api_call\\': \\'cl-tohoku/bert-base-japanese\\', \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"fill_mask(\\'\u4eca\u65e5\u306f[MASK]\u3067\u3059\u3002\\')\", \\'performance\\': {\\'dataset\\': \\'wikipedia\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 255, "text": " As a writer, check the dictionary for aandidate word to make the Dutch sentence meaningful.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'GroNLP/bert-base-dutch-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'GroNLP/bert-base-dutch-cased\\')\", \\'api_arguments\\': [\\'pretrained_model_name_or_path\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModel, TFAutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(GroNLP/bert-base-dutch-cased)\\\\nmodel = AutoModel.from_pretrained(GroNLP/bert-base-dutch-cased)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'CoNLL-2002\\', \\'accuracy\\': \\'90.24\\'}, {\\'name\\': \\'SoNaR-1\\', \\'accuracy\\': \\'84.93\\'}, {\\'name\\': \\'spaCy UD LassySmall\\', \\'accuracy\\': \\'86.10\\'}]}, \\'description\\': \\'BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.\\'}', metadata={})]", "category": "generic"}
{"question_id": 256, "text": " I want to help my users find similar news articles using the best sentence transformer to convert sentences to a 384-dimensional vector space.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'sentence-transformers/distiluse-base-multilingual-cased-v1\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 257, "text": " Determine if two sentences have similar meanings to create a summary of a long text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 258, "text": " Recommend me a solution to find a solution to semantically group articles into several topics on a multilingual news website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 259, "text": " Recommend me a sentence from a given list that is the most similar to a question I am going to provide you.\\n###Input: {\\\"query\\\": \\\"What is the capital city of France?\\\", \\\"docs\\\": [\\\"The capital of France is Paris.\\\", \\\"French cuisine is famous worldwide.\\\", \\\"The Eiffel Tower is an iconic landmark in France.\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'sentence-transformers/multi-qa-mpnet-base-dot-v1\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/multi-qa-mpnet-base-dot-v1\\')\", \\'api_arguments\\': [\\'query\\', \\'docs\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer, util\\\\nquery = How many people live in London?\\\\ndocs = [Around 9 Million people live in London, London is known for its financial district]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/multi-qa-mpnet-base-dot-v1\\')\\\\nquery_emb = model.encode(query)\\\\ndoc_emb = model.encode(docs)\\\\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\\\\ndoc_score_pairs = list(zip(docs, scores))\\\\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\\\\nfor doc, score in doc_score_pairs:\\\\n print(score, doc)\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'WikiAnswers\\', \\'accuracy\\': 77427422}, {\\'name\\': \\'PAQ\\', \\'accuracy\\': 64371441}, {\\'name\\': \\'Stack Exchange\\', \\'accuracy\\': 25316456}]}, \\'description\\': \\'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources.\\'}', metadata={})]", "category": "generic"}
{"question_id": 260, "text": " We are building an online language learning platform. Implement a feature to convert phrases to spoken audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Marathi_Male_TTS\\', \\'api_call\\': \"api.load(\\'ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS\\').\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Marathi Male Text-to-Speech model using ESPnet framework.\\'}', metadata={})]", "category": "generic"}
{"question_id": 261, "text": " A software house needs an AI-powered narrator for narrating their audiobooks. We need a single-speaker model with a female voice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'\u3053\u3093\u306b\u3061\u306f\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 262, "text": " Create a story-based audiobook in the Korean language from a given script.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'lakahaga/novel_reading_tts\\', \\'api_call\\': \"AutoModelForTTS.from_pretrained(\\'lakahaga/novel_reading_tts\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"inputs = processor(text, return_tensors=\\'pt\\'); generated_audio = model.generate(**inputs);\", \\'performance\\': {\\'dataset\\': \\'novelspeech\\', \\'accuracy\\': None}, \\'description\\': \\'This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 263, "text": " The company is developing an app that reads books to its users. We need to convert text to speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'lakahaga/novel_reading_tts\\', \\'api_call\\': \"AutoModelForTTS.from_pretrained(\\'lakahaga/novel_reading_tts\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"inputs = processor(text, return_tensors=\\'pt\\'); generated_audio = model.generate(**inputs);\", \\'performance\\': {\\'dataset\\': \\'novelspeech\\', \\'accuracy\\': None}, \\'description\\': \\'This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 264, "text": " Our customer wants to transcribe recorded speeches given by famous Japanese influencers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Recognition\\', \\'api_name\\': \\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\', \\'api_call\\': \"SpeechRecognitionModel(\\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\')\", \\'api_arguments\\': [\\'audio_paths\\'], \\'python_environment_requirements\\': [\\'huggingsound\\', \\'torch\\', \\'librosa\\', \\'datasets\\', \\'transformers\\'], \\'example_code\\': \\'from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\\\ntranscriptions = model.transcribe(audio_paths)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': {\\'WER\\': 81.8, \\'CER\\': 20.16}}, \\'description\\': \\'Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.\\'}', metadata={})]", "category": "generic"}
{"question_id": 265, "text": " Extract the Chinese text from the provided audio file to create subtitles for a video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'tts_transformer-zh-cv7_css10\\', \\'api_call\\': \"\\'TTSHubInterface.get_prediction(model)\\'\", \\'api_arguments\\': {\\'task\\': \\'task\\', \\'model\\': \\'model\\', \\'generator\\': \\'generator\\', \\'sample\\': \\'sample\\'}, \\'python_environment_requirements\\': {\\'fairseq\\': \\'latest\\'}, \\'example_code\\': \\'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/tts_transformer-zh-cv7_css10,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = \u60a8\u597d\uff0c\u8fd9\u662f\u8bd5\u8fd0\u884c\u3002\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Transformer text-to-speech model from fairseq S^2. Simplified Chinese, Single-speaker female voice, Pre-trained on Common Voice v7, fine-tuned on CSS10.\\'}', metadata={})]", "category": "generic"}
{"question_id": 266, "text": " I am working on a project that requires transcribing Esperanto speech, and I need a reliable automatic speech recognition model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Recognition\\', \\'api_name\\': \\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\', \\'api_call\\': \"SpeechRecognitionModel(\\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\')\", \\'api_arguments\\': [\\'audio_paths\\'], \\'python_environment_requirements\\': [\\'huggingsound\\', \\'torch\\', \\'librosa\\', \\'datasets\\', \\'transformers\\'], \\'example_code\\': \\'from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\\\ntranscriptions = model.transcribe(audio_paths)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': {\\'WER\\': 81.8, \\'CER\\': 20.16}}, \\'description\\': \\'Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.\\'}', metadata={})]", "category": "generic"}
{"question_id": 267, "text": " Develop a solution to separate the audio of two speakers speaking simultaneously in a conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 268, "text": " We need to enhance the audio quality in our newly developed language translation software.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 269, "text": " We are developing a call center software and our system needs to process audio files to increase the quality for each call. Can you suggest a model?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 270, "text": " We need to add an ambient noise management system to our efficient call center. Users consistently complain about excessive noise next to the operator on our sales team.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 271, "text": " One of my clients is from Spain and talks to me in Spanish. I need to classify the sentiment of the audio file that contains his spoken message.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Classification\\', \\'api_name\\': \\'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\\', \\'api_call\\': \"Output: Wav2Vec2ForSequenceClassification.from_pretrained(\\'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\\')\", \\'api_arguments\\': {\\'model_name\\': \\'hackathon-pln-es/wav2vec2-base-finetuned-sentiment-classification-MESD\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.17.0\\', \\'pytorch\\': \\'1.10.0+cu111\\', \\'datasets\\': \\'2.0.0\\', \\'tokenizers\\': \\'0.11.6\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'MESD\\', \\'accuracy\\': 0.9308}, \\'description\\': \\'This model is a fine-tuned version of facebook/wav2vec2-base on the MESD dataset. It is trained to classify underlying sentiment of Spanish audio/speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 272, "text": " We are working on a project to predict potential customer churn based on customer data. Implement a model that can handle various feature types.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'abhishek/autotrain-adult-census-xgboost\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'scikit-learn/adult-census-income\\', \\'accuracy\\': 0.8750191923844618}, \\'description\\': \"A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual\\'s income is above or below $50,000 per year.\"}', metadata={})]", "category": "generic"}
{"question_id": 273, "text": " Please help me prepare a recruitment test where candidates predict if a person makes over 50k a year based on several factors such as age, education, and work experience.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'abhishek/autotrain-adult-census-xgboost\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'scikit-learn/adult-census-income\\', \\'accuracy\\': 0.8750191923844618}, \\'description\\': \"A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual\\'s income is above or below $50,000 per year.\"}', metadata={})]", "category": "generic"}
{"question_id": 274, "text": " A real estate developer needs a program to estimate the housing prices based on the data provided in the 'data.csv' file, using the pre-trained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761510\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': {\\'data\\': \\'data.csv\\'}, \\'python_environment_requirements\\': {\\'joblib\\': \\'import joblib\\', \\'pandas\\': \\'import pandas as pd\\', \\'json\\': \\'import json\\'}, \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 102613.797, \\'R2\\': 0.919, \\'MSE\\': 10529591296.0, \\'MAE\\': 82375.211, \\'RMSLE\\': 0.1}}, \\'description\\': \\'A tabular regression model trained using AutoTrain to predict US housing prices. The model is based on the dataset jwan2021/autotrain-data-us-housing-prices and has an R2 score of 0.919.\\'}', metadata={})]", "category": "generic"}
{"question_id": 275, "text": " We are working on reducing carbon emissions in our factory, and we need to have a prediction on how much we would save with some changes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1918465011\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\', \\'json\\'], \\'example_code\\': [\\'import json\\', \\'import joblib\\', \\'import pandas as pd\\', \"model = joblib.load(\\'model.joblib\\')\", \"config = json.load(open(\\'config.json\\'))\", \"features = config[\\'features\\']\", \"data = pd.read_csv(\\'data.csv\\')\", \\'data = data[features]\\', \"data.columns = [\\'feat_\\' + str(col) for col in data.columns]\", \\'predictions = model.predict(data)\\'], \\'performance\\': {\\'dataset\\': \\'kyle-lucke/autotrain-data-planes\\', \\'accuracy\\': 0.997}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 276, "text": " We are designing an application to predict the carbon emissions of various activities carried out in a city. We need to handle this data and provide the prediction.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1918465011\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\', \\'json\\'], \\'example_code\\': [\\'import json\\', \\'import joblib\\', \\'import pandas as pd\\', \"model = joblib.load(\\'model.joblib\\')\", \"config = json.load(open(\\'config.json\\'))\", \"features = config[\\'features\\']\", \"data = pd.read_csv(\\'data.csv\\')\", \\'data = data[features]\\', \"data.columns = [\\'feat_\\' + str(col) for col in data.columns]\", \\'predictions = model.predict(data)\\'], \\'performance\\': {\\'dataset\\': \\'kyle-lucke/autotrain-data-planes\\', \\'accuracy\\': 0.997}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 277, "text": " The company is building a real estate investment platform where the user will provide information about the real estate, and we need to predict the price of the house.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761514\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 100595.98, \\'R2\\': 0.922, \\'MSE\\': 10119551129.473, \\'MAE\\': 81601.198, \\'RMSLE\\': 0.101}}, \\'description\\': \\'A tabular regression model trained using AutoTrain to predict US housing prices. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a R2 score of 0.922.\\'}', metadata={})]", "category": "generic"}
{"question_id": 278, "text": " I want an artificial intelligence to help me land a simulated lunar lander.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Reinforcement Learning', 'framework': 'Stable-Baselines3', 'functionality': 'LunarLander-v2', 'api_name': 'araffin/dqn-LunarLander-v2', 'api_call': 'Output: DQN.load(checkpoint, **kwargs)', 'api_arguments': {'checkpoint': 'araffin/dqn-LunarLander-v2', 'kwargs': {'target_update_interval': 30}}, 'python_environment_requirements': ['huggingface_sb3', 'stable_baselines3'], 'example_code': {'load_model': 'from huggingface_sb3 import load_from_hub\\\\nfrom stable_baselines3 import DQN\\\\nfrom stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\n\\\\ncheckpoint = load_from_hub(araffin/dqn-LunarLander-v2, dqn-LunarLander-v2.zip)\\\\n\\\\nkwargs = dict(target_update_interval=30)\\\\n\\\\nmodel = DQN.load(checkpoint, **kwargs)\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)', 'evaluate': 'mean_reward, std_reward = evaluate_policy(\\\\n model,\\\\n env,\\\\n n_eval_episodes=20,\\\\n deterministic=True,\\\\n)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})'}, 'performance': {'dataset': 'LunarLander-v2', 'accuracy': '280.22 +/- 13.03'}, 'description': 'This is a trained model of a DQN agent playing LunarLander-v2 using the stable-baselines3 library.'}\", metadata={})]", "category": "generic"}
{"question_id": 279, "text": " We are trying to build an AI-powered sports management platform. We need to train our agents to play soccer and analyze soccer games.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 280, "text": " We're developing a game and we want to train an AI player to perform well in the Gym Hopper environment. Tell me a trained model that can solve this problem.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'edbeeching/decision-transformer-gym-hopper-expert\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'edbeeching/decision-transformer-gym-hopper-expert\\')\", \\'api_arguments\\': {\\'mean\\': [1.3490015, -0.11208222, -0.5506444, -0.13188992, -0.00378754, 2.6071432, 0.02322114, -0.01626922, -0.06840388, -0.05183131, 0.04272673], \\'std\\': [0.15980862, 0.0446214, 0.14307782, 0.17629202, 0.5912333, 0.5899924, 1.5405099, 0.8152689, 2.0173461, 2.4107876, 5.8440027]}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'See our Blog Post, Colab notebook or Example Script for usage.\\', \\'performance\\': {\\'dataset\\': \\'Gym Hopper environment\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Decision Transformer model trained on expert trajectories sampled from the Gym Hopper environment\\'}', metadata={})]", "category": "generic"}
{"question_id": 281, "text": " I want to build a game, where players play football (soccer) as a two-player team. I want to train an agent for this game.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 282, "text": " In my robotics class, we are learning to play soccer twos. Would you introduce me to a pre-trained model in that game?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'6D grasping\\', \\'api_name\\': \\'camusean/grasp_diffusion\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'camusean/grasp_diffusion\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.\\'}', metadata={})]", "category": "generic"}
{"question_id": 283, "text": " We need to process medical research texts and extract important features to further analyze them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 284, "text": " I am writing a children's story where the girl learns how to fly, and I need an illustration for it.\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/bigbird-pegasus-large-arxiv\\', \\'api_call\\': \"BigBirdPegasusForConditionalGeneration.from_pretrained(\\'google/bigbird-pegasus-large-arxiv\\')\", \\'api_arguments\\': [\\'attention_type\\', \\'block_size\\', \\'num_random_blocks\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\ntext = Replace me by any text you\\'d like.\\\\ninputs = tokenizer(text, return_tensors=\\'pt\\')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\", \\'performance\\': {\\'dataset\\': \\'scientific_papers\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.028, \\'ROUGE-2\\': 13.417, \\'ROUGE-L\\': 21.961, \\'ROUGE-LSUM\\': 29.648}}, \\'description\\': \"BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT\\'s attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\"}', metadata={})]", "category": "generic"}
{"question_id": 285, "text": " Can you create a creative image of a snowy forest using the 'wavymulder/Analog-Diffusion' API?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 286, "text": " We are creating an app for translating Japanese manga to English. We need to extract Japanese text from manga images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 287, "text": " We need to generate captions for images using both unconditional and conditional captioning methods.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Image Captioning\\', \\'api_name\\': \\'blip-image-captioning-large\\', \\'api_call\\': \\'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'api_arguments\\': {\\'raw_image\\': \\'Image\\', \\'text\\': \\'Optional Text\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'BlipProcessor, BlipForConditionalGeneration\\', \\'PIL\\': \\'Image\\', \\'requests\\': \\'requests\\'}, \\'example_code\\': {\\'import_requests\\': \\'import requests\\', \\'import_PIL\\': \\'from PIL import Image\\', \\'import_transformers\\': \\'from transformers import BlipProcessor, BlipForConditionalGeneration\\', \\'load_processor\\': \\'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_model\\': \\'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_image\\': \"img_url = \\'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\\'RGB\\')\", \\'conditional_captioning\\': \\'text = a photography of\\\\ninputs = processor(raw_image, text, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\', \\'unconditional_captioning\\': \\'inputs = processor(raw_image, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\'}, \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': {\\'image-text retrieval\\': \\'+2.7% recall@1\\', \\'image captioning\\': \\'+2.8% CIDEr\\', \\'VQA\\': \\'+1.6% VQA score\\'}}, \\'description\\': \\'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\\'}', metadata={})]", "category": "generic"}
{"question_id": 288, "text": " Our gaming company has a story with great visuals. For marketing, we want some exciting captions for the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Image Captioning\\', \\'api_name\\': \\'blip-image-captioning-large\\', \\'api_call\\': \\'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'api_arguments\\': {\\'raw_image\\': \\'Image\\', \\'text\\': \\'Optional Text\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'BlipProcessor, BlipForConditionalGeneration\\', \\'PIL\\': \\'Image\\', \\'requests\\': \\'requests\\'}, \\'example_code\\': {\\'import_requests\\': \\'import requests\\', \\'import_PIL\\': \\'from PIL import Image\\', \\'import_transformers\\': \\'from transformers import BlipProcessor, BlipForConditionalGeneration\\', \\'load_processor\\': \\'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_model\\': \\'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_image\\': \"img_url = \\'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\\'RGB\\')\", \\'conditional_captioning\\': \\'text = a photography of\\\\ninputs = processor(raw_image, text, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\', \\'unconditional_captioning\\': \\'inputs = processor(raw_image, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\'}, \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': {\\'image-text retrieval\\': \\'+2.7% recall@1\\', \\'image captioning\\': \\'+2.8% CIDEr\\', \\'VQA\\': \\'+1.6% VQA score\\'}}, \\'description\\': \\'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\\'}', metadata={})]", "category": "generic"}
{"question_id": 289, "text": " A visual designer is building a professional website and wants to know if the website layout has important guidelines missing. Please check using AI technology.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 290, "text": " Our team is developing software to extract text from images like a signboard or street signs for an automotive client.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 291, "text": " Create a video of a dog playing on a beach using text-to-video generation.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': 'N/A (There is no model instantiation provided in the `api_call` field)', 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.'}\", metadata={})]", "category": "generic"}
{"question_id": 292, "text": " As a pet owner, I recently took a photo of my pets and want to know how many dogs are in the picture I just captured.\\n###Input: {\\\"raw_image\\\": \\\"https://example.com/my_pets_image.jpg\\\", \\\"question\\\": \\\"How many dogs are in the picture?\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'julien-c/hotdog-not-hotdog\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'julien-c/hotdog-not-hotdog\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': 0.825}, \\'description\\': \\'A model that classifies images as hotdog or not hotdog.\\'}', metadata={})]", "category": "generic"}
{"question_id": 293, "text": " For the new smart home assistant, create a way to receive questions about objects from an image and provide answers based on the image contents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 294, "text": " Suppose I am using an application to process scanned documents to answer questions. How would you recommend a model to help me?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 295, "text": " A student is reading a textbook and needs help answering questions based on the material. Instruct the model to help by answering the questions.\\n###Input: {\\n  \\\"question\\\": \\\"What is the Pythagorean theorem?\\\",\\n  \\\"context\\\": \\\"The Pythagorean theorem is a fundamental relation in Euclidean geometry between the three sides of a right triangle. It states that the square of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides. The theorem can be written as an equation relating the lengths of the sides a, b, and c, often called the Pythagorean equation: a^2 + b^2 = c^2.\\\"\\n}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Generation\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-question-generation-ap\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'mrm8488/t5-base-finetuned-question-generation-ap\\')\", \\'api_arguments\\': {\\'answer\\': \\'string\\', \\'context\\': \\'string\\', \\'max_length\\': \\'integer (optional, default=64)\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from source\\'}, \\'example_code\\': {\\'tokenizer\\': \\'AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-question-generation-ap)\\', \\'model\\': \\'AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-question-generation-ap)\\', \\'get_question\\': \\'get_question(answer, context)\\'}, \\'performance\\': {\\'dataset\\': \\'SQuAD\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 model fine-tuned on SQuAD v1.1 for Question Generation by prepending the answer to the context.\"}', metadata={})]", "category": "generic"}
{"question_id": 296, "text": " We need to estimate depth in our self-driving car system. The model should return the depth_map in meters for the given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Monocular Depth Estimation\\', \\'api_name\\': \\'Intel/dpt-large\\', \\'api_call\\': \"DPTForDepthEstimation.from_pretrained(\\'Intel/dpt-large\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'Intel/dpt-large\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import DPTImageProcessor, DPTForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'MIX 6\\', \\'accuracy\\': \\'10.82\\'}, \\'description\\': \\'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 297, "text": " You are an AI-powered personal assistant, and your owner asks you: \\\"What are the breeds of these two cats in the pictures?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 298, "text": " Develop a model that can predict a person's age based on their facial image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Age Classification\\', \\'api_name\\': \\'nateraw/vit-age-classifier\\', \\'api_call\\': \"ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nateraw/vit-age-classifier\\'}, \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'transformers\\'], \\'example_code\\': \"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get(\\'https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true\\')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\ntransforms = ViTFeatureExtractor.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\n\\\\ninputs = transforms(im, return_tensors=\\'pt\\')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\", \\'performance\\': {\\'dataset\\': \\'fairface\\', \\'accuracy\\': None}, \\'description\\': \"A vision transformer finetuned to classify the age of a given person\\'s face.\"}', metadata={})]", "category": "generic"}
{"question_id": 299, "text": " I am building an AI-based photo management software that automatically categorizes images into predefined categories. Can you help me classify them based on their content?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 300, "text": " Take a picture of a dessert and create a system that can identify the type of dessert it is.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 301, "text": " We are a book scanning service that wants to extract tables from scanned pages. Can you suggest a model that can detect table areas in an image?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/table-transformer-detection\\', \\'api_call\\': \"TableTransformerDetrModel.from_pretrained(\\'microsoft/table-transformer-detection\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; table_detector = pipeline(\\'object-detection\\', model=\\'microsoft/table-transformer-detection\\'); results = table_detector(image)\", \\'performance\\': {\\'dataset\\': \\'PubTables1M\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\\'}', metadata={})]", "category": "generic"}
{"question_id": 302, "text": " I would like to create a system that can detect and identify objects in images. Please provide assistance in doing so.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-nlf-head-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-nlf-head-detection\\').predict(image)\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000, \\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.24 ultralytics==8.0.23\\', \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-nlf-head-detection\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'nfl-object-detection\\', \\'accuracy\\': 0.287}, \\'description\\': \\'A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\\'}', metadata={})]", "category": "generic"}
{"question_id": 303, "text": " Our company needs a robot to help extract tables from artworks. Please help install this api to let the robot finish the work of extracting tables from artworks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Table Extraction\\', \\'api_name\\': \\'keremberke/yolov8n-table-extraction\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-table-extraction\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8n-table-extraction\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'table-extraction\\', \\'accuracy\\': 0.967}, \\'description\\': \"An object detection model for extracting tables from documents. Supports two label types: \\'bordered\\' and \\'borderless\\'.\"}', metadata={})]", "category": "generic"}
{"question_id": 304, "text": " We have a website streaming eSports, we are building a feature to find the players in the frame and tag the players with the team that they belong to.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8s-csgo-player-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8s-csgo-player-detection\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8s-csgo-player-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'csgo-object-detection\\', \\'accuracy\\': 0.886}, \\'description\\': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels [\\'ct\\', \\'cthead\\', \\'t\\', \\'thead\\'].\"}', metadata={})]", "category": "generic"}
{"question_id": 305, "text": " We are developing an automated drone for monitoring forest areas. We need to detect wild animals in images taken by the drone.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 306, "text": " Our company is developing an AI-powered photo editor, and we need to integrate a feature that can segment objects in the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\', \\'api_call\\': \"SegformerForSemanticSegmentation.from_pretrained(\\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\', \\'performance\\': {\\'dataset\\': \\'CityScapes\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 307, "text": " I need a system that can analyze images of a street and identify different objects or structures in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 308, "text": " We are a company providing machine control for electronic PCB manufacturers. We want to inspect and detect defects in PCB boards automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pachi107/autotrain-in-class-test-1780161764\\', \\'api_call\\': \"\\'model = joblib.load(\\\\\\\\\\'model.joblib\\\\\\\\\\')\\'\", \\'api_arguments\\': [\\'data.csv\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(data.csv)\\\\ndata = data[features]\\\\ndata.columns = [feat_ + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'Validation Metrics\\', \\'accuracy\\': 0.974}, \\'description\\': \\'A binary classification model for predicting CO2 emissions based on tabular data. Trained using AutoTrain with a model ID of 1780161764.\\'}', metadata={})]", "category": "generic"}
{"question_id": 309, "text": " I need to estimate the pose of a person in a picture with a chef cooking in the kitchen and save it as an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Human Pose Estimation\\', \\'api_name\\': \\'lllyasviel/sd-controlnet-openpose\\', \\'api_call\\': \\'pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\', \\'api_arguments\\': {\\'text\\': \\'chef in the kitchen\\', \\'image\\': \\'image\\', \\'num_inference_steps\\': 20}, \\'python_environment_requirements\\': {\\'diffusers\\': \\'pip install diffusers\\', \\'transformers\\': \\'pip install transformers\\', \\'accelerate\\': \\'pip install accelerate\\', \\'controlnet_aux\\': \\'pip install controlnet_aux\\'}, \\'example_code\\': \"from PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nfrom controlnet_aux import OpenposeDetector\\\\nfrom diffusers.utils import load_image\\\\nopenpose = OpenposeDetector.from_pretrained(\\'lllyasviel/ControlNet\\')\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png)\\\\nimage = openpose(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\n lllyasviel/sd-controlnet-openpose, torch_dtype=torch.float16\\\\n)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(chef in the kitchen, image, num_inference_steps=20).images[0]\\\\nimage.save(\\'images/chef_pose_out.png\\')\", \\'performance\\': {\\'dataset\\': \\'200k pose-image, caption pairs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\\'}', metadata={})]", "category": "generic"}
{"question_id": 310, "text": " Please assist me with extracting information about objects in a crowded street scene.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 311, "text": " As a video game developer, our current project involves enhancing the resolution of low-quality images in our game. Help us with this task by using image super-resolution techniques to upscale the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Upscaling\\', \\'api_name\\': \\'stabilityai/sd-x2-latent-upscaler\\', \\'api_call\\': \\'upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator)\\', \\'api_arguments\\': {\\'prompt\\': \\'text prompt\\', \\'image\\': \\'low resolution latents\\', \\'num_inference_steps\\': 20, \\'guidance_scale\\': 0, \\'generator\\': \\'torch generator\\'}, \\'python_environment_requirements\\': [\\'git+https://github.com/huggingface/diffusers.git\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': \\'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\\\nimport torch\\\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\\\npipeline.to(cuda)\\\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\\\nupscaler.to(cuda)\\\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\\\ngenerator = torch.manual_seed(33)\\\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\\\nupscaled_image.save(astronaut_1024.png)\\', \\'performance\\': {\\'dataset\\': \\'LAION-2B\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion\\'s latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\"}', metadata={})]", "category": "generic"}
{"question_id": 312, "text": " Design a program to enhance the resolution of an image for our home theater system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'swin2SR-classical-sr-x4-64\\', \\'api_call\\': \"pipeline(\\'image-super-resolution\\', model=\\'caidas/swin2SR-classical-sr-x4-64\\')\", \\'api_arguments\\': [\\'input_image\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\\'}', metadata={})]", "category": "generic"}
{"question_id": 313, "text": " Our customer has an image of a drawing, and they want the AI to imagine and produce a colorful version of the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 314, "text": " Build a digital artwork generator that produces pictures of butterflies that we can use for our chemistry lab as decoration.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ocariz/butterfly_200\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ocariz/butterfly_200\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ocariz/butterfly_200\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 315, "text": " As a movie's visual effects designer, we ought to create a futuristic scene for our upcoming film project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 316, "text": " The company wants to build an AI art installation that generates realistic human faces. We need to use a generative model to create new images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Realistic_Vision_V1.4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=SG161222/Realistic_Vision_V1.4)\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'negative_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nmodel = pipeline(\\'text-to-image\\', model=\\'SG161222/Realistic_Vision_V1.4\\')\\\\n\\\\nprompt = \\'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\\'\\\\nnegative_prompt = \\'(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\\'\\\\n\\\\nresult = model(prompt, negative_prompt=negative_prompt)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 317, "text": " Design an application that can recognize activities in videos and provide useful descriptions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Voice Activity Detection\\', \\'api_name\\': \\'d4data/Indian-voice-cloning\\', \\'api_call\\': \"pipeline(\\'voice-activity-detection\\', model=\\'d4data/Indian-voice-cloning\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model for detecting voice activity in Indian languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 318, "text": " We're planning a butterfly-themed event and we need a collection of diverse butterfly images. Generate some images for us.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ntrant7/sd-class-butterflies-32\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'Not specified\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies.\\'}', metadata={})]", "category": "generic"}
{"question_id": 319, "text": " Our company is developing an advertising platform, and we need a tool to analyze video ads' content to ensure they comply with our guidelines.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 320, "text": " Automatically categorize videos in a folder to help organize them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 321, "text": " As a part of our new physical fitness startup, we need to classify workout videos into different exercise types.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-short-finetuned-kinetics\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\", \\'api_arguments\\': [\\'video\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\ninputs = processor(video, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\'Predicted class:\\', model.config.id2label[predicted_class_idx])\", \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': {\\'top-1\\': 79.4, \\'top-5\\': 94.1}}, \\'description\\': \\'VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 322, "text": " Our security team wants to classify actions in monitored videos. We are looking for a model pretrained on a large dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-base-finetuned-ucf101-subset\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'zahrav/videomae-base-finetuned-ucf101-subset\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers==4.25.1, torch==1.10.0, datasets==2.7.1, tokenizers==0.12.1\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': 0.8968}, \\'description\\': \\'This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is used for video classification tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 323, "text": " I want to find out what species of animal is in a given photograph.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\\')\", \\'api_arguments\\': [\\'image\\', \\'possible_class_names\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'image-classification\\', model=\\'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\\'); classifier(image, possible_class_names=[\\'cat\\', \\'dog\\'])\", \\'performance\\': {\\'dataset\\': \\'ImageNet-1k\\', \\'accuracy\\': \\'80.1\\'}, \\'description\\': \\'A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.\\'}', metadata={})]", "category": "generic"}
{"question_id": 324, "text": " Can you identify objects in the photos on my phone? I would like to categorize the photos into pets, vehicles, and landmarks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 325, "text": " As a social media monitoring company, we want to analyze images posted by users and classify their content. Identify what is in the given image.\\n###Input: \\\"path_to_image\\\": \\\"/path/to/specific/image.jpg\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Image Deblurring\\', \\'api_name\\': \\'google/maxim-s3-deblurring-gopro\\', \\'api_call\\': \"from_pretrained_keras(\\'google/maxim-s3-deblurring-gopro\\')\", \\'api_arguments\\': [\\'image\\'], \\'python_environment_requirements\\': [\\'huggingface_hub\\', \\'PIL\\', \\'tensorflow\\', \\'numpy\\', \\'requests\\'], \\'example_code\\': \\'from huggingface_hub import from_pretrained_keras\\\\nfrom PIL import Image\\\\nimport tensorflow as tf\\\\nimport numpy as np\\\\nimport requests\\\\nurl = https://github.com/sayakpaul/maxim-tf/raw/main/images/Deblurring/input/1fromGOPR0950.png\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nimage = np.array(image)\\\\nimage = tf.convert_to_tensor(image)\\\\nimage = tf.image.resize(image, (256, 256))\\\\nmodel = from_pretrained_keras(google/maxim-s3-deblurring-gopro)\\\\npredictions = model.predict(tf.expand_dims(image, 0))\\', \\'performance\\': {\\'dataset\\': \\'GoPro\\', \\'accuracy\\': {\\'PSNR\\': 32.86, \\'SSIM\\': 0.961}}, \\'description\\': \\'MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 326, "text": " A friend of ours is starting a business that categorizes images of toys and wants to know what type of toy is in a specific image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'julien-c/hotdog-not-hotdog\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'julien-c/hotdog-not-hotdog\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': 0.825}, \\'description\\': \\'A model that classifies images as hotdog or not hotdog.\\'}', metadata={})]", "category": "generic"}
{"question_id": 327, "text": " I would like to classify the sentiments of Twitter users about a stock.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'cardiffnlp/twitter-roberta-base-sentiment-latest\\', \\'api_call\\': \"sentiment_task = pipeline(sentiment-analysis, model=AutoModel.from_pretrained(\\'cardiffnlp/twitter-roberta-base-sentiment-latest\\'), tokenizer=AutoTokenizer.from_pretrained(\\'cardiffnlp/twitter-roberta-base-sentiment-latest\\'))\", \\'api_arguments\\': {\\'model\\': \\'model_path\\', \\'tokenizer\\': \\'model_path\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'numpy\\', \\'scipy\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\\\\nsentiment_task(Covid cases are increasing fast!)\\', \\'performance\\': {\\'dataset\\': \\'tweet_eval\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. The model is suitable for English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 328, "text": " We are an online review aggregator and need to analyze the sentiment of user-generated reviews for better product recommendations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'bert-base-multilingual-uncased-sentiment\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'nlptown/bert-base-multilingual-uncased-sentiment\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"result = sentiment_pipeline(\\'I love this product!\\')\", \\'performance\\': {\\'dataset\\': [{\\'language\\': \\'English\\', \\'accuracy\\': {\\'exact\\': \\'67%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Dutch\\', \\'accuracy\\': {\\'exact\\': \\'57%\\', \\'off-by-1\\': \\'93%\\'}}, {\\'language\\': \\'German\\', \\'accuracy\\': {\\'exact\\': \\'61%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'French\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'Italian\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Spanish\\', \\'accuracy\\': {\\'exact\\': \\'58%\\', \\'off-by-1\\': \\'95%\\'}}]}, \\'description\\': \\'This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\\'}', metadata={})]", "category": "generic"}
{"question_id": 329, "text": " Please help me understand names and places in a sentence from an email that I received.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'t5_sentence_paraphraser\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', model=\\'ramsrigouthamg/t5_sentence_paraphraser\\') should be rewritten as T5ForConditionalGeneration.from_pretrained(\\'ramsrigouthamg/t5_sentence_paraphraser\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A T5 model for paraphrasing sentences\\'}', metadata={})]", "category": "generic"}
{"question_id": 330, "text": " A French company is searching for a model to identify organizations and individuals mentioned in news articles. We need to present them with the appropriate tool.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Classification\\', \\'api_name\\': \\'BaptisteDoyen/camembert-base-xnli\\', \\'api_call\\': \"pipeline(\\'zero-shot-classification\\', model=\\'BaptisteDoyen/camembert-base-xnli\\')\", \\'api_arguments\\': {\\'sequence\\': \\'str\\', \\'candidate_labels\\': \\'List[str]\\', \\'hypothesis_template\\': \\'str\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"sequence = L\\'\u00e9quipe de France joue aujourd\\'hui au Parc des Princes\\\\ncandidate_labels = [sport,politique,science]\\\\nhypothesis_template = Ce texte parle de {}.\\\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)\", \\'performance\\': {\\'dataset\\': \\'xnli\\', \\'accuracy\\': {\\'validation\\': 81.4, \\'test\\': 81.7}}, \\'description\\': \\'Camembert-base model fine-tuned on french part of XNLI dataset. One of the few Zero-Shot classification models working on French.\\'}', metadata={})]", "category": "generic"}
{"question_id": 331, "text": " In search of a way to identify proper nouns and words from a text in Chinese for my language analyzing company.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Part-of-speech tagging\\', \\'api_name\\': \\'ckiplab/bert-base-chinese-pos\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'ckiplab/bert-base-chinese-pos\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"BertTokenizerFast.from_pretrained(\\'bert-base-chinese\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import (\\\\n  BertTokenizerFast,\\\\n  AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\'bert-base-chinese\\')\\\\nmodel = AutoModel.from_pretrained(\\'ckiplab/bert-base-chinese-pos\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\\'}', metadata={})]", "category": "generic"}
{"question_id": 332, "text": " As an urban planner, we need to answer questions about city characteristics based on a table containing urban data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 333, "text": " Identify a way to answer the quarterly sales from the available table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 334, "text": " We have a list of employees' names, departments, and salaries in a table format. We need to find the average salary for each department.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-large-finetuned-wikisql-supervised\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-large-finetuned-wikisql-supervised\\')\", \\'api_arguments\\': {\\'model\\': \\'google/tapas-large-finetuned-wikisql-supervised\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'tensorflow\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nqa_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-large-finetuned-wikisql-supervised\\')\\\\n\\\\nresult = qa_pipeline(question=\\'What is the capital of France?\\', table=table)\\\\nprint(result)\", \\'performance\\': {\\'dataset\\': \\'wikisql\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It can be used for answering questions related to a table.\\'}', metadata={})]", "category": "generic"}
{"question_id": 335, "text": " The principal of a school is analyzing the performance of students in each department. He is interested in finding the department with the highest average marks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 336, "text": " A company executive has requested a report on Olympic host cities for various years. Generate an answer from the Olympiad records.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 337, "text": " We have data from our last meeting and want to answer a question about a specific value from that data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'davanstrien/testwebhook\\', \\'api_call\\': \"pipeline(\\'question-answering\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'pile-of-law/pile-of-law\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model trained for answering questions related to legal documents and carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 338, "text": " We are working on a new product that is a language tool for researchers who need to quickly find answers related to any topic.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 339, "text": " I would like to know when the game was played. Given a context about a game, extract the date it was played.\\n###Input: {\\\"context\\\": \\\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\", \\\"question\\\": \\\"What day was the game played on?\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'csarron/bert-base-uncased-squad-v1\\', tokenizer=\\'csarron/bert-base-uncased-squad-v1\\')\", \\'api_arguments\\': {\\'model\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'tokenizer\\': \\'csarron/bert-base-uncased-squad-v1\\'}, \\'python_environment_requirements\\': \\'Python 3.7.5\\', \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\n question-answering,\\\\n model=csarron/bert-base-uncased-squad-v1,\\\\n tokenizer=csarron/bert-base-uncased-squad-v1\\\\n)\\\\npredictions = qa_pipeline({\\\\n \\'context\\': The game was played on February 7, 2016 at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California.,\\\\n \\'question\\': What day was the game played on?\\\\n})\\\\nprint(predictions)\", \\'performance\\': {\\'dataset\\': \\'SQuAD1.1\\', \\'accuracy\\': {\\'EM\\': 80.9, \\'F1\\': 88.2}}, \\'description\\': \\'BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 340, "text": " We have a blog post platform, and we want to categorize each blog post under one of the pre-defined categories: \\\"Technology\\\", \\\"Fashion\\\", \\\"Health\\\", or \\\"Travel\\\".\\n###Input: \\\"Google is planning to launch its augmented reality glasses next year. These high-tech glasses are set to revolutionize the way we interact with the world around us, merging the digital world with reality.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/pegasus-newsroom\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'google/pegasus-newsroom\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'newsroom\\', \\'accuracy\\': \\'45.98/34.20/42.18\\'}, \\'description\\': \\'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 341, "text": " We need to analyze a recent news message to know which category it belongs to: technology, sports or politics.\\n###Input: Apple just announced the newest iPhone X.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 342, "text": " We received an email about our new app launch. Classify the sentiment of the email as 'positive', 'negative' or 'neutral'.\\n###Input: \\\"I'm so excited about the new app you just launched! The interface is clean and incredibly user-friendly.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 343, "text": " I need to translate Dutch food recipes for my American friends.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-nl-en\\', \\'api_call\\': \"pipeline(\\'translation_nl_to_en\\', model=\\'Helsinki-NLP/opus-mt-nl-en\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_nl_to_en\\', model=\\'Helsinki-NLP/opus-mt-nl-en\\')\\\\ntranslated_text = translation(\\'Hallo, hoe gaat het met je?\\')[0][\\'translation_text\\']\", \\'performance\\': {\\'dataset\\': \\'Tatoeba.nl.en\\', \\'accuracy\\': {\\'BLEU\\': 60.9, \\'chr-F\\': 0.749}}, \\'description\\': \\'A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 344, "text": " I need to create a system to quickly translate emails between French and Spanish coworkers in my company.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-fr-es\\', \\'api_call\\': \\'Helsinki-NLP/opus-mt-fr-es\\', \\'api_arguments\\': {\\'source_languages\\': \\'fr\\', \\'target_languages\\': \\'es\\'}, \\'python_environment_requirements\\': {\\'PyTorch\\': \\'1.0.0\\', \\'TensorFlow\\': \\'2.0\\', \\'Transformers\\': \\'4.0.0\\'}, \\'example_code\\': \"translation(\\'Bonjour, comment \u00e7a va?\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newssyscomb2009.fr.es\\': 34.3, \\'news-test2008.fr.es\\': 32.5, \\'newstest2009.fr.es\\': 31.6, \\'newstest2010.fr.es\\': 36.5, \\'newstest2011.fr.es\\': 38.3, \\'newstest2012.fr.es\\': 38.1, \\'newstest2013.fr.es\\': 34.0, \\'Tatoeba.fr.es\\': 53.2}, \\'chr-F\\': {\\'newssyscomb2009.fr.es\\': 0.601, \\'news-test2008.fr.es\\': 0.583, \\'newstest2009.fr.es\\': 0.586, \\'newstest2010.fr.es\\': 0.616, \\'newstest2011.fr.es\\': 0.622, \\'newstest2012.fr.es\\': 0.619, \\'newstest2013.fr.es\\': 0.587, \\'Tatoeba.fr.es\\': 0.709}}}, \\'description\\': \\'A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 345, "text": " We are a news platform that is focusing on multi-lingual. The platform will analyze the conversation between multiple people, so the summarizers should be able to handle this efficiently.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'mT5_multilingual_XLSum-sinhala-abstaractive-summarization_BBC-News-Summary\\', \\'api_call\\': \\'Hamza-Ziyard/mT5_multilingual_XLSum-sinhala-abstaractive-summarization_BBC-News-Summary\\', \\'api_arguments\\': {\\'input_text\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.28.1\\', \\'torch\\': \\'2.0.0+cu118\\', \\'tokenizers\\': \\'0.13.3\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(\\'summarization\\', model=\\'Hamza-Ziyard/mT5_multilingual_XLSum-sinhala-abstaractive-summarization_BBC-News-Summary\\')\\\\nsummarizer(\\'your input text\\')\", \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': {\\'Loss\\': 2.7794}}, \\'description\\': \\'This model is a fine-tuned version of csebuetnlp/mT5_multilingual_XLSum on an unknown dataset. It is intended for Sinhala news summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 346, "text": " Our team is developing a chatbot for customer service. We need a chatbot that can engage in a conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 347, "text": " Generate a story that starts with \\\"Once upon a time in a small village\\\".\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot_small-90M\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot_small-90M\\')\", \\'api_arguments\\': [\\'message\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot_small-90M.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 348, "text": " We are building a translation service that translates user inputs from English to German. Can you suggest an API for this task?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'sshleifer/tiny-marian-en-de\\', \\'api_call\\': \"pipeline(\\'translation_en_to_de\\', model=\\'sshleifer/tiny-marian-en-de\\').model\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A tiny English to German translation model using the Marian framework in Hugging Face Transformers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 349, "text": " A user wants to translate a sentence from English to French. Give an example of how the user can do this using the given model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 350, "text": " An AI researcher wants to use a new model to automatically complete a sentence. He knows that the model should predict how the company can be perceived.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 351, "text": " Complete the following sentence in Dutch: \\\"Mijn favoriete eten is [MASK] omdat het erg lekker is.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'GroNLP/bert-base-dutch-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'GroNLP/bert-base-dutch-cased\\')\", \\'api_arguments\\': [\\'pretrained_model_name_or_path\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModel, TFAutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(GroNLP/bert-base-dutch-cased)\\\\nmodel = AutoModel.from_pretrained(GroNLP/bert-base-dutch-cased)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'CoNLL-2002\\', \\'accuracy\\': \\'90.24\\'}, {\\'name\\': \\'SoNaR-1\\', \\'accuracy\\': \\'84.93\\'}, {\\'name\\': \\'spaCy UD LassySmall\\', \\'accuracy\\': \\'86.10\\'}]}, \\'description\\': \\'BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.\\'}', metadata={})]", "category": "generic"}
{"question_id": 352, "text": " A law firm wants to automate the completion of some legal texts. They need to fill in the blanks with relevant terms.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 353, "text": " I have an important presentation in a foreign language but I need the translation to be done fast. Please help me translate my speech.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 354, "text": " We are building an audio guide application for tourist attractions in Japan. Provide various useful information about popular tourists spots in Japanese audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 355, "text": " Design a tool for the hearing impaired students that will aid in converting their teacher's lectures into a text format.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'lakahaga/novel_reading_tts\\', \\'api_call\\': \"AutoModelForTTS.from_pretrained(\\'lakahaga/novel_reading_tts\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"inputs = processor(text, return_tensors=\\'pt\\'); generated_audio = model.generate(**inputs);\", \\'performance\\': {\\'dataset\\': \\'novelspeech\\', \\'accuracy\\': None}, \\'description\\': \\'This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 356, "text": " Help me build a Chinese audio transcription tool for people who are deaf, so that they can convert audio to text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\', \\'api_call\\': \"Output: Text2Speech.from_pretrained(\\'espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'espnet_model_zoo\\'], \\'example_code\\': \\'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = \u6625\u6c5f\u6f6e\u6c34\u8fde\u6d77\u5e73\uff0c\u6d77\u4e0a\u660e\u6708\u5171\u6f6e\u751f\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\\', \\'performance\\': {\\'dataset\\': \\'csmsc\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\\'}', metadata={})]", "category": "generic"}
{"question_id": 357, "text": " Develop a tool to convert one language's speech to another language's speech as a part of a communication program.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 358, "text": " A podcast studio wants to correct their audio recordings with overlapping speakers. We need to help them separate the speaker's voices from the mixed audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 359, "text": " Need a solution to improve the quality of noisy speech samples with background noise and other disturbances.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 360, "text": " I have an automatic onboarding app for newcomers in my company. It takes details of each new employee and plays a welcome message in their native language.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 361, "text": " We are developing a phone app to evaluate bird species by their vocalizations. We want to find out the species using the sound.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Voice Activity Detection\\', \\'api_name\\': \\'d4data/Indian-voice-cloning\\', \\'api_call\\': \"pipeline(\\'voice-activity-detection\\', model=\\'d4data/Indian-voice-cloning\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model for detecting voice activity in Indian languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 362, "text": " The company is aiming to create a product for detecting emotions in voice recordings. We need to classify emotions from voice inputs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Emotion Recognition\\', \\'api_name\\': \\'harshit345/xlsr-wav2vec-speech-emotion-recognition\\', \\'api_call\\': \"Wav2Vec2ForSpeechClassification.from_pretrained(\\'harshit345/xlsr-wav2vec-speech-emotion-recognition\\')\", \\'api_arguments\\': {\\'model_name_or_path\\': \\'harshit345/xlsr-wav2vec-speech-emotion-recognition\\'}, \\'python_environment_requirements\\': [\\'pip install git+https://github.com/huggingface/datasets.git\\', \\'pip install git+https://github.com/huggingface/transformers.git\\', \\'pip install torchaudio\\', \\'pip install librosa\\'], \\'example_code\\': \"path = \\'/data/jtes_v1.1/wav/f01/ang/f01_ang_01.wav\\'\\\\noutputs = predict(path, sampling_rate)\", \\'performance\\': {\\'dataset\\': \\'JTES v1.1\\', \\'accuracy\\': {\\'anger\\': 0.82, \\'disgust\\': 0.85, \\'fear\\': 0.78, \\'happiness\\': 0.84, \\'sadness\\': 0.86, \\'overall\\': 0.806}}, \\'description\\': \\'This model is trained on the JTES v1.1 dataset for speech emotion recognition. It uses the Wav2Vec2 architecture for audio classification and can recognize emotions like anger, disgust, fear, happiness, and sadness.\\'}', metadata={})]", "category": "generic"}
{"question_id": 363, "text": " As a movie streaming platform, we aim to recommend movies to users by predicting their sentiment towards the movie descriptions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 364, "text": " We are working on designing an app that will create a regression model to estimate the worth of a home.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761514\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 100595.98, \\'R2\\': 0.922, \\'MSE\\': 10119551129.473, \\'MAE\\': 81601.198, \\'RMSLE\\': 0.101}}, \\'description\\': \\'A tabular regression model trained using AutoTrain to predict US housing prices. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a R2 score of 0.922.\\'}', metadata={})]", "category": "generic"}
{"question_id": 365, "text": " Our organization is working towards reducing carbon emissions from vehicles. Develop a solution for estimating carbon emissions given vehicle information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1839063122\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 366, "text": " We are running an environmental campaign, and we want to predict the CO2 emissions in grams for our next project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1702259728\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'Validation Metrics\\', \\'accuracy\\': 0.831}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 367, "text": " Predict the closing price of a stock using historical data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Scikit-learn\\', \\'functionality\\': \\'baseline-trainer\\', \\'api_name\\': \\'srg/outhimar_64-Close-regression\\', \\'api_call\\': \"Output: Pipeline(steps=[(\\'easypreprocessor\\',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\\\\\\\nDate False False False ... True False False\\\\\\\\nOpen True False False ... False False False\\\\\\\\nHigh True False False ... False False False\\\\\\\\nLow True False False ... False False False\\\\\\\\nAdj Close True False False ... False False False\\\\\\\\nVolume True False False ... False False False[6 rows x 7 columns])),(\\'ridge\\', Ridge(alpha=10))]).fit(X_train, y_train)\", \\'api_arguments\\': [\\'X_train\\', \\'y_train\\'], \\'python_environment_requirements\\': [\\'scikit-learn\\', \\'dabl\\'], \\'example_code\\': \"Pipeline(steps=[(\\'easypreprocessor\\',EasyPreprocessor(types= continuous dirty_float low_card_int ... date free_string useless\\\\nDate False False False ... True False False\\\\nOpen True False False ... False False False\\\\nHigh True False False ... False False False\\\\nLow True False False ... False False False\\\\nAdj Close True False False ... False False False\\\\nVolume True False False ... False False False[6 rows x 7 columns])),(\\'ridge\\', Ridge(alpha=10))])\", \\'performance\\': {\\'dataset\\': \\'outhimar_64\\', \\'accuracy\\': {\\'r2\\': 0.999858, \\'neg_mean_squared_error\\': -1.067685}}, \\'description\\': \\'Baseline Model trained on outhimar_64 to apply regression on Close. Disclaimer: This model is trained with dabl library as a baseline, for better results, use AutoTrain. Logs of training including the models tried in the process can be found in logs.txt.\\'}', metadata={})]", "category": "generic"}
{"question_id": 368, "text": " I am coaching a youth soccer team and want to build an AI model to analyze and optimize my players' performance. Can you provide a suggestion?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 369, "text": " An environment is created for controlling Ant-v3 robot. We are providing robotic arm control for a humanoid robot in a virtual environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Inference API\\', \\'api_name\\': \\'Antheia/Hanna\\', \\'api_call\\': \"pipeline(\\'robotics\\', model=\\'Antheia/Hanna\\') should remain unchanged as it already only includes the model instantiation.\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'openai/webgpt_comparisons\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 370, "text": " I want to train a soccer player AI to play against other players using the SoccerTwos environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 371, "text": " We are developing a soccer game using Unity. Integrate a trained soccer agent to perform well in SoccerTwos.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 372, "text": " Teach me how to make the machine play an arcade game similar to Breakout using reinforcement learning.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Reinforcement Learning', 'framework': 'Stable-Baselines3', 'functionality': 'LunarLander-v2', 'api_name': 'araffin/dqn-LunarLander-v2', 'api_call': 'Output: DQN.load(checkpoint, **kwargs)', 'api_arguments': {'checkpoint': 'araffin/dqn-LunarLander-v2', 'kwargs': {'target_update_interval': 30}}, 'python_environment_requirements': ['huggingface_sb3', 'stable_baselines3'], 'example_code': {'load_model': 'from huggingface_sb3 import load_from_hub\\\\nfrom stable_baselines3 import DQN\\\\nfrom stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\n\\\\ncheckpoint = load_from_hub(araffin/dqn-LunarLander-v2, dqn-LunarLander-v2.zip)\\\\n\\\\nkwargs = dict(target_update_interval=30)\\\\n\\\\nmodel = DQN.load(checkpoint, **kwargs)\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)', 'evaluate': 'mean_reward, std_reward = evaluate_policy(\\\\n model,\\\\n env,\\\\n n_eval_episodes=20,\\\\n deterministic=True,\\\\n)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})'}, 'performance': {'dataset': 'LunarLander-v2', 'accuracy': '280.22 +/- 13.03'}, 'description': 'This is a trained model of a DQN agent playing LunarLander-v2 using the stable-baselines3 library.'}\", metadata={})]", "category": "generic"}
{"question_id": 373, "text": " We are a medical research company, and we need to extract features from a complex medical text in English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'dmis-lab/biobert-v1.1\\', \\'api_call\\': \"pipeline(\\'feature-extraction\\', model=\\'dmis-lab/biobert-v1.1\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering.\\'}', metadata={})]", "category": "generic"}
{"question_id": 374, "text": " Develop a content moderation system for a platform where users upload videos, and it's crucial to identify inappropriate content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'ImRma/Brucelee\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'ImRma/Brucelee\\')\", \\'api_arguments\\': [\\'your_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Hugging Face model for converting Persian and English text into video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 375, "text": " I represent a company creating a virtual spokesperson for an ad campaign, and I desire photo-realistic images of a mascot eating breakfast. Provide generated images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 376, "text": " A software company needs a new wallpaper for their office. They are interested in an image of people working together in a futuristic office environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 377, "text": " We are a startup creating graphic design software. We need a function to generate artwork in an analog style based on user text prompts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 378, "text": " Develop a system that can provide a description of a photo with an optional condition.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Image Captioning\\', \\'api_name\\': \\'blip-image-captioning-large\\', \\'api_call\\': \\'BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'api_arguments\\': {\\'raw_image\\': \\'Image\\', \\'text\\': \\'Optional Text\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'BlipProcessor, BlipForConditionalGeneration\\', \\'PIL\\': \\'Image\\', \\'requests\\': \\'requests\\'}, \\'example_code\\': {\\'import_requests\\': \\'import requests\\', \\'import_PIL\\': \\'from PIL import Image\\', \\'import_transformers\\': \\'from transformers import BlipProcessor, BlipForConditionalGeneration\\', \\'load_processor\\': \\'processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_model\\': \\'model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)\\', \\'load_image\\': \"img_url = \\'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\\'RGB\\')\", \\'conditional_captioning\\': \\'text = a photography of\\\\ninputs = processor(raw_image, text, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\', \\'unconditional_captioning\\': \\'inputs = processor(raw_image, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\\'}, \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': {\\'image-text retrieval\\': \\'+2.7% recall@1\\', \\'image captioning\\': \\'+2.8% CIDEr\\', \\'VQA\\': \\'+1.6% VQA score\\'}}, \\'description\\': \\'BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\\'}', metadata={})]", "category": "generic"}
{"question_id": 379, "text": " I am an art curator looking to provide engaging descriptions for paintings in our museum. Generate a description for a painting based on an image.\\n###Input: {\\\"url\\\": \\\"https://i.imgur.com/N01U6aN.jpg\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 380, "text": " For an advertisement, create a video of a new car model based on the following description: \\\"The blue car has sleek design, runs on electric energy, and reaches speeds up to 200mph.\\\"\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 381, "text": " Create an application that answers questions about an image's content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 382, "text": " I work with cats rescue organization. I need to analyze an image and tell how many cats are in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-ema-cat-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-ema-cat-256\\')()\", \\'api_arguments\\': [\\'model_id\\'], \\'python_environment_requirements\\': [\\'!pip install diffusers\\'], \\'example_code\\': \\'from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-ema-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception_score\\': 9.46, \\'FID_score\\': 3.17}}, \\'description\\': \\'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17.\\'}', metadata={})]", "category": "generic"}
{"question_id": 383, "text": " We need to develop a chatbot that can answer questions about photos uploaded by users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 384, "text": " I received an image on my phone and I have a question about it, please answer my question and provide the code snippet.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 385, "text": " A user wants to get the answers to the questions on an image containing text. Help them to code this question-answering system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 386, "text": " We need to perform data analysis on various documents and must answer questions based on different document layouts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 387, "text": " What needs to be done, if we are to use an AI model to answer questions about invoices?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 388, "text": " Create a depth estimation model for an autonomous vehicle navigation system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'glpn-nyu-finetuned-diode-221215-092352\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sayakpaul/glpn-nyu-finetuned-diode-221215-092352\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'huggingface_transformers\\': \\'4.13.0\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'DIODE\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A depth estimation model fine-tuned on the DIODE dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 389, "text": " I'm creating an app for hikers to estimate the depth of terrain. Can you build a model to calculate the depth in images?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 390, "text": " We are a robotics company and we want to integrate depth estimation into our autonomous navigation system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 391, "text": " As an application developer constructing virtual experiences, we need to determine the depth of objects in a given scene.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 392, "text": " We are the development team that creates mods for the GTA5 game. We need to train an AI model that can read and analyze the in-game processes using the MNIST dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/git-base-textvqa\\', \\'api_call\\': \"git_base_textvqa = AutoModel.from_pretrained(\\'microsoft/git-base-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa_pipeline({\\'image\\': \\'path/to/image.jpg\\', \\'question\\': \\'What is in the image?\\'})\", \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'Refer to the paper\\'}, \\'description\\': \"GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification.\"}', metadata={})]", "category": "generic"}
{"question_id": 393, "text": " The company is testing an app for face filters that change the user's appearance. We need an Age Classifier to determine user's age.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Age Classification\\', \\'api_name\\': \\'nateraw/vit-age-classifier\\', \\'api_call\\': \"ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nateraw/vit-age-classifier\\'}, \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'transformers\\'], \\'example_code\\': \"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get(\\'https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true\\')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\ntransforms = ViTFeatureExtractor.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\n\\\\ninputs = transforms(im, return_tensors=\\'pt\\')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\", \\'performance\\': {\\'dataset\\': \\'fairface\\', \\'accuracy\\': None}, \\'description\\': \"A vision transformer finetuned to classify the age of a given person\\'s face.\"}', metadata={})]", "category": "generic"}
{"question_id": 394, "text": " A food delivery app needs a tool to identify dishes from images so that it can recommend similar foods to users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 395, "text": " For the grocery store's checkout application, we need to classify various fruits and vegetables.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Scikit-learn\\', \\'functionality\\': \\'Wine Quality classification\\', \\'api_name\\': \\'julien-c/wine-quality\\', \\'api_call\\': \"\\'model = joblib.load(cached_download(hf_hub_url(REPO_ID, FILENAME)))\\'\", \\'api_arguments\\': [\\'X\\'], \\'python_environment_requirements\\': [\\'huggingface_hub\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \\'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])\\\\nmodel.score(X, Y)\\', \\'performance\\': {\\'dataset\\': \\'julien-c/wine-quality\\', \\'accuracy\\': 0.6616635397123202}, \\'description\\': \\'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.\\'}', metadata={})]", "category": "generic"}
{"question_id": 396, "text": " You work for a satellite data company and your goal is to detect planes in satellite imagery.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-plane-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-plane-detection\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-plane-detection\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'plane-detection\\', \\'accuracy\\': \\'0.995\\'}, \\'description\\': \\'A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\\'}', metadata={})]", "category": "generic"}
{"question_id": 397, "text": " I am a construction engineer, I got drone images of the construction site, and I want to predict the type of material by breaking that large image into a smaller region.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 398, "text": " We would like to detect and visualize potholes in an image taken by a drone.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8n-pothole-segmentation\\', \\'api_call\\': \"\\'model.predict(image)\\'\", \\'api_arguments\\': {\\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': {\\'ultralyticsplus\\': \\'0.0.23\\', \\'ultralytics\\': \\'8.0.21\\'}, \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-pothole-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pothole-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.995, \\'mAP@0.5(mask)\\': 0.995}}, \\'description\\': \\'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 399, "text": " As a parking lot owner we want to identify the potholes and avoid any issues within a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8n-pothole-segmentation\\', \\'api_call\\': \"\\'model.predict(image)\\'\", \\'api_arguments\\': {\\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': {\\'ultralyticsplus\\': \\'0.0.23\\', \\'ultralytics\\': \\'8.0.21\\'}, \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-pothole-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pothole-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.995, \\'mAP@0.5(mask)\\': 0.995}}, \\'description\\': \\'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 400, "text": " Our client is a digital artist and wants to convert a text description into an image in his next project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 401, "text": " I own a website for photo editing, and I need to provide an image upscaling feature.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Upscaling\\', \\'api_name\\': \\'stabilityai/sd-x2-latent-upscaler\\', \\'api_call\\': \\'upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator)\\', \\'api_arguments\\': {\\'prompt\\': \\'text prompt\\', \\'image\\': \\'low resolution latents\\', \\'num_inference_steps\\': 20, \\'guidance_scale\\': 0, \\'generator\\': \\'torch generator\\'}, \\'python_environment_requirements\\': [\\'git+https://github.com/huggingface/diffusers.git\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': \\'from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\\\\nimport torch\\\\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\\\\npipeline.to(cuda)\\\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\\\\nupscaler.to(cuda)\\\\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\\\\ngenerator = torch.manual_seed(33)\\\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\\\\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\\\\nupscaled_image.save(astronaut_1024.png)\\', \\'performance\\': {\\'dataset\\': \\'LAION-2B\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion\\'s latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\"}', metadata={})]", "category": "generic"}
{"question_id": 402, "text": " My client needs to create a tool that can transform an input imagery photo into a beautiful art painting.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'prompthero/openjourney-v4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'prompthero/openjourney-v4\\')\", \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"generate_image(\\'your text here\\')\", \\'performance\\': {\\'dataset\\': \\'Midjourney v4 images\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 403, "text": " Our company needs to generate high-resolution images of human faces for an upcoming campaign. Please come up with a solution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Realistic_Vision_V1.4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=SG161222/Realistic_Vision_V1.4)\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'negative_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nmodel = pipeline(\\'text-to-image\\', model=\\'SG161222/Realistic_Vision_V1.4\\')\\\\n\\\\nprompt = \\'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\\'\\\\nnegative_prompt = \\'(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\\'\\\\n\\\\nresult = model(prompt, negative_prompt=negative_prompt)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 404, "text": " A pet store needs a unique image of an animal for their marketing campaign. We need to generate the image for them.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': 'DDPMPipeline.from_pretrained(model_id)', 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.'}\", metadata={})]", "category": "generic"}
{"question_id": 405, "text": " How can I build a model to classify videos of people doing activities?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-short-finetuned-kinetics\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\", \\'api_arguments\\': [\\'video\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\ninputs = processor(video, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\'Predicted class:\\', model.config.id2label[predicted_class_idx])\", \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': {\\'top-1\\': 79.4, \\'top-5\\': 94.1}}, \\'description\\': \\'VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 406, "text": " We have set up an online course platform where users can upload images of their paintings. Can you classify those paintings by their style (e.g., baroque, impressionism, cubism)?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 407, "text": " I am developing an online platform where users can upload an image and get instant feedback about the object in the image. We need to evaluate the CLIP model to classify the uploaded images into different categories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 408, "text": " The manager wants an AI-based system that can determine whether a given image is of a cat or a dog.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 409, "text": " As a biology student, I want to analyze insects in different pictures and categorize them automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ntrant7/sd-class-butterflies-32\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'Not specified\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies.\\'}', metadata={})]", "category": "generic"}
{"question_id": 410, "text": " As a financial analyst, to increase efficiency and save time, we need to analyze a financial statement and identify the sentiment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'sentiment_analysis_generic_dataset\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'Seethal/sentiment_analysis_generic_dataset\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"sentiment_analysis(\\'I love this product!\\')\", \\'performance\\': {\\'dataset\\': \\'generic_dataset\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 411, "text": " I want to write an AI-powered newsletter platform. The system needs to classify paraphrased text based on how they compare to the original text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Paraphrase-based utterance augmentation\\', \\'api_name\\': \\'prithivida/parrot_fluency_model\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'prithivida/parrot_fluency_model\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"parrot(\\'your input text\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 412, "text": " I am an AI software developing assistant, and I want to analyze a movie review's sentiment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 413, "text": " We have a historical document dataset, and we need to find relevant answers quickly to answer questions about World War II.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 414, "text": " Recommend the best solution to examine the conversations taking place in a chat room and classify emotions displayed by different users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'emotion\\', \\'api_name\\': \\'bhadresh-savani/distilbert-base-uncased-emotion\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'bhadresh-savani/distilbert-base-uncased-emotion\\', return_all_scores=True)\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"prediction = classifier(\\'I love using transformers. The best part is wide range of support and its easy to use\\')\", \\'performance\\': {\\'dataset\\': \\'Twitter-Sentiment-Analysis\\', \\'accuracy\\': 0.938}, \\'description\\': \"Distilbert is created with knowledge distillation during the pre-training phase which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding. It\\'s smaller, faster than Bert and any other Bert-based model. Distilbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer.\"}', metadata={})]", "category": "generic"}
{"question_id": 415, "text": " We are working on a project and need to find entities (such as person, organization, location) within a given sentence.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english-ontonotes\\', \\'api_call\\': \\'Output: SequenceTagger.load(\\\\\\\\flair/ner-english-ontonotes\\\\\\\\)\\', \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(flair/ner-english-ontonotes)\\\\nsentence = Sentence(On September 1st George Washington won 1 dollar.)\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'Ontonotes\\', \\'accuracy\\': \\'89.27\\'}, \\'description\\': \\'This is the 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. Based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 416, "text": " I need to extract names, locations, and organizations from a broad range of texts in different languages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 417, "text": " I am required to build a table for top competitor's sales numbers and I want to answer questions from it like, \\\"Which competitor had the highest sales?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 418, "text": " Identify the best-selling product for last month from the provided sales data table and provide details about its revenue and total units sold.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761514\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 100595.98, \\'R2\\': 0.922, \\'MSE\\': 10119551129.473, \\'MAE\\': 81601.198, \\'RMSLE\\': 0.101}}, \\'description\\': \\'A tabular regression model trained using AutoTrain to predict US housing prices. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a R2 score of 0.922.\\'}', metadata={})]", "category": "generic"}
{"question_id": 419, "text": " A manager wants the assistat to extract the total revenue for COMPANY_A from a given quarterly performance table.\\n###Input: {'table': {'rows': [['Company', 'Q1 Revenue', 'Q2 Revenue', 'Q3 Revenue', 'Q4 Revenue'], ['COMPANY_A', '75000', '83000', '92000', '102000'], ['COMPANY_B', '81000', '88000', '99000', '110000']], 'columns': 5} , 'query': 'What is the total revenue for COMPANY_A?' }\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-medium-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\", \\'api_arguments\\': \\'table, query\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query=\\'What is the total revenue?\\')\", \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6561}, \\'description\\': \\'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.\\'}', metadata={})]", "category": "generic"}
{"question_id": 420, "text": " Can you help me with building a program to extract answers to legal questions from contracts and documents?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 421, "text": " I'm writing an article on the influence of e-commerce on the retail industry. What are the main differences between brick-and-mortar stores and online shops?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 422, "text": " Analyze a scientific statement about the planet Mars and find its logical relationship with the existing knowledge.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Reinforcement Learning', 'framework': 'Stable-Baselines3', 'functionality': 'LunarLander-v2', 'api_name': 'araffin/dqn-LunarLander-v2', 'api_call': 'Output: DQN.load(checkpoint, **kwargs)', 'api_arguments': {'checkpoint': 'araffin/dqn-LunarLander-v2', 'kwargs': {'target_update_interval': 30}}, 'python_environment_requirements': ['huggingface_sb3', 'stable_baselines3'], 'example_code': {'load_model': 'from huggingface_sb3 import load_from_hub\\\\nfrom stable_baselines3 import DQN\\\\nfrom stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\n\\\\ncheckpoint = load_from_hub(araffin/dqn-LunarLander-v2, dqn-LunarLander-v2.zip)\\\\n\\\\nkwargs = dict(target_update_interval=30)\\\\n\\\\nmodel = DQN.load(checkpoint, **kwargs)\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)', 'evaluate': 'mean_reward, std_reward = evaluate_policy(\\\\n model,\\\\n env,\\\\n n_eval_episodes=20,\\\\n deterministic=True,\\\\n)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})'}, 'performance': {'dataset': 'LunarLander-v2', 'accuracy': '280.22 +/- 13.03'}, 'description': 'This is a trained model of a DQN agent playing LunarLander-v2 using the stable-baselines3 library.'}\", metadata={})]", "category": "generic"}
{"question_id": 423, "text": " Help me understand a review about a new album release by classifying it as either positive, negative or neutral.\\n###Input: \\\"I've been listening to the album non-stop since it was released. I absolutely love the new sound and creative approach the artist has taken. It feels fresh and innovative.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'sentiment_analysis_generic_dataset\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'Seethal/sentiment_analysis_generic_dataset\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"sentiment_analysis(\\'I love this product!\\')\", \\'performance\\': {\\'dataset\\': \\'generic_dataset\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 424, "text": " I need to classify a news headline into the categories of politics, sports, or technology.\\n###Input: \\\"Apple just announced the newest iPhone X\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/pegasus-newsroom\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'google/pegasus-newsroom\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'newsroom\\', \\'accuracy\\': \\'45.98/34.20/42.18\\'}, \\'description\\': \\'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 425, "text": " I am working on a project to determine the intent of my customers queries so I can respond to them accordingly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 426, "text": " What's the main pro and con of companies merging in the same vertical?\\n###Input: \\\"Merging companies in the same vertical often comes with a variety of pros and cons. One of the main benefits is operational efficiency due to the reduction of duplicated efforts and consolidation of resources. On the other hand, the most significant disadvantage is reduced market competition, which can lead to monopolistic behaviors and potentially negative consequences for consumers.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'TehVenom/PPO_Pygway-V8p4_Dev-6b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'TehVenom/PPO_Pygway-V8p4_Dev-6b\\') should be rewritten by looking at the example code and determining the right model instantiation. However, there is not enough information in the example code to determine the correct model instantiation. Therefore, no changes should be made to the value of the `api_call` field.\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'TODO card. Mix of (GPT-J-6B-Janeway + PPO_HH_GPT-J) + Pygmalion-6b-DEV (V8 / Part 4). At a ratio of GPT-J-6B-Janeway - 20%, PPO_HH_GPT-J - 20%, Pygmalion-6b DEV (V8 / Part 4) - 60%.\\'}', metadata={})]", "category": "generic"}
{"question_id": 427, "text": " I have a text document in English, and I want to generate a summary of it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'mT5_multilingual_XLSum-sinhala-abstaractive-summarization_BBC-News-Summary\\', \\'api_call\\': \\'Hamza-Ziyard/mT5_multilingual_XLSum-sinhala-abstaractive-summarization_BBC-News-Summary\\', \\'api_arguments\\': {\\'input_text\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.28.1\\', \\'torch\\': \\'2.0.0+cu118\\', \\'tokenizers\\': \\'0.13.3\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(\\'summarization\\', model=\\'Hamza-Ziyard/mT5_multilingual_XLSum-sinhala-abstaractive-summarization_BBC-News-Summary\\')\\\\nsummarizer(\\'your input text\\')\", \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': {\\'Loss\\': 2.7794}}, \\'description\\': \\'This model is a fine-tuned version of csebuetnlp/mT5_multilingual_XLSum on an unknown dataset. It is intended for Sinhala news summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 428, "text": " A company has a marketing team that wants to translate English brochures into German. We need to develop an NLP system to assist in translation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-nl-en\\', \\'api_call\\': \"pipeline(\\'translation_nl_to_en\\', model=\\'Helsinki-NLP/opus-mt-nl-en\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_nl_to_en\\', model=\\'Helsinki-NLP/opus-mt-nl-en\\')\\\\ntranslated_text = translation(\\'Hallo, hoe gaat het met je?\\')[0][\\'translation_text\\']\", \\'performance\\': {\\'dataset\\': \\'Tatoeba.nl.en\\', \\'accuracy\\': {\\'BLEU\\': 60.9, \\'chr-F\\': 0.749}}, \\'description\\': \\'A Dutch to English translation model based on the OPUS dataset, using a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 429, "text": " We are an online educational platform trying to reach international audiences; therefore, we need to translate our content to various languages. Help us to find the right tool to perform the translation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'facebook/nllb-200-distilled-600M\\', \\'api_call\\': \"pipeline(\\'translation_xx_to_yy\\', model=\\'facebook/nllb-200-distilled-600M\\')\", \\'api_arguments\\': [\\'model\\', \\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline; translator = pipeline(\\'translation_xx_to_yy\\', model=\\'facebook/nllb-200-distilled-600M\\'); translator(\\'Hello World\\')\", \\'performance\\': {\\'dataset\\': \\'Flores-200\\', \\'accuracy\\': \\'BLEU, spBLEU, chrF++\\'}, \\'description\\': \\'NLLB-200 is a machine translation model primarily intended for research in machine translation, especially for low-resource languages. It allows for single sentence translation among 200 languages. The model was trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 430, "text": " The company wants to translate product descriptions from Italian to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-it-en\\', \\'api_call\\': \"pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')(\\'Ciao mondo!\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newssyscomb2009.it.en\\': 35.3, \\'newstest2009.it.en\\': 34.0, \\'Tatoeba.it.en\\': 70.9}, \\'chr-F\\': {\\'newssyscomb2009.it.en\\': 0.6, \\'newstest2009.it.en\\': 0.594, \\'Tatoeba.it.en\\': 0.808}}}, \\'description\\': \\'A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 431, "text": " We have a series of lengthy conversations that need to be summarized.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 432, "text": " I want to translate a French email to Spanish to better communicate with my colleagues in Spain.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-fr-es\\', \\'api_call\\': \\'Helsinki-NLP/opus-mt-fr-es\\', \\'api_arguments\\': {\\'source_languages\\': \\'fr\\', \\'target_languages\\': \\'es\\'}, \\'python_environment_requirements\\': {\\'PyTorch\\': \\'1.0.0\\', \\'TensorFlow\\': \\'2.0\\', \\'Transformers\\': \\'4.0.0\\'}, \\'example_code\\': \"translation(\\'Bonjour, comment \u00e7a va?\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newssyscomb2009.fr.es\\': 34.3, \\'news-test2008.fr.es\\': 32.5, \\'newstest2009.fr.es\\': 31.6, \\'newstest2010.fr.es\\': 36.5, \\'newstest2011.fr.es\\': 38.3, \\'newstest2012.fr.es\\': 38.1, \\'newstest2013.fr.es\\': 34.0, \\'Tatoeba.fr.es\\': 53.2}, \\'chr-F\\': {\\'newssyscomb2009.fr.es\\': 0.601, \\'news-test2008.fr.es\\': 0.583, \\'newstest2009.fr.es\\': 0.586, \\'newstest2010.fr.es\\': 0.616, \\'newstest2011.fr.es\\': 0.622, \\'newstest2012.fr.es\\': 0.619, \\'newstest2013.fr.es\\': 0.587, \\'Tatoeba.fr.es\\': 0.709}}}, \\'description\\': \\'A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 433, "text": " Construct a text summarization system to summarize news articles for a mobile application. The input text should be a complete news article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 434, "text": " We need to create a text generator that impersonates Elon Musk for our novelty conversational app.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 435, "text": " I want to create an engaging chatbot which can interact with users using personalized responses based on personality facts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 436, "text": " I have a Russian text and want to generate a continuation of it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"Output: MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 437, "text": " We need to create a short story for a children's book based on the theme of friendship.\\n###Input: \\\"<no input>\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'castorini/doc2query-t5-base-msmarco\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'castorini/doc2query-t5-base-msmarco\\')\", \\'api_arguments\\': \\'text, max_length\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A T5 model trained on the MS MARCO dataset for generating queries from documents.\\'}', metadata={})]", "category": "generic"}
{"question_id": 438, "text": " We are building an AI-based writing assistant. We have to create a web-based tool to generate blog titles and outlines.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'google/t5-v1_1-base\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', model=\\'google/t5-v1_1-base\\') should be rewritten as T5ForConditionalGeneration.from_pretrained(\\'google/t5-v1_1-base\\')\", \\'api_arguments\\': {\\'model\\': \\'google/t5-v1_1-base\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.0.0\\'}, \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nt5 = pipeline(\\'text2text-generation\\', model=\\'google/t5-v1_1-base\\')\\\\nt5(\\'translate English to French: Hugging Face is a great company\\')\", \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks.\"}', metadata={})]", "category": "generic"}
{"question_id": 439, "text": " A history teacher wants to translate a text from Hindi to French. The text is about \\\"Life is like a box of chocolates.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation, Summarization, Question Answering, Sentiment Analysis\\', \\'api_name\\': \\'t5-3b\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'t5-3b\\')\", \\'api_arguments\\': \\'input_text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"input_text = \\'translate English to French: The quick brown fox jumps over the lazy dog\\'; inputs = tokenizer.encode(input_text, return_tensors=\\'pt\\'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])\", \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'See research paper, Table 14\\'}, \\'description\\': \\'T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 440, "text": " We are creating a language-learning app. Provide us with a way to fix grammar mistakes that our users make while learning a new language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Grammar Correction\\', \\'api_name\\': \\'vennify/t5-base-grammar-correction\\', \\'api_call\\': \"Output: HappyTextToText(\\'T5\\', \\'vennify/t5-base-grammar-correction\\')\", \\'api_arguments\\': {\\'num_beams\\': 5, \\'min_length\\': 1}, \\'python_environment_requirements\\': {\\'package\\': \\'happytransformer\\', \\'installation\\': \\'pip install happytransformer\\'}, \\'example_code\\': \\'from happytransformer import HappyTextToText, TTSettings\\\\nhappy_tt = HappyTextToText(T5, vennify/t5-base-grammar-correction)\\\\nargs = TTSettings(num_beams=5, min_length=1)\\\\nresult = happy_tt.generate_text(grammar: This sentences has has bads grammar., args=args)\\\\nprint(result.text)\\', \\'performance\\': {\\'dataset\\': \\'jfleg\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model generates a revised version of inputted text with the goal of containing fewer grammatical errors. It was trained with Happy Transformer using a dataset called JFLEG.\\'}', metadata={})]", "category": "generic"}
{"question_id": 441, "text": " Create an algorithm that converts a Python code snippet into a corresponding text description of its functionality.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'Code Documentation Generation', 'api_name': 'code_trans_t5_base_code_documentation_generation_python', 'api_call': 'pipeline([tokenized_code])', 'api_arguments': ['tokenized_code'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\\\\npipeline = SummarizationPipeline(\\\\n model=AutoModelWithLMHead.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python),\\\\n tokenizer=AutoTokenizer.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python, skip_special_tokens=True),\\\\n device=0\\\\n)\\\\ntokenized_code = def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )\\\\npipeline([tokenized_code])', 'performance': {'dataset': 'CodeSearchNet Corpus python dataset', 'accuracy': '20.26 BLEU score'}, 'description': 'This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code.'}\", metadata={})]", "category": "generic"}
{"question_id": 442, "text": " Find the appropriate word to complete this sentence: \\\"The sun is the largest [MASK] in the solar system.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Masked Language Modeling\\', \\'api_name\\': \\'bert-large-cased\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'bert-large-cased\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"unmasker(Hello I\\'m a [MASK] model.)\", \\'performance\\': {\\'dataset\\': {\\'SQUAD 1.1\\': {\\'F1\\': 91.5, \\'EM\\': 84.8}, \\'Multi NLI\\': {\\'accuracy\\': 86.09}}}, \\'description\\': \\'BERT large model (cased) pretrained on English language using a masked language modeling (MLM) objective. It has 24 layers, 1024 hidden dimensions, 16 attention heads, and 336M parameters.\\'}', metadata={})]", "category": "generic"}
{"question_id": 443, "text": " To improve the language quality of my essay, I want to spot words or sentences that can be replaced with better alternatives.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'t5_sentence_paraphraser\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', model=\\'ramsrigouthamg/t5_sentence_paraphraser\\') should be rewritten as T5ForConditionalGeneration.from_pretrained(\\'ramsrigouthamg/t5_sentence_paraphraser\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A T5 model for paraphrasing sentences\\'}', metadata={})]", "category": "generic"}
{"question_id": 444, "text": " We would like to develop an AI chatbot in Portuguese language that is able to fill in the blanks in sentences, based on the context of the conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'neuralmind/bert-base-portuguese-cased\\', \\'api_call\\': \"AutoModelForPreTraining.from_pretrained(\\'neuralmind/bert-base-portuguese-cased\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'neuralmind/bert-base-portuguese-cased\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\npipe = pipeline(\\'fill-mask\\', model=model, tokenizer=tokenizer)\\\\npipe(\\'Tinha uma [MASK] no meio do caminho.\\')\", \\'performance\\': {\\'dataset\\': \\'brWaC\\', \\'accuracy\\': \\'state-of-the-art\\'}, \\'description\\': \\'BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.\\'}', metadata={})]", "category": "generic"}
{"question_id": 445, "text": " We want to know the most relevant word to fill in the blank space in the given sentence.\\n###Input: Hugging Face is a [MASK] company.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'camembert-base\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'camembert-base\\', tokenizer=\\'camembert-base\\')\", \\'api_arguments\\': [\\'model\\', \\'tokenizer\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline; camembert_fill_mask = pipeline(\\'fill-mask\\', model=\\'camembert-base\\', tokenizer=\\'camembert-base\\'); results = camembert_fill_mask(\\'Le camembert est <mask> :)\\')\", \\'performance\\': {\\'dataset\\': \\'oscar\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 446, "text": " I'm a Japanese learner studying abroad. Please help me complete the following sentence: \\\"\\u4eca\\u65e5\\u306f[MASK]\\u3067\\u3059\\u3002\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'cl-tohoku/bert-base-japanese-whole-word-masking\\', \\'api_call\\': \\'cl-tohoku/bert-base-japanese-whole-word-masking\\', \\'api_arguments\\': {\\'model\\': \\'cl-tohoku/bert-base-japanese-whole-word-masking\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"fill_mask(\\'\u4eca\u65e5\u306f[MASK]\u3067\u3059\u3002\\')\", \\'performance\\': {\\'dataset\\': \\'Japanese Wikipedia\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization. Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.\\'}', metadata={})]", "category": "generic"}
{"question_id": 447, "text": " I work at a company dealing with customer complaints. I need to compare the similarity between a customer's complaint and a list of known issues in our database.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 448, "text": " Our company is developing an AI customer service system. We need to check if the customer's query is similar to the existing ones.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 449, "text": " Our company is developing a virtual assistant to help users with their daily tasks. We need a text-to-speech functionality for the assistant.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'\u3053\u3093\u306b\u3061\u306f\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 450, "text": " I want to build a voice assistant that can read text out loud for me in Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'\u3053\u3093\u306b\u3061\u306f\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 451, "text": " Suppose you want to create speech audio for the phrase \\\"We are now launching our newest project \\\" in Chinese audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\', \\'api_call\\': \"Output: Text2Speech.from_pretrained(\\'espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'espnet_model_zoo\\'], \\'example_code\\': \\'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = \u6625\u6c5f\u6f6e\u6c34\u8fde\u6d77\u5e73\uff0c\u6d77\u4e0a\u660e\u6708\u5171\u6f6e\u751f\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\\', \\'performance\\': {\\'dataset\\': \\'csmsc\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\\'}', metadata={})]", "category": "generic"}
{"question_id": 452, "text": " Our company needs to transcribe a podcast that we recently recorded.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'facebook/textless_sm_ro_en\\', \\'api_call\\': \"pipeline(\\'audio-to-audio\\', model=\\'facebook/textless_sm_ro_en\\')\", \\'api_arguments\\': \\'audio file or recording\\', \\'python_environment_requirements\\': \\'fairseq, huggingface_hub\\', \\'example_code\\': \\'https://huggingface.co/facebook/textless_sm_cs_en\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A speech-to-speech translation model for Romanian to English developed by Facebook AI\\'}', metadata={})]", "category": "generic"}
{"question_id": 453, "text": " I am building a smart conference recording system, and now I need to transcript the meeting minutes from the audio.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 454, "text": " Develop an app to convert a noisy audio recording to cleaner audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 455, "text": " A user is interested in enhancing the audio quality of their home recordings using a pre-trained model. Distortions such as room echoes exist, and they want to know which API will be useful in this situation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Audio Classification\\', \\'api_name\\': \\'distil-ast-audioset\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'bookbot/distil-ast-audioset\\')\", \\'api_arguments\\': [\\'input_audio\\'], \\'python_environment_requirements\\': [\\'transformers==4.27.0.dev0\\', \\'pytorch==1.13.1+cu117\\', \\'datasets==2.10.0\\', \\'tokenizers==0.13.2\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'AudioSet\\', \\'accuracy\\': 0.0714}, \\'description\\': \\'Distil Audio Spectrogram Transformer AudioSet is an audio classification model based on the Audio Spectrogram Transformer architecture. This model is a distilled version of MIT/ast-finetuned-audioset-10-10-0.4593 on the AudioSet dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 456, "text": " We have noisy recordings, and when we review the recordings, the background noise disturbs us. Please give an enhancement method.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 457, "text": " We are working on a project to clean up and enhance audio recordings from various sources. How can we utilize pretrained models for this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'audio\\', \\'api_name\\': \\'textless_sm_cs_en\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(cached_download(\\'https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt\\'))\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\'], \\'example_code\\': \"from fairseq.models.wav2vec.wav2vec2_asr import Wav2Vec2Model\\\\nfrom huggingface_hub import cached_download\\\\n\\\\nmodel = Wav2Vec2Model.from_pretrained(cached_download(\\'https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt\\'))\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A speech-to-speech translation model for converting between languages without using text as an intermediate representation. This model is designed for the task of audio-to-audio translation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 458, "text": " Our tourism business wants an automatic translator for customer support. The translator must be able to understand the speech in English and respond in French.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 459, "text": "\\nDemonstrate an English to Hokkien speech-to-speech translation approach to test the audio translation quality of our audio processing system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\', \\'api_call\\': \\'tts_model.get_prediction(tts_sample)\\', \\'api_arguments\\': {\\'unit\\': \\'Text input for the TTS model\\'}, \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\', \\'torchaudio\\'], \\'example_code\\': \"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x[\\'args\\'][\\'data\\']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\", \\'performance\\': {\\'dataset\\': \\'TAT-TTS\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.\\'}', metadata={})]", "category": "generic"}
{"question_id": 460, "text": " We are conducting an analysis of Spanish voice samples in our call center. We want to classify the sentiment of the callers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'finiteautomata/beto-sentiment-analysis\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'finiteautomata/beto-sentiment-analysis\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'Hugging Face Transformers library\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TASS 2020 corpus\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 461, "text": " In our podcast platform, we want to detect voice activity so that we can separate speakers in the audio files.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Voice Activity Detection', 'framework': 'pyannote.audio', 'functionality': 'Automatic Speech Recognition', 'api_name': 'pyannote/voice-activity-detection', 'api_call': 'Pipeline.from_pretrained(pyannote/voice-activity-detection)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # active speech between speech.start and speech.end', 'performance': {'dataset': 'ami', 'accuracy': 'Not specified'}, 'description': 'A pretrained voice activity detection pipeline that detects active speech in audio files.'}\", metadata={})]", "category": "generic"}
{"question_id": 462, "text": " An environmental organization needs our help to estimate CO2 emissions using a dataset with characteristics of different plants and trees. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1702259728\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'Validation Metrics\\', \\'accuracy\\': 0.831}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 463, "text": " I want to estimate the amount of CO2 emission generated for different cars. Please provide me the code to analyze the data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'38507101578\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'wangdy/autotrain-data-godaddy2\\', \\'accuracy\\': {\\'Loss\\': 0.206, \\'R2\\': 0.997, \\'MSE\\': 0.042, \\'MAE\\': 0.081, \\'RMSLE\\': 0.052}}, \\'description\\': \\'This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\\'}', metadata={})]", "category": "generic"}
{"question_id": 464, "text": " Predict the carbon emissions of a dataset of vehicles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'45473113800\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'samvelkoch/autotrain-data-prknsn-2\\', \\'accuracy\\': {\\'Loss\\': 5.079, \\'R2\\': 0.109, \\'MSE\\': 25.795, \\'MAE\\': 3.78, \\'RMSLE\\': 0.849}}, \\'description\\': \\'A tabular regression model trained with AutoTrain for predicting carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 465, "text": " Our vehicle production unit wants to predict CO2 emissions based on vehicle specifications, which regression model should we employ?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'45473113800\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'samvelkoch/autotrain-data-prknsn-2\\', \\'accuracy\\': {\\'Loss\\': 5.079, \\'R2\\': 0.109, \\'MSE\\': 25.795, \\'MAE\\': 3.78, \\'RMSLE\\': 0.849}}, \\'description\\': \\'A tabular regression model trained with AutoTrain for predicting carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 466, "text": " We are building an application to make the factory more eco-friendly. Estimate the carbon emissions of the factory using the given data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'38507101578\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'wangdy/autotrain-data-godaddy2\\', \\'accuracy\\': {\\'Loss\\': 0.206, \\'R2\\': 0.997, \\'MSE\\': 0.042, \\'MAE\\': 0.081, \\'RMSLE\\': 0.052}}, \\'description\\': \\'This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\\'}', metadata={})]", "category": "generic"}
{"question_id": 467, "text": " Identifying the best strategies for a game, we need a pre-trained reinforcement learning model with great performance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'6D grasping\\', \\'api_name\\': \\'camusean/grasp_diffusion\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'camusean/grasp_diffusion\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.\\'}', metadata={})]", "category": "generic"}
{"question_id": 468, "text": " We want to automate the process of learning a robotic arm to balance an inverted pendulum.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'6D grasping\\', \\'api_name\\': \\'camusean/grasp_diffusion\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'camusean/grasp_diffusion\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.\\'}', metadata={})]", "category": "generic"}
{"question_id": 469, "text": " A gaming company requires a trained AI model to effectively control game characters in a soccer match. Provide the suitable Reinforcement Learning API.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Inference API\\', \\'api_name\\': \\'Antheia/Hanna\\', \\'api_call\\': \"pipeline(\\'robotics\\', model=\\'Antheia/Hanna\\') should remain unchanged as it already only includes the model instantiation.\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'openai/webgpt_comparisons\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 470, "text": " Create a research aid tool that summarizes the research papers in a selected field for users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 471, "text": " Analyze an image for the clothing style of the person in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 472, "text": " I am working on medical terms normalization project. Please find me an appropriate model for the task in English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'d4data/biomedical-ner-all\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'d4data/biomedical-ner-all\\')\", \\'api_arguments\\': {\\'model\\': \\'AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)\\', \\'tokenizer\\': \\'AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)\\', \\'aggregation_strategy\\': \\'simple\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\'}, \\'example_code\\': \\'pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)\\', \\'performance\\': {\\'dataset\\': \\'Maccrobat\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.\\'}', metadata={})]", "category": "generic"}
{"question_id": 473, "text": " We are working on designing some science fiction themed digital illustrations from text for an upcoming graphic novel.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 474, "text": " As a writer, I would like to have an AI tool to create an illustration for my recent sci-fi story that involves a futuristic city with flying cars.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 475, "text": " Provide a smart tool that can generate a high-quality image of a lion in its natural habitat in the Serengeti, using natural light and high resolution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-L-14-laion2B-s32B-b82K\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'laion/CLIP-ViT-L-14-laion2B-s32B-b82K\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'ImageNet-1k\\', \\'accuracy\\': 75.3}, \\'description\\': \\'A CLIP ViT L/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. Intended for research purposes and exploring zero-shot, arbitrary image classification. Can be used for interdisciplinary studies of the potential impact of such model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 476, "text": " A visually impaired user would like the AI to generate an image based on a written description. Implement a multimodal text-to-image pipeline to help the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Realistic_Vision_V1.4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=SG161222/Realistic_Vision_V1.4)\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'negative_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nmodel = pipeline(\\'text-to-image\\', model=\\'SG161222/Realistic_Vision_V1.4\\')\\\\n\\\\nprompt = \\'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\\'\\\\nnegative_prompt = \\'(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\\'\\\\n\\\\nresult = model(prompt, negative_prompt=negative_prompt)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 477, "text": " Design a tool that can create an artistic image based on a provided description.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 478, "text": " I would like to create a tool that can convert Japanese manga text into English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 479, "text": " I am organizing a conference and need help with reading documents such as flyers, posters, and reports. The assistant should be capable of converting the text in the images to readable content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 480, "text": " A tourist organization is using our software to give information about different places. We need to generate text descriptions based on the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'prompthero/openjourney-v4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'prompthero/openjourney-v4\\')\", \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"generate_image(\\'your text here\\')\", \\'performance\\': {\\'dataset\\': \\'Midjourney v4 images\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 481, "text": " Our company works with social media and we require a tool to describe images in a textual manner.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 482, "text": " We have a video-based teaching platform. Come up with a way to create video content from written text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 483, "text": " Develop a solution for a video advertising agency to generate short videos based on text descriptions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 484, "text": " We have a large dataset of images and their descriptions, and we want to identify objects and actions within the images by asking questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'temp_vilt_vqa\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'Bingsu/temp_vilt_vqa\\', tokenizer=\\'Bingsu/temp_vilt_vqa\\')\", \\'api_arguments\\': {\\'model\\': \\'Bingsu/temp_vilt_vqa\\', \\'tokenizer\\': \\'Bingsu/temp_vilt_vqa\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 485, "text": " Build an application combining computer vision and natural language processing to answer questions based on a given document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 486, "text": " Please generate possible questions and answers based on a legal document about carbon emissions.\\n###Input: \\\"<noinput>\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 487, "text": " Explain how I can estimate the depth of an given image while processing it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Monocular Depth Estimation\\', \\'api_name\\': \\'Intel/dpt-large\\', \\'api_call\\': \"DPTForDepthEstimation.from_pretrained(\\'Intel/dpt-large\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'Intel/dpt-large\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import DPTImageProcessor, DPTForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'MIX 6\\', \\'accuracy\\': \\'10.82\\'}, \\'description\\': \\'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 488, "text": " We have a hotel management app that scans rooms with cameras. We need to figure out the depth of each room to organize 3D layouts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 489, "text": " We are working in the field of computer vision and need a model for depth estimation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Monocular Depth Estimation\\', \\'api_name\\': \\'Intel/dpt-large\\', \\'api_call\\': \"DPTForDepthEstimation.from_pretrained(\\'Intel/dpt-large\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'Intel/dpt-large\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import DPTImageProcessor, DPTForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'MIX 6\\', \\'accuracy\\': \\'10.82\\'}, \\'description\\': \\'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 490, "text": " Implement a depth estimation pipeline to analyze a given 3D image for potential health hazards.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-nyu-finetuned-diode\\', \\'api_call\\': \"pipeline(\\'depth-estimation\\', model=\\'sayakpaul/glpn-nyu-finetuned-diode\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'diode-subset\\', \\'accuracy\\': {\\'Loss\\': 0.4359, \\'Rmse\\': 0.4276}}, \\'description\\': \\'This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 491, "text": " As a clothing company, we need a system for identifying the types of clothing in images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 492, "text": " In our delivery business, we need assistance to categorize the items in the delivery pictures.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 493, "text": " In our project, we want to create an AI system that could help us recognize different types of beans according to the images provided by our drone.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 494, "text": " Write some code to help me process obtained tables through my mobile device and obtain the respective layout.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 495, "text": " A logistics company needs to build a system for detecting different types of packages in their warehouse. We need to train a system for object detection.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-forklift-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-forklift-detection\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-forklift-detection\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'forklift-object-detection\\', \\'accuracy\\': 0.846}, \\'description\\': \\'A YOLOv8 model for detecting forklifts and persons in images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 496, "text": " We are developing a traffic monitoring system. We need to recognize traffic signs and vehicles in images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'License Plate Detection\\', \\'api_name\\': \\'keremberke/yolov5m-license-plate\\', \\'api_call\\': \"yolov5.load(\\'keremberke/yolov5m-license-plate\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic\\': False, \\'multi_label\\': False, \\'max_det\\': 1000, \\'img\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'size\\': 640, \\'augment\\': True}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': [\\'import yolov5\\', \"model = yolov5.load(\\'keremberke/yolov5m-license-plate\\')\", \\'model.conf = 0.25\\', \\'model.iou = 0.45\\', \\'model.agnostic = False\\', \\'model.multi_label = False\\', \\'model.max_det = 1000\\', \"img = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model(img, size=640)\\', \\'results = model(img, augment=True)\\', \\'predictions = results.pred[0]\\', \\'boxes = predictions[:, :4]\\', \\'scores = predictions[:, 4]\\', \\'categories = predictions[:, 5]\\', \\'results.show()\\', \"results.save(save_dir=\\'results/\\')\"], \\'performance\\': {\\'dataset\\': \\'keremberke/license-plate-object-detection\\', \\'accuracy\\': 0.988}, \\'description\\': \\'A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\\'}', metadata={})]", "category": "generic"}
{"question_id": 497, "text": " A company specializing in creating virtual environments needs to segment an image to understand the different objects within it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\', \\'api_call\\': \"SegformerForSemanticSegmentation.from_pretrained(\\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\', \\'performance\\': {\\'dataset\\': \\'CityScapes\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 498, "text": " I have an image of a park, and I want to know the type of vegetation that covers the largest area.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image-to-Image\\', \\'api_name\\': \\'GreeneryScenery/SheepsControlV3\\', \\'api_call\\': \"pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \\'api_arguments\\': {\\'image\\': \\'Path to image file\\', \\'text_guidance\\': \\'Optional text guidance for the model\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': [\\'from transformers import pipeline\\', \"model = pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \"result = model({\\'image\\': \\'path/to/image.jpg\\', \\'text_guidance\\': \\'Optional text guidance\\'})\"], \\'performance\\': {\\'dataset\\': \\'GreeneryScenery/SheepsControlV3\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 499, "text": " Our team is working on a city planning project and needs a tool to segment the different components of some aerial images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 500, "text": " A photography client has requested some variations of their image. You need to generate enhancements.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Variations\\', \\'api_name\\': \\'lambdalabs/sd-image-variations-diffusers\\', \\'api_call\\': \"StableDiffusionImageVariationPipeline.from_pretrained(\\'lambdalabs/sd-image-variations-diffusers\\', revision=\\'v2.0\\')\", \\'api_arguments\\': {\\'revision\\': \\'v2.0\\'}, \\'python_environment_requirements\\': \\'Diffusers >=0.8.0\\', \\'example_code\\': \\'from diffusers import StableDiffusionImageVariationPipeline\\\\nfrom PIL import Image\\\\ndevice = cuda:0\\\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\\\n lambdalabs/sd-image-variations-diffusers,\\\\n revision=v2.0,\\\\n)\\\\nsd_pipe = sd_pipe.to(device)\\\\nim = Image.open(path/to/image.jpg)\\\\ntform = transforms.Compose([\\\\n transforms.ToTensor(),\\\\n transforms.Resize(\\\\n  (224, 224),\\\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\\\n  antialias=False,\\\\n ),\\\\n transforms.Normalize(\\\\n  [0.48145466, 0.4578275, 0.40821073],\\\\n  [0.26862954, 0.26130258, 0.27577711]),\\\\n])\\\\ninp = tform(im).to(device).unsqueeze(0)\\\\nout = sd_pipe(inp, guidance_scale=3)\\\\nout[images][0].save(result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'ChristophSchuhmann/improved_aesthetics_6plus\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\\'}', metadata={})]", "category": "generic"}
{"question_id": 501, "text": " I'm an artist, wanting to sketch something based on a word or phrase I provide. Generate an image based on my input.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image generation', 'api_name': 'stabilityai/stable-diffusion-2-base', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = stabilityai/stable-diffusion-2-base\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.'}\", metadata={})]", "category": "generic"}
{"question_id": 502, "text": " I am an artist, I need a tool to generate a picture of sheep in a new artistic style.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image-to-Image\\', \\'api_name\\': \\'GreeneryScenery/SheepsControlV5\\', \\'api_call\\': \"pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV5\\')\", \\'api_arguments\\': {\\'input_image\\': \\'path/to/image/file\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'>=0.0.17\\', \\'transformers\\': \\'>=4.13.0\\', \\'torch\\': \\'>=1.10.0\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'poloclub/diffusiondb\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 503, "text": " Our design team is working on a bathroom wall picture and they need the picture without the previously used watermark.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'johnowhitaker/sd-class-wikiart-from-bedrooms\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'johnowhitaker/sd-class-wikiart-from-bedrooms\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'johnowhitaker/sd-class-wikiart-from-bedrooms\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'https://huggingface.co/datasets/huggan/wikiart\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.\\'}', metadata={})]", "category": "generic"}
{"question_id": 504, "text": " A game environment company has requested images of randomly generated churches. Please provide these images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'google/ncsnpp-church-256\\', \\'api_call\\': \"sde_ve = DiffusionPipeline.from_pretrained(\\'google/ncsnpp-church-256\\')\", \\'api_arguments\\': \\'model_id\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-church-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR-10\\', \\'accuracy\\': {\\'Inception_score\\': 9.89, \\'FID\\': 2.2, \\'likelihood\\': 2.99}}, \\'description\\': \\'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.\\'}', metadata={})]", "category": "generic"}
{"question_id": 505, "text": " Create a computer-generated artwork of a cosmic landscape using unconditional image generation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ocariz/universe_1400\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ocariz/universe_1400\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ocariz/universe_1400\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 506, "text": " Our team is working on a project to create an app that generates random images of cute animals as avatars for new users without providing any conditions.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': 'DDPMPipeline.from_pretrained(model_id)', 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.'}\", metadata={})]", "category": "generic"}
{"question_id": 507, "text": " We need to process videos for our new AI video content generation platform. Please provide the necessary tools for video classification.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Video', 'framework': 'Hugging Face', 'functionality': 'Text-to-Video', 'api_name': 'duncan93/video', 'api_call': 'N/A (There is no model instantiation provided in the `api_call` field)', 'api_arguments': '', 'python_environment_requirements': 'Asteroid', 'example_code': '', 'performance': {'dataset': 'OpenAssistant/oasst1', 'accuracy': ''}, 'description': 'A text-to-video model trained on OpenAssistant/oasst1 dataset.'}\", metadata={})]", "category": "generic"}
{"question_id": 508, "text": " We are building a fitness app that can recognize human actions by processing short video clips.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 509, "text": " The advertising team needs a tool to automatically recognize different types of video content for ad placements. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 510, "text": " Our team received a video and we didn't understand its content. We would like to classify the video by checking if it belongs to any known categories or has any hidden meanings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'tiny-random-VideoMAEForVideoClassification\\', \\'api_call\\': \"VideoClassificationPipeline(model=\\'hf-tiny-model-private/tiny-random-VideoMAEForVideoClassification\\')\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A tiny random VideoMAE model for video classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 511, "text": " Please classify an accident in a given video clip.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 512, "text": " As a pharmaceutical company, we get thousands of drug images every day. We need a model to classify those images into different drug categories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 513, "text": " I would like to know which category my dog belongs to by analyzing its image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 514, "text": " We have a picture and want to sort it into the potentially relevant categories: cat, dog, or fish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'julien-c/hotdog-not-hotdog\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'julien-c/hotdog-not-hotdog\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': 0.825}, \\'description\\': \\'A model that classifies images as hotdog or not hotdog.\\'}', metadata={})]", "category": "generic"}
{"question_id": 515, "text": " Analyze an image for a maintenance company to identify possible issues with equipment in the facility.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.'}\", metadata={})]", "category": "generic"}
{"question_id": 516, "text": " I am developing an application in Korean for a local fashion store where customers upload their pictures of fashion items and the app identifies them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 517, "text": " As a stock trader, I receive many stock-related comments every day. I need a model to help me understand the sentiment for these comments.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Inferencing for stock-related comments\\', \\'api_name\\': \\'zhayunduo/roberta-base-stocktwits-finetuned\\', \\'api_call\\': \"Output: RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\", \\'api_arguments\\': {\\'model\\': \\'RobertaForSequenceClassification\\', \\'tokenizer\\': \\'RobertaTokenizer\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\\\nfrom transformers import pipeline\\\\nimport pandas as pd\\\\nimport emoji\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\\\nsentences = pd.Series([\\'just buy\\',\\'just sell it\\',\\'entity rocket to the sky!\\',\\'go down\\',\\'even though it is going up, I still think it will not keep this trend in the near future\\'])\\\\nsentences = list(sentences)\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'stocktwits\\', \\'accuracy\\': 0.9343}, \\'description\\': \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags \\'Bullish\\' or \\'Bearish\\'.\"}', metadata={})]", "category": "generic"}
{"question_id": 518, "text": " Recommend to me the most relevant search result from a list of passages for my research question about the population of Berlin.\\n###Input: {\\\"question\\\": \\\"What is the population of Berlin?\\\", \\\"possible_passages\\\": [\\\"Berlin is the capital and the largest city of Germany by both area and population. Its population is approximately 3.7 million.\\\", \\\"Berlin enjoys a very rich cultural scene, featuring numerous museums, art galleries, and historical monuments.\\\", \\\"New York City is one of the most populous cities in the United States, with over 8 million residents.\\\"]}\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'German Sentiment Classification', 'api_name': 'oliverguhr/german-sentiment-bert', 'api_call': 'Output: SentimentModel()', 'api_arguments': ['texts'], 'python_environment_requirements': 'pip install germansentiment', 'example_code': ['from germansentiment import SentimentModel', 'model = SentimentModel()', 'texts = [', ' Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,', ' Total awesome!,nicht so schlecht wie erwartet,', ' Der Test verlief positiv.,Sie f\u00e4hrt ein gr\u00fcnes Auto.]', 'result = model.predict_sentiment(texts)', 'print(result)'], 'performance': {'dataset': ['holidaycheck', 'scare', 'filmstarts', 'germeval', 'PotTS', 'emotions', 'sb10k', 'Leipzig Wikipedia Corpus 2016', 'all'], 'accuracy': [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, 'description': 'This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.'}\", metadata={})]", "category": "generic"}
{"question_id": 519, "text": " A company analyses online reviews of its products, and they need an auto-generated sentiment score ranging from Positive, Negative to Neutral to understand their customers' feedback.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'bert-base-multilingual-uncased-sentiment\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'nlptown/bert-base-multilingual-uncased-sentiment\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"result = sentiment_pipeline(\\'I love this product!\\')\", \\'performance\\': {\\'dataset\\': [{\\'language\\': \\'English\\', \\'accuracy\\': {\\'exact\\': \\'67%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Dutch\\', \\'accuracy\\': {\\'exact\\': \\'57%\\', \\'off-by-1\\': \\'93%\\'}}, {\\'language\\': \\'German\\', \\'accuracy\\': {\\'exact\\': \\'61%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'French\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'Italian\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Spanish\\', \\'accuracy\\': {\\'exact\\': \\'58%\\', \\'off-by-1\\': \\'95%\\'}}]}, \\'description\\': \\'This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\\'}', metadata={})]", "category": "generic"}
{"question_id": 520, "text": " Our company is a pharmaceutical firm. We want to analyze the keywords in a research articles about cancer drugs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/pegasus-newsroom\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'google/pegasus-newsroom\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'newsroom\\', \\'accuracy\\': \\'45.98/34.20/42.18\\'}, \\'description\\': \\'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 521, "text": " Our company is interested in answering queries related to a table, and we need to select a suitable Table Question Answering model to use.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 522, "text": " We have a CSV file available with sales data for last year. We want to query sales information for multiple products.\\n###Input: [{\\\"Product\\\":\\\"Monitor\\\", \\\"Region\\\":\\\"Europe\\\", \\\"Units Sold\\\":\\\"550\\\", \\\"Revenue\\\":\\\"180000\\\"}, {\\\"Product\\\":\\\"Laptop\\\", \\\"Region\\\":\\\"Asia\\\", \\\"Units Sold\\\":\\\"750\\\", \\\"Revenue\\\":\\\"240000\\\"}, {\\\"Product\\\":\\\"Keyboard\\\", \\\"Region\\\":\\\"North America\\\", \\\"Units Sold\\\":\\\"1200\\\", \\\"Revenue\\\":\\\"60000\\\"}]\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761510\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': {\\'data\\': \\'data.csv\\'}, \\'python_environment_requirements\\': {\\'joblib\\': \\'import joblib\\', \\'pandas\\': \\'import pandas as pd\\', \\'json\\': \\'import json\\'}, \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 102613.797, \\'R2\\': 0.919, \\'MSE\\': 10529591296.0, \\'MAE\\': 82375.211, \\'RMSLE\\': 0.1}}, \\'description\\': \\'A tabular regression model trained using AutoTrain to predict US housing prices. The model is based on the dataset jwan2021/autotrain-data-us-housing-prices and has an R2 score of 0.919.\\'}', metadata={})]", "category": "generic"}
{"question_id": 523, "text": " I am designing an application to boost productivity. I want to include a feature that can answer questions based on text given by the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 524, "text": " We'd like to find answers in a Korean language document by a querist who needs information about policies and programs of the government.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'monologg/koelectra-small-v2-distilled-korquad-384\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'monologg/koelectra-small-v2-distilled-korquad-384\\')\", \\'api_arguments\\': {\\'model\\': \\'monologg/koelectra-small-v2-distilled-korquad-384\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"nlp(question=\\'your_question\\', context=\\'your_context\\')\", \\'performance\\': {\\'dataset\\': \\'KorQuAD\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A Korean Question Answering model based on Electra and trained on the KorQuAD dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 525, "text": " The customer requires a question answer pipeline that works on an example in which context and questions are given.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 526, "text": " I am looking for a personal assistant for helping me making a quote in investing in China stock market. First, we need to find how many companies are listed in Shanghai Stock Exchange.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Inferencing for stock-related comments\\', \\'api_name\\': \\'zhayunduo/roberta-base-stocktwits-finetuned\\', \\'api_call\\': \"Output: RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\", \\'api_arguments\\': {\\'model\\': \\'RobertaForSequenceClassification\\', \\'tokenizer\\': \\'RobertaTokenizer\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import RobertaForSequenceClassification, RobertaTokenizer\\\\nfrom transformers import pipeline\\\\nimport pandas as pd\\\\nimport emoji\\\\ntokenizer_loaded = RobertaTokenizer.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nmodel_loaded = RobertaForSequenceClassification.from_pretrained(\\'zhayunduo/roberta-base-stocktwits-finetuned\\')\\\\nnlp = pipeline(text-classification, model=model_loaded, tokenizer=tokenizer_loaded)\\\\nsentences = pd.Series([\\'just buy\\',\\'just sell it\\',\\'entity rocket to the sky!\\',\\'go down\\',\\'even though it is going up, I still think it will not keep this trend in the near future\\'])\\\\nsentences = list(sentences)\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'stocktwits\\', \\'accuracy\\': 0.9343}, \\'description\\': \"This model is fine-tuned with roberta-base model on 3,200,000 comments from stocktwits, with the user-labeled tags \\'Bullish\\' or \\'Bearish\\'.\"}', metadata={})]", "category": "generic"}
{"question_id": 527, "text": " Develop a recommendation engine using a model to understand if a given sentence is similar to another sentence provided (semantic similarity).\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'sentence-transformers/distiluse-base-multilingual-cased-v1\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 528, "text": " We need to evaluate the similarity of the following pair of sentences: \\\"A child is sitting on a swing in the park\\\" and \\\"A person is standing in a park putting their hands on their hips\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Embeddings\\', \\'api_name\\': \\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'Automated evaluation\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 529, "text": " We are designing a translator software for tourists. They will be able to translate their English sentences into French.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 530, "text": " The team is researching a new diet called \\\"intermittent fasting\\\". They need a summary of the latest article about intermittent fasting they just found.\\n###Input: \\\"Intermittent fasting is a popular dietary trend that involves cycling between periods of fasting and eating. The most common methods include the 16:8 method, which involves fasting for 16 hours a day and eating during an eight-hour window, and the 5:2 method, which involves eating normally for five days a week and restricting calorie intake to 500-600 calories for the remaining two days. Advocates of intermittent fasting claim that it can lead to weight loss, improved metabolism, and increased energy levels. Some research supports these claims, with studies showing that intermittent fasting can help individuals lose weight, improve insulin sensitivity, and lower inflammation levels. However, not all experts agree on the benefits of intermittent fasting, as some argue that it can be difficult to adhere to for extended periods of time and may lead to disordered eating patterns or nutritional deficiencies. It is essential for individuals considering intermittent fasting to consult with a healthcare professional before making significant changes to their diet.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 531, "text": " Provide a method to translate a given Spanish text to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-de-es\\', \\'api_call\\': \\'Helsinki-NLP/opus-mt-de-es\\', \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_de_to_es\\', model=\\'Helsinki-NLP/opus-mt-de-es\\')\\\\ntranslated_text = translation(\\'Guten Tag\\')\", \\'performance\\': {\\'dataset\\': \\'Tatoeba.de.es\\', \\'accuracy\\': {\\'BLEU\\': 48.5, \\'chr-F\\': 0.676}}, \\'description\\': \\'A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 532, "text": " Please convert a message from Italian to English.\\n###Input: \\\"Ciao! Come stai? Ho sentito che il tuo viaggio stato fantastico.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-it-en\\', \\'api_call\\': \"pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')(\\'Ciao mondo!\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newssyscomb2009.it.en\\': 35.3, \\'newstest2009.it.en\\': 34.0, \\'Tatoeba.it.en\\': 70.9}, \\'chr-F\\': {\\'newssyscomb2009.it.en\\': 0.6, \\'newstest2009.it.en\\': 0.594, \\'Tatoeba.it.en\\': 0.808}}}, \\'description\\': \\'A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 533, "text": " Our mobile app includes a chat feature, and we want to translate user messages from German to Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-de-es\\', \\'api_call\\': \\'Helsinki-NLP/opus-mt-de-es\\', \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_de_to_es\\', model=\\'Helsinki-NLP/opus-mt-de-es\\')\\\\ntranslated_text = translation(\\'Guten Tag\\')\", \\'performance\\': {\\'dataset\\': \\'Tatoeba.de.es\\', \\'accuracy\\': {\\'BLEU\\': 48.5, \\'chr-F\\': 0.676}}, \\'description\\': \\'A German to Spanish translation model based on the OPUS dataset and trained using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece tokenization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 534, "text": " We have a Swedish company and for our international expansion, we are in need of a solution that can help us to send English translated emails to our clients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-es-en\\', \\'api_call\\': \"pipeline(\\'translation_es_to_en\\', model=\\'Helsinki-NLP/opus-mt-es-en\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_es_to_en\\', model=\\'Helsinki-NLP/opus-mt-es-en\\')(\\'Hola, \u00bfc\u00f3mo est\u00e1s?\\')\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'newssyscomb2009-spaeng.spa.eng\\', \\'accuracy\\': {\\'BLEU\\': 30.6, \\'chr-F\\': 0.57}}, {\\'name\\': \\'news-test2008-spaeng.spa.eng\\', \\'accuracy\\': {\\'BLEU\\': 27.9, \\'chr-F\\': 0.553}}, {\\'name\\': \\'newstest2009-spaeng.spa.eng\\', \\'accuracy\\': {\\'BLEU\\': 30.4, \\'chr-F\\': 0.572}}, {\\'name\\': \\'newstest2010-spaeng.spa.eng\\', \\'accuracy\\': {\\'BLEU\\': 36.1, \\'chr-F\\': 0.614}}, {\\'name\\': \\'newstest2011-spaeng.spa.eng\\', \\'accuracy\\': {\\'BLEU\\': 34.2, \\'chr-F\\': 0.599}}, {\\'name\\': \\'newstest2012-spaeng.spa.eng\\', \\'accuracy\\': {\\'BLEU\\': 37.9, \\'chr-F\\': 0.624}}, {\\'name\\': \\'newstest2013-spaeng.spa.eng\\', \\'accuracy\\': {\\'BLEU\\': 35.3, \\'chr-F\\': 0.609}}, {\\'name\\': \\'Tatoeba-test.spa.eng\\', \\'accuracy\\': {\\'BLEU\\': 59.6, \\'chr-F\\': 0.739}}]}, \\'description\\': \\'Helsinki-NLP/opus-mt-es-en is a machine translation model trained to translate from Spanish to English using the Hugging Face Transformers library. The model is based on the Marian framework and was trained on the OPUS dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 535, "text": " Can you help us come up with a brief summary of an input passage for our busy editor?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 536, "text": " We are building a chatting product that translates the user's feed written in French into Spanish to reach users in Latin America.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-fr-es\\', \\'api_call\\': \\'Helsinki-NLP/opus-mt-fr-es\\', \\'api_arguments\\': {\\'source_languages\\': \\'fr\\', \\'target_languages\\': \\'es\\'}, \\'python_environment_requirements\\': {\\'PyTorch\\': \\'1.0.0\\', \\'TensorFlow\\': \\'2.0\\', \\'Transformers\\': \\'4.0.0\\'}, \\'example_code\\': \"translation(\\'Bonjour, comment \u00e7a va?\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newssyscomb2009.fr.es\\': 34.3, \\'news-test2008.fr.es\\': 32.5, \\'newstest2009.fr.es\\': 31.6, \\'newstest2010.fr.es\\': 36.5, \\'newstest2011.fr.es\\': 38.3, \\'newstest2012.fr.es\\': 38.1, \\'newstest2013.fr.es\\': 34.0, \\'Tatoeba.fr.es\\': 53.2}, \\'chr-F\\': {\\'newssyscomb2009.fr.es\\': 0.601, \\'news-test2008.fr.es\\': 0.583, \\'newstest2009.fr.es\\': 0.586, \\'newstest2010.fr.es\\': 0.616, \\'newstest2011.fr.es\\': 0.622, \\'newstest2012.fr.es\\': 0.619, \\'newstest2013.fr.es\\': 0.587, \\'Tatoeba.fr.es\\': 0.709}}}, \\'description\\': \\'A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 537, "text": " Our client is looking for an AI chatbot. Can you create one for us?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 538, "text": " The company needs a way to improve customer support. We want a conversational agent to talk to the users and provide them with helpful information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 539, "text": " We want to build a virtual travel advisor assistant that would participate in a conversation with its client.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 540, "text": " Our customer needs a brief overview of how artificial intelligence has evolved over the years.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 541, "text": " We are developing a content-creation tool which can be used by users to generate academic articles on various topics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 542, "text": " Consdier the type of job you have. Generate a fictional story about a language model that comes to life.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'model.generate(input_ids, max_length=128)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}\", metadata={})]", "category": "generic"}
{"question_id": 543, "text": " Our company aims to generate conversational text based on provided prompts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 544, "text": " As a language learning platform, we want to offer a feature where users can ask for translations of sentences from their native language into the language they are trying to learn.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 545, "text": " I want a ready-to-use NLP model to complete this sentence: \\\"Today, I went to the [MASK] to buy some groceries.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'cl-tohoku/bert-base-japanese-whole-word-masking\\', \\'api_call\\': \\'cl-tohoku/bert-base-japanese-whole-word-masking\\', \\'api_arguments\\': {\\'model\\': \\'cl-tohoku/bert-base-japanese-whole-word-masking\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"fill_mask(\\'\u4eca\u65e5\u306f[MASK]\u3067\u3059\u3002\\')\", \\'performance\\': {\\'dataset\\': \\'Japanese Wikipedia\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization. Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.\\'}', metadata={})]", "category": "generic"}
{"question_id": 546, "text": " I am working on a chatbot that is supposed to have knowledge about geography. One question it should be able to answer is, what is the capital city of France?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'ahotrod/electra_large_discriminator_squad2_512\\', \\'api_call\\': \\'ahotrod/electra_large_discriminator_squad2_512\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'tensorflow\\'], \\'example_code\\': \"qa_pipeline({\\'question\\': \\'What is the capital of France?\\', \\'context\\': \\'France is a country in Europe. Its capital is Paris.\\'})\", \\'performance\\': {\\'dataset\\': \\'SQuAD2.0\\', \\'accuracy\\': {\\'exact\\': 87.09677419354838, \\'f1\\': 89.98343832723452}}, \\'description\\': \\'ELECTRA_large_discriminator language model fine-tuned on SQuAD2.0 for question answering tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 547, "text": " Analyze the following user comments and compare their similarity to find out which comments have the closest meaning.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'martin-ha/toxic-comment-model\\', \\'api_call\\': \"pipeline(model=AutoModelForSequenceClassification.from_pretrained(\\'martin-ha/toxic-comment-model\\'), tokenizer=AutoTokenizer.from_pretrained(\\'martin-ha/toxic-comment-model\\')).__call__(\\'This is a test text.\\')\", \\'api_arguments\\': {\\'model_path\\': \\'martin-ha/toxic-comment-model\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\\\nmodel_path = martin-ha/toxic-comment-model\\\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\\\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\\\nprint(pipeline(\\'This is a test text.\\'))\", \\'performance\\': {\\'dataset\\': \\'held-out test set\\', \\'accuracy\\': 0.94, \\'f1-score\\': 0.59}, \\'description\\': \\'This model is a fine-tuned version of the DistilBERT model to classify toxic comments.\\'}', metadata={})]", "category": "generic"}
{"question_id": 548, "text": " I want to assess the similarity between various sentences to find related content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'sentence-transformers/distiluse-base-multilingual-cased-v1\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 549, "text": " Can you help me find the most relevant document among these three Chinese sentences about my business proposal?\\n###Input: {\\\"source_sentence\\\": \\\"\\u6211\\u6b63\\u5728\\u4e3a\\u4e00\\u5bb6\\u65b0\\u7684\\u79d1\\u6280\\u516c\\u53f8\\u5236\\u5b9a\\u4e1a\\u52a1\\u8ba1\\u5212\\u3002\\\",  \\\"sentences_to_compare\\\": [\\\"\\u5173\\u4e8e\\u65b0\\u516c\\u53f8\\u7684\\u4e1a\\u52a1\\u8ba1\\u5212\\u3002\\\", \\\"\\u79d1\\u6280\\u884c\\u4e1a\\u7684\\u53d1\\u5c55\\u8d8b\\u52bf\\u3002\\\", \\\"\\u7b80\\u4ecb\\u53ca\\u4e86\\u89e3\\u516c\\u53f8\\u3002\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'text2vec-large-chinese\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'GanymedeNil/text2vec-large-chinese\\')\", \\'api_arguments\\': \\'source_sentence, sentences_to_compare\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import AutoModel, AutoTokenizer\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'GanymedeNil/text2vec-large-chinese\\')\\\\nmodel = AutoModel.from_pretrained(\\'GanymedeNil/text2vec-large-chinese\\')\", \\'performance\\': {\\'dataset\\': \\'https://huggingface.co/shibing624/text2vec-base-chinese\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A Chinese sentence similarity model based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replacing MacBERT with LERT, and keeping other training conditions unchanged.\\'}', metadata={})]", "category": "generic"}
{"question_id": 550, "text": " We are a library, and we want to find the similarity between two sentences about books.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'nikcheerla/nooks-amd-detection-v2-full\\', \\'api_call\\': \"SentenceTransformer(\\'{MODEL_NAME}\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'sentence-transformers\\', \\'transformers\\'], \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'{MODEL_NAME}\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 551, "text": " An AI recommended me an Indian male voice for reading Marathi text loudly. I will use Text-to-Speech conversion for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Marathi_Male_TTS\\', \\'api_call\\': \"api.load(\\'ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS\\').\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Marathi Male Text-to-Speech model using ESPnet framework.\\'}', metadata={})]", "category": "generic"}
{"question_id": 552, "text": " Convert a text written in the Telugu language to an audio file, spoken by a male voice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Telugu_Male_TTS\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'SYSPIN/Telugu_Male_TTS\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.\\'}', metadata={})]", "category": "generic"}
{"question_id": 553, "text": " Our team builds a mobile application that reads out news articles. We want to include audio output from text read by a text-to-speech engine.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'lakahaga/novel_reading_tts\\', \\'api_call\\': \"AutoModelForTTS.from_pretrained(\\'lakahaga/novel_reading_tts\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"inputs = processor(text, return_tensors=\\'pt\\'); generated_audio = model.generate(**inputs);\", \\'performance\\': {\\'dataset\\': \\'novelspeech\\', \\'accuracy\\': None}, \\'description\\': \\'This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 554, "text": " Write an email for a marketing campaign and create an audio file to check its effectiveness.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 555, "text": " Our company is developing an app for learning Dutch. We would like to implement a feature to auto-convert the user's speech into Dutch text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'GroNLP/bert-base-dutch-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'GroNLP/bert-base-dutch-cased\\')\", \\'api_arguments\\': [\\'pretrained_model_name_or_path\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModel, TFAutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(GroNLP/bert-base-dutch-cased)\\\\nmodel = AutoModel.from_pretrained(GroNLP/bert-base-dutch-cased)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'CoNLL-2002\\', \\'accuracy\\': \\'90.24\\'}, {\\'name\\': \\'SoNaR-1\\', \\'accuracy\\': \\'84.93\\'}, {\\'name\\': \\'spaCy UD LassySmall\\', \\'accuracy\\': \\'86.10\\'}]}, \\'description\\': \\'BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.\\'}', metadata={})]", "category": "generic"}
{"question_id": 556, "text": " Our company is developing an AI assistant to perform live transcription of meetings. We need to convert recorded speech into text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'\u3053\u3093\u306b\u3061\u306f\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 557, "text": " A web browser extension automatically transcribes meetings in real-time to help people with hearing disabilities or those who join late to understand what is being discussed.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Text-to-Speech', 'framework': 'ESPnet', 'functionality': 'Text-to-Speech', 'api_name': 'kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan', 'api_call': 'espnet/kan-bayashi_ljspeech_joint_finetune_conformer_fastspeech2_hifigan', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers', 'torch'], 'example_code': '', 'performance': {'dataset': 'LJSpeech', 'accuracy': ''}, 'description': 'A pretrained Text-to-Speech model based on the ESPnet framework, fine-tuned on the LJSpeech dataset. This model is capable of converting text input into synthesized speech.'}\", metadata={})]", "category": "generic"}
{"question_id": 558, "text": " As an international company, our phone conference calls happen in multiple languages. We want to create a tool to convert English speech input to French speech output for our meetings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 559, "text": " Create a tourist-oriented mobile app that translates spoken statements from a Romanian-speaking tour guide to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'facebook/textless_sm_ro_en\\', \\'api_call\\': \"pipeline(\\'audio-to-audio\\', model=\\'facebook/textless_sm_ro_en\\')\", \\'api_arguments\\': \\'audio file or recording\\', \\'python_environment_requirements\\': \\'fairseq, huggingface_hub\\', \\'example_code\\': \\'https://huggingface.co/facebook/textless_sm_cs_en\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A speech-to-speech translation model for Romanian to English developed by Facebook AI\\'}', metadata={})]", "category": "generic"}
{"question_id": 560, "text": " We would like to improve the customer support quality of our call center by understanding the emotions of our clients in realtime.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Emotion Classification\\', \\'api_name\\': \\'j-hartmann/emotion-english-distilroberta-base\\', \\'api_call\\': \\'text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True\\', \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \\'from transformers import pipeline\\\\nclassifier = pipeline(text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True)\\\\nclassifier(I love this!)\\', \\'performance\\': {\\'dataset\\': \\'Balanced subset from 6 diverse datasets\\', \\'accuracy\\': \\'66%\\'}, \\'description\\': \"This model classifies emotions in English text data. It predicts Ekman\\'s 6 basic emotions, plus a neutral class: anger, disgust, fear, joy, neutral, sadness, and surprise. The model is a fine-tuned checkpoint of DistilRoBERTa-base.\"}', metadata={})]", "category": "generic"}
{"question_id": 561, "text": " Develop a software for classifying keywords spoken by users. It should predict the top five keywords.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Paraphrase-based utterance augmentation\\', \\'api_name\\': \\'prithivida/parrot_fluency_model\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'prithivida/parrot_fluency_model\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"parrot(\\'your input text\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 562, "text": " Automate the process of categorizing noises on a blog with video content, analyzing and describing the type of sound in the video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 563, "text": " We are an e-Learning platform providing language lessons. Detect the speaker from a given speech segment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 564, "text": " An audio application is developed which requires speaker verification. Can you include the appropriate API call and example code?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 565, "text": " I am a software engineer, and I want to build an application that can control smart home devices using voice commands. The model must identify which command has been given by the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'\u3053\u3093\u306b\u3061\u306f\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 566, "text": " I am building a web application to predict a person's income based on adult census data, and I need to know how to use the XGBoost model to make the predictions. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'abhishek/autotrain-adult-census-xgboost\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'scikit-learn/adult-census-income\\', \\'accuracy\\': 0.8750191923844618}, \\'description\\': \"A binary classification model trained on the Adult Census Income dataset using the XGBoost algorithm. The model predicts whether an individual\\'s income is above or below $50,000 per year.\"}', metadata={})]", "category": "generic"}
{"question_id": 567, "text": " We need an AI model that can predict the carbon emission levels of different products based on certain factors.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'45473113800\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'samvelkoch/autotrain-data-prknsn-2\\', \\'accuracy\\': {\\'Loss\\': 5.079, \\'R2\\': 0.109, \\'MSE\\': 25.795, \\'MAE\\': 3.78, \\'RMSLE\\': 0.849}}, \\'description\\': \\'A tabular regression model trained with AutoTrain for predicting carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 568, "text": " As a speaker factory, we want to know the correlation beteween components weight and carbon emmisions during production.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'38507101578\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'wangdy/autotrain-data-godaddy2\\', \\'accuracy\\': {\\'Loss\\': 0.206, \\'R2\\': 0.997, \\'MSE\\': 0.042, \\'MAE\\': 0.081, \\'RMSLE\\': 0.052}}, \\'description\\': \\'This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\\'}', metadata={})]", "category": "generic"}
{"question_id": 569, "text": " Generate the amount of carbon emissions for given building data in order to predict the efficiency at which it produces.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'45473113800\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'samvelkoch/autotrain-data-prknsn-2\\', \\'accuracy\\': {\\'Loss\\': 5.079, \\'R2\\': 0.109, \\'MSE\\': 25.795, \\'MAE\\': 3.78, \\'RMSLE\\': 0.849}}, \\'description\\': \\'A tabular regression model trained with AutoTrain for predicting carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 570, "text": " I oversee management of a company with warehouse robots. I want our robots to move in a more natural gait. Can you provide a suitable model for this purpose?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 571, "text": " Our software team wants to implement an AI-driven multi-agent soccer game. Identify an existing model to use.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 572, "text": " We'd like to analyze a customer review in Korean and help identify the most relevant parts for potential improvements.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 573, "text": " Extract linguistic features from an audio sample.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'audio-spectrogram-transformer\\', \\'api_call\\': \"AutoFeatureExtractor.from_pretrained(\\'Ericwang/tiny-random-ast\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'One custom ast model for testing of HF repos\\'}', metadata={})]", "category": "generic"}
{"question_id": 574, "text": " Analyze the trend of people's fashion style based on the designer's description and generate related images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 575, "text": " An online manga translation platform needs to recognize text in Japanese manga and want to convert it into plain text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 576, "text": " Recognize and extract text from an image that contains a printed sample.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 577, "text": " A magazine editor is interested in extracting text from a printed image of a handwritten manuscript to include in the publication.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'vintedois-diffusion-v0-1\\', \\'api_call\\': \"text2img = pipeline(\\'text-to-image\\', model=\\'22h/vintedois-diffusion-v0-1\\')\", \\'api_arguments\\': [\\'prompt\\', \\'CFG Scale\\', \\'Scheduler\\', \\'Steps\\', \\'Seed\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text2img(\\'photo of an old man in a jungle, looking at the camera\\', CFG Scale=7.5, Scheduler=\\'diffusers.EulerAncestralDiscreteScheduler\\', Steps=30, Seed=44)\", \\'performance\\': {\\'dataset\\': \\'large amount of high quality images\\', \\'accuracy\\': \\'not specified\\'}, \\'description\\': \\'Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\\'}', metadata={})]", "category": "generic"}
{"question_id": 578, "text": " I want to create a security system based on text identification in my company. It will be used for reading texts from images captured by the security system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 579, "text": " A marketing team needs to create high-quality content for product promotion. They want to turn text descriptions into short video clips to engage their audience on social media.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 580, "text": " As part of a social-aware project, we are analyzing images and answering questions based on those images using pretrained models.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'temp_vilt_vqa\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'Bingsu/temp_vilt_vqa\\', tokenizer=\\'Bingsu/temp_vilt_vqa\\')\", \\'api_arguments\\': {\\'model\\': \\'Bingsu/temp_vilt_vqa\\', \\'tokenizer\\': \\'Bingsu/temp_vilt_vqa\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 581, "text": " We want to build a program that would answer questions based on visuals.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 582, "text": " Create an AI tool that answers simple questions related to an image provided as input.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'blip-vqa-base\\', \\'api_call\\': \"Output: BlipForQuestionAnswering.from_pretrained(\\'Salesforce/blip-vqa-base\\').generate(**inputs)\", \\'api_arguments\\': {\\'raw_image\\': \\'Image\\', \\'question\\': \\'String\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'BlipProcessor, BlipForQuestionAnswering\\', \\'PIL\\': \\'Image\\', \\'requests\\': \\'requests\\'}, \\'example_code\\': \"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-base)\\\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-base)\\\\nimg_url = \\'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\\'RGB\\')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \\'performance\\': {\\'dataset\\': \\'VQA\\', \\'accuracy\\': \\'+1.6% in VQA score\\'}, \\'description\\': \\'BLIP is a Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is trained on visual question answering with a base architecture (using ViT base backbone).\\'}', metadata={})]", "category": "generic"}
{"question_id": 583, "text": " Someone is sending me pictures of famous places, but they want me to guess the place by asking questions. Can you help me respond to the vital question?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': 'Not specified.', 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = prompthero/openjourney\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.'}\", metadata={})]", "category": "generic"}
{"question_id": 584, "text": " An accounting firm would like to extract specific information, such as invoice numbers, from scanned images of documents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 585, "text": " Can you explain how I can extract answers from a document image with a specific question?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 586, "text": " I have some scanned documents, and I want a tool that extracts information from these images to answer questions related to the content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 587, "text": " We would like to extract important information from an invoice for bookkeeping and accounting purposes.\\n###Input: {\\\"url\\\": \\\"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\\", \\\"question\\\": \\\"What is the total amount due?\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 588, "text": " A robot company wants to create a model to predict the distance of objects from the camera. They want to know which API would be best suitable for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'openai/clip-vit-large-patch14-336\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'openai/clip-vit-large-patch14\\').\", \\'api_arguments\\': \\'image_path, tokenizer, model\\', \\'python_environment_requirements\\': \\'Transformers 4.21.3, TensorFlow 2.8.2, Tokenizers 0.12.1\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This model was trained from scratch on an unknown dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 589, "text": " We need to classify images of animals as a part of our game development process. How can we use an ML model to achieve this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 590, "text": " For an article about popular street foods, we need to find out if the given image is a hotdog.\\n###Input: path/to/image\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'julien-c/hotdog-not-hotdog\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'julien-c/hotdog-not-hotdog\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': 0.825}, \\'description\\': \\'A model that classifies images as hotdog or not hotdog.\\'}', metadata={})]", "category": "generic"}
{"question_id": 591, "text": " Our company sells custom-made aprons. We want to use an AI tool to identify images of aprons and analyze the customer feedback.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 592, "text": " We need to automatically segment items in images to improve our warehouse management system. Help us to detect objects in the images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-pcb-defect-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-pcb-defect-segmentation\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-pcb-defect-segmentation\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'print(results[0].masks)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'pcb-defect-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.568, \\'mAP@0.5(mask)\\': 0.557}}, \\'description\\': \\'A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\\'}', metadata={})]", "category": "generic"}
{"question_id": 593, "text": " I want a chat product to classify whether an image is healthy by checking the presence of red blood cells, white blood cells and platelets in the bloodstream.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Blood Cell Detection\\', \\'api_name\\': \\'keremberke/yolov8n-blood-cell-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'blood-cell-object-detection\\', \\'accuracy\\': 0.893}, \\'description\\': \\'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 594, "text": " My client manages a real estate company, they want to segment the rooms in their apartment pictures to showcase them to their potential customers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\', \\'api_call\\': \"SegformerForSemanticSegmentation.from_pretrained(\\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nvidia/segformer-b0-finetuned-cityscapes-1024-1024\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nfeature_extractor = SegformerFeatureExtractor.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-cityscapes-1024-1024)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\', \\'performance\\': {\\'dataset\\': \\'CityScapes\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 595, "text": " Our city management wants to analyze urban landscape images to identify various elements such as buildings, roads, and trees.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 596, "text": " We need to detect potholes in a provided image to help the city government take necessary actions in fixing damaged roads.\\n###Input: {\\\"image\\\": \\\"https://example.com/pothole_image.jpg\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8s-pothole-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8s-pothole-segmentation\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': {\\'ultralyticsplus\\': \\'0.0.23\\', \\'ultralytics\\': \\'8.0.21\\'}, \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8s-pothole-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pothole-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.928, \\'mAP@0.5(mask)\\': 0.928}}, \\'description\\': \\'A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 597, "text": " You are working with a fashion magazine that needs a tool to generate a variation of a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 598, "text": " A travel agency wants to use computer vision for generating virtual tours of tourist attractions based on the provided text description.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'prompthero/openjourney-v4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'prompthero/openjourney-v4\\')\", \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"generate_image(\\'your text here\\')\", \\'performance\\': {\\'dataset\\': \\'Midjourney v4 images\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 599, "text": " Generate an image for marketing purposes depicting a concert of a famous pop star based on the line art as input.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 600, "text": " We are aiming to help a designer to come up with a poster visual for their new home decor product line inspired by a royal chamber with a fancy bed.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 601, "text": " We are building a website to showcase various churches. Write a Python script that generates a high-quality image of a church using Denoising Diffusion Probabilistic Models (DDPM).\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 602, "text": " How can I generate an image using the provided API for a creative project?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image Generation', 'api_name': 'stabilityai/stable-diffusion-2', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': {'prompt': 'a photo of an astronaut riding a horse on mars'}, 'python_environment_requirements': ['diffusers', 'transformers', 'accelerate', 'scipy', 'safetensors'], 'example_code': 'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)', 'performance': {'dataset': 'COCO2017 validation set', 'accuracy': 'Not optimized for FID scores'}, 'description': 'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.'}\", metadata={})]", "category": "generic"}
{"question_id": 603, "text": " We want to prepare some Minecraft skin images for a gaming event. Use an API to generate some unique skins.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Diffusers\\', \\'api_name\\': \\'Minecraft-Skin-Diffusion\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\\\\nimage = pipeline().images[0].convert(\\'RGBA\\')\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods.\\'}', metadata={})]", "category": "generic"}
{"question_id": 604, "text": " Generate an image of a galaxy using a pre-trained diffusion model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Diffusers\\', \\'api_name\\': \\'myunus1/diffmodels_galaxies_scratchbook\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'myunus1/diffmodels_galaxies_scratchbook\\')\", \\'api_arguments\\': {\\'from_pretrained\\': \\'myunus1/diffmodels_galaxies_scratchbook\\'}, \\'python_environment_requirements\\': {\\'package\\': \\'diffusers\\', \\'import\\': \\'from diffusers import DDPMPipeline\\'}, \\'example_code\\': {\\'initialize_pipeline\\': \"pipeline = DDPMPipeline.from_pretrained(\\'myunus1/diffmodels_galaxies_scratchbook\\')\", \\'generate_image\\': \\'image = pipeline().images[0]\\', \\'display_image\\': \\'image\\'}, \\'performance\\': {\\'dataset\\': \\'Not provided\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b.\\'}', metadata={})]", "category": "generic"}
{"question_id": 605, "text": " Develop a method for us to categorize videos according to their contents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 606, "text": " An application that can identify human actions in short video clips is needed. For this task, select a model suitable for video classification.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-short-finetuned-kinetics\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\", \\'api_arguments\\': [\\'video\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\ninputs = processor(video, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\'Predicted class:\\', model.config.id2label[predicted_class_idx])\", \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': {\\'top-1\\': 79.4, \\'top-5\\': 94.1}}, \\'description\\': \\'VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 607, "text": " Our content provides physical fitness videos to customers. Identifying and classifying the type of exercise in each video would help provide personalized recommendations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-short-finetuned-kinetics\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\", \\'api_arguments\\': [\\'video\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\ninputs = processor(video, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\'Predicted class:\\', model.config.id2label[predicted_class_idx])\", \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': {\\'top-1\\': 79.4, \\'top-5\\': 94.1}}, \\'description\\': \\'VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 608, "text": " We need to classify some video clips for the exercise videos' application. Use a suitable video model to do this.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/xclip-base-patch16-zero-shot\\', \\'api_call\\': \"XClipModel.from_pretrained(\\'microsoft/xclip-base-patch16-zero-shot\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'For code examples, we refer to the documentation.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'HMDB-51\\', \\'accuracy\\': 44.6}, {\\'name\\': \\'UCF-101\\', \\'accuracy\\': 72.0}, {\\'name\\': \\'Kinetics-600\\', \\'accuracy\\': 65.2}]}, \\'description\\': \\'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\\'}', metadata={})]", "category": "generic"}
{"question_id": 609, "text": " I'm creating a security system for my company. I want to analyze video feeds and decide the main action occurring in the video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 610, "text": " A member of a museum curatorial team needs to categorize new art pieces recently acquired for their online database. They want to classify them automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 611, "text": " Imagine you are building an app that identifies dog breed from a photo. We need to know the breed when user uploads a new photo.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 612, "text": " We have a finance startup working on making investing easy. We want to analyze sentiment of financial news.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'text2text-generation', 'api_name': 'financial-summarization-pegasus', 'api_call': 'PegasusForConditionalGeneration.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))', 'performance': {'dataset': 'xsum', 'accuracy': {'ROUGE-1': 35.206, 'ROUGE-2': 16.569, 'ROUGE-L': 30.128, 'ROUGE-LSUM': 30.171}}, 'description': 'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 613, "text": " I would like to create a tool for investors to analyze stock-related comments and classify them as \\\"Bullish\\\" or \\\"Bearish\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 614, "text": " In my department store, analyze the customers' reviews in the German language for better feedback.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'German Sentiment Classification', 'api_name': 'oliverguhr/german-sentiment-bert', 'api_call': 'Output: SentimentModel()', 'api_arguments': ['texts'], 'python_environment_requirements': 'pip install germansentiment', 'example_code': ['from germansentiment import SentimentModel', 'model = SentimentModel()', 'texts = [', ' Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,', ' Total awesome!,nicht so schlecht wie erwartet,', ' Der Test verlief positiv.,Sie f\u00e4hrt ein gr\u00fcnes Auto.]', 'result = model.predict_sentiment(texts)', 'print(result)'], 'performance': {'dataset': ['holidaycheck', 'scare', 'filmstarts', 'germeval', 'PotTS', 'emotions', 'sb10k', 'Leipzig Wikipedia Corpus 2016', 'all'], 'accuracy': [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, 'description': 'This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.'}\", metadata={})]", "category": "generic"}
{"question_id": 615, "text": " Our software system has communication interactions with users. We need to identify whether the received message is gibberish or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'madhurjindal/autonlp-Gibberish-Detector-492513457\\', \\'api_call\\': \"Output: AutoModelForSequenceClassification.from_pretrained(\\'madhurjindal/autonlp-Gibberish-Detector-492513457\\', use_auth_token=True)\", \\'api_arguments\\': {\\'inputs\\': \\'I love AutoNLP\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoModelForSequenceClassification\\', \\'AutoTokenizer\\': \\'from_pretrained\\'}, \\'example_code\\': \\'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoNLP, return_tensors=pt)\\\\noutputs = model(**inputs)\\', \\'performance\\': {\\'dataset\\': \\'madhurjindal/autonlp-data-Gibberish-Detector\\', \\'accuracy\\': 0.9735624586913417}, \\'description\\': \\'A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT.\\'}', metadata={})]", "category": "generic"}
{"question_id": 616, "text": " We have online medical records and we want to remove sensitive information like names, dates, addresses, etc. Is there any API available to simplify this process?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'De-identification\\', \\'api_name\\': \\'StanfordAIMI/stanford-deidentifier-base\\', \\'api_call\\': \"pipeline(\\'ner\\', model=\\'StanfordAIMI/stanford-deidentifier-base\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"deidentifier(\\'Your input text here\\')\", \\'performance\\': {\\'dataset\\': \\'radreports\\', \\'accuracy\\': {\\'known_institution_F1\\': 97.9, \\'new_institution_F1\\': 99.6, \\'i2b2_2006_F1\\': 99.5, \\'i2b2_2014_F1\\': 98.9}}, \\'description\\': \\'Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.\\'}', metadata={})]", "category": "generic"}
{"question_id": 617, "text": " I have a list of information on the Olympic Games and I need to find the cities and years they were held in. Can you generate a table for me?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'neulab/omnitab-large-finetuned-wtq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'neulab/omnitab-large-finetuned-wtq\\').generate(**encoding)\", \\'api_arguments\\': {\\'table\\': \\'pd.DataFrame\\', \\'query\\': \\'str\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\', \\'pandas\\': \\'pd\\'}, \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport pandas as pd\\\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-finetuned-wtq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-finetuned-wtq)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'wikitablequestions\\', \\'accuracy\\': None}, \\'description\\': \\'OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. The original Github repository is https://github.com/jzbjyb/OmniTab.\\'}', metadata={})]", "category": "generic"}
{"question_id": 618, "text": " The company we are working with is trying to get information about the medical industry. They want to get answers to their questions based on information from tables.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 619, "text": " A medical research team needs an AI model to extract answers from medical articles for their research project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-base-squad2-covid\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=RobertaForQuestionAnswering.from_pretrained(\\'deepset/roberta-base-squad2-covid\\'), tokenizer=RobertaTokenizer.from_pretrained(\\'deepset/roberta-base-squad2-covid\\'))\", \\'api_arguments\\': {\\'model_name\\': \\'deepset/roberta-base-squad2-covid\\', \\'tokenizer\\': \\'deepset/roberta-base-squad2-covid\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': {\\'QA_input\\': {\\'question\\': \\'Why is model conversion important?\\', \\'context\\': \\'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\\'}, \\'res\\': \\'nlp(QA_input)\\'}, \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': {\\'XVAL_EM\\': 0.17890995260663506, \\'XVAL_f1\\': 0.49925444207319924, \\'XVAL_top_3_recall\\': 0.8021327014218009}}, \\'description\\': \\'This model is a Roberta-based model fine-tuned on SQuAD-style CORD-19 annotations for the task of extractive question answering in the context of COVID-19. It can be used with the Hugging Face Transformers library for question answering tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 620, "text": " I need information about Mount Everest in Chinese. Please tell me how high it is.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\\', \\'api_call\\': \\'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large.from_pretrained()\\', \\'api_arguments\\': \\'context, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\'question-answering\\', model=\\'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\\')\\\\nresult = qa_pipeline({\\'context\\': \\'your_context_here\\', \\'question\\': \\'your_question_here\\'})\", \\'performance\\': {\\'dataset\\': \\'Dureader-2021\\', \\'accuracy\\': \\'83.1\\'}, \\'description\\': \\'A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition.\\'}', metadata={})]", "category": "generic"}
{"question_id": 621, "text": " I'm a retinal surgeon, and I want to detect which diseases have been cured or quite stable based on a given clinical note.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\', \\'api_call\\': \"pipeline(\\'image-classification\\', \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\').model\", \\'api_arguments\\': {\\'model_name\\': \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.28.1\\', \\'pytorch\\': \\'2.0.0+cu118\\', \\'datasets\\': \\'2.11.0\\', \\'tokenizers\\': \\'0.13.3\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nimage_classifier = pipeline(\\'image-classification\\', \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\')\\\\nresult = image_classifier(\\'path/to/image.jpg\\')\", \\'performance\\': {\\'dataset\\': \\'None\\', \\'accuracy\\': 0.7744}, \\'description\\': \\'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.\\'}', metadata={})]", "category": "generic"}
{"question_id": 622, "text": " Assess the compatibility of two Russian sentences you are given.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'nikcheerla/nooks-amd-detection-v2-full\\', \\'api_call\\': \"SentenceTransformer(\\'{MODEL_NAME}\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'sentence-transformers\\', \\'transformers\\'], \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'{MODEL_NAME}\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a sentence-transformers model that maps sentences and paragraphs to a 768-dimensional dense vector space. It can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 623, "text": " I need to display this phrase \\\"Bonjour, comment  va?\\\" in English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation, Summarization, Question Answering, Sentiment Analysis\\', \\'api_name\\': \\'t5-3b\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'t5-3b\\')\", \\'api_arguments\\': \\'input_text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"input_text = \\'translate English to French: The quick brown fox jumps over the lazy dog\\'; inputs = tokenizer.encode(input_text, return_tensors=\\'pt\\'); outputs = model.generate(inputs); translated_text = tokenizer.decode(outputs[0])\", \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'See research paper, Table 14\\'}, \\'description\\': \\'T5-3B is a Text-To-Text Transfer Transformer (T5) model with 3 billion parameters. It is designed for various NLP tasks such as translation, summarization, question answering, and sentiment analysis. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on multiple supervised and unsupervised tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 624, "text": " We need to translate a recipe from our Italian cookbook to English for international customers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-it-en\\', \\'api_call\\': \"pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')(\\'Ciao mondo!\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newssyscomb2009.it.en\\': 35.3, \\'newstest2009.it.en\\': 34.0, \\'Tatoeba.it.en\\': 70.9}, \\'chr-F\\': {\\'newssyscomb2009.it.en\\': 0.6, \\'newstest2009.it.en\\': 0.594, \\'Tatoeba.it.en\\': 0.808}}}, \\'description\\': \\'A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 625, "text": " Can you help me to create an implementation to translate English text to French text?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 626, "text": " A tourist company requested that for each destination, they need to include a detailed brief written in different languages. They want you to create a system that will handle this.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'mT5_multilingual_XLSum-sinhala-abstaractive-summarization_BBC-News-Summary\\', \\'api_call\\': \\'Hamza-Ziyard/mT5_multilingual_XLSum-sinhala-abstaractive-summarization_BBC-News-Summary\\', \\'api_arguments\\': {\\'input_text\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.28.1\\', \\'torch\\': \\'2.0.0+cu118\\', \\'tokenizers\\': \\'0.13.3\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(\\'summarization\\', model=\\'Hamza-Ziyard/mT5_multilingual_XLSum-sinhala-abstaractive-summarization_BBC-News-Summary\\')\\\\nsummarizer(\\'your input text\\')\", \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': {\\'Loss\\': 2.7794}}, \\'description\\': \\'This model is a fine-tuned version of csebuetnlp/mT5_multilingual_XLSum on an unknown dataset. It is intended for Sinhala news summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 627, "text": " As a content-manager in the field of computer AI, I want to generate summaries for blog posts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 628, "text": " My company receives many lengthy reports, and we need a tool to automatically generate summaries.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 629, "text": " I'm working on a research project about patents in the tech industry. I need help summarizing lengthy patent documents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 630, "text": " Our client, a news website, is looking for an automated system to provide summaries of long articles in Russian. Generate a summary for a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 631, "text": " I want to create a conversation with a fictional character I am designing. Can you help me generate text in the character's voice?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 632, "text": " We are looking to implement a chatbot that can engage in open-domain conversations with our customers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 633, "text": " Generate a short summary of the highlighted benefits of a low-carb diet and the possible drawbacks.\\n###Input: A low-carb diet can bring numerous benefits, including weight loss, improved blood sugar control, enhanced mental focus, lower blood pressure, and reduced hunger. However, there are potential drawbacks, such as nutrient deficiencies, increased risk of heart disease due to higher saturated fat consumption, and an initial period of low energy and mood swings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1702259728\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'Validation Metrics\\', \\'accuracy\\': 0.831}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 634, "text": " I have limited computational resources. Design an AI that can complete short stories from given prompts.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': 'Not specified.', 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = prompthero/openjourney\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.'}\", metadata={})]", "category": "generic"}
{"question_id": 635, "text": " Greg desires a translation of the following sentence \\\"What is your favorite color?\\\" from English to German.\\n###Input: What is your favorite color?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'German Sentiment Classification', 'api_name': 'oliverguhr/german-sentiment-bert', 'api_call': 'Output: SentimentModel()', 'api_arguments': ['texts'], 'python_environment_requirements': 'pip install germansentiment', 'example_code': ['from germansentiment import SentimentModel', 'model = SentimentModel()', 'texts = [', ' Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,', ' Total awesome!,nicht so schlecht wie erwartet,', ' Der Test verlief positiv.,Sie f\u00e4hrt ein gr\u00fcnes Auto.]', 'result = model.predict_sentiment(texts)', 'print(result)'], 'performance': {'dataset': ['holidaycheck', 'scare', 'filmstarts', 'germeval', 'PotTS', 'emotions', 'sb10k', 'Leipzig Wikipedia Corpus 2016', 'all'], 'accuracy': [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, 'description': 'This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.'}\", metadata={})]", "category": "generic"}
{"question_id": 636, "text": " Give me a brief summary of a long conversation between five people discussing the future of technology and privacy issues.\\n###Input: '<long_conversation>'\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"Output: long_t5_tglobal_base_16384_book_summary = T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\\\\\\\\nsummarizer = pipeline(\\\\\\\\n    summarization,\\\\\\\\n    model=long_t5_tglobal_base_16384_book_summary,\\\\\\\\n    tokenizer=long_t5_tglobal_base_16384_book_summary.tokenizer,\\\\\\\\n    device=0 if torch.cuda.is_available() else -1,\\\\\\\\n)\\\\\\\\nsummarizer(long_text)\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 637, "text": " Create a simple text-based calculator that uses a language model for arithmetic operations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \\'AutoModelForSeq2SeqLM.from_pretrained(model_name)\\', \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 638, "text": " Create a program that will predict what word is needed to fill in the blanks in the following incomplete sentence. \\\"She decided to ____ a movie tonight because she was tired.\\\"\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 639, "text": " I have a sentence about my cat. The cat is a missing word in Spanish. Help me complete the sentence.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 640, "text": " We need a model to evaluate the similarity between two sentences for a text-analysis application.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\', \\'api_call\\': \\'model.encode(text)\\', \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'sentence-transformers library\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\\\\ntext = Replace me by any text you\\'d like.\\\\ntext_embbedding = model.encode(text)\", \\'performance\\': {\\'dataset\\': \\'1,097,953,922\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 641, "text": " Can you generate Japanese audio files of the given text?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 642, "text": " Setup a voice assistant to play Korean novel audiobooks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'lakahaga/novel_reading_tts\\', \\'api_call\\': \"AutoModelForTTS.from_pretrained(\\'lakahaga/novel_reading_tts\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"inputs = processor(text, return_tensors=\\'pt\\'); generated_audio = model.generate(**inputs);\", \\'performance\\': {\\'dataset\\': \\'novelspeech\\', \\'accuracy\\': None}, \\'description\\': \\'This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 643, "text": " The language learning app developers have requested that we implement TTS functionality for their Chinese language study materials.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\', \\'api_call\\': \"Output: Text2Speech.from_pretrained(\\'espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'espnet_model_zoo\\'], \\'example_code\\': \\'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = \u6625\u6c5f\u6f6e\u6c34\u8fde\u6d77\u5e73\uff0c\u6d77\u4e0a\u660e\u6708\u5171\u6f6e\u751f\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\\', \\'performance\\': {\\'dataset\\': \\'csmsc\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\\'}', metadata={})]", "category": "generic"}
{"question_id": 644, "text": " I need a solution to transcribe voice memos from my phone into written text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'\u3053\u3093\u306b\u3061\u306f\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 645, "text": " Develop an automatic speech recognition system for a customer service hotline.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 646, "text": " I would like to clean up some noisy audio recordings so they can be played at a conference.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 647, "text": " Our company is developing an app that separates different speakers in a podcast recording. We need to find a transformer model that can help us achieve this.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'facebook/xm_transformer_sm_all-en\\', \\'api_call\\': \"pipeline(\\'translation\\', model=\\'facebook/xm_transformer_sm_all-en\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"translation_pipeline(\\'Hello, how are you?\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A speech-to-speech translation model that can be loaded on the Inference API on-demand.\\'}', metadata={})]", "category": "generic"}
{"question_id": 648, "text": " Convert the Hokkien speech into English speech so that everyone can understand it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\', \\'api_call\\': \\'tts_model.get_prediction(tts_sample)\\', \\'api_arguments\\': {\\'unit\\': \\'Text input for the TTS model\\'}, \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\', \\'torchaudio\\'], \\'example_code\\': \"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x[\\'args\\'][\\'data\\']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\", \\'performance\\': {\\'dataset\\': \\'TAT-TTS\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.\\'}', metadata={})]", "category": "generic"}
{"question_id": 649, "text": " Develop an audio-to-audio translation script that converts a short audio file from one language to another without using text.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 650, "text": " The company's call center needs a solution to enhance the quality of noisy recordings for better customer interactions.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 651, "text": " I want to create an app for emotion recognition, which can detect emotions in people's voices based on a given audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'SpeechBrain\\', \\'functionality\\': \\'Emotion Recognition\\', \\'api_name\\': \\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP\\', \\'api_call\\': \"classifier = foreign_class(source=\\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP\\', pymodule_file=\\'custom_interface.py\\', classname=\\'CustomEncoderWav2vec2Classifier\\')\\\\\\\\nclassifier.classify_file(\\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav\\')\", \\'api_arguments\\': [\\'file_path\\'], \\'python_environment_requirements\\': \\'pip install speechbrain\\', \\'example_code\\': \"from speechbrain.pretrained.interfaces import foreign_class\\\\nclassifier = foreign_class(source=\\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP\\', pymodule_file=\\'custom_interface.py\\', classname=\\'CustomEncoderWav2vec2Classifier\\')\\\\nout_prob, score, index, text_lab = classifier.classify_file(\\'speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav\\')\\\\nprint(text_lab)\", \\'performance\\': {\\'dataset\\': \\'IEMOCAP\\', \\'accuracy\\': \\'78.7%\\'}, \\'description\\': \\'This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. It is trained on IEMOCAP training data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 652, "text": " Our company provides a global voice assistant. We need to identify the language used in incoming calls to provide the right service to callers in their language. \\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 653, "text": " Analyze a recording of a Russian customer service representative to determine their emotion during the conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'wav2vec2-xlsr-53-russian-emotion-recognition\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'facebook/wav2vec2-large-xlsr-53\\') .predict(\\'/path/to/russian_audio_speech.wav\\', sampling_rate=16000)\", \\'api_arguments\\': {\\'path\\': \\'/path/to/russian_audio_speech.wav\\', \\'sampling_rate\\': 16000}, \\'python_environment_requirements\\': [\\'torch\\', \\'torchaudio\\', \\'transformers\\', \\'librosa\\', \\'numpy\\'], \\'example_code\\': \"result = predict(\\'/path/to/russian_audio_speech.wav\\', 16000)\\\\nprint(result)\", \\'performance\\': {\\'dataset\\': \\'Russian Emotional Speech Dialogs\\', \\'accuracy\\': \\'72%\\'}, \\'description\\': \\'A model trained to recognize emotions in Russian speech using wav2vec2. It can classify emotions such as anger, disgust, enthusiasm, fear, happiness, neutral, and sadness.\\'}', metadata={})]", "category": "generic"}
{"question_id": 654, "text": " Create a system that can detect voice activity in audio files, estimate the speech-to-noise ratio, and measure the C50 room acoustics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 655, "text": " I have a set of plant observations with various features, and I need to categorize them into different species. What classifier can you suggest and what do I need to implement it?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'abhishek/autotrain-iris-knn\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'scikit-learn/iris\\', \\'accuracy\\': 0.9}, \\'description\\': \\'A K-Nearest Neighbors (KNN) model trained on the Iris dataset for multi-class classification. The model is trained using AutoTrain and has an accuracy of 0.9.\\'}', metadata={})]", "category": "generic"}
{"question_id": 656, "text": " Generate predictions for carbon emissions from a dataset provided as a .csv file to make our factory more eco-friendly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'45473113800\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'samvelkoch/autotrain-data-prknsn-2\\', \\'accuracy\\': {\\'Loss\\': 5.079, \\'R2\\': 0.109, \\'MSE\\': 25.795, \\'MAE\\': 3.78, \\'RMSLE\\': 0.849}}, \\'description\\': \\'A tabular regression model trained with AutoTrain for predicting carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 657, "text": " I want to predict future carbon emissions within a location. Develop an automation tool that provides continuous data analysis and estimates future changes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'45473113800\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'samvelkoch/autotrain-data-prknsn-2\\', \\'accuracy\\': {\\'Loss\\': 5.079, \\'R2\\': 0.109, \\'MSE\\': 25.795, \\'MAE\\': 3.78, \\'RMSLE\\': 0.849}}, \\'description\\': \\'A tabular regression model trained with AutoTrain for predicting carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 658, "text": " I am an environmental expert, and my job is to predict carbon emissions given a dataset. The dataset has columns for different factors affecting emissions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1918465011\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\', \\'json\\'], \\'example_code\\': [\\'import json\\', \\'import joblib\\', \\'import pandas as pd\\', \"model = joblib.load(\\'model.joblib\\')\", \"config = json.load(open(\\'config.json\\'))\", \"features = config[\\'features\\']\", \"data = pd.read_csv(\\'data.csv\\')\", \\'data = data[features]\\', \"data.columns = [\\'feat_\\' + str(col) for col in data.columns]\", \\'predictions = model.predict(data)\\'], \\'performance\\': {\\'dataset\\': \\'kyle-lucke/autotrain-data-planes\\', \\'accuracy\\': 0.997}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 659, "text": " Can you analyze the dataset containing information about electric cars and predict the carbon emissions for specific vehicles?\\n###Input: {'data': 'data.csv'}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'45473113800\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'samvelkoch/autotrain-data-prknsn-2\\', \\'accuracy\\': {\\'Loss\\': 5.079, \\'R2\\': 0.109, \\'MSE\\': 25.795, \\'MAE\\': 3.78, \\'RMSLE\\': 0.849}}, \\'description\\': \\'A tabular regression model trained with AutoTrain for predicting carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 660, "text": " You are designing a gaming AI and you want to use the existing sb3/dqn-Acrobot-v1 model as your starting point. Train your gaming AI using this model.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Reinforcement Learning', 'framework': 'Stable-Baselines3', 'functionality': 'LunarLander-v2', 'api_name': 'araffin/dqn-LunarLander-v2', 'api_call': 'Output: DQN.load(checkpoint, **kwargs)', 'api_arguments': {'checkpoint': 'araffin/dqn-LunarLander-v2', 'kwargs': {'target_update_interval': 30}}, 'python_environment_requirements': ['huggingface_sb3', 'stable_baselines3'], 'example_code': {'load_model': 'from huggingface_sb3 import load_from_hub\\\\nfrom stable_baselines3 import DQN\\\\nfrom stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\n\\\\ncheckpoint = load_from_hub(araffin/dqn-LunarLander-v2, dqn-LunarLander-v2.zip)\\\\n\\\\nkwargs = dict(target_update_interval=30)\\\\n\\\\nmodel = DQN.load(checkpoint, **kwargs)\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)', 'evaluate': 'mean_reward, std_reward = evaluate_policy(\\\\n model,\\\\n env,\\\\n n_eval_episodes=20,\\\\n deterministic=True,\\\\n)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})'}, 'performance': {'dataset': 'LunarLander-v2', 'accuracy': '280.22 +/- 13.03'}, 'description': 'This is a trained model of a DQN agent playing LunarLander-v2 using the stable-baselines3 library.'}\", metadata={})]", "category": "generic"}
{"question_id": 661, "text": " A gaming company is eager to develop a new reinforcement learning game based on ant locomotion. We need to use the pre-trained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'6D grasping\\', \\'api_name\\': \\'camusean/grasp_diffusion\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'camusean/grasp_diffusion\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.\\'}', metadata={})]", "category": "generic"}
{"question_id": 662, "text": " I have a Soccer simulation with two teams, and I would like to train agents to play effectively in the simulation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'6D grasping\\', \\'api_name\\': \\'camusean/grasp_diffusion\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'camusean/grasp_diffusion\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.\\'}', metadata={})]", "category": "generic"}
{"question_id": 663, "text": " The factory needs an intelligent robot to perform complex tasks with real-time interaction. The robot needs to have a visual model that can manipulate objects and navigate in an indoor environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 664, "text": " Please generate a representation of an image from the URL: 'http://images.cocodataset.org/val2017/000000039769.jpg'\\n###Input: http://images.cocodataset.org/val2017/000000039769.jpg\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': 'DDPMPipeline.from_pretrained(model_id)', 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.'}\", metadata={})]", "category": "generic"}
{"question_id": 665, "text": " Classify the content of the video clip.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/xclip-base-patch16-zero-shot\\', \\'api_call\\': \"XClipModel.from_pretrained(\\'microsoft/xclip-base-patch16-zero-shot\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'For code examples, we refer to the documentation.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'HMDB-51\\', \\'accuracy\\': 44.6}, {\\'name\\': \\'UCF-101\\', \\'accuracy\\': 72.0}, {\\'name\\': \\'Kinetics-600\\', \\'accuracy\\': 65.2}]}, \\'description\\': \\'X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.\\'}', metadata={})]", "category": "generic"}
{"question_id": 666, "text": " Our client wants to create an image from a text description, and also needs to inpaint a specific part of the image using a mask.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Image Generation', 'api_name': 'runwayml/stable-diffusion-inpainting', 'api_call': 'pipe(prompt=prompt, image=image, mask_image=mask_image)', 'api_arguments': {'prompt': 'Text prompt', 'image': 'PIL image', 'mask_image': 'PIL image (mask)'}, 'python_environment_requirements': {'diffusers': 'from diffusers import StableDiffusionInpaintPipeline'}, 'example_code': {'import_code': 'from diffusers import StableDiffusionInpaintPipeline', 'instantiate_code': 'pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)', 'generate_image_code': 'image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]', 'save_image_code': 'image.save(./yellow_cat_on_park_bench.png)'}, 'performance': {'dataset': {'name': 'LAION-2B (en)', 'accuracy': 'Not optimized for FID scores'}}, 'description': 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.'}\", metadata={})]", "category": "generic"}
{"question_id": 667, "text": " Develop an AI service that generates pictures of people in the context of various professions, such as a doctor, a teacher, or a chef.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 668, "text": " I need the GPT model to generate a higher-resolution image of a small simple yellow bird in a forest.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/bigbird-pegasus-large-bigpatent\\', \\'api_call\\': \"BigBirdPegasusForConditionalGeneration.from_pretrained(\\'google/bigbird-pegasus-large-bigpatent\\')\", \\'api_arguments\\': {\\'attention_type\\': \\'original_full\\', \\'block_size\\': 16, \\'num_random_blocks\\': 2}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-bigpatent)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-bigpatent)\\\\ntext = Replace me by any text you\\'d like.\\\\ninputs = tokenizer(text, return_tensors=\\'pt\\')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\", \\'performance\\': {\\'dataset\\': \\'big_patent\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BigBird, a sparse-attention based transformer, extends Transformer-based models like BERT to much longer sequences. It can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. BigBird has achieved state-of-the-art results on various tasks involving very long sequences such as long documents summarization and question-answering with long contexts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 669, "text": " I would like to recognize the handwritten text in an image in the future with a given URL.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 670, "text": " At a party, people want to know what is in the image displayed in the living room. Could you help them answer some questions related to the image?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 671, "text": " Our clothes store client wants to build a webpage that suggests the right combination or matching colors for various types of fashion items.\\n###Input: {\\\"image_path\\\": \\\"path/to/fashion_item_image.jpg\\\", \\\"question\\\": \\\"What colors should be combined with the color in this fashion item?\\\"}\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': 'Not specified.', 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = prompthero/openjourney\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.'}\", metadata={})]", "category": "generic"}
{"question_id": 672, "text": " Can you guide me on how to extract specific information from a document using a model that can answer questions based on the document layout?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 673, "text": " We are an educational institution experiencing so many forms getting filled out. For our research purpose we want to pick a particular answer \\\"parent's name\\\" given a picture of form filled out.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'temp_vilt_vqa\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'Bingsu/temp_vilt_vqa\\', tokenizer=\\'Bingsu/temp_vilt_vqa\\')\", \\'api_arguments\\': {\\'model\\': \\'Bingsu/temp_vilt_vqa\\', \\'tokenizer\\': \\'Bingsu/temp_vilt_vqa\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A visual question answering model for answering questions related to images using the Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 674, "text": " I am an accountant in a company, and I need to extract information from invoices. Can you please assist me in designing a solution for it?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 675, "text": " Design a system that takes document images and user questions to provide answers from the document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 676, "text": " I have a document containing a college transcript. I'd like to know the student's overall GPA displayed on the transcript.\\n###Input: \\\"your_question\\\": \\\"What is the overall GPA on the transcript?\\\", \\\"your_context\\\": \\\"Multimodal Document Question Answer\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 677, "text": " Our client wants to categorize their product images into different categories. Prepare a solution to classify those images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 678, "text": " I need to find a way to automatically detect objects in images, ideally using a model suited towards smaller images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'hustvl/yolos-tiny\\', \\'api_call\\': \"YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-tiny\\')\", \\'api_arguments\\': {\\'images\\': \\'image\\', \\'return_tensors\\': \\'pt\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained(\\'hustvl/yolos-tiny\\')\\\\nmodel = YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-tiny\\')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\", \\'performance\\': {\\'dataset\\': \\'COCO 2017 validation\\', \\'accuracy\\': \\'28.7 AP\\'}, \\'description\\': \\'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 679, "text": " Our e-sports team needs to detect objects like enemies, teammates, and dropped spike in the Valorant game.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-valorant-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-valorant-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-valorant-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'valorant-object-detection\\', \\'accuracy\\': 0.965}, \\'description\\': \\'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\\'}', metadata={})]", "category": "generic"}
{"question_id": 680, "text": " Our company is responsible for the safety of workers in a warehouse. We need to detect forklifts and persons in the surveillance images to prevent accidents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-forklift-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-forklift-detection\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-forklift-detection\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'forklift-object-detection\\', \\'accuracy\\': 0.846}, \\'description\\': \\'A YOLOv8 model for detecting forklifts and persons in images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 681, "text": " We want to extract the outline and label each element in an image of a factory.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 682, "text": " A client needs to generate handwritten texts using controlnets with diffusion. Implement the controlnet model with canny edges and generate a colorized image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image-to-Image\\', \\'api_name\\': \\'lllyasviel/sd-controlnet-canny\\', \\'api_call\\': \"ControlNetModel.from_pretrained(\\'lllyasviel/sd-controlnet-canny\\', torch_dtype=torch.float16)\", \\'api_arguments\\': {\\'torch_dtype\\': \\'torch.float16\\'}, \\'python_environment_requirements\\': {\\'opencv\\': \\'pip install opencv-contrib-python\\', \\'diffusers\\': \\'pip install diffusers transformers accelerate\\'}, \\'example_code\\': \"import cv2\\\\nfrom PIL import Image\\\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\\\\nimport torch\\\\nimport numpy as np\\\\nfrom diffusers.utils import load_image\\\\nimage = load_image(https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png)\\\\nimage = np.array(image)\\\\nlow_threshold = 100\\\\nhigh_threshold = 200\\\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\\\nimage = image[:, :, None]\\\\nimage = np.concatenate([image, image, image], axis=2)\\\\nimage = Image.fromarray(image)\\\\ncontrolnet = ControlNetModel.from_pretrained(\\\\n lllyasviel/sd-controlnet-canny, torch_dtype=torch.float16\\\\n)\\\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\\\n runwayml/stable-diffusion-v1-5, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\\\\n)\\\\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\npipe.enable_model_cpu_offload()\\\\nimage = pipe(bird, image, num_inference_steps=20).images[0]\\\\nimage.save(\\'images/bird_canny_out.png\\')\", \\'performance\\': {\\'dataset\\': \\'3M edge-image, caption pairs\\', \\'accuracy\\': \\'600 GPU-hours with Nvidia A100 80G\\'}, \\'description\\': \\'ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\\'}', metadata={})]", "category": "generic"}
{"question_id": 683, "text": " For my online art gallery, I need to create an image of a \\\"mystical forest with colorful animals\\\" based on textual description.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 684, "text": " As a photographer, I would like to generate an image of \\\"a tropical beach at sunset\\\" using text-to-image generation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'prompthero/openjourney-v4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'prompthero/openjourney-v4\\')\", \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"generate_image(\\'your text here\\')\", \\'performance\\': {\\'dataset\\': \\'Midjourney v4 images\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 685, "text": " A group of gamers needs to generate unique Minecraft skins for their Minecraft characters.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Diffusers\\', \\'api_name\\': \\'Minecraft-Skin-Diffusion\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\\\\nimage = pipeline().images[0].convert(\\'RGBA\\')\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods.\\'}', metadata={})]", "category": "generic"}
{"question_id": 686, "text": " I have video frames and I am going to build a tool to classify their activities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb\\', \\'api_call\\': \"AutoModelForVideoClassification.from_pretrained(\\'lmazzon70/videomae-base-finetuned-kinetics-finetuned-rwf2000mp4-epochs8-batch8-kb\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': 0.7453}, \\'description\\': \\'This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 687, "text": " Create a smart security system that can detect any suspicious person inside an indoor building captured in a video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8s-csgo-player-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8s-csgo-player-detection\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8s-csgo-player-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'csgo-object-detection\\', \\'accuracy\\': 0.886}, \\'description\\': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels [\\'ct\\', \\'cthead\\', \\'t\\', \\'thead\\'].\"}', metadata={})]", "category": "generic"}
{"question_id": 688, "text": " An art gallery requires a tool for automatically classifying pictures based on their content. Use an appropriate API to predict the content of an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 689, "text": " I have an image and want to know whether it is a picture of a car, a bicycle, or a person. How can I do this?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'nitrosocke/nitro-diffusion', 'api_call': 'pipe(prompt).images[0]', 'api_arguments': ['prompt'], 'python_environment_requirements': ['torch', 'diffusers'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = nitrosocke/nitro-diffusion\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = archer arcane style magical princess with golden hair\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./magical_princess.png)', 'performance': {'dataset': 'Stable Diffusion', 'accuracy': 'N/A'}, 'description': 'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.'}\", metadata={})]", "category": "generic"}
{"question_id": 690, "text": " Implement a program that can identify if an image is showing animals, food, or vehicles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 691, "text": " Can you create a function to identify the presence of animals in photos, like cats, dogs, and birds? The model must also classify the animal type.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 692, "text": " Develop a simple text box for users to test the language detection model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Language Detection\\', \\'api_name\\': \\'papluca/xlm-roberta-base-language-detection\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'papluca/xlm-roberta-base-language-detection\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"language_detection(\\'Hello, how are you?\\')\", \\'performance\\': {\\'dataset\\': \\'Language Identification\\', \\'accuracy\\': 0.996}, \\'description\\': \\'This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset. It is an XLM-RoBERTa transformer model with a classification head on top, and can be used as a language detector for sequence classification tasks. It supports 20 languages including Arabic, Bulgarian, German, Greek, English, Spanish, French, Hindi, Italian, Japanese, Dutch, Polish, Portuguese, Russian, Swahili, Thai, Turkish, Urdu, Vietnamese, and Chinese.\\'}', metadata={})]", "category": "generic"}
{"question_id": 693, "text": " As a support agent for a streaming service, we want to know how the users feel about their content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 694, "text": " A manager of an e-commerce store needs to automatically classify customer reviews based on their emotional content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Emotion Classification\\', \\'api_name\\': \\'j-hartmann/emotion-english-distilroberta-base\\', \\'api_call\\': \\'text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True\\', \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \\'from transformers import pipeline\\\\nclassifier = pipeline(text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True)\\\\nclassifier(I love this!)\\', \\'performance\\': {\\'dataset\\': \\'Balanced subset from 6 diverse datasets\\', \\'accuracy\\': \\'66%\\'}, \\'description\\': \"This model classifies emotions in English text data. It predicts Ekman\\'s 6 basic emotions, plus a neutral class: anger, disgust, fear, joy, neutral, sadness, and surprise. The model is a fine-tuned checkpoint of DistilRoBERTa-base.\"}', metadata={})]", "category": "generic"}
{"question_id": 695, "text": " I have recently authored an online article, and I need to make sure it's safe for a work environment. Can you check if it contains explicit content?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 696, "text": " The company wants to build a product that answers user queries based on provided tables. How to approach this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 697, "text": " A news agency wants to extract names of people, organizations, and locations mentioned in a given article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/pegasus-newsroom\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'google/pegasus-newsroom\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'newsroom\\', \\'accuracy\\': \\'45.98/34.20/42.18\\'}, \\'description\\': \\'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 698, "text": " Our users need to answer questions based on the given data in a table format. We need a model to assist them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 699, "text": " We are a sports analytics company. I need to know in which year Beijing hosted the Olympic Games from a given table of Olympic years and cities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table-based QA\\', \\'api_name\\': \\'neulab/omnitab-large-1024shot\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'neulab/omnitab-large-1024shot\\')\", \\'api_arguments\\': {\\'table\\': \\'pd.DataFrame.from_dict(data)\\', \\'query\\': \\'str\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'pandas\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport pandas as pd\\\\ntokenizer = AutoTokenizer.from_pretrained(neulab/omnitab-large-1024shot)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(neulab/omnitab-large-1024shot)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = In which year did beijing host the Olympic Games?\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'wikitablequestions\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OmniTab is a table-based QA model proposed in OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. neulab/omnitab-large-1024shot (based on BART architecture) is initialized with microsoft/tapex-large and continuously pretrained on natural and synthetic data (SQL2NL model trained in the 1024-shot setting).\\'}', metadata={})]", "category": "generic"}
{"question_id": 700, "text": " We are designing a website that offers a question and answer service for users to provide information based on tables of data. Help us generate answers from given tables.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 701, "text": " I have tables in my app and I want to create a functionality when users can ask questions and the system can pick the correct value from the table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 702, "text": " We need to create a table-based assistant to respond to queries related to a specific dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 703, "text": " We need an AI-based voice assistant for answering questions from a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 704, "text": " A student wants help identifying the main subject of a given text passage. Assist the student with the information they need.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'joeddav/distilbert-base-uncased-go-emotions-student\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'joeddav/distilbert-base-uncased-go-emotions-student\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'tensorflow\\'], \\'example_code\\': \"from transformers import pipeline\\\\nnlp = pipeline(\\'text-classification\\', model=\\'joeddav/distilbert-base-uncased-go-emotions-student\\')\\\\nresult = nlp(\\'I am so happy today!\\')\", \\'performance\\': {\\'dataset\\': \\'go_emotions\\'}, \\'description\\': \\'This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 705, "text": " Review this article on cancer treatment, identify the types of therapies mentioned, and answer any questions I might have.\\n###Input: Cancer therapy has come a long way and has given rise to multiple treatment options. Some common cancer treatments include surgery, chemotherapy, and radiation-based treatments. Immunotherapy, an area gaining more attention, uses the body's immune system to fight cancer cells. Targeted therapies are another option, which selectively block the growth and spread of cancer cells. Finally, hormonal therapy helps regulate hormone levels to slow the growth of hormone-sensitive cancers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'t5_sentence_paraphraser\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', model=\\'ramsrigouthamg/t5_sentence_paraphraser\\') should be rewritten as T5ForConditionalGeneration.from_pretrained(\\'ramsrigouthamg/t5_sentence_paraphraser\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A T5 model for paraphrasing sentences\\'}', metadata={})]", "category": "generic"}
{"question_id": 706, "text": " Determine whether the following statement is true, false, or unrelated: \\\"The earth is flat.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'philschmid/distilbert-onnx\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'philschmid/distilbert-onnx\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbert-onnx\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'onnx\\'], \\'example_code\\': {\\'Compute\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\'question-answering\\', model=\\'philschmid/distilbert-onnx\\')\\\\nqa_pipeline({\\'context\\': \\'This is a context\\', \\'question\\': \\'What is this?\\'})\"}, \\'performance\\': {\\'dataset\\': \\'squad\\', \\'accuracy\\': \\'F1 score: 87.1\\'}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-cased, fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.\\'}', metadata={})]", "category": "generic"}
{"question_id": 707, "text": " Please create an AI that reads deployment meeting schedule requests and classifies them into high, medium, or low priority.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 708, "text": " I'm working on a text completion software that helps users finish their sentences. The model needs to generate the rest of given incomplete sentence.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 709, "text": " Can you please help me communicate to my workers in Germany? My message to them is \\\"Gute Arbeit, weiter so!\\\"\\n###Input: Gute Arbeit, weiter so!\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'German Sentiment Classification', 'api_name': 'oliverguhr/german-sentiment-bert', 'api_call': 'Output: SentimentModel()', 'api_arguments': ['texts'], 'python_environment_requirements': 'pip install germansentiment', 'example_code': ['from germansentiment import SentimentModel', 'model = SentimentModel()', 'texts = [', ' Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,', ' Total awesome!,nicht so schlecht wie erwartet,', ' Der Test verlief positiv.,Sie f\u00e4hrt ein gr\u00fcnes Auto.]', 'result = model.predict_sentiment(texts)', 'print(result)'], 'performance': {'dataset': ['holidaycheck', 'scare', 'filmstarts', 'germeval', 'PotTS', 'emotions', 'sb10k', 'Leipzig Wikipedia Corpus 2016', 'all'], 'accuracy': [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, 'description': 'This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.'}\", metadata={})]", "category": "generic"}
{"question_id": 710, "text": " One of our customers submitted a request in French, and we need to translate it into English.\\n###Input: Bonjour, j'aimerais connare les dimensions de ce produit, s'il vous pla.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 711, "text": " Your manager requested you to automate sending replies to Brazilian wholesalers' emails reporting the availability of goods.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 712, "text": " Translate the following Italian sentence into English: \\\"Mi chiamo Marco e lavoro come ingegnere.\\\"\\n###Input:  Mi chiamo Marco e lavoro come ingegnere.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-it-en\\', \\'api_call\\': \"pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_it_to_en\\', model=\\'Helsinki-NLP/opus-mt-it-en\\')(\\'Ciao mondo!\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newssyscomb2009.it.en\\': 35.3, \\'newstest2009.it.en\\': 34.0, \\'Tatoeba.it.en\\': 70.9}, \\'chr-F\\': {\\'newssyscomb2009.it.en\\': 0.6, \\'newstest2009.it.en\\': 0.594, \\'Tatoeba.it.en\\': 0.808}}}, \\'description\\': \\'A transformer model for Italian to English translation trained on the OPUS dataset. It can be used for translating Italian text to English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 713, "text": " A team working on news articles requests an AI tool to summarize articles automatically so they can save time digesting.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 714, "text": " As a news agency, we need a summary of this long article: \\\"In many cities around the world, urban parks provide an opportunity for residents to escape the hustle and bustle of daily life, allowing them to relax and recharge. Whether walking the dog or playing with the kids, city dwellers can forget about the stresses of work and enjoy the simple pleasures nature provides. London's eight Royal Parks, for example, are a true urban oasis, covering over 5,000 acres of parkland and attracting millions of visitors each year. From Hyde Park to Kensington Gardens, these green spaces offer a range of activities and experiences, including boating on the Serpentine, picnicking in the sun or visiting numerous cultural attractions such as art galleries and historical landmarks.\\\"\\n###Input: \\\"In many cities around the world, urban parks provide an opportunity for residents to escape the hustle and bustle of daily life, allowing them to relax and recharge. Whether walking the dog or playing with the kids, city dwellers can forget about the stresses of work and enjoy the simple pleasures nature provides. London's eight Royal Parks, for example, are a true urban oasis, covering over 5,000 acres of parkland and attracting millions of visitors each year. From Hyde Park to Kensington Gardens, these green spaces offer a range of activities and experiences, including boating on the Serpentine, picnicking in the sun or visiting numerous cultural attractions such as art galleries and historical landmarks.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 715, "text": " Translate a sentence from French to Spanish.\\n###Input: Bonjour, comment  va?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-fr-es\\', \\'api_call\\': \\'Helsinki-NLP/opus-mt-fr-es\\', \\'api_arguments\\': {\\'source_languages\\': \\'fr\\', \\'target_languages\\': \\'es\\'}, \\'python_environment_requirements\\': {\\'PyTorch\\': \\'1.0.0\\', \\'TensorFlow\\': \\'2.0\\', \\'Transformers\\': \\'4.0.0\\'}, \\'example_code\\': \"translation(\\'Bonjour, comment \u00e7a va?\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newssyscomb2009.fr.es\\': 34.3, \\'news-test2008.fr.es\\': 32.5, \\'newstest2009.fr.es\\': 31.6, \\'newstest2010.fr.es\\': 36.5, \\'newstest2011.fr.es\\': 38.3, \\'newstest2012.fr.es\\': 38.1, \\'newstest2013.fr.es\\': 34.0, \\'Tatoeba.fr.es\\': 53.2}, \\'chr-F\\': {\\'newssyscomb2009.fr.es\\': 0.601, \\'news-test2008.fr.es\\': 0.583, \\'newstest2009.fr.es\\': 0.586, \\'newstest2010.fr.es\\': 0.616, \\'newstest2011.fr.es\\': 0.622, \\'newstest2012.fr.es\\': 0.619, \\'newstest2013.fr.es\\': 0.587, \\'Tatoeba.fr.es\\': 0.709}}}, \\'description\\': \\'A French to Spanish translation model trained on the OPUS dataset using the Hugging Face Transformers library. The model is based on the transformer-align architecture and uses normalization and SentencePiece for pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 716, "text": " Give me the summary of the following news article: IBM has announced its plan to acquire Bluestacks, a leading mobile virtualization software, to integrate with its Bigfix endpoint management platform. The goal of this acquisition is to strengthen IBM's mobile device management capabilities and expand the services offered to commercial and government clients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'facebook/bart-large-cnn\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'facebook/bart-large-cnn\\')\", \\'api_arguments\\': [\\'ARTICLE\\', \\'max_length\\', \\'min_length\\', \\'do_sample\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\\\\nARTICLE = ...\\\\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\\', \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 42.949, \\'ROUGE-2\\': 20.815, \\'ROUGE-L\\': 30.619, \\'ROUGE-LSUM\\': 40.038}}, \\'description\\': \\'BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 717, "text": " I have a long academic article due tomorrow, and it's too long. Help me summarize it into a more digestible format.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 718, "text": " I need to find inspiration for a speech I will give at my company's annual meeting. Create a conversation that starts with \\\"Our company is future driven.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 719, "text": " I want a model that can help me generate text to finish my sentences on topics related to artificial intelligence, particularly generative AI.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 720, "text": " Write a Python function that takes in a text and generates 10 lines of different programming languages code snippets.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Program Synthesis', 'api_name': 'Salesforce/codegen-350M-multi', 'api_call': 'model.generate(input_ids, max_length=128)', 'api_arguments': ['text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))', 'performance': {'dataset': 'HumanEval and MTPB', 'accuracy': 'Refer to the paper for accuracy details'}, 'description': 'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.'}\", metadata={})]", "category": "generic"}
{"question_id": 721, "text": " Generate a paragraph describing the benefits of using renewable energy sources.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'t5_sentence_paraphraser\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', model=\\'ramsrigouthamg/t5_sentence_paraphraser\\') should be rewritten as T5ForConditionalGeneration.from_pretrained(\\'ramsrigouthamg/t5_sentence_paraphraser\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A T5 model for paraphrasing sentences\\'}', metadata={})]", "category": "generic"}
{"question_id": 722, "text": " The company plans to build a text-to-text translation system using artificial intelligence, focusing on Korean language conversion. How do we accomplish that?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 723, "text": " Generate a list of questions from a document. The document describes the history of the company Apple Inc.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 724, "text": " I am a high school student and want to train a language model that can complete my sentences. For instance, when I say \\\"I am a\\\", the model should be able to predict the next word.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 725, "text": " I work in a pharmaceutical company and have a text with a missing word related to our field. Please help me find the most suitable word for the context.\\n###Input: \\\"report showed an increase in [MASK] levels after exposure to the new drug.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Masked Language Modeling\\', \\'api_name\\': \\'bert-base-multilingual-cased\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'bert-base-multilingual-cased\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nunmasker = pipeline(\\'fill-mask\\', model=\\'bert-base-multilingual-cased\\')\\\\nunmasker(Hello I\\'m a [MASK] model.)\", \\'performance\\': {\\'dataset\\': \\'wikipedia\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.\\'}', metadata={})]", "category": "generic"}
{"question_id": 726, "text": " I need a software tool to help me complete and format code snippets in various programming languages like Python, Java, and Ruby.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Transformers', 'functionality': 'Code Documentation Generation', 'api_name': 'code_trans_t5_base_code_documentation_generation_python', 'api_call': 'pipeline([tokenized_code])', 'api_arguments': ['tokenized_code'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline\\\\npipeline = SummarizationPipeline(\\\\n model=AutoModelWithLMHead.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python),\\\\n tokenizer=AutoTokenizer.from_pretrained(SEBIS/code_trans_t5_base_code_documentation_generation_python, skip_special_tokens=True),\\\\n device=0\\\\n)\\\\ntokenized_code = def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )\\\\npipeline([tokenized_code])', 'performance': {'dataset': 'CodeSearchNet Corpus python dataset', 'accuracy': '20.26 BLEU score'}, 'description': 'This CodeTrans model is based on the t5-base model and is trained on tokenized python code functions. It can be used to generate descriptions for python functions or be fine-tuned on other python code tasks. The model works best with tokenized python functions but can also be used on unparsed and untokenized python code.'}\", metadata={})]", "category": "generic"}
{"question_id": 727, "text": " We need a powerful AI to predict the missing words in this statement: \\\"Bob went to the ___ to buy some groceries.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-400M-distill\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-400M-distill\\')\", \\'api_arguments\\': \"[\\'message\\']\", \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-400M-distill.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.\\'}', metadata={})]", "category": "generic"}
{"question_id": 728, "text": " I want to find the most similar sentences among a list of sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'sentence-transformers/distiluse-base-multilingual-cased-v1\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 729, "text": " We need to find the similarity between multiple sentences for use in our article summarization tool.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'sentence-transformers/distiluse-base-multilingual-cased-v1\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/distiluse-base-multilingual-cased-v1\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 730, "text": " Our organization is creating a search filter to find relevant tweets about a specific topic on Twitter.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'cardiffnlp/twitter-roberta-base-sentiment-latest\\', \\'api_call\\': \"sentiment_task = pipeline(sentiment-analysis, model=AutoModel.from_pretrained(\\'cardiffnlp/twitter-roberta-base-sentiment-latest\\'), tokenizer=AutoTokenizer.from_pretrained(\\'cardiffnlp/twitter-roberta-base-sentiment-latest\\'))\", \\'api_arguments\\': {\\'model\\': \\'model_path\\', \\'tokenizer\\': \\'model_path\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'numpy\\', \\'scipy\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\\\\nsentiment_task(Covid cases are increasing fast!)\\', \\'performance\\': {\\'dataset\\': \\'tweet_eval\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. The model is suitable for English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 731, "text": " Help us implement a semantic search model to find relevant information in our internal document repository.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/pegasus-newsroom\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'google/pegasus-newsroom\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'newsroom\\', \\'accuracy\\': \\'45.98/34.20/42.18\\'}, \\'description\\': \\'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. The model is trained on both C4 and HugeNews datasets and is designed for summarization tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 732, "text": " You are running an e-commerce, you need to cluster similar customer questions together to enhance the product accuracy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 733, "text": " Our company is creating a virtual assistant, and we want to integrate text-to-speech functionality into our product for better user experience.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'\u3053\u3093\u306b\u3061\u306f\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 734, "text": " I need an AI that would convert written words into spoken words for audiobooks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'\u3053\u3093\u306b\u3061\u306f\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 735, "text": " We are adding a feature for converting speech to text in meetings to create transcripts automatically. How can we implement this?\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 736, "text": " We aim to develop a tool to remove background noise from voice recordings. Let's access the available enhancement model to preprocess the recordings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Enhancement\\', \\'api_name\\': \\'speechbrain/mtl-mimic-voicebank\\', \\'api_call\\': \"WaveformEnhancement.from_hparams(\\'speechbrain/mtl-mimic-voicebank\\', \\'pretrained_models/mtl-mimic-voicebank\\').enhance_file(\\'speechbrain/mtl-mimic-voicebank/example.wav\\')\", \\'api_arguments\\': [\\'source\\', \\'savedir\\'], \\'python_environment_requirements\\': \\'pip install speechbrain\\', \\'example_code\\': \"import torchaudio\\\\nfrom speechbrain.pretrained import WaveformEnhancement\\\\nenhance_model = WaveformEnhancement.from_hparams(\\\\n source=speechbrain/mtl-mimic-voicebank,\\\\n savedir=pretrained_models/mtl-mimic-voicebank,\\\\n)\\\\nenhanced = enhance_model.enhance_file(speechbrain/mtl-mimic-voicebank/example.wav)\\\\ntorchaudio.save(\\'enhanced.wav\\', enhanced.unsqueeze(0).cpu(), 16000)\", \\'performance\\': {\\'dataset\\': \\'Voicebank\\', \\'accuracy\\': {\\'Test PESQ\\': 3.05, \\'Test COVL\\': 3.74, \\'Valid WER\\': 2.89, \\'Test WER\\': 2.8}}, \\'description\\': \\'This repository provides all the necessary tools to perform enhancement and\\\\nrobust ASR training (EN) within\\\\nSpeechBrain. For a better experience we encourage you to learn more about\\\\nSpeechBrain. The model performance is:\\\\nRelease\\\\nTest PESQ\\\\nTest COVL\\\\nValid WER\\\\nTest WER\\\\n22-06-21\\\\n3.05\\\\n3.74\\\\n2.89\\\\n2.80\\\\nWorks with SpeechBrain v0.5.12\\'}', metadata={})]", "category": "generic"}
{"question_id": 737, "text": " I would like to create an application that allows me to translate someone's speech in Hokkien into English audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\', \\'api_call\\': \\'tts_model.get_prediction(tts_sample)\\', \\'api_arguments\\': {\\'unit\\': \\'Text input for the TTS model\\'}, \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\', \\'torchaudio\\'], \\'example_code\\': \"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x[\\'args\\'][\\'data\\']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\", \\'performance\\': {\\'dataset\\': \\'TAT-TTS\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.\\'}', metadata={})]", "category": "generic"}
{"question_id": 738, "text": " We are a live streaming company with a lot of environmental noises. Find a way to classify the noises present.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 739, "text": " We need a way to classify audio tracks from smartphones or smart audio devices to recognize if the audio is coming from a particular speaker or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 740, "text": " Analyze audio recordings from company meetings and identify the segments where multiple people are talking at the same time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 741, "text": " We are developing an application to help wine enthusiasts determine the quality of wines. Use the provided API to classify the input wine samples based on their properties.\\n###Input: {\\\"samples\\\": [{\\\"fixed_acidity\\\": 7.4, \\\"volatile_acidity\\\": 0.7, \\\"citric_acid\\\": 0.0, \\\"residual_sugar\\\": 1.9, \\\"chlorides\\\": 0.076, \\\"free_sulfur_dioxide\\\": 11.0, \\\"total_sulfur_dioxide\\\": 34.0, \\\"density\\\": 0.9978, \\\"pH\\\": 3.51, \\\"sulphates\\\": 0.56, \\\"alcohol\\\": 9.4}, {\\\"fixed_acidity\\\": 7.8, \\\"volatile_acidity\\\": 0.88, \\\"citric_acid\\\": 0.0, \\\"residual_sugar\\\": 2.6, \\\"chlorides\\\": 0.098, \\\"free_sulfur_dioxide\\\": 25.0, \\\"total_sulfur_dioxide\\\": 67.0, \\\"density\\\": 0.9968, \\\"pH\\\": 3.2, \\\"sulphates\\\": 0.68, \\\"alcohol\\\": 9.8}]}\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Tabular Tabular Classification', 'framework': 'Scikit-learn', 'functionality': 'Wine Quality classification', 'api_name': 'osanseviero/wine-quality', 'api_call': 'model.predict(X[:3])', 'api_arguments': 'X', 'python_environment_requirements': ['huggingface_hub', 'joblib', 'pandas'], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])', 'performance': {'dataset': 'winequality-red.csv', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.'}\", metadata={})]", "category": "generic"}
{"question_id": 742, "text": " Estimate the carbon emissions of the factory from the provided data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'38507101578\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'wangdy/autotrain-data-godaddy2\\', \\'accuracy\\': {\\'Loss\\': 0.206, \\'R2\\': 0.997, \\'MSE\\': 0.042, \\'MAE\\': 0.081, \\'RMSLE\\': 0.052}}, \\'description\\': \\'This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\\'}', metadata={})]", "category": "generic"}
{"question_id": 743, "text": " We need to estimate the carbon emissions of a fleet of vehicles based on the dataset containing various features.\\n###Input: data = pd.read_csv('data.csv')\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1860863627\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': {\\'data\\': \\'data.csv\\'}, \\'python_environment_requirements\\': {\\'import json\\': \\'\\', \\'import joblib\\': \\'\\', \\'import pandas as pd\\': \\'\\'}, \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-dragino-7-7-max_495m\\', \\'accuracy\\': {\\'Loss\\': 72.73, \\'R2\\': 0.386, \\'MSE\\': 5289.6, \\'MAE\\': 60.23, \\'RMSLE\\': 0.436}}, \\'description\\': \\'A tabular regression model for predicting carbon emissions using the Joblib framework. Trained on the pcoloc/autotrain-data-dragino-7-7-max_495m dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 744, "text": " I am building a game and I would like to train an AI character with reinforcement learning in the CartPole environment using PPO.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Stable-Baselines3\\', \\'functionality\\': \\'LunarLander-v2\\', \\'api_name\\': \\'araffin/ppo-LunarLander-v2\\', \\'api_call\\': \"Output: model = PPO.load_from_hub(\\'araffin/ppo-LunarLander-v2\\', \\'ppo-LunarLander-v2.zip\\')\", \\'api_arguments\\': {\\'checkpoint\\': \\'araffin/ppo-LunarLander-v2.zip\\'}, \\'python_environment_requirements\\': [\\'huggingface_sb3\\', \\'stable_baselines3\\'], \\'example_code\\': {\\'load_model\\': \\'from huggingface_sb3 import load_from_hub\\\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\\\nmodel = PPO.load(checkpoint)\\', \\'evaluate\\': \\'from stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})\\'}, \\'performance\\': {\\'dataset\\': \\'LunarLander-v2\\', \\'accuracy\\': \\'283.49 +/- 13.74\\'}, \\'description\\': \\'This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 745, "text": " I'm building a game based on Lunar Lander. I want to use your model for testing the game performance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Stable-Baselines3\\', \\'functionality\\': \\'LunarLander-v2\\', \\'api_name\\': \\'araffin/ppo-LunarLander-v2\\', \\'api_call\\': \"Output: model = PPO.load_from_hub(\\'araffin/ppo-LunarLander-v2\\', \\'ppo-LunarLander-v2.zip\\')\", \\'api_arguments\\': {\\'checkpoint\\': \\'araffin/ppo-LunarLander-v2.zip\\'}, \\'python_environment_requirements\\': [\\'huggingface_sb3\\', \\'stable_baselines3\\'], \\'example_code\\': {\\'load_model\\': \\'from huggingface_sb3 import load_from_hub\\\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\\\nmodel = PPO.load(checkpoint)\\', \\'evaluate\\': \\'from stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})\\'}, \\'performance\\': {\\'dataset\\': \\'LunarLander-v2\\', \\'accuracy\\': \\'283.49 +/- 13.74\\'}, \\'description\\': \\'This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 746, "text": " We need a program that will be able to learn how to control a robotic arm for pick and place operations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'6D grasping\\', \\'api_name\\': \\'camusean/grasp_diffusion\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'camusean/grasp_diffusion\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.\\'}', metadata={})]", "category": "generic"}
{"question_id": 747, "text": " An AI company is developing a soccer game. They want to train AI for playing Soccer Twos using your model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 748, "text": " I'm looking for a useful agent for Ant-v3 environment to use the agent, train it, evaluate it, and add it to my hub.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Stable-Baselines3\\', \\'functionality\\': \\'LunarLander-v2\\', \\'api_name\\': \\'araffin/ppo-LunarLander-v2\\', \\'api_call\\': \"Output: model = PPO.load_from_hub(\\'araffin/ppo-LunarLander-v2\\', \\'ppo-LunarLander-v2.zip\\')\", \\'api_arguments\\': {\\'checkpoint\\': \\'araffin/ppo-LunarLander-v2.zip\\'}, \\'python_environment_requirements\\': [\\'huggingface_sb3\\', \\'stable_baselines3\\'], \\'example_code\\': {\\'load_model\\': \\'from huggingface_sb3 import load_from_hub\\\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\\\nmodel = PPO.load(checkpoint)\\', \\'evaluate\\': \\'from stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})\\'}, \\'performance\\': {\\'dataset\\': \\'LunarLander-v2\\', \\'accuracy\\': \\'283.49 +/- 13.74\\'}, \\'description\\': \\'This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 749, "text": " Our company is developing a soccer training application. We need an agent that can learn and play soccer effectively.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 750, "text": " Please find a method to identify the similarity of English phrases using a feature extraction model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sentence-transformers/distilbert-base-nli-mean-tokens\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/distilbert-base-nli-mean-tokens\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': \\'pip install -U sentence-transformers\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/distilbert-base-nli-mean-tokens\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 751, "text": " Can you please generate an image of a mysterious knight with a shining armor in a gothic artstyle?\\n###Input: \\\"mysterious knight with shining armor in gothic artstyle\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 752, "text": " We need to extract the text from the image URL for an OCR application.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'kha-white/manga-ocr-base\\', \\'api_call\\': \"pipeline(\\'ocr\\', model=\\'kha-white/manga-ocr-base\\').model\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'manga109s\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 753, "text": " Imagine you are designing an app that would allow the user to take a photo and ask questions about it. They are going to take a picture of their living room and have questions about the objects in the room.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 754, "text": " Design an interface to provide the user with the best image captions and interactive responses to questions considering a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 755, "text": " We want to build a system that takes input as a rendered image and generates nice looking textual descriptions for the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 756, "text": " Let's create a Python function that will take an image URL as an input and returns the recognized text in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'prompthero/openjourney-v4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'prompthero/openjourney-v4\\')\", \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"generate_image(\\'your text here\\')\", \\'performance\\': {\\'dataset\\': \\'Midjourney v4 images\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 757, "text": " Design a tool for hearing-impaired people where they can visualize spoken words.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Enhancement\\', \\'api_name\\': \\'speechbrain/mtl-mimic-voicebank\\', \\'api_call\\': \"WaveformEnhancement.from_hparams(\\'speechbrain/mtl-mimic-voicebank\\', \\'pretrained_models/mtl-mimic-voicebank\\').enhance_file(\\'speechbrain/mtl-mimic-voicebank/example.wav\\')\", \\'api_arguments\\': [\\'source\\', \\'savedir\\'], \\'python_environment_requirements\\': \\'pip install speechbrain\\', \\'example_code\\': \"import torchaudio\\\\nfrom speechbrain.pretrained import WaveformEnhancement\\\\nenhance_model = WaveformEnhancement.from_hparams(\\\\n source=speechbrain/mtl-mimic-voicebank,\\\\n savedir=pretrained_models/mtl-mimic-voicebank,\\\\n)\\\\nenhanced = enhance_model.enhance_file(speechbrain/mtl-mimic-voicebank/example.wav)\\\\ntorchaudio.save(\\'enhanced.wav\\', enhanced.unsqueeze(0).cpu(), 16000)\", \\'performance\\': {\\'dataset\\': \\'Voicebank\\', \\'accuracy\\': {\\'Test PESQ\\': 3.05, \\'Test COVL\\': 3.74, \\'Valid WER\\': 2.89, \\'Test WER\\': 2.8}}, \\'description\\': \\'This repository provides all the necessary tools to perform enhancement and\\\\nrobust ASR training (EN) within\\\\nSpeechBrain. For a better experience we encourage you to learn more about\\\\nSpeechBrain. The model performance is:\\\\nRelease\\\\nTest PESQ\\\\nTest COVL\\\\nValid WER\\\\nTest WER\\\\n22-06-21\\\\n3.05\\\\n3.74\\\\n2.89\\\\n2.80\\\\nWorks with SpeechBrain v0.5.12\\'}', metadata={})]", "category": "generic"}
{"question_id": 758, "text": " I have a small company making movies. I want to generate different movie scenes from text input describing the scenes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video Generation\\', \\'api_name\\': \\'redshift-man-skiing\\', \\'api_call\\': \\'pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5)\\', \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'video_length\\': \\'int\\', \\'height\\': \\'int\\', \\'width\\': \\'int\\', \\'num_inference_steps\\': \\'int\\', \\'guidance_scale\\': \\'float\\'}, \\'python_environment_requirements\\': [\\'torch\\', \\'tuneavideo\\'], \\'example_code\\': \"from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\\\\nfrom tuneavideo.models.unet import UNet3DConditionModel\\\\nfrom tuneavideo.util import save_videos_grid\\\\nimport torch\\\\npretrained_model_path = nitrosocke/redshift-diffusion\\\\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\\\\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder=\\'unet\\', torch_dtype=torch.float16).to(\\'cuda\\')\\\\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\\\\npipe.enable_xformers_memory_efficient_attention()\\\\nprompt = (redshift style) spider man is skiing\\\\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\\\\nsave_videos_grid(video, f./{prompt}.gif)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as \\'a man is skiing\\' or \\'(redshift style) spider man is skiing\\'.\"}', metadata={})]", "category": "generic"}
{"question_id": 759, "text": " I want to design a tool that will take an image and a question as inputs and provide an answer to the question based on the contents of the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 760, "text": " Develop software that answers questions based on an input image for an educational app.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'blip-vqa-base\\', \\'api_call\\': \"Output: BlipForQuestionAnswering.from_pretrained(\\'Salesforce/blip-vqa-base\\').generate(**inputs)\", \\'api_arguments\\': {\\'raw_image\\': \\'Image\\', \\'question\\': \\'String\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'BlipProcessor, BlipForQuestionAnswering\\', \\'PIL\\': \\'Image\\', \\'requests\\': \\'requests\\'}, \\'example_code\\': \"import requests\\\\nfrom PIL import Image\\\\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\\\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-base)\\\\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-base)\\\\nimg_url = \\'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\'\\\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\\'RGB\\')\\\\nquestion = how many dogs are in the picture?\\\\ninputs = processor(raw_image, question, return_tensors=pt)\\\\nout = model.generate(**inputs)\\\\nprint(processor.decode(out[0], skip_special_tokens=True))\", \\'performance\\': {\\'dataset\\': \\'VQA\\', \\'accuracy\\': \\'+1.6% in VQA score\\'}, \\'description\\': \\'BLIP is a Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is trained on visual question answering with a base architecture (using ViT base backbone).\\'}', metadata={})]", "category": "generic"}
{"question_id": 761, "text": " Using a Visual Question Answering model, provide me with details on where I can find the price of a product on a webpage.\\n###Input: <<<image>>>: image_path, <<<question>>>: \\\"Where can I find the price of a product on this webpage?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\')\", \\'api_arguments\\': {\\'model\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'tokenizer\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': \\'vqa(image_path, question)\\', \\'performance\\': {\\'dataset\\': \\'JosephusCheung/GuanacoVQADataset\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 762, "text": " I want to develop an AI-powered program that can scan images of documents with questions and automatically answer them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 763, "text": " The HR department receives a large number of documents every day. They need to find answers to specific questions from these documents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 764, "text": " We are working on a drug discovery application, and we need to classify molecules based on their molecular structure.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 765, "text": " While creating virtual reality games, I need to estimate the depth of objects in 3D space.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 766, "text": " Add a feature to our drone that allows it to estimate the depth of different elements in its field of view.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 767, "text": " I need an AI system that can predict the depth map of a single color image. Can you suggest any models?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'glpn-nyu-finetuned-diode-221215-092352\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sayakpaul/glpn-nyu-finetuned-diode-221215-092352\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'huggingface_transformers\\': \\'4.13.0\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'DIODE\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A depth estimation model fine-tuned on the DIODE dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 768, "text": " I want to analyze the depth of objects in an image using AI.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Monocular Depth Estimation\\', \\'api_name\\': \\'Intel/dpt-large\\', \\'api_call\\': \"DPTForDepthEstimation.from_pretrained(\\'Intel/dpt-large\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'Intel/dpt-large\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import DPTImageProcessor, DPTForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\\\\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'MIX 6\\', \\'accuracy\\': \\'10.82\\'}, \\'description\\': \\'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 769, "text": " We are building a robot to follow a person while maintaining a specific distance. We need to estimate the depth of each object in the scene.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 770, "text": " I am building an app that can classify the type of animal in a given image. Can you help me configure the code for this?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 771, "text": " Detect the severity of Diabetic Retinopathy in the eye images of patients.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\', \\'api_call\\': \"pipeline(\\'image-classification\\', \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\').model\", \\'api_arguments\\': {\\'model_name\\': \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.28.1\\', \\'pytorch\\': \\'2.0.0+cu118\\', \\'datasets\\': \\'2.11.0\\', \\'tokenizers\\': \\'0.13.3\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nimage_classifier = pipeline(\\'image-classification\\', \\'martinezomg/vit-base-patch16-224-diabetic-retinopathy\\')\\\\nresult = image_classifier(\\'path/to/image.jpg\\')\", \\'performance\\': {\\'dataset\\': \\'None\\', \\'accuracy\\': 0.7744}, \\'description\\': \\'This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.\\'}', metadata={})]", "category": "generic"}
{"question_id": 772, "text": " We want to create a machine learning model that can identify objects in photographs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 773, "text": " I need to create a tool that identifies objects in an image to help blind people see the world more effectively.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-base-patch16\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-base-patch16\\')\", \\'api_arguments\\': [\\'texts\\', \\'images\\'], \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'torch\\', \\'transformers\\'], \\'example_code\\': \\'processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\\\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = [[a photo of a cat, a photo of a dog]]\\\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\\'}', metadata={})]", "category": "generic"}
{"question_id": 774, "text": " I'm developing a mobile app that classifies whether an image contains a cat or not. I need to use an API that can provide excellent accuracy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 775, "text": " There is a security breach in the company's server room. We must detect any unauthorized personnel in the CCTV footage.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k\\', \\'api_call\\': \"clip.load(\\'timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k\\')\", \\'api_arguments\\': \\'image, class_names\\', \\'python_environment_requirements\\': \\'huggingface_hub, openai, transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This model is a zero-shot image classification model based on OpenCLIP. It can be used for classifying images into various categories without any additional training.\\'}', metadata={})]", "category": "generic"}
{"question_id": 776, "text": " Our company works with processing various types of documents. We need a solution to detect tables in documents, including bordered and borderless ones.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Computer Vision Object Detection', 'framework': 'Hugging Face Transformers', 'functionality': 'Detect Bordered and Borderless tables in documents', 'api_name': 'TahaDouaji/detr-doc-table-detection', 'api_call': 'DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)', 'api_arguments': ['images', 'return_tensors', 'threshold'], 'python_environment_requirements': ['transformers', 'torch', 'PIL', 'requests'], 'example_code': 'from transformers import DetrImageProcessor, DetrForObjectDetection\\\\nimport torch\\\\nfrom PIL import Image\\\\nimport requests\\\\nimage = Image.open(IMAGE_PATH)\\\\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\\\\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\ntarget_sizes = torch.tensor([image.size[::-1]])\\\\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\\\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\\\\n box = [round(i, 2) for i in box.tolist()]\\\\n print(\\\\n fDetected {model.config.id2label[label.item()]} with confidence \\\\n f{round(score.item(), 3)} at location {box}\\\\n )', 'performance': {'dataset': 'ICDAR2019 Table Dataset', 'accuracy': 'Not provided'}, 'description': 'detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.'}\", metadata={})]", "category": "generic"}
{"question_id": 777, "text": " Monitor the work of your warehouse employees by detecting forklifts and persons on your security camera images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-forklift-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-forklift-detection\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': [\\'from ultralyticsplus import YOLO, render_result\\', \"model = YOLO(\\'keremberke/yolov8m-forklift-detection\\')\", \"model.overrides[\\'conf\\'] = 0.25\", \"model.overrides[\\'iou\\'] = 0.45\", \"model.overrides[\\'agnostic_nms\\'] = False\", \"model.overrides[\\'max_det\\'] = 1000\", \"image = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\", \\'results = model.predict(image)\\', \\'print(results[0].boxes)\\', \\'render = render_result(model=model, image=image, result=results[0])\\', \\'render.show()\\'], \\'performance\\': {\\'dataset\\': \\'forklift-object-detection\\', \\'accuracy\\': 0.846}, \\'description\\': \\'A YOLOv8 model for detecting forklifts and persons in images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 778, "text": " There is a need to segment specific objects in a landscape image for better visual processing.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-pothole-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-pothole-segmentation\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local image path\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-pothole-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pothole-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.858, \\'mAP@0.5(mask)\\': 0.895}}, \\'description\\': \\'A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 779, "text": " We are developing an autonomous car and we need to segment the road and surrounding environment in real-time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8n-pothole-segmentation\\', \\'api_call\\': \"\\'model.predict(image)\\'\", \\'api_arguments\\': {\\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': {\\'ultralyticsplus\\': \\'0.0.23\\', \\'ultralytics\\': \\'8.0.21\\'}, \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-pothole-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pothole-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.995, \\'mAP@0.5(mask)\\': 0.995}}, \\'description\\': \\'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 780, "text": " Generate a high-quality image of a human face using a Denoising Diffusion Probabilistic Model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"Output: DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')()\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 781, "text": " A e-commerce platform wants to generate images of butterflies to boost sales of butterfly related products. Please provide suitable recommendations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ntrant7/sd-class-butterflies-32\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'Not specified\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies.\\'}', metadata={})]", "category": "generic"}
{"question_id": 782, "text": " I want to classify an online video about traveling without any label and find out if it's about nature, food, or city life.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Geolocalization\\', \\'api_name\\': \\'geolocal/StreetCLIP\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'geolocal/StreetCLIP\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'geolocal/StreetCLIP\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(geolocal/StreetCLIP)\\\\nprocessor = CLIPProcessor.from_pretrained(geolocal/StreetCLIP)\\\\nurl = https://huggingface.co/geolocal/StreetCLIP/resolve/main/sanfrancisco.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nchoices = [San Jose, San Diego, Los Angeles, Las Vegas, San Francisco]\\\\ninputs = processor(text=choices, images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'IM2GPS\\', \\'accuracy\\': {\\'25km\\': 28.3, \\'200km\\': 45.1, \\'750km\\': 74.7, \\'2500km\\': 88.2}}, {\\'name\\': \\'IM2GPS3K\\', \\'accuracy\\': {\\'25km\\': 22.4, \\'200km\\': 37.4, \\'750km\\': 61.3, \\'2500km\\': 80.4}}]}, \\'description\\': \\'StreetCLIP is a robust foundation model for open-domain image geolocalization and other geographic and climate-related tasks. Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, outperforming supervised models trained on millions of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 783, "text": " Create a program that can classify a given video file into various categories like sports, music, etc.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 784, "text": " Create a model to classify short videos based on their content for our video-sharing application.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-short-finetuned-kinetics\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\", \\'api_arguments\\': [\\'video\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-short-finetuned-kinetics\\')\\\\ninputs = processor(video, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(\\'Predicted class:\\', model.config.id2label[predicted_class_idx])\", \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': {\\'top-1\\': 79.4, \\'top-5\\': 94.1}}, \\'description\\': \\'VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 785, "text": " I want to build an app that can recognize images and classify them into categories I choose. The content of the image is like a tool for work, a handheld device, or an instrument for entertainment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 786, "text": " I am an art collector and want to create a recommendation system for categorizing and recognizing diverse art styles. I want to distinguish between different styles, such as impressionism, cubism, and abstract art.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'abhishek/autotrain-iris-knn\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'scikit-learn/iris\\', \\'accuracy\\': 0.9}, \\'description\\': \\'A K-Nearest Neighbors (KNN) model trained on the Iris dataset for multi-class classification. The model is trained using AutoTrain and has an accuracy of 0.9.\\'}', metadata={})]", "category": "generic"}
{"question_id": 787, "text": " I have a photo archive that I need to label with different collectibles, such as stamps, coins, and antiques. Automate the process of identifying what's in each picture using a zero-shot image classification model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shor Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 788, "text": " Design a system to identify the category of the given image from a collection of categories including animals, vehicles, and nature.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 789, "text": " I am building a food recommendation service from images. For that, I would like you to determine what the dish is supposed to be based on the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 790, "text": " We need to analyze multiple financial news headlines to conclude the sentiment of each headline.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 791, "text": " Could you please analyze the sentiment of the provided customer review text?\\n###Input: \\\"El producto llegen perfectas condiciones, pero tardmucho en llegar.\\\" \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'finiteautomata/beto-sentiment-analysis\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'finiteautomata/beto-sentiment-analysis\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'Hugging Face Transformers library\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TASS 2020 corpus\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 792, "text": " I am building an app to read tweets and help users understand the general sentiment of those tweets. We want to know if tweets are positive, negative, or neutral.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'cardiffnlp/twitter-roberta-base-sentiment-latest\\', \\'api_call\\': \"sentiment_task = pipeline(sentiment-analysis, model=AutoModel.from_pretrained(\\'cardiffnlp/twitter-roberta-base-sentiment-latest\\'), tokenizer=AutoTokenizer.from_pretrained(\\'cardiffnlp/twitter-roberta-base-sentiment-latest\\'))\", \\'api_arguments\\': {\\'model\\': \\'model_path\\', \\'tokenizer\\': \\'model_path\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'numpy\\', \\'scipy\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nsentiment_task = pipeline(sentiment-analysis, model=model_path, tokenizer=model_path)\\\\nsentiment_task(Covid cases are increasing fast!)\\', \\'performance\\': {\\'dataset\\': \\'tweet_eval\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. The model is suitable for English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 793, "text": " Categorize this input comment as toxic or non-toxic.\\n###Input: You're an absolute idiot and I hate everything about you.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'martin-ha/toxic-comment-model\\', \\'api_call\\': \"pipeline(model=AutoModelForSequenceClassification.from_pretrained(\\'martin-ha/toxic-comment-model\\'), tokenizer=AutoTokenizer.from_pretrained(\\'martin-ha/toxic-comment-model\\')).__call__(\\'This is a test text.\\')\", \\'api_arguments\\': {\\'model_path\\': \\'martin-ha/toxic-comment-model\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\\\nmodel_path = martin-ha/toxic-comment-model\\\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\\\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\\\nprint(pipeline(\\'This is a test text.\\'))\", \\'performance\\': {\\'dataset\\': \\'held-out test set\\', \\'accuracy\\': 0.94, \\'f1-score\\': 0.59}, \\'description\\': \\'This model is a fine-tuned version of the DistilBERT model to classify toxic comments.\\'}', metadata={})]", "category": "generic"}
{"question_id": 794, "text": " The company received emails from customers. We need to classify them into questions or statements.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'shahrukhx01/question-vs-statement-classifier\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'shahrukhx01/question-vs-statement-classifier\\')\", \\'api_arguments\\': {\\'tokenizer\\': \\'AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\'}, \\'example_code\\': \\'tokenizer = AutoTokenizer.from_pretrained(shahrukhx01/question-vs-statement-classifier)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(shahrukhx01/question-vs-statement-classifier)\\', \\'performance\\': {\\'dataset\\': \\'Haystack\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Trained to add the feature for classifying queries between Question Query vs Statement Query using classification in Haystack\\'}', metadata={})]", "category": "generic"}
{"question_id": 795, "text": " As a company focusing on cybersecurity, we would like to integrate a model into our email filter to detect gibberish text for better scanning.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'madhurjindal/autonlp-Gibberish-Detector-492513457\\', \\'api_call\\': \"Output: AutoModelForSequenceClassification.from_pretrained(\\'madhurjindal/autonlp-Gibberish-Detector-492513457\\', use_auth_token=True)\", \\'api_arguments\\': {\\'inputs\\': \\'I love AutoNLP\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoModelForSequenceClassification\\', \\'AutoTokenizer\\': \\'from_pretrained\\'}, \\'example_code\\': \\'from transformers import AutoModelForSequenceClassification, AutoTokenizer\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(madhurjindal/autonlp-Gibberish-Detector-492513457, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoNLP, return_tensors=pt)\\\\noutputs = model(**inputs)\\', \\'performance\\': {\\'dataset\\': \\'madhurjindal/autonlp-data-Gibberish-Detector\\', \\'accuracy\\': 0.9735624586913417}, \\'description\\': \\'A multi-class text classification model for detecting gibberish text. Trained using AutoNLP and DistilBERT.\\'}', metadata={})]", "category": "generic"}
{"question_id": 796, "text": " A content moderation company requires an efficient way to detect inappropriate text in the chat rooms. They need help.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 797, "text": " Clients ask us to identify persons, businesses and regions in a text to provide data-driven analytics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'joeddav/distilbert-base-uncased-go-emotions-student\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'joeddav/distilbert-base-uncased-go-emotions-student\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\', \\'tensorflow\\'], \\'example_code\\': \"from transformers import pipeline\\\\nnlp = pipeline(\\'text-classification\\', model=\\'joeddav/distilbert-base-uncased-go-emotions-student\\')\\\\nresult = nlp(\\'I am so happy today!\\')\", \\'performance\\': {\\'dataset\\': \\'go_emotions\\'}, \\'description\\': \\'This model is distilled from the zero-shot classification pipeline on the unlabeled GoEmotions dataset. It is primarily intended as a demo of how an expensive NLI-based zero-shot model can be distilled to a more efficient student, allowing a classifier to be trained with only unlabeled data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 798, "text": " As part of our customer support platform, we need to develop a service that can detect companies mentioned in users' questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 799, "text": " Our company is planning to analyze customer feedback from users in China. We want a system to perform part-of-speech tagging on the Chinese text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Part-of-speech tagging\\', \\'api_name\\': \\'ckiplab/bert-base-chinese-pos\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'ckiplab/bert-base-chinese-pos\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"BertTokenizerFast.from_pretrained(\\'bert-base-chinese\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import (\\\\n  BertTokenizerFast,\\\\n  AutoModel,\\\\n)\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\'bert-base-chinese\\')\\\\nmodel = AutoModel.from_pretrained(\\'ckiplab/bert-base-chinese-pos\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\\'}', metadata={})]", "category": "generic"}
{"question_id": 800, "text": " We are developing a grammar-checking application. We need to identify the part-of-speech tags of input text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Flair\\', \\'functionality\\': \\'Part-of-Speech Tagging\\', \\'api_name\\': \\'flair/upos-english\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/upos-english\\')\", \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': \\'pip install flair\\', \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(\\'flair/upos-english\\')\\\\nsentence = Sentence(\\'I love Berlin.\\')\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans(\\'pos\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'ontonotes\\', \\'accuracy\\': 98.6}, \\'description\\': \\'This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 801, "text": " Identify a solution for classifying user's question and their corresponding data in a given table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 802, "text": " Our research team needs assistance in answering questions from a given table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 803, "text": " Our task is to quickly analyze various datasets for an upcoming business presentation. We need answers for several questions related to the dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 804, "text": " We are a translation agency focusing on various languages. We have a document containing Korean content, and we need to answer questions based on the context.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'monologg/koelectra-small-v2-distilled-korquad-384\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'monologg/koelectra-small-v2-distilled-korquad-384\\')\", \\'api_arguments\\': {\\'model\\': \\'monologg/koelectra-small-v2-distilled-korquad-384\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"nlp(question=\\'your_question\\', context=\\'your_context\\')\", \\'performance\\': {\\'dataset\\': \\'KorQuAD\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A Korean Question Answering model based on Electra and trained on the KorQuAD dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 805, "text": " Create a legal-question answering system to extract information from contracts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 806, "text": " I am an AI scientist checking COVID-19 impacts on different industries. I have some documents that I want to analyze whether the text mentions the impact of the virus on a specific industry.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 807, "text": " I am a Spanish editor, I need to figure out the best category for a news article for subsequent publishing and promotion.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'Recognai/bert-base-spanish-wwm-cased-xnli', 'api_call': 'Recognai/bert-base-spanish-wwm-cased-xnli.from_pretrained()', 'api_arguments': ['sequence', 'candidate_labels', 'hypothesis_template'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Recognai/bert-base-spanish-wwm-cased-xnli)\\\\nclassifier(\\\\nEl autor se perfila, a los 50 a\u00f1os de su muerte, como uno de los grandes de su siglo,\\\\ncandidate_labels=[cultura, sociedad, economia, salud, deportes],\\\\nhypothesis_template=Este ejemplo es {}. \\\\n)', 'performance': {'dataset': 'XNLI-es', 'accuracy': '79.9%'}, 'description': 'This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training.'}\", metadata={})]", "category": "generic"}
{"question_id": 808, "text": " Our team is working on a new app to encourage people to recycle more. We would like to use AI to classify user submissions in categories like : 'plastic', 'paper', 'glass', 'metal', 'electronics'.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'1702259725\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': {\\'accuracy\\': 0.827}}, \\'description\\': \\'Multi-class Classification Model for Carbon Emissions\\'}', metadata={})]", "category": "generic"}
{"question_id": 809, "text": " Our customer is an international law firm, we need to translate contracts from English to French. Make sure the translations are accurate and context-based.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-fr-en\\', \\'api_call\\': \"pipeline(\\'translation_fr_to_en\\', model=\\'Helsinki-NLP/opus-mt-fr-en\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"translation_pipeline(\\'Bonjour, comment \u00e7a va?\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.fr.en\\': 33.1, \\'newsdiscusstest2015-enfr.fr.en\\': 38.7, \\'newssyscomb2009.fr.en\\': 30.3, \\'news-test2008.fr.en\\': 26.2, \\'newstest2009.fr.en\\': 30.2, \\'newstest2010.fr.en\\': 32.2, \\'newstest2011.fr.en\\': 33.0, \\'newstest2012.fr.en\\': 32.8, \\'newstest2013.fr.en\\': 33.9, \\'newstest2014-fren.fr.en\\': 37.8, \\'Tatoeba.fr.en\\': 57.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 810, "text": " I have a customer inquiry asking about a refund policy. I need the answer in my native language, French. Can you help me get the details from the company policy?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 811, "text": " Our Team needs a group translator to convert all project documents from listed Romance languages (French, Spanish, Portuguese, Italian, Romanian) to English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-en-ROMANCE\\', \\'api_call\\': \"pipeline(\\'translation_en_to_ROMANCE\\', model=\\'Helsinki-NLP/opus-mt-en-ROMANCE\\')\", \\'api_arguments\\': \\'source languages, target languages\\', \\'python_environment_requirements\\': \\'transformers, torch\\', \\'example_code\\': \"from transformers import pipeline\\\\ntranslation = pipeline(\\'translation_en_to_ROMANCE\\', model=\\'Helsinki-NLP/opus-mt-en-ROMANCE\\')\\\\ntranslated_text = translation(\\'Hello, how are you?\\', tgt_lang=\\'es\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': 50.1, \\'chr-F\\': 0.693}}, \\'description\\': \\'A translation model trained on the OPUS dataset that supports translation between English and various Romance languages. It uses a transformer architecture and requires a sentence initial language token in the form of >>id<< (id = valid target language ID).\\'}', metadata={})]", "category": "generic"}
{"question_id": 812, "text": " As a news organization, we require a system to summarize long articles for our readers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 813, "text": " You are helping a journalist to generate a concise news summary to be posted on social media.\\n###Input: <<<ARTICLE>>>: \\\"Tech giant Apple Inc. announced today that the all-new MacBook Pro will feature the company's latest in-house ARM-based M1 chip, dramatically improving performance and battery life. The new MacBook Pro is expected to be up to 3.5 times faster in processing power, and up to six times faster in graphics, compared to its predecessors powered by Intel processors. Apple's shift from Intel processors to its own custom-designed chips marks a significant change in the company's approach to hardware manufacturing. The move is set to make production more efficient, cost-effective, and environmentally friendly. However, the transition is not without its challenges, as software developers will need to adapt their applications to run smoothly on the new hardware. Apple plans to complete the two-year transition period by the end of 2022, with more products featuring the M1 chip expected to be unveiled in the coming months.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'facebook/bart-large-cnn\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'facebook/bart-large-cnn\\')\", \\'api_arguments\\': [\\'ARTICLE\\', \\'max_length\\', \\'min_length\\', \\'do_sample\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\\\\nARTICLE = ...\\\\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\\', \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 42.949, \\'ROUGE-2\\': 20.815, \\'ROUGE-L\\': 30.619, \\'ROUGE-LSUM\\': 40.038}}, \\'description\\': \\'BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 814, "text": " We are a news agency looking to automatically summarize lengthy news articles while maintaining the original meaning.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 815, "text": " Suppose we are building a friendly chatbot to keep clients engaged while waiting for our service, and we want to generate suggestions for their next vacations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 816, "text": " The company wants an AI bot that replies to its employees to answer their questions without human intervention.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 817, "text": " Help me build a solution that recognizes historical figures, and retrieves interesting details on their achievements. Write a python code as an example.\\n###Input: instruction=\\\"Who was Albert Einstein?\\\", knowledge=\\\"Albert Einstein was a theoretical physicist who developed the theory of relativity. His work is also known for its influence on the philosophy of science. He received the Nobel Prize in Physics in 1921 for his services to theoretical physics, and especially for his discovery of the photoelectric effect, a pivotal step in the development of quantum theory.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 818, "text": " Let's make some interesting stories for young kids.Get the beginning lines of 5 stories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 819, "text": " The author can't continue their story. Help them with writing the next few lines.\\n###Input: Hello, I'm am conscious and\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 820, "text": " We are developing an AI model to communicate with humans, give me an example based on the prompt \\\"What is your purpose?\\\"\\n###Input: What is your purpose?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 821, "text": " My son has a difficult time translating English to German, and he needs help with his homework. Develop a translator which can assist.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'Sahajtomar/German_Zeroshot', 'api_call': 'classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'api_arguments': {'sequence': 'string', 'candidate_labels': 'list of strings', 'hypothesis_template': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Sahajtomar/German_Zeroshot)\\\\nsequence = Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie\\\\ncandidate_labels = [Verbrechen,Trag\u00f6die,Stehlen]\\\\nhypothesis_template = In deisem geht es um {}. ## Since monolingual model,its sensitive to hypothesis template. This can be experimented\\\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'performance': {'dataset': {'XNLI DEV (german)': {'accuracy': 85.5}, 'XNLI TEST (german)': {'accuracy': 83.6}}}, 'description': 'This model has GBERT Large as base model and fine-tuned it on xnli de dataset. The default hypothesis template is in English: This text is {}. While using this model, change it to In deisem geht es um {}. or something different. While inferencing through huggingface api may give poor results as it uses by default english template. Since model is monolingual and not multilingual, hypothesis template needs to be changed accordingly.'}\", metadata={})]", "category": "generic"}
{"question_id": 822, "text": " We are supporting French language learning. The students should fill the blank in the sentence: \\\"Le camembert est ____\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'camembert-base\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'camembert-base\\', tokenizer=\\'camembert-base\\')\", \\'api_arguments\\': [\\'model\\', \\'tokenizer\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline; camembert_fill_mask = pipeline(\\'fill-mask\\', model=\\'camembert-base\\', tokenizer=\\'camembert-base\\'); results = camembert_fill_mask(\\'Le camembert est <mask> :)\\')\", \\'performance\\': {\\'dataset\\': \\'oscar\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. It is available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data, and pretraining data source domains. It can be used for Fill-Mask tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 823, "text": " Predict the missing word in the following text: \\\"I am an AI who can perform various NLP [MASK].\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Masked Language Modeling, Next Sentence Prediction\\', \\'api_name\\': \\'bert-base-uncased\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'bert-base-uncased\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nunmasker = pipeline(\\'fill-mask\\', model=\\'bert-base-uncased\\')\\\\nunmasker(Hello I\\'m a [MASK] model.)\", \\'performance\\': {\\'dataset\\': \\'GLUE\\', \\'accuracy\\': 79.6}, \\'description\\': \\'BERT base model (uncased) is a transformer model pretrained on a large corpus of English data using a masked language modeling (MLM) objective. It can be used for masked language modeling, next sentence prediction, and fine-tuning on downstream tasks such as sequence classification, token classification, or question answering.\\'}', metadata={})]", "category": "generic"}
{"question_id": 824, "text": " Calculate the similarity between two movie reviews to find out if two users have similar opinions on the film.\\n###Input: \\nReview 1: \\\"I thoroughly enjoyed this movie - the acting was great, the storyline was interesting, and the cinematography was stunning.\\\"\\nReview 2: \\\"The film was a delightful experience, with outstanding performances, an engaging plot, and exceptional visual effects.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'lvwerra/distilbert-imdb\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'lvwerra/distilbert-imdb\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\', \\'pytorch\\'], \\'example_code\\': \"classifier(\\'I love this movie!\\')\", \\'performance\\': {\\'dataset\\': \\'imdb\\', \\'accuracy\\': 0.928}, \\'description\\': \\'This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\\'}', metadata={})]", "category": "generic"}
{"question_id": 825, "text": " We are building a chatbot and we need to be able to compare which response is the most suitable match for the input message from the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 826, "text": " I am building transcript summaries of meeting audio recordings. I need to extract only the most important sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 827, "text": " I have several paragraphs of text from different sources. I want to analyze their similarity in content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'t5_sentence_paraphraser\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', model=\\'ramsrigouthamg/t5_sentence_paraphraser\\') should be rewritten as T5ForConditionalGeneration.from_pretrained(\\'ramsrigouthamg/t5_sentence_paraphraser\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A T5 model for paraphrasing sentences\\'}', metadata={})]", "category": "generic"}
{"question_id": 828, "text": " Create a prototype artificial voice for the novel character that we described in the product description. It will be used in an audiobook.\\n###Input: \\\"<noinput>\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'lakahaga/novel_reading_tts\\', \\'api_call\\': \"AutoModelForTTS.from_pretrained(\\'lakahaga/novel_reading_tts\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"inputs = processor(text, return_tensors=\\'pt\\'); generated_audio = model.generate(**inputs);\", \\'performance\\': {\\'dataset\\': \\'novelspeech\\', \\'accuracy\\': None}, \\'description\\': \\'This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 829, "text": " A client is launching their website in Arabic and wants a voice-over for their promotional video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'ImRma/Brucelee\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'ImRma/Brucelee\\')\", \\'api_arguments\\': [\\'your_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Hugging Face model for converting Persian and English text into video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 830, "text": " In a podcast with 3 speakers, we want to separate the speakers by segmenting the audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 831, "text": " Design a system to transcribe a podcast episode in a format that can be used for closed captions, including accurate punctuation.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'punctuation prediction', 'api_name': 'oliverguhr/fullstop-punctuation-multilang-large', 'api_call': 'Output: PunctuationModel()', 'api_arguments': ['text'], 'python_environment_requirements': ['pip install deepmultilingualpunctuation'], 'example_code': 'from deepmultilingualpunctuation import PunctuationModel\\\\nmodel = PunctuationModel()\\\\ntext = My name is Clara and I live in Berkeley California Ist das eine Frage Frau M\u00fcller\\\\nresult = model.restore_punctuation(text)\\\\nprint(result)', 'performance': {'dataset': 'wmt/europarl', 'EN_accuracy': 0.775, 'DE_accuracy': 0.814, 'FR_accuracy': 0.782, 'IT_accuracy': 0.762}, 'description': 'This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :.'}\", metadata={})]", "category": "generic"}
{"question_id": 832, "text": " A company wants to develop a voice assistant to be integrated into their web-based services. The main objective of this voice assistant is to transcribe user's voice commands and generate appropriate text inputs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'\u3053\u3093\u306b\u3061\u306f\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 833, "text": " Analyze customer calls and identify the spoken words in various languages from an audio file.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 834, "text": " My team is building a customer support chatbot that can detect customer's emotions from their voice messages. We need to evaluate emotions from an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'padmalcom/wav2vec2-large-emotion-detection-german\\', \\'api_call\\': \"pipeline(\\'audio-classification\\', model=Wav2Vec2ForCTC.from_pretrained(\\'padmalcom/wav2vec2-large-emotion-detection-german\\'))\", \\'api_arguments\\': \\'audio_file\\', \\'python_environment_requirements\\': \\'transformers, torch\\', \\'example_code\\': \"from transformers import pipeline\\\\naudio_classifier = pipeline(\\'audio-classification\\', model=\\'padmalcom/wav2vec2-large-emotion-detection-german\\')\\\\nresult = audio_classifier(audio_file)\", \\'performance\\': {\\'dataset\\': \\'emo-DB\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This wav2vec2 based emotion detection model is trained on the emo-DB dataset. It can classify emotions in German audio files into seven classes: anger, boredom, disgust, fear, happiness, sadness, and neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 835, "text": " Develop a voice interface for our robotic vacuum. The robot should recognize different commands from pre-recorded audio files.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Voice Activity Detection\\', \\'api_name\\': \\'d4data/Indian-voice-cloning\\', \\'api_call\\': \"pipeline(\\'voice-activity-detection\\', model=\\'d4data/Indian-voice-cloning\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model for detecting voice activity in Indian languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 836, "text": " A German dentist needs help during sessions with his patients. Create an emotion detection system to analyze the emotions in his patients' responses.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Emotion Classification\\', \\'api_name\\': \\'j-hartmann/emotion-english-distilroberta-base\\', \\'api_call\\': \\'text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True\\', \\'api_arguments\\': {\\'text\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \\'from transformers import pipeline\\\\nclassifier = pipeline(text-classification, model=j-hartmann/emotion-english-distilroberta-base, return_all_scores=True)\\\\nclassifier(I love this!)\\', \\'performance\\': {\\'dataset\\': \\'Balanced subset from 6 diverse datasets\\', \\'accuracy\\': \\'66%\\'}, \\'description\\': \"This model classifies emotions in English text data. It predicts Ekman\\'s 6 basic emotions, plus a neutral class: anger, disgust, fear, joy, neutral, sadness, and surprise. The model is a fine-tuned checkpoint of DistilRoBERTa-base.\"}', metadata={})]", "category": "generic"}
{"question_id": 837, "text": " We are working on an application to transcribe large conference calls. We need an algorithm to detect when people are speaking in the recording.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 838, "text": " We are a food technology firm, and we want to explore possible options to classify the quality of wine.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Tabular Tabular Classification', 'framework': 'Scikit-learn', 'functionality': 'Wine Quality classification', 'api_name': 'osanseviero/wine-quality', 'api_call': 'model.predict(X[:3])', 'api_arguments': 'X', 'python_environment_requirements': ['huggingface_hub', 'joblib', 'pandas'], 'example_code': 'from huggingface_hub import hf_hub_url, cached_download\\\\nimport joblib\\\\nimport pandas as pd\\\\nREPO_ID = julien-c/wine-quality\\\\nFILENAME = sklearn_model.joblib\\\\nmodel = joblib.load(cached_download(\\\\n hf_hub_url(REPO_ID, FILENAME)\\\\n))\\\\ndata_file = cached_download(\\\\n hf_hub_url(REPO_ID, winequality-red.csv)\\\\n)\\\\nwinedf = pd.read_csv(data_file, sep=;)\\\\nX = winedf.drop([quality], axis=1)\\\\nY = winedf[quality]\\\\nprint(X[:3])\\\\nlabels = model.predict(X[:3])', 'performance': {'dataset': 'winequality-red.csv', 'accuracy': 0.6616635397123202}, 'description': 'A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.'}\", metadata={})]", "category": "generic"}
{"question_id": 839, "text": " We are currently developing a flower delivery app, and it should take user's input and classify which type of flower it is.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Paraphrase-based utterance augmentation\\', \\'api_name\\': \\'prithivida/parrot_fluency_model\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'prithivida/parrot_fluency_model\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"parrot(\\'your input text\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 840, "text": " The company is interested in predicting the carbon emissions of a new warehouse building.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'38507101578\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'wangdy/autotrain-data-godaddy2\\', \\'accuracy\\': {\\'Loss\\': 0.206, \\'R2\\': 0.997, \\'MSE\\': 0.042, \\'MAE\\': 0.081, \\'RMSLE\\': 0.052}}, \\'description\\': \\'This model is trained using AutoTrain for single column regression on the problem of CO2 emissions prediction. The model ID is 38507101578 and has an R2 score of 0.997.\\'}', metadata={})]", "category": "generic"}
{"question_id": 841, "text": " We are an organization working on reducing carbon emissions. We have collected data on carbon emission factors and features. Estimate the carbon emissions using this given data.\\n###Input: Please provide a .csv file named \\\"data.csv\\\" containing the necessary features for carbon emission estimation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'40376105012\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data.csv\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(data.csv)\\\\ndata = data[features]\\\\ndata.columns = [feat_ + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'bibekbehera/autotrain-data-numeric_prediction\\', \\'accuracy\\': {\\'Loss\\': 0.211, \\'R2\\': 0.339, \\'MSE\\': 0.045, \\'MAE\\': 0.104, \\'RMSLE\\': 0.145}}, \\'description\\': \\'A model trained using AutoTrain for single column regression. The model predicts CO2 emissions in grams.\\'}', metadata={})]", "category": "generic"}
{"question_id": 842, "text": " I would like to train a reinforcement learning model to play CartPole using pre-trained PPO.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Stable-Baselines3\\', \\'functionality\\': \\'LunarLander-v2\\', \\'api_name\\': \\'araffin/ppo-LunarLander-v2\\', \\'api_call\\': \"Output: model = PPO.load_from_hub(\\'araffin/ppo-LunarLander-v2\\', \\'ppo-LunarLander-v2.zip\\')\", \\'api_arguments\\': {\\'checkpoint\\': \\'araffin/ppo-LunarLander-v2.zip\\'}, \\'python_environment_requirements\\': [\\'huggingface_sb3\\', \\'stable_baselines3\\'], \\'example_code\\': {\\'load_model\\': \\'from huggingface_sb3 import load_from_hub\\\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\\\nmodel = PPO.load(checkpoint)\\', \\'evaluate\\': \\'from stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})\\'}, \\'performance\\': {\\'dataset\\': \\'LunarLander-v2\\', \\'accuracy\\': \\'283.49 +/- 13.74\\'}, \\'description\\': \\'This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 843, "text": " During a showcase of your company, you are asked to demonstrate an interactive experience with an already trained model of a player playing Pong. How would you use the API to replay the game with the saved game data?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Stable-Baselines3\\', \\'functionality\\': \\'LunarLander-v2\\', \\'api_name\\': \\'araffin/ppo-LunarLander-v2\\', \\'api_call\\': \"Output: model = PPO.load_from_hub(\\'araffin/ppo-LunarLander-v2\\', \\'ppo-LunarLander-v2.zip\\')\", \\'api_arguments\\': {\\'checkpoint\\': \\'araffin/ppo-LunarLander-v2.zip\\'}, \\'python_environment_requirements\\': [\\'huggingface_sb3\\', \\'stable_baselines3\\'], \\'example_code\\': {\\'load_model\\': \\'from huggingface_sb3 import load_from_hub\\\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\\\nmodel = PPO.load(checkpoint)\\', \\'evaluate\\': \\'from stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})\\'}, \\'performance\\': {\\'dataset\\': \\'LunarLander-v2\\', \\'accuracy\\': \\'283.49 +/- 13.74\\'}, \\'description\\': \\'This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 844, "text": " We are developing a kids app that teaches soccer strategies through reinforcement learning. We need a model to demonstrate the gameplay.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'6D grasping\\', \\'api_name\\': \\'camusean/grasp_diffusion\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'camusean/grasp_diffusion\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.\\'}', metadata={})]", "category": "generic"}
{"question_id": 845, "text": " Develop a tool that helps users find relevant code snippets online when given a prompt in natural language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"\\'model.caption(prompt, image)\\'\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 846, "text": " We have to create a Q&A system for our educational platform for the high school students. The students should be able to ask questions and get relevant answers from the textbook.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 847, "text": " We are building an art product that merges Art and Literature. We need to convert painting images into meaningful text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 848, "text": " We are exploring marketing opportunities and our company wants to create a video advertisement. The ad illustrates an athlete running while promoting our brand new shoes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'chavinlo/TempoFunk\\', \\'api_call\\': \"text_to_video = pipeline(\\'text-to-video\\', model=\\'chavinlo/TempoFunk\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 849, "text": " Could you help me find the invoice number from an invoice image?\\n###Input: image_url=https://templates.invoicehome.com/invoice-template-us-neat-750px.png, question=What is the invoice number?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 850, "text": " Audrey is responsible for invoice management at her company. She needs a tool to assist with the extraction of invoice details such as the invoice number, total, and due date.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 851, "text": " Look at the image for me and find out what's the title of this document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 852, "text": " My company needs to extract information from invoices to analyze their business expenses. We need the algorithm to provide detailed information about each invoice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \\'faisalraza/layoutlm-invoices\\', \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 853, "text": " We would like to perform graph classification using the Graphormer model for our medical research project to analyze molecular structures.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Graph Machine Learning', 'framework': 'Hugging Face Transformers', 'functionality': 'Transformers', 'api_name': 'graphormer-base-pcqm4mv1', 'api_call': 'Output: AutoModel.from_pretrained(model_name)', 'api_arguments': ['model_name'], 'python_environment_requirements': ['transformers'], 'example_code': 'See the Graph Classification with Transformers tutorial', 'performance': {'dataset': 'PCQM4M-LSC', 'accuracy': '1st place on the KDD CUP 2021 (quantum prediction track)'}, 'description': 'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.'}\", metadata={})]", "category": "generic"}
{"question_id": 854, "text": " Our client is a startup working on a mobile navigation system. They need to calculate depth estimations from input images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 855, "text": " I need to remember the perfect birthday cake for my party; could you help me compute the depth estimations from an image of a birthday cake?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 856, "text": " A company focuses on providing suitable advertisements for their customers based on their age. We should assess people's age.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Age Classification\\', \\'api_name\\': \\'nateraw/vit-age-classifier\\', \\'api_call\\': \"ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nateraw/vit-age-classifier\\'}, \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'transformers\\'], \\'example_code\\': \"import requests\\\\nfrom PIL import Image\\\\nfrom io import BytesIO\\\\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\\\\n\\\\nr = requests.get(\\'https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true\\')\\\\nim = Image.open(BytesIO(r.content))\\\\n\\\\nmodel = ViTForImageClassification.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\ntransforms = ViTFeatureExtractor.from_pretrained(\\'nateraw/vit-age-classifier\\')\\\\n\\\\ninputs = transforms(im, return_tensors=\\'pt\\')\\\\noutput = model(**inputs)\\\\n\\\\nproba = output.logits.softmax(1)\\\\npreds = proba.argmax(1)\", \\'performance\\': {\\'dataset\\': \\'fairface\\', \\'accuracy\\': None}, \\'description\\': \"A vision transformer finetuned to classify the age of a given person\\'s face.\"}', metadata={})]", "category": "generic"}
{"question_id": 857, "text": " Detect the main subject of the image in a photo album website.\\n###Input: Image URL: 'http://images.cocodataset.org/val2017/000000039769.jpg'\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-csgo-player-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-csgo-player-detection\\').predict(image)\", \\'api_arguments\\': {\\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-csgo-player-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'csgo-object-detection\\', \\'accuracy\\': 0.892}, \\'description\\': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify \\'ct\\', \\'cthead\\', \\'t\\', and \\'thead\\' labels.\"}', metadata={})]", "category": "generic"}
{"question_id": 858, "text": " A clothing store needs help to identify and label different parts of clothes. Help them with a program that will automatically segment the clothes in an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'mattmdjaga/segformer_b2_clothes\\', \\'api_call\\': \"SegformerForSemanticSegmentation.from_pretrained(\\'mattmdjaga/segformer_b2_clothes\\')\", \\'api_arguments\\': [\\'image\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\', \\'matplotlib\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nimport matplotlib.pyplot as plt\\\\nimport torch.nn as nn\\\\nextractor = AutoFeatureExtractor.from_pretrained(\\'mattmdjaga/segformer_b2_clothes\\')\\\\nmodel = SegformerForSemanticSegmentation.from_pretrained(\\'mattmdjaga/segformer_b2_clothes\\')\\\\nurl = \\'https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&amp;w=1000&amp;q=80\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = extractor(images=image, return_tensors=\\'pt\\')\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits.cpu()\\\\nupsampled_logits = nn.functional.interpolate(logits, size=image.size[::-1], mode=\\'bilinear\\', align_corners=False)\\\\npred_seg = upsampled_logits.argmax(dim=1)[0]\\\\nplt.imshow(pred_seg)\", \\'performance\\': {\\'dataset\\': \\'mattmdjaga/human_parsing_dataset\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'SegFormer model fine-tuned on ATR dataset for clothes segmentation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 859, "text": " The CEO wants us to develop a single model that can perform multiple image segmentation tasks accurately.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'clipseg-rd64-refined\\', \\'api_call\\': \"pipeline(\\'image-segmentation\\', model=\\'CIDAS/clipseg-rd64-refined\\')\", \\'api_arguments\\': {\\'model\\': \\'CIDAS/clipseg-rd64-refined\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L\u00fcddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 860, "text": " We are an infrastructure management company. We need to detect potholes in images from the road network.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8n-pothole-segmentation\\', \\'api_call\\': \"\\'model.predict(image)\\'\", \\'api_arguments\\': {\\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': {\\'ultralyticsplus\\': \\'0.0.23\\', \\'ultralytics\\': \\'8.0.21\\'}, \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-pothole-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pothole-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.995, \\'mAP@0.5(mask)\\': 0.995}}, \\'description\\': \\'A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 861, "text": " A film production company wants to estimate the depth of objects in an image taken from one of their video footages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Depth Estimation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Depth Estimation\\', \\'api_name\\': \\'glpn-kitti\\', \\'api_call\\': \"\\'GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\'\", \\'api_arguments\\': \\'images, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\\\\nimport torch\\\\nimport numpy as np\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\\\\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n outputs = model(**inputs)\\\\n predicted_depth = outputs.predicted_depth\\\\nprediction = torch.nn.functional.interpolate(\\\\n predicted_depth.unsqueeze(1),\\\\n size=image.size[::-1],\\\\n mode=bicubic,\\\\n align_corners=False,\\\\n)\\\\noutput = prediction.squeeze().cpu().numpy()\\\\nformatted = (output * 255 / np.max(output)).astype(uint8)\\\\ndepth = Image.fromarray(formatted)\\', \\'performance\\': {\\'dataset\\': \\'KITTI\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 862, "text": " We are working on a project that involves converting a text description into its corresponding images. The model should also maintain the context of the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Linaqruf/anything-v3.0\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'Linaqruf/anything-v3.0\\') should be rewritten as Text2ImagePipeline(model=\\'Linaqruf/anything-v3.0\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A text-to-image model that generates images from text descriptions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 863, "text": " We are creating a digital art show and require random, high-quality computer-generated images to display.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-anime-1.0\\', \\'api_call\\': \\'pipe(prompt, negative_prompt=negative_prompt)\\', \\'api_arguments\\': [\\'prompt\\', \\'negative_prompt\\'], \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \"from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-anime-1.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\\\\nnegative_prompt = \\'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry\\'\\\\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\\\\nimage.save(./result.jpg)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include \\'photo anime, masterpiece, high quality, absurdres\\'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 864, "text": " An architect firm is looking forward to generating artificial images of church layouts. They mainly focus on the facade of the buildings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'google/ncsnpp-church-256\\', \\'api_call\\': \"sde_ve = DiffusionPipeline.from_pretrained(\\'google/ncsnpp-church-256\\')\", \\'api_arguments\\': \\'model_id\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = google/ncsnpp-church-256\\\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = sde_ve()[sample]\\\\nimage[0].save(sde_ve_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR-10\\', \\'accuracy\\': {\\'Inception_score\\': 9.89, \\'FID\\': 2.2, \\'likelihood\\': 2.99}}, \\'description\\': \\'Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.\\'}', metadata={})]", "category": "generic"}
{"question_id": 865, "text": " I am writing a book and I need references which helps me to create illustrations of insects, especially butterflies.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ntrant7/sd-class-butterflies-32\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ntrant7/sd-class-butterflies-32\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'Not specified\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies.\\'}', metadata={})]", "category": "generic"}
{"question_id": 866, "text": " Create a solution for an app that categorizes videos into either violent or non-violent footage.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'videomae-base-finetuned-RealLifeViolenceSituations-subset\\', \\'api_call\\': \"AutoModelForVideoClassification.from_pretrained(\\'dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset\\')\", \\'api_arguments\\': {\\'model_name\\': \\'dangle124/videomae-base-finetuned-RealLifeViolenceSituations-subset\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.27.2\\', \\'pytorch\\': \\'1.13.1\\', \\'datasets\\': \\'2.10.1\\', \\'tokenizers\\': \\'0.13.2\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'unknown\\', \\'accuracy\\': 0.9533}, \\'description\\': \\'This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is trained for video classification task, specifically for RealLifeViolenceSituations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 867, "text": " Create a model that predicts whether an image contains a dog or a cat given an image url.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Computer Vision Unconditional Image Generation', 'framework': 'Hugging Face Transformers', 'functionality': 'Unconditional Image Generation', 'api_name': 'google/ddpm-cat-256', 'api_call': 'DDPMPipeline.from_pretrained(model_id)', 'api_arguments': ['model_id'], 'python_environment_requirements': ['diffusers'], 'example_code': '!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)', 'performance': {'dataset': 'CIFAR10', 'accuracy': {'Inception_score': 9.46, 'FID_score': 3.17}}, 'description': 'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.'}\", metadata={})]", "category": "generic"}
{"question_id": 868, "text": " Create an image classifier that can identify diverse animals like cats, dogs, and birds.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'abhishek/autotrain-dog-vs-food\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'abhishek/autotrain-dog-vs-food\\')\", \\'api_arguments\\': \\'image_path\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'sasha/dog-food\\', \\'accuracy\\': 0.998}, \\'description\\': \"A pre-trained model for classifying images as either dog or food using Hugging Face\\'s AutoTrain framework.\"}', metadata={})]", "category": "generic"}
{"question_id": 869, "text": " We are developing an app to classify what type of food is being consumed for a calorie-tracking feature. Implement a model to perform Zero-Shot Image Classification for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 870, "text": " In our online platform, we need to determine if customer reviews are positive or negative.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'desertdev/autotrain-imdb-sentiment-analysis-44994113085\\', \\'api_call\\': \\'model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'desertdev/autotrain-data-imdb-sentiment-analysis\\', \\'accuracy\\': 0.565}, \\'description\\': \\'A binary classification model trained on the IMDb sentiment analysis dataset using AutoTrain. The model is capable of predicting sentiment (positive or negative) for movie reviews.\\'}', metadata={})]", "category": "generic"}
{"question_id": 871, "text": " Identify if a user inquiry is a question or a statement for a customer support chatbot.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 872, "text": " I am a restaurant owner, I manage a series of comments, and I have a hard time detecting if the comment is good or bad. I need a simple script to detect if my customer is satisfied or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 873, "text": " We need a name entity recognition program to extract information included in a text document, which are English, German, or Spanish. \\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text Classification', 'framework': 'Hugging Face Transformers', 'functionality': 'German Sentiment Classification', 'api_name': 'oliverguhr/german-sentiment-bert', 'api_call': 'Output: SentimentModel()', 'api_arguments': ['texts'], 'python_environment_requirements': 'pip install germansentiment', 'example_code': ['from germansentiment import SentimentModel', 'model = SentimentModel()', 'texts = [', ' Mit keinem guten Ergebniss,Das ist gar nicht mal so gut,', ' Total awesome!,nicht so schlecht wie erwartet,', ' Der Test verlief positiv.,Sie f\u00e4hrt ein gr\u00fcnes Auto.]', 'result = model.predict_sentiment(texts)', 'print(result)'], 'performance': {'dataset': ['holidaycheck', 'scare', 'filmstarts', 'germeval', 'PotTS', 'emotions', 'sb10k', 'Leipzig Wikipedia Corpus 2016', 'all'], 'accuracy': [0.9568, 0.9418, 0.9021, 0.7536, 0.678, 0.9649, 0.7376, 0.9967, 0.9639]}, 'description': 'This model was trained for sentiment classification of German language texts. The model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews.'}\", metadata={})]", "category": "generic"}
{"question_id": 874, "text": " I need help to identify the entities in the given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Detect GPT-2 generated text\\', \\'api_name\\': \\'roberta-base-openai-detector\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'roberta-base-openai-detector\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))\\', \\'performance\\': {\\'dataset\\': \\'WebText\\', \\'accuracy\\': \\'95%\\'}, \\'description\\': \\'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 875, "text": " The team wants to implement a tool that can analyze the part-of-speech of a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'finiteautomata/beto-sentiment-analysis\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'finiteautomata/beto-sentiment-analysis\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'Hugging Face Transformers library\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'TASS 2020 corpus\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is BETO, a BERT model trained in Spanish. Uses POS, NEG, NEU labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 876, "text": " Answer a question related to a table/list of data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 877, "text": " We are running a sports trivia night at a fundraiser event, and we would like to answer questions based on Olympic Games data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \\'get_response(context)\\', \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 878, "text": " I am working on a project to create a chatbot for a college library. I'd like the chatbot to be able to answer questions related to specific data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 879, "text": " An education service provider wants to find a method to answer the questions based on the contents of a given table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-small-finetuned-sqa\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6155}, \\'description\\': \\'TAPAS small model fine-tuned on Sequential Question Answering (SQA). It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 880, "text": " We have an inventory database and we need to answer some questions for our customers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 881, "text": " An intern in my company is learning Python programming. They are asking for a function that can answer natural language questions given a specific context.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\\\\\\\question-answering\\\\\\\\, model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 882, "text": " Our company is expanding into France and we need a model that can classify French texts into categories such as sports, politics, and science.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Paraphrase-based utterance augmentation\\', \\'api_name\\': \\'prithivida/parrot_fluency_model\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'prithivida/parrot_fluency_model\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"parrot(\\'your input text\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 883, "text": " Our company wants to develop multilingual conversational agents and we want to examine the sentiments in the German news article. I need a code snippet for zero-shot classification.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Zero-Shot Classification', 'framework': 'Transformers', 'functionality': 'Zero-Shot Classification', 'api_name': 'Sahajtomar/German_Zeroshot', 'api_call': 'classifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'api_arguments': {'sequence': 'string', 'candidate_labels': 'list of strings', 'hypothesis_template': 'string'}, 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Sahajtomar/German_Zeroshot)\\\\nsequence = Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie\\\\ncandidate_labels = [Verbrechen,Trag\u00f6die,Stehlen]\\\\nhypothesis_template = In deisem geht es um {}. ## Since monolingual model,its sensitive to hypothesis template. This can be experimented\\\\nclassifier(sequence, candidate_labels, hypothesis_template=hypothesis_template)', 'performance': {'dataset': {'XNLI DEV (german)': {'accuracy': 85.5}, 'XNLI TEST (german)': {'accuracy': 83.6}}}, 'description': 'This model has GBERT Large as base model and fine-tuned it on xnli de dataset. The default hypothesis template is in English: This text is {}. While using this model, change it to In deisem geht es um {}. or something different. While inferencing through huggingface api may give poor results as it uses by default english template. Since model is monolingual and not multilingual, hypothesis template needs to be changed accordingly.'}\", metadata={})]", "category": "generic"}
{"question_id": 884, "text": " We need an AI to help our students to answer their questions in different subjects.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"layoutlm_document_qa = pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 885, "text": " My science department is about to release a new research paper. I need to summarize the findings before presenting it to the team.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"summarizer = pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\').model\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 886, "text": " I am a researcher working in the medical field, and there're some research articles that we would like to summarize to save time for our team.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 887, "text": " One of my friends is a robot fan, I want to send him a gift which is a robot. Please write me a message to ask him if he is interested.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 888, "text": " Assist me in engaging in a conversation with my brother about his favorite food for dinner and recommend a restaurant.\\n###Input: My brother's favorite food is pizza.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 889, "text": " Create a coherent story with a prompt \\\"Once upon a time, in a faraway land\\\".\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Multimodal Text-to-Image', 'framework': 'Hugging Face', 'functionality': 'Text-to-Image', 'api_name': 'prompthero/openjourney', 'api_call': 'Not specified.', 'api_arguments': {'prompt': 'string'}, 'python_environment_requirements': ['diffusers', 'torch'], 'example_code': 'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = prompthero/openjourney\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./retro_cars.png)', 'performance': {'dataset': 'Midjourney images', 'accuracy': 'Not specified'}, 'description': 'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.'}\", metadata={})]", "category": "generic"}
{"question_id": 890, "text": " Imagine you are working as a newspaper editor, and you need to find a quick and brief summary of a long news article.\\n###Input: In a landmark decision by the United Nations General Assembly, global leaders have unanimously agreed to significantly reduce plastic waste production by the end of the decade in an effort to combat the devastating effects of plastic pollution on the environment. This historic announcement comes as a result of years-long negotiations and increasing public awareness of the environmental toll that plastic waste takes on the planet. By adopting a global framework for action, the UN aims to eliminate single-use plastics, improve waste management, and encourage sustainable alternatives. Countries are now expected to develop national action plans reflecting the goals of the agreement, and the UN will develop a monitoring system to evaluate progress in meeting these objectives.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 891, "text": " A website owner wants to generate a short summary of their article in French by translating it from English.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Summarization', 'framework': 'Hugging Face Transformers', 'functionality': 'Abstractive Text Summarization', 'api_name': 'plguillou/t5-base-fr-sum-cnndm', 'api_call': 'T5ForConditionalGeneration.from_pretrained(\\\\\\\\plguillou/t5-base-fr-sum-cnndm\\\\\\\\)', 'api_arguments': {'input_text': 'summarize: ARTICLE'}, 'python_environment_requirements': {'transformers': 'from transformers import T5Tokenizer, T5ForConditionalGeneration'}, 'example_code': 'tokenizer = T5Tokenizer.from_pretrained(plguillou/t5-base-fr-sum-cnndm)\\\\nmodel = T5ForConditionalGeneration.from_pretrained(plguillou/t5-base-fr-sum-cnndm)', 'performance': {'dataset': 'cnn_dailymail', 'ROUGE-1': 44.5252, 'ROUGE-2': 22.652, 'ROUGE-L': 29.8866}, 'description': 'This model is a T5 Transformers model (JDBN/t5-base-fr-qg-fquad) that was fine-tuned in French for abstractive text summarization.'}\", metadata={})]", "category": "generic"}
{"question_id": 892, "text": " We are trying to build a chatbot for customer support. However, it's important that the language used by the chatbot is grammatically correct. So, in order to correct the grammar, I need a text-based generative model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 893, "text": " Given an incomplete sentence during an online science exam, please find the appropriate word to complete the statement.\\n###Input: \\\"During photosynthesis, plants convert sunlight, carbon dioxide, and water into glucose and [MASK].\\\"\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Text2Text Generation', 'framework': 'Transformers', 'functionality': 'Sentence Correction', 'api_name': 'flexudy/t5-base-multi-sentence-doctor', 'api_call': 'model.generate(input_ids, max_length=32, num_beams=1)', 'api_arguments': ['input_text'], 'python_environment_requirements': ['transformers'], 'example_code': 'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.', 'performance': {'dataset': 'tatoeba', 'accuracy': 'Not specified'}, 'description': 'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.'}\", metadata={})]", "category": "generic"}
{"question_id": 894, "text": " Your task is to create a language model to cluster news articles into different categories based on similarity.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 895, "text": " I want to find two similar sentence's score in the Chinese language more efficiently.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'shibing624/text2vec-base-chinese\\', \\'api_call\\': \"Output: SentenceModel(\\'shibing624/text2vec-base-chinese\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'text2vec\\', \\'transformers\\'], \\'example_code\\': \"from text2vec import SentenceModel\\\\nsentences = [\\'\u5982\u4f55\u66f4\u6362\u82b1\u5457\u7ed1\u5b9a\u94f6\u884c\u5361\\', \\'\u82b1\u5457\u66f4\u6539\u7ed1\u5b9a\u94f6\u884c\u5361\\']\\\\nmodel = SentenceModel(\\'shibing624/text2vec-base-chinese\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'ATEC\\', \\'accuracy\\': \\'31.93\\'}, {\\'name\\': \\'BQ\\', \\'accuracy\\': \\'42.67\\'}, {\\'name\\': \\'LCQMC\\', \\'accuracy\\': \\'70.16\\'}, {\\'name\\': \\'PAWSX\\', \\'accuracy\\': \\'17.21\\'}, {\\'name\\': \\'STS-B\\', \\'accuracy\\': \\'79.30\\'}]}, \\'description\\': \\'This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese. It maps sentences to a 768 dimensional dense vector space and can be used for tasks like sentence embeddings, text matching or semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 896, "text": " I'm a content moderator, looking to detect similarity between two pieces of text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 897, "text": " Can you compare the similarity between different news articles to help me find related content to read?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-summarize-news\\', \\'api_call\\': \"\\'summarize(text, max_length=150)\\'\", \\'api_arguments\\': [\\'text\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-summarize-news)\\\\ndef summarize(text, max_length=150):\\\\n input_ids = tokenizer.encode(text, return_tensors=pt, add_special_tokens=True)\\\\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\\\\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\\\\nreturn preds[0]\\', \\'performance\\': {\\'dataset\\': \\'News Summary\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Google\\'s T5 base fine-tuned on News Summary dataset for summarization downstream task. The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. Time period ranges from February to August 2017.\"}', metadata={})]", "category": "generic"}
{"question_id": 898, "text": " You have been asked to find the similarity between an instruction and a sentence for an educational text resource.\\n###Input: {\\\"instruction\\\": \\\"Explain the main idea of the research paper:\\\", \\\"sentence\\\": \\\"This study investigates the impact of climate change on agricultural productivity.\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'hkunlp/instructor-base\\', \\'api_call\\': \"Output: INSTRUCTOR(\\'hkunlp/instructor-base\\').encode([[instruction,sentence]])\", \\'api_arguments\\': {\\'instruction\\': \\'string\\', \\'sentence\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install InstructorEmbedding\\', \\'example_code\\': \"from InstructorEmbedding import INSTRUCTOR\\\\nmodel = INSTRUCTOR(\\'hkunlp/instructor-base\\')\\\\nsentence = 3D ActionSLAM: wearable person tracking in multi-floor environments\\\\ninstruction = Represent the Science title:\\\\nembeddings = model.encode([[instruction,sentence]])\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'MTEB AmazonCounterfactualClassification (en)\\', \\'accuracy\\': 86.209}, \\'description\\': \\'Instructor is an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor achieves state-of-the-art performance on 70 diverse embedding tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 899, "text": " We are planning to create an audiobook from a given text. Can you provide a Text-To-Speech model that could help?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'lakahaga/novel_reading_tts\\', \\'api_call\\': \"AutoModelForTTS.from_pretrained(\\'lakahaga/novel_reading_tts\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"inputs = processor(text, return_tensors=\\'pt\\'); generated_audio = model.generate(**inputs);\", \\'performance\\': {\\'dataset\\': \\'novelspeech\\', \\'accuracy\\': None}, \\'description\\': \\'This model was trained by lakahaga using novelspeech recipe in espnet. It is designed for Korean text-to-speech tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 900, "text": " Create a voice assistant that reads out this article summary: \\\"The tech giant announced its latest innovation in AI, which is set to revolutionize the world of technology.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'it5-base-news-summarization\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'it5/it5-base-news-summarization\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'newsum(Dal 31 maggio \u00e8 infine partita la piattaforma ITsART, a pi\u00f9 di un anno da quando \u2013 durante il primo lockdown \u2013 il ministro della Cultura Dario Franceschini ne aveva parlato come di \u00abuna sorta di Netflix della cultura\u00bb, pensata per \u00aboffrire a tutto il mondo la cultura italiana a pagamento\u00bb. \u00c8 presto per dare giudizi definitivi sulla piattaforma, e di certo sar\u00e0 difficile farlo anche pi\u00f9 avanti senza numeri precisi. Al momento, l\u2019unica cosa che si pu\u00f2 fare \u00e8 guardare com\u2019\u00e8 fatto il sito, contare quanti contenuti ci sono (circa 700 \u201ctitoli\u201d, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro variet\u00e0. Intanto, una cosa notata da pi\u00f9 parti \u00e8 che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente.)\\', \\'performance\\': {\\'dataset\\': \\'NewsSum-IT\\', \\'accuracy\\': {\\'Rouge1\\': 0.339, \\'Rouge2\\': 0.16, \\'RougeL\\': 0.263}}, \\'description\\': \\'IT5 Base model fine-tuned on news summarization on the Fanpage and Il Post corpora for Italian Language Understanding and Generation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 901, "text": " Create an advertisement for a gardening company, and we need a voice-over with the following sentence in Chinese: \\\"\\u7ed9\\u4f60\\u7684\\u82b1\\u56ed\\u5e26\\u6765\\u9c9c\\u82b1\\u548c\\u7eff\\u610f\\u76ce\\u7136\\u7684\\u751f\\u547d\\u529b.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\', \\'api_call\\': \"Output: Text2Speech.from_pretrained(\\'espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'espnet_model_zoo\\'], \\'example_code\\': \\'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = \u6625\u6c5f\u6f6e\u6c34\u8fde\u6d77\u5e73\uff0c\u6d77\u4e0a\u660e\u6708\u5171\u6f6e\u751f\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\\', \\'performance\\': {\\'dataset\\': \\'csmsc\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\\'}', metadata={})]", "category": "generic"}
{"question_id": 902, "text": " We need a system that can convert text to speech in Taiwanese Hokkien accent for our language learning app.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\', \\'api_call\\': \\'tts_model.get_prediction(tts_sample)\\', \\'api_arguments\\': {\\'unit\\': \\'Text input for the TTS model\\'}, \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\', \\'torchaudio\\'], \\'example_code\\': \"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x[\\'args\\'][\\'data\\']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\", \\'performance\\': {\\'dataset\\': \\'TAT-TTS\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.\\'}', metadata={})]", "category": "generic"}
{"question_id": 903, "text": " I'm the CEO of an international e-commerce company. For voice assistants, there is a need to translate product names and descriptions from English to Japanese.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Recognition\\', \\'api_name\\': \\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\', \\'api_call\\': \"SpeechRecognitionModel(\\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\')\", \\'api_arguments\\': [\\'audio_paths\\'], \\'python_environment_requirements\\': [\\'huggingsound\\', \\'torch\\', \\'librosa\\', \\'datasets\\', \\'transformers\\'], \\'example_code\\': \\'from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\\\ntranscriptions = model.transcribe(audio_paths)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': {\\'WER\\': 81.8, \\'CER\\': 20.16}}, \\'description\\': \\'Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.\\'}', metadata={})]", "category": "generic"}
{"question_id": 904, "text": " A customer wants to create an audio file from a German text sentence for an advertisement.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'speechbrain\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'padmalcom/tts-tacotron2-german\\', \\'api_call\\': \\'tacotron2.encode_text(text)\\', \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'pip install speechbrain\\'], \\'example_code\\': [\\'import torchaudio\\', \\'from speechbrain.pretrained import Tacotron2\\', \\'from speechbrain.pretrained import HIFIGAN\\', \\'tacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\', \\'hifi_gan = HIFIGAN.from_hparams(source=speechbrain/tts-hifigan-ljspeech, savedir=tmpdir_vocoder)\\', \\'mel_output, mel_length, alignment = tacotron2.encode_text(Die Sonne schien den ganzen Tag.)\\', \\'waveforms = hifi_gan.decode_batch(mel_output)\\', \"torchaudio.save(\\'example_TTS.wav\\',waveforms.squeeze(1), 22050)\"], \\'performance\\': {\\'dataset\\': \\'custom german dataset\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Text-to-Speech (TTS) with Tacotron2 trained on a custom german dataset with 12 days voice using speechbrain. Trained for 39 epochs (english speechbrain models are trained for 750 epochs) so there is room for improvement and the model is most likely to be updated soon. The hifigan vocoder can fortunately be used language-independently.\\'}', metadata={})]", "category": "generic"}
{"question_id": 905, "text": " We are building a platform where users can join a conference call. A feature we need is the ability to detect when multiple people talk over each other.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'pyannote.audio', 'functionality': 'overlapped-speech-detection', 'api_name': 'pyannote/overlapped-speech-detection', 'api_call': 'pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)', 'api_arguments': ['audio.wav'], 'python_environment_requirements': ['pyannote.audio 2.1'], 'example_code': 'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...', 'performance': {'dataset': 'ami', 'accuracy': None}, 'description': 'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.'}\", metadata={})]", "category": "generic"}
{"question_id": 906, "text": " Our latest AI-based software aims to convert large audiobooks to text. Write a code to transcribe an audiobook.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Audio Automatic Speech Recognition', 'framework': 'CTranslate2', 'functionality': 'Automatic Speech Recognition', 'api_name': 'guillaumekln/faster-whisper-large-v2', 'api_call': 'WhisperModel(large-v2)', 'api_arguments': ['audio.mp3'], 'python_environment_requirements': ['faster_whisper'], 'example_code': 'from faster_whisper import WhisperModel\\\\nmodel = WhisperModel(large-v2)\\\\nsegments, info = model.transcribe(audio.mp3)\\\\nfor segment in segments:\\\\n print([%.2fs -&gt; %.2fs] %s % (segment.start, segment.end, segment.text))', 'performance': {'dataset': '99 languages', 'accuracy': 'Not provided'}, 'description': 'Whisper large-v2 model for CTranslate2. This model can be used in CTranslate2 or projets based on CTranslate2 such as faster-whisper.'}\", metadata={})]", "category": "generic"}
{"question_id": 907, "text": " Our team is creating an application for automated Marathi language transcription of recorded audio files. Develop the appropriate code snippet for this task.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Marathi_Male_TTS\\', \\'api_call\\': \"api.load(\\'ESPnet/espnet_model_zoo:SYSPIN/Marathi_Male_TTS\\').\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Marathi Male Text-to-Speech model using ESPnet framework.\\'}', metadata={})]", "category": "generic"}
{"question_id": 908, "text": " I am building an application for foreign language learners. One of its features is to generate transcriptions of the words the users are practicing in Japanese language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Recognition\\', \\'api_name\\': \\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\', \\'api_call\\': \"SpeechRecognitionModel(\\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\')\", \\'api_arguments\\': [\\'audio_paths\\'], \\'python_environment_requirements\\': [\\'huggingsound\\', \\'torch\\', \\'librosa\\', \\'datasets\\', \\'transformers\\'], \\'example_code\\': \\'from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\\\ntranscriptions = model.transcribe(audio_paths)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': {\\'WER\\': 81.8, \\'CER\\': 20.16}}, \\'description\\': \\'Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.\\'}', metadata={})]", "category": "generic"}
{"question_id": 909, "text": " Tell me the steps to transcribe an audio file in English and provide the text result using the whisper-medium model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transcription and Translation\\', \\'api_name\\': \\'openai/whisper-medium\\', \\'api_call\\': \"WhisperForConditionalGeneration.from_pretrained(\\'openai/whisper-medium\\')\", \\'api_arguments\\': [\\'sample\\', \\'sampling_rate\\', \\'language\\', \\'task\\', \\'skip_special_tokens\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'datasets\\'], \\'example_code\\': \\'from transformers import WhisperProcessor, WhisperForConditionalGeneration\\\\nfrom datasets import load_dataset\\\\n\\\\nprocessor = WhisperProcessor.from_pretrained(openai/whisper-medium)\\\\nmodel = WhisperForConditionalGeneration.from_pretrained(openai/whisper-medium)\\\\n\\\\nmodel.config.forced_decoder_ids = None\\\\n\\\\nds = load_dataset(hf-internal-testing/librispeech_asr_dummy, clean, split=validation)\\\\nsample = ds[0][audio]\\\\ninput_features = processor(sample[array], sampling_rate=sample[sampling_rate], return_tensors=pt).input_features\\\\n\\\\npredicted_ids = model.generate(input_features)\\\\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'LibriSpeech (clean)\\', \\'accuracy\\': 2.9}, {\\'name\\': \\'LibriSpeech (other)\\', \\'accuracy\\': 5.9}, {\\'name\\': \\'Common Voice 11.0\\', \\'accuracy\\': 53.87}]}, \\'description\\': \\'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model and was trained on either English-only data or multilingual data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 910, "text": " We have a platform that offers tutoring services. We need to transcribe a tutor's speech to provide subtitles for deaf students.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'tts_transformer-ar-cv7\\', \\'api_call\\': \"\\'TTSHubInterface.get_model_input(task, text)\\'\", \\'api_arguments\\': {\\'text\\': \\'input text\\'}, \\'python_environment_requirements\\': [\\'fairseq\\', \\'IPython\\'], \\'example_code\\': \\'from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.text_to_speech.hub_interface import TTSHubInterface\\\\nimport IPython.display as ipd\\\\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\\\\n facebook/tts_transformer-ar-cv7,\\\\n arg_overrides={vocoder: hifigan, fp16: False}\\\\n)\\\\nmodel = models[0]\\\\nTTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\\\\ngenerator = task.build_generator(model, cfg)\\\\ntext = \u0645\u0631\u062d\u0628\u064b\u0627 \u060c \u0647\u0630\u0627 \u0627\u062e\u062a\u0628\u0627\u0631 \u062a\u0634\u063a\u064a\u0644.\\\\nsample = TTSHubInterface.get_model_input(task, text)\\\\nwav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\\\\nipd.Audio(wav, rate=rate)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Transformer text-to-speech model for Arabic language with a single-speaker male voice, trained on Common Voice v7 dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 911, "text": " Determine the different audio sources in a noisy environment and isolate those sources from the original audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Speech-to-Noise Ratio, and C50 Room Acoustics Estimation\\', \\'api_name\\': \\'pyannote/brouhaha\\', \\'api_call\\': \"Output: Model.from_pretrained(\\'pyannote/brouhaha\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote-audio\\', \\'brouhaha-vad\\'], \\'example_code\\': [\\'from pyannote.audio import Model\\', \\'model = Model.from_pretrained(pyannote/brouhaha, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'from pyannote.audio import Inference\\', \\'inference = Inference(model)\\', \\'output = inference(audio.wav)\\', \\'for frame, (vad, snr, c50) in output:\\', \\'  t = frame.middle\\', \\'  print(f{t:8.3f} vad={100*vad:.0f}% snr={snr:.0f} c50={c50:.0f})\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech, AudioSet, EchoThief, MIT-Acoustical-Reverberation-Scene\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Brouhaha is a joint voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation model. It is based on the PyTorch framework and uses the pyannote.audio library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 912, "text": " A conference call with multiple speakers has just ended, and we need to separate the voices.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker diarization\\', \\'api_name\\': \\'johnislarry/cloned-pyannote-speaker-diarization-endpoint\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2022.07\\')\", \\'api_arguments\\': [\\'num_speakers\\', \\'min_speakers\\', \\'max_speakers\\', \\'segmentation_onset\\'], \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': {\\'load_pipeline\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2022.07)\\', \\'apply_pipeline\\': \\'diarization = pipeline(audio.wav)\\', \\'save_output\\': \\'with open(audio.rttm, w) as rttm:\\\\n  diarization.write_rttm(rttm)\\'}, \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'AISHELL-4\\', \\'accuracy\\': {\\'DER%\\': 14.61, \\'FA%\\': 3.31, \\'Miss%\\': 4.35, \\'Conf%\\': 6.95}}, {\\'name\\': \\'AMI Mix-Headset only_words\\', \\'accuracy\\': {\\'DER%\\': 18.21, \\'FA%\\': 3.28, \\'Miss%\\': 11.07, \\'Conf%\\': 3.87}}, {\\'name\\': \\'AMI Array1-01 only_words\\', \\'accuracy\\': {\\'DER%\\': 29.0, \\'FA%\\': 2.71, \\'Miss%\\': 21.61, \\'Conf%\\': 4.68}}, {\\'name\\': \\'CALLHOME Part2\\', \\'accuracy\\': {\\'DER%\\': 30.24, \\'FA%\\': 3.71, \\'Miss%\\': 16.86, \\'Conf%\\': 9.66}}, {\\'name\\': \\'DIHARD 3 Full\\', \\'accuracy\\': {\\'DER%\\': 20.99, \\'FA%\\': 4.25, \\'Miss%\\': 10.74, \\'Conf%\\': 6.0}}, {\\'name\\': \\'REPERE Phase 2\\', \\'accuracy\\': {\\'DER%\\': 12.62, \\'FA%\\': 1.55, \\'Miss%\\': 3.3, \\'Conf%\\': 7.76}}, {\\'name\\': \\'VoxConverse v0.0.2\\', \\'accuracy\\': {\\'DER%\\': 12.76, \\'FA%\\': 3.45, \\'Miss%\\': 3.85, \\'Conf%\\': 5.46}}]}, \\'description\\': \\'This API provides speaker diarization functionality using the pyannote.audio framework. It is capable of processing audio files and outputting speaker diarization results in RTTM format. The API supports providing the number of speakers, minimum and maximum number of speakers, and adjusting the segmentation onset threshold.\\'}', metadata={})]", "category": "generic"}
{"question_id": 913, "text": " Help me build a speech-to-speech translation tool for translating English to Hokkien language, using an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'unit_hifigan_HK_layer12.km2500_frame_TAT-TTS\\', \\'api_call\\': \\'tts_model.get_prediction(tts_sample)\\', \\'api_arguments\\': {\\'unit\\': \\'Text input for the TTS model\\'}, \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\', \\'torchaudio\\'], \\'example_code\\': \"import json\\\\nimport os\\\\nfrom pathlib import Path\\\\nimport IPython.display as ipd\\\\nfrom fairseq import hub_utils\\\\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\\\\nfrom fairseq.models.speech_to_text.hub_interface import S2THubInterface\\\\nfrom fairseq.models.text_to_speech import CodeHiFiGANVocoder\\\\nfrom fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\\\\nfrom huggingface_hub import snapshot_download\\\\nimport torchaudio\\\\ncache_dir = os.getenv(HUGGINGFACE_HUB_CACHE)\\\\nlibrary_name = fairseq\\\\ncache_dir = (\\\\n cache_dir or (Path.home() / .cache / library_name).as_posix()\\\\n)\\\\ncache_dir = snapshot_download(\\\\n ffacebook/unit_hifigan_HK_layer12.km2500_frame_TAT-TTS, cache_dir=cache_dir, library_name=library_name\\\\n)\\\\nx = hub_utils.from_pretrained(\\\\n cache_dir,\\\\n model.pt,\\\\n .,\\\\n archive_map=CodeHiFiGANVocoder.hub_models(),\\\\n config_yaml=config.json,\\\\n fp16=False,\\\\n is_vocoder=True,\\\\n)\\\\nwith open(f{x[\\'args\\'][\\'data\\']}/config.json) as f:\\\\n vocoder_cfg = json.load(f)\\\\nassert (\\\\n len(x[args][model_path]) == 1\\\\n), Too many vocoder models in the input\\\\nvocoder = CodeHiFiGANVocoder(x[args][model_path][0], vocoder_cfg)\\\\ntts_model = VocoderHubInterface(vocoder_cfg, vocoder)\\\\ntts_sample = tts_model.get_model_input(unit)\\\\nwav, sr = tts_model.get_prediction(tts_sample)\\\\nipd.Audio(wav, rate=sr)\", \\'performance\\': {\\'dataset\\': \\'TAT-TTS\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Hokkien unit HiFiGAN based vocoder from fairseq. Trained with TAT-TTS data with 4 speakers in Taiwanese Hokkien accent.\\'}', metadata={})]", "category": "generic"}
{"question_id": 914, "text": " Supervise a remote control system to react optimally to the emotions expressed by people.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'emotion\\', \\'api_name\\': \\'bhadresh-savani/distilbert-base-uncased-emotion\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'bhadresh-savani/distilbert-base-uncased-emotion\\', return_all_scores=True)\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"prediction = classifier(\\'I love using transformers. The best part is wide range of support and its easy to use\\')\", \\'performance\\': {\\'dataset\\': \\'Twitter-Sentiment-Analysis\\', \\'accuracy\\': 0.938}, \\'description\\': \"Distilbert is created with knowledge distillation during the pre-training phase which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding. It\\'s smaller, faster than Bert and any other Bert-based model. Distilbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer.\"}', metadata={})]", "category": "generic"}
{"question_id": 915, "text": " An acting agency needs to analyze voice samples from aspirant actors to detect their predominant emotions in order to select the best-suited actors for various roles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Voice Activity Detection\\', \\'api_name\\': \\'d4data/Indian-voice-cloning\\', \\'api_call\\': \"pipeline(\\'voice-activity-detection\\', model=\\'d4data/Indian-voice-cloning\\').model\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A model for detecting voice activity in Indian languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 916, "text": " We are a real estate company, we have all historical data of our deals, and we would like to predict the price of a specific house using a machine learning model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761514\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 100595.98, \\'R2\\': 0.922, \\'MSE\\': 10119551129.473, \\'MAE\\': 81601.198, \\'RMSLE\\': 0.101}}, \\'description\\': \\'A tabular regression model trained using AutoTrain to predict US housing prices. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a R2 score of 0.922.\\'}', metadata={})]", "category": "generic"}
{"question_id": 917, "text": " A real estate company would like to display estimated home prices on their website based on a set of basic features. Help them integrate the prediction model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'1771761514\\', \\'api_call\\': \\'Output: model.predict(data)\\', \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'json\\', \\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 100595.98, \\'R2\\': 0.922, \\'MSE\\': 10119551129.473, \\'MAE\\': 81601.198, \\'RMSLE\\': 0.101}}, \\'description\\': \\'A tabular regression model trained using AutoTrain to predict US housing prices. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a R2 score of 0.922.\\'}', metadata={})]", "category": "generic"}
{"question_id": 918, "text": " I want to predict the CO2 emissions for a list of cars given their features.\\n###Input: data.csv - a CSV file containing car features such as engine size, fuel type, and weight\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'40376105012\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data.csv\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(data.csv)\\\\ndata = data[features]\\\\ndata.columns = [feat_ + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'bibekbehera/autotrain-data-numeric_prediction\\', \\'accuracy\\': {\\'Loss\\': 0.211, \\'R2\\': 0.339, \\'MSE\\': 0.045, \\'MAE\\': 0.104, \\'RMSLE\\': 0.145}}, \\'description\\': \\'A model trained using AutoTrain for single column regression. The model predicts CO2 emissions in grams.\\'}', metadata={})]", "category": "generic"}
{"question_id": 919, "text": " A startup is building new arcade games that incorporate artificial intelligence. We want to implement a PPO agent to play seals/CartPole-v0.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Stable-Baselines3\\', \\'functionality\\': \\'LunarLander-v2\\', \\'api_name\\': \\'araffin/ppo-LunarLander-v2\\', \\'api_call\\': \"Output: model = PPO.load_from_hub(\\'araffin/ppo-LunarLander-v2\\', \\'ppo-LunarLander-v2.zip\\')\", \\'api_arguments\\': {\\'checkpoint\\': \\'araffin/ppo-LunarLander-v2.zip\\'}, \\'python_environment_requirements\\': [\\'huggingface_sb3\\', \\'stable_baselines3\\'], \\'example_code\\': {\\'load_model\\': \\'from huggingface_sb3 import load_from_hub\\\\ncheckpoint = load_from_hub(araffin/ppo-LunarLander-v2, ppo-LunarLander-v2.zip)\\\\nmodel = PPO.load(checkpoint)\\', \\'evaluate\\': \\'from stable_baselines3.common.env_util import make_vec_env\\\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\\\nenv = make_vec_env(LunarLander-v2, n_envs=1)\\\\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\\\\nprint(fMean reward = {mean_reward:.2f} +/- {std_reward:.2f})\\'}, \\'performance\\': {\\'dataset\\': \\'LunarLander-v2\\', \\'accuracy\\': \\'283.49 +/- 13.74\\'}, \\'description\\': \\'This is a trained model of a PPO agent playing LunarLander-v2 using the stable-baselines3 library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 920, "text": " A game developer desires to make a game user friendly using decision transformer in a hopping environment to design a responsive AI.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 921, "text": " Can you help our engineers to train and run a SoccerTwos simulation and use a trained model of a poca agent?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pcoloc/autotrain-mikrotik-7-7-1860563597\\', \\'api_call\\': \"Output: model = joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-mikrotik-7-7\\', \\'accuracy\\': {\\'Loss\\': 49.757, \\'R2\\': 0.632, \\'MSE\\': 2475.747, \\'MAE\\': 33.327, \\'RMSLE\\': 0.587}}, \\'description\\': \\'A tabular regression model trained with AutoTrain to predict carbon emissions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 922, "text": " We are a technology solutions provider, and we need to help our client develop a reinforcement learning model for a Hopper robot.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Inference API\\', \\'api_name\\': \\'Antheia/Hanna\\', \\'api_call\\': \"pipeline(\\'robotics\\', model=\\'Antheia/Hanna\\') should remain unchanged as it already only includes the model instantiation.\", \\'api_arguments\\': \\'model\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'openai/webgpt_comparisons\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Antheia/Hanna is a reinforcement learning model for robotics tasks, trained on the openai/webgpt_comparisons dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 923, "text": " Create a model for controlling a robotic cheetah capable of navigating through challenging terrain.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'edbeeching/decision-transformer-gym-halfcheetah-expert\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'edbeeching/decision-transformer-gym-halfcheetah-expert\\')\", \\'api_arguments\\': {\\'mean\\': [-0.04489148, 0.03232588, 0.06034835, -0.17081226, -0.19480659, -0.05751596, 0.09701628, 0.03239211, 11.047426, -0.07997331, -0.32363534, 0.36297753, 0.42322603, 0.40836546, 1.1085187, -0.4874403, -0.0737481], \\'std\\': [0.04002118, 0.4107858, 0.54217845, 0.41522816, 0.23796624, 0.62036866, 0.30100912, 0.21737163, 2.2105937, 0.572586, 1.7255033, 11.844218, 12.06324, 7.0495934, 13.499867, 7.195647, 5.0264325]}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'See our Blog Post, Colab notebook or Example Script for usage.\\', \\'performance\\': {\\'dataset\\': \\'Gym HalfCheetah environment\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Decision Transformer model trained on expert trajectories sampled from the Gym HalfCheetah environment\\'}', metadata={})]", "category": "generic"}
{"question_id": 924, "text": " We are developing a soccer game AI using reinforcement learning. How do we run the training with the provided configuration file and run ID?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'abhishek/autotrain-iris-logistic-regression\\', \\'api_call\\': \"Output: joblib.load(\\'model.joblib\\').predict(data)\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'scikit-learn/iris\\', \\'accuracy\\': 0.9}, \\'description\\': \\'This model is trained for multi-class classification using logistic regression on the iris dataset. It is trained with AutoTrain and has a CO2 emissions of 0.0006300767567816624 grams. The model has an accuracy of 0.9 and can be used with the Hugging Face Inference API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 925, "text": " I have a robotic arm in my warehouse, and I want it to perform efficient 6D grasping on objects.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'6D grasping\\', \\'api_name\\': \\'camusean/grasp_diffusion\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'camusean/grasp_diffusion\\')\", \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Trained Models for Grasp SE(3) DiffusionFields. Check SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion for additional details.\\'}', metadata={})]", "category": "generic"}
