export PYTHONPATH=$PYTHONPATH:~/Longchat/
torchrun --nproc_per_node=8 longchat/train/fine_tune/train_condense_16K.py --model_name_or_path ~/hf_llama/ --data_path data/dummy_conversation.json --bf16 --output_dir outputs --num_train_epochs 3 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 1000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type "cosine" --logging_steps 1 --fsdp "full_shard auto_wrap" --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' --tf32 True --model_max_length 16384 --gradient_checkpointing True --lazy_preprocess True
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python longeval/get_model_answer.py --model-id bm25-llama-RT-hf-may20 --model-name-or-path hf_longchat_bm25/ --question-file ~/FastChat/fastchat/eval/table/questions_hf_july03_eval.jsonl --answer-file table/answer_bm25_llama-RT_hf_may20_eval.jsonl --num-gpus 8
